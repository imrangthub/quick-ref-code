#################################################
#         MICROSERVICE-TECHSTACK                #
#################################################


=================================================
#                  Basic                                          
================================================= 






=================================================
#Spring-caching                                   
================================================= 
 In Spring, caching can be easily implemented using in-memory caching providers:
Generic
JCache (JSR-107) (EhCache 3, Hazelcast, Infinispan, and others)
EhCache 2.x
Hazelcast
Infinispan
Couchbase
Redis
Caffeine
Simple or the default ConcurrentMapCacheManager.


















https://www.itpanther.com/category/redis/

https://www.youtube.com/watch?v=tip2mgC6rwQ&list=PLVCgi5HZ0-YtWh-PD8LUCbdsG7oDIfGlD&index=13



Redis couse:
-------------------------------------------------
https://www.itpanther.com/installing-redis-on-linux-using-a-package-manager/









1. Visit https://www.itpanther.com/product/redis-virtual-lab/

2. Add product to cart

3. Move to Cart from the menu bar or https://www.itpanther.com/cart/

4. Apply coupon code: QEPKNXWJ

5. Checkout

6. Wait for an email containing your Redis environment details.



Remember:

- Each registered email ID can avail of up to 5 sessions.

- Each session grants access to the Lab environment for 1 hour.

- To start a new session, repeat these steps.



Happy Learning !!!



Regards,

Vikas Kumar Jha



https://www.youtube.com/playlist?list=PLVCgi5HZ0-YtWh-PD8LUCbdsG7oDIfGlD
=================================================
#redis | Redis                                     
================================================= 
=>sudo apt install redis-tools
=>redis-cli --version
Run a Redis Server using docker and install resis-toos for redis-cli in ubuntu

=>sudo systemctl status redis
=>sudo vi /etc/redis/redis.conf
Edit Config file, thne restart redis-service
=>sudo systemctl restart redis-server

Is reis run:
=>ps -ef | grep redis
=>ps aux | grep redis

Find/Check redis in hostmachine:
=>find / -name redis.conf 2>/dev/null
sudo find / -type f -name "redis-server" 2>/dev/null
sudo find / -type f -name "redis-cli" 2>/dev/null



=>redis-cli ping
=>redis-cli
=>redis-cli -p 8888

redis-server -v
redis-server --check-system


=>CONFIG GET "port"
127.0.0.1:8888> CONFIG GET *
Show currrent server configuration 

docker run -v /home/bs960/imranMadbar/myPROJECT/redisWorkshop/redisLocalVol:/usr/local/etc/redis -p8080:8080 -d  --name myredis redis redis-server /usr/local/etc/redis/redis.conf
=>docker run -p 6379:6379 --name my-redis1 -d redis
=>docker run -p 8888:6379 --name my-redis1 -d my-redis




=>set mykey myvalue
=>get mykey

=>keys *
=>dbsize
Show list of key

=>del keyName
Delete single key

=>flushall
Remove all data


=>redis-cli INFO MEMORY
Check Memory

=>redis-cli --latency-history
For Latency Check


=>CLIENT LIST
It is possible to verify the number of active connections using the CLIENT LIST command.



=>redis-cli --bigkeys
redis-cli -h <hostname> -p <port> -a <password> --bigkeys
For information about all key in redis (size, number of key etc)



#Redis HA 
=================================================

Master Node:
docker run -v /home/bs960/imranMadbar/myPROJECT/redisWorkshop/haV/master/config:/usr/local/etc/redis -v /home/bs960/imranMadbar/myPROJECT/redisWorkshop/haV/master/data:/data -p8080:8080 -d  --name redis_master redis redis-server /usr/local/etc/redis/redis.conf

Master: redis.conf file
port 8080
bind 0.0.0.0
protected-mode no
save 5 2
rdbcompression yes
dbfilename redis-master-dump.rdb
dir /data



Slave1
docker run -v /home/bs960/imranMadbar/myPROJECT/redisWorkshop/haV/slave/config:/usr/local/etc/redis -v /home/bs960/imranMadbar/myPROJECT/redisWorkshop/haV/slave/data:/data -p8181:8181 -d  --name redis_slave redis redis-server /usr/local/etc/redis/redis.conf

Slave: redis.conf file
port 8181
bind 0.0.0.0
protected-mode no
save 5 2
rdbcompression yes
dbfilename redis-slave-dump.rdb
dir /data
replicaof 192.168.217.145 8080




redis-cli -p 8080
redis-cli -p 8181
=>info replication




#Redis Sentinel 
=================================================

Create Doecker Network:
docker network create redis-net



Create masterNode
docker run --name redis-master -p 6379:6379 --network redis-net -d redis redis-server
docker run --name redis-master -p 6379:6379 --network redis-net -d redis redis-server --appendonly yes



Create Replica Node:
docker run --name redis-replica1 -p 6380:6379 --network redis-net -d redis redis-server --appendonly yes --slaveof redis-master 6379
docker run --name redis-replica2 -p 6381:6379 --network redis-net -d redis redis-server --appendonly yes --slaveof redis-master 6379

docker run --name redis-replica1 -p 6380:6379 --network redis-net -d redis redis-server --slaveof redis-master 6379
docker run --name redis-replica2 -p 6381:6379 --network redis-net -d redis redis-server --slaveof redis-master 6379



Login to Master slave, master allow write/read and slave allow only read(get).
redis-cli -p 6379

redis-cli -p 6380
redis-cli -p 6381




Redis Sentinel using Docker:
------------------------
Get IP address of all the containers:
docker inspect -f '{{.Name}} - {{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' $(docker ps -aq)



Create sentinel.conf file and provide master server details with master-node (container) IP.
echo "sentinel monitor mymaster 172.19.0.2 6379 2" > /home/bs960/imranMadbar/myPROJECT/redisWorkshop/redisSentinel/sntnl1/sentinel.conf
echo "sentinel monitor mymaster 172.19.0.2 6379 2" > /home/bs960/imranMadbar/myPROJECT/redisWorkshop/redisSentinel/sntnl2/sentinel.conf
echo "sentinel monitor mymaster 172.19.0.2 6379 2" > /home/bs960/imranMadbar/myPROJECT/redisWorkshop/redisSentinel/sntnl3/sentinel.conf



Create three docker containers for Sentinels:
docker run -d --name redis-sentinel_1 -p 26381:26379 --network redis-net -v /home/bs960/imranMadbar/myPROJECT/redisWorkshop/redisSentinel/sntnl1:/data redis redis-sentinel /data/sentinel.conf
docker run -d --name redis-sentinel_2 -p 26382:26379 --network redis-net -v /home/bs960/imranMadbar/myPROJECT/redisWorkshop/redisSentinel/sntnl2:/data redis redis-sentinel /data/sentinel.conf
docker run -d --name redis-sentinel_3 -p 26383:26379 --network redis-net -v /home/bs960/imranMadbar/myPROJECT/redisWorkshop/redisSentinel/sntnl3:/data redis redis-sentinel /data/sentinel.conf






=>redis-cli -p 26381
=>redis-cli -h my_host -p my_port -a my_password
=>redis-cli -h 172.16.7.161 -p 26379 -a pass
Login to sentinel


=>sentinel masters  
=>sentinel master mymaster
Info about all master

=>sentinel get-master-addr-by-name mymaster
Get current master and port


=>sentinel slaves mymaster
=>SENTINEL REPLICAS mymaster
Show connected replicas



docker run --name myng -d --network redis-net imranmadbar/nginx 
docker run --name mypy -d --network redis-net python sleep 99999
Run a simple client ng service, and install python

Install Python on DockerContainer for checkRedis
apt-get update
apt-get install python3
pip install redis
apt-get install nano -y



Get Master and Slave Info (in side from docker):
import redis
sentinel = redis.sentinel.Sentinel([('172.19.0.5', 26381),('localhost', 26382),('localhost', 26383)], socket_timeout=0.1)
master = sentinel.discover_master('mymaster')
print (master)
replicas = sentinel.discover_slaves('mymaster')
print(replicas)
OR
import redis
from redis.sentinel import Sentinel

# Define the list of Sentinel instances
sentinel_servers = [('172.19.0.5', 26379),
                    ('172.19.0.6', 26379),
                    ('172.19.0.7', 26379)]

# Define the service name
service_name = 'mymaster'

# Create a Sentinel object
sentinel = Sentinel(sentinel_servers, socket_timeout=0.1)

# Try to get the master
try:
    master = sentinel.master_for(service_name)
    master.ping()  # Test the connection
    print("Connected to the Redis master successfully.")
except redis.exceptions.ConnectionError as e:
    print(f"Failed to connect to the Redis master. Error: {e}")
except Exception as e:
    print(f"An error occurred: {e}")


WriteDatausing Sentanels
-----------------------------------------------------
import redis
from redis.sentinel import Sentinel

sentinel_servers = [('172.19.0.5', 26379),
                    ('172.19.0.6', 26379),
                    ('172.19.0.7', 26379)]
service_name = 'mymaster'

sentinel = Sentinel(sentinel_servers, socket_timeout=0.1)
master = sentinel.master_for(service_name)
replicas = sentinel.discover_slaves(service_name)

try:
    # Print current master and replicas
    print("Master:", master.connection_pool.get_master_address())

    print("Replicas:", replicas)

    # Attempt to ping master
    master.ping()
    print("Connected to the Redis master successfully.")

    # Ask for a key value and set it if provided
    key = input("Enter the key: ").strip()
    if key:
        value = input("Enter the value: ").strip()
        master.set(key, value)
        print(f"Key '{key}' set successfully.")
    else:
        print("No key provided. Exiting.")

    # Retrieve a key value if it exists
    key_to_retrieve = input("Enter the key to retrieve: ").strip()
    if key_to_retrieve:
        retrieved_value = master.get(key_to_retrieve)
        if retrieved_value:
            print(f"Retrieved value: {retrieved_value.decode('utf-8')}")
        else:
            print(f"No value found for '{key_to_retrieve}'")
    else:
        print("No key provided for retrieval.")

except redis.exceptions.ConnectionError as e:
    print(f"Failed to connect to the Redis master. Error: {e}")
except Exception as e:
    print(f"An error occurred: {e}")








==============================================
#Redis Performance Benchmarking | Tuning
==============================================


redis-benchmark -q -n 1000000
redis-benchmark -q -n 1000000 -p 8888
redis-benchmark -q -n 1000000 -h 10.128.0.2
redis-benchmark -q -n 100 -t get,set
  -q means run this in a quit mode and just display us the output.
  -n means the total number of queries to run during this test.


Tuning
---------------------------------------------------

RDB Persistence and Append Only File
If you are using the cluster mode of Redis then the RDB persistence and AOF is not required. So simply comment out these lines in redis.conf
sudo vim /etc/redis/redis.conf

# Comment out these lines
save 900 1
save 300 10
save 60 10000

rdbcompression no
rdbchecksum no
appendonly no



Memory Usage
Redis will use all of your available memory in the server unless this is configured. 
edit your redis.conf just like below:
	# Setting it to 16Gib
	maxmemory 17179869184
	
maxmemory 500mb
maxmemory-policy allkeys-lru


	
	
TCP-KeepAlive
Keepalive is a method to allow the same TCP connection for HTTP conversation instead of opening a new one with each new request.
## Editing default config file /etc/redis/redis.conf
# Update the value to 0
tcp-keepalive 0



TCP-backlog
you will need this to be higher if you have many connections. To do this, edit your redis config file
# TCP listen() backlog.
# In high requests-per-second environments you need an high backlog in order
# make sure to raise both the value of somaxconn and tcp_max_syn_backlog
tcp-backlog 65536


Set maxclients
The default is 10000 and if you have many connections you may need to go higher
# Once the limit is reached Redis will close all the new connections sending
# an error 'max number of clients reached'.
maxclients 10000

HostSide:
========
Max-Connection
sudo vim /etc/rc.local
# make sure this line is just before of exit 0.
sysctl -w net.core.somaxconn=65365


Overcommit Memory
Overcommit memory is a kernel parameter which checks if the memory is available or not.
If the overcommit memory value is 0 then there is a chance that your Redis will get OOM (Out of Memory) error.
echo 'vm.overcommit_memory = 1' >> /etc/sysctl.conf



Transparent Huge Page(THP)
but somehow it slows down the databases which are memory-based (for example — in the case of Redis). To overcome this issue you can disable THP.
sudo vim /etc/rc.local

# Add this line before exit 0
echo never > /sys/kernel/mm/transparent_hugepage/enabled


















Redis Cluster Mode :
=================================================
In a Redis Cluster setup, the application does not explicitly differentiate between master and replica nodes. 
Instead, Redis Cluster itself manages the distribution of read and write operations across the master and replica nodes 
transparently to the client application. 

Redis Cluster Mode: Horizontally scalable, suitable for large datasets and high throughput.
Redis Sentinel Mode: Ensures high availability and fault tolerance, ideal for mission-critical applications with 
single-master or master-replica setups.




CreateClusterWay2
--------------------------------------------------
Create Doecker Network:
docker network create redis-cluster


There are 2 images that we need to pull first
docker pull redis
docker pull redislabs/redisinsight


mkdir -p {7000..7005}
mkdir 7000 7001 7002 7003 7004 7005
Redis recommends nodes with a minimum number of 6. So we will create those nodes 

Create a config file in parant directory:
vi redis.conf
bs960@BS-960:~/imranMadbar/myPROJECT/redisWorkshop/redisCluster$ cat redis.conf 
# redis.conf file
port 7000
cluster-enabled yes
cluster-config-file nodes.conf
cluster-node-timeout 5000
appendonly yes
bind 0.0.0.0
protected-mode no


Then create the minimal configuration redis.conf and copy all directory
for i in {7000..7005}; do cp redis.conf $i; done
Update redis.conf file port as directory name




Thne run this for each directory (redis instance, as node-1, node-1 ... etc.)
docker run -v `pwd`/7001:/redis --name node-1 -p 7001:7001 --network redis-cluster redis redis-server /redis/redis.conf
docker run -v `pwd`/7002:/redis --name node-2 -p 7002:7002 --network redis-cluster redis redis-server /redis/redis.conf
docker run -v `pwd`/7003:/redis --name node-3 -p 7003:7003 --network redis-cluster redis redis-server /redis/redis.conf

docker run -v `pwd`/7004:/redis --name node-4 -p 7004:7004 --network redis-cluster redis redis-server /redis/redis.conf
docker run -v `pwd`/7005:/redis --name node-5 -p 7005:7005 --network redis-cluster redis redis-server /redis/redis.conf
docker run -v `pwd`/7006:/redis --name node-6 -p 7006:7006 --network redis-cluster redis redis-server /redis/redis.conf



Now create the cluster:
docker exec -it node-1 bash
redis-cli -p 7001 --cluster create node-1:7001 node-2:7002 node-3:7003 node-4:7004 node-5:7005 node-6:7006 --cluster-replicas 1 --cluster-yes
Successfully Done if no error



For UI View:RedisInsight


 mkdir redisinsight
 docker run -v `pwd`/redisinsight:/db -p 8001:8001 --network redis-cluster redislabs/redis-insight


useCompose: docker-compose.yml
version: '3'
networks:
  redis-cluster-compose:
    driver: bridge

services:
  redis-node-1:
    image: redis:latest
    ports:
      - 7000:7000
    networks:
      - redis-cluster-compose
    hostname: redis-node-1
    volumes:
      - ./7000:/redis
    command: redis-server /redis/redis.conf
  redis-node-2:
    image: redis:latest
    ports:
      - 7001:7001
    networks:
      - redis-cluster-compose
    hostname: redis-node-2
    volumes:
      - ./7001:/redis
    command: redis-server /redis/redis.conf

  redis-node-3:
    image: redis:latest
    ports:
      - 7002:7002
    networks:
      - redis-cluster-compose
    hostname: redis-node-3
    volumes:
      - ./7002:/redis
    command: redis-server /redis/redis.conf

  redis-node-4:
    image: redis:latest
    ports:
      - 7003:7003
    networks:
      - redis-cluster-compose
    hostname: redis-node-4
    volumes:
      - ./7003:/redis
    command: redis-server /redis/redis.conf

  redis-node-5:
    image: redis:latest
    ports:
      - 7004:7004
    networks:
      - redis-cluster-compose
    hostname: redis-node-5
    volumes:
      - ./7004:/redis
    command: redis-server /redis/redis.conf
  
  redis-node-6:
    image: redis:latest
    ports:
      - 7005:7005
    networks:
      - redis-cluster-compose
    hostname: redis-node-6
    volumes:
      - ./7005:/redis
    command: redis-server /redis/redis.conf
  
  redis-cluster-creator:
    image: redis:latest
    ports:
      - 6999:6999
    networks:
      - redis-cluster-compose
    command: redis-cli -p 7000 --cluster create redis-node-1:7000 redis-node-2:7001 redis-node-3:7002 redis-node-4:7003 redis-node-5:7004 redis-node-6:7005 --cluster-replicas 1 --cluster-yes
    depends_on:
      - redis-node-1
      - redis-node-2
      - redis-node-3
      - redis-node-4
      - redis-node-5
      - redis-node-6
  
  redis-insight:
    image: redislabs/redisinsight
    ports:
      - 8001:8001
    networks:
      - redis-cluster-compose
    volumes:
      - ./redisinsight:/db
    depends_on:
      - redis-cluster-creator






Doc
-------------------------
https://docs.spring.io/spring-data/redis/reference/redis/connection-modes.html
https://medium.com/@htyesilyurt/spring-boot-3-redis-sentinel-lettuce-client-and-docker-compose-for-high-availability-1f1e3c372a5a
https://www.youtube.com/watch?v=Pj8Q_9dovgY&list=PLq3uEqRnr_2HY6LMQsbvsK4btj51sWhBS&index=2


Spring Boot, both Lettuce and Jedis are popular choices for interacting with Redis, which is an open-source, in-memory data structure store. 

Lettuce:
  Asynchronous Support
  Thread Safety
  Connection Pooling
  Cluster Support

Jedis:
  Synchronous and Asynchronous Support:
  Connection Pooling:
  Maturity:

Choosing Between Lettuce and Jedis:
Performance:
Lettuce is often considered more performant than Jedis, especially in scenarios where high concurrency and asynchronous behavior are important.

Reactive Programming:
If your application is built using reactive programming paradigms (e.g., Spring WebFlux), Lettuce may be a more natural fit.
Ease of Use:

Jedis has a simpler API, and if you are looking for a straightforward and easy-to-use client, Jedis might be a good choice.






Caching Topology:
-Standalone         
 Application inMemory cashing

-Distributed
 Central case system

-Replication
 Chahe replicas every node

UdamyCourse:
https://gale.udemy.com/course/redis-latest/learn/lecture/21936116#overview


Example:
Assuming you have a Redis configuration as follows:

Master (Writer): 192.168.1.1:8080
Replicas (Readers):
192.168.3.1:8082
192.168.2.1:8082
Here's a configuration for a Spring Cloud microservice with 10 application instances:

java
Copy code
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.data.redis.connection.RedisStandaloneConfiguration;
import org.springframework.data.redis.connection.RedisStaticMasterReplicaConfiguration;
import org.springframework.data.redis.connection.lettuce.LettuceClientConfiguration;
import org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory;

@Configuration
public class RedisConfig {
    
    @Bean
    public LettuceClientConfiguration lettuceClientConfiguration() {
        return LettuceClientConfiguration.builder()
            .readFrom(ReadFrom.REPLICA_PREFERRED)
            .build();
    }
    
    @Bean
    public LettuceConnectionFactory lettuceConnectionFactory(LettuceClientConfiguration clientConfig) {
        RedisStandaloneConfiguration serverConfig = new RedisStandaloneConfiguration("192.168.1.1", 8080);
        RedisStaticMasterReplicaConfiguration staticMasterReplicaConfig = new RedisStaticMasterReplicaConfiguration(
            serverConfig,
            Arrays.asList(
                new RedisNode("192.168.3.1", 8082),
                new RedisNode("192.168.2.1", 8082)
            )
        );
        
        return new LettuceConnectionFactory(staticMasterReplicaConfig, clientConfig);
    }
}



Read from replica:
-------------------------------------
mport org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.data.redis.connection.RedisSentinelConfiguration;
import org.springframework.data.redis.connection.lettuce.LettuceClientConfiguration;
import org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory;
import org.springframework.data.redis.repository.configuration.EnableRedisRepositories;
import io.lettuce.core.ReadFrom;

@Configuration
@EnableRedisRepositories
public class RedisConfig {

    @Bean
    public LettuceConnectionFactory redisConnectionFactory() {
        RedisSentinelConfiguration sentinelConfig = new RedisSentinelConfiguration()
            .master("mymaster")
            .sentinel("host1", 26379)
            .sentinel("host2", 26379)
            .sentinel("host3", 26379);

        LettuceClientConfiguration clientConfig = LettuceClientConfiguration.builder()
            .readFrom(ReadFrom.REPLICA_PREFERRED)  // Configures reading from replicas
            .build();

        return new LettuceConnectionFactory(sentinelConfig, clientConfig);
    }
}









=================================================
##Event-driven Architecture | EDA
=================================================

Event-driven Architecture alternate of loss cupling or temp-cuplinc(Synchronous communitation)

Key Components of Event-driven Architecture:
  Event Producers: Components or services that generate events based on changes in the system.
  Event Consumers: Services that listen for and react to these events.
  Event Brokers: Middleware that routes events from producers to consumers (e.g., Kafka, RabbitMQ).


Example Use Case:
Suppose an e-commerce application has the following components:
  An Order Service that produces an event when a new order is placed.
  A Payment Service that consumes the event to process the payment.
  An Inventory Service that consumes the event to update stock levels.




Two even-driven model/technology:
  1) Pub/sub model
  2) Event Streming model

1)Pub/sub model: Once an event is dreceived, its cannot be replayed, which means new 
  subscribers join later will not have acccess paset event.

2)Event Streming model: this model events are writeen to a log in a sequential manner.
  Producer publish events as they occur, and these events are stored in a well-ordered fashion
  consumer have the ability to read from any part of the event stream.

One advantage of this model is thet events cna be replayed, allowing clients to join at 
any time and received all past events.



MQ-Types of Messaging:
  Point to Point
    In this type of communication, the broker sends messages to only one consumer, while the other consumers will wait till they get 
  the messages from the broker. No consumer will get the same message.

  If there are no consumers, the Broker will hold the messages till it gets a consumer (Queue based communication ). 
  If there is more than one consumer, they may get the next message but they won’t get the same message as the other consumer.


  Publish/Subscribe
    In this type of communication, the Broker sends same copy of messages to all the active consumers. 
    This type of communication is also known as Topic based communication where broker sends same message to all active 
    consumer who has subscribed for particular Topic. 



=================================================
##rabitmq |  Message broker  | RabbitMQ                                        
================================================= 
The AMQP operates on the principales of exchanges and queues for all queue operation.

RabitMQ design overview (4 actors):                              
  publisher(producer) --------------> exchange(routes)-----------> queue ----------> subscriber(consumer)


Installing and Run RabbitMQ: 
-------------------------------------------------
Docker:
docker run -it --rm --name rabbitmq -p 5672:5672 -p 15672:15672 rabbitmq:3.12-management

OR
=>rabbitmq-server start
Start rabbitMQ Server, Open RabbitMQ command prompt

=>localhost:15672
guest/guest



Example: Geting Start with Simple Queue
------------------------------------------------
Creating a Quequ:
Queues and Streams->add a new queque->add Queue

Publish to Quequ:
Click to queue->	my-test-queue1 -> Puhlish Message
Now it will show Messsage(Ready)

Received Msg from Quequ:
Click Get Message from Quew

OR
Using console: sudo apt install rabbitmq-server

Step 1:Create an Exchange:
=>rabbitmqadmin declare exchange name=my_exchange type=direct
=>rabbitmqadmin declare exchange name=imm-app-exc1 type=direct

  First, you need to create an exchange in RabbitMQ. You can do this with the rabbitmqadmin tool.
  name=my_exchange: The name of the exchange.
  type=direct: The type of the exchange (you can also use fanout, topic, etc.).


Step 2: Create a Queue
=>rabbitmqadmin declare queue name=my_queue durable=true
=>rabbitmqadmin declare queue name=imm-app-que1 durable=true
  Next, declare a queue to which messages will be sent:
  name=my_queue: The name of the queue.
  durable=true: This makes the queue persistent, so it survives server restarts.


Step 3: Bind the Queue to the Exchange
=>rabbitmqadmin declare binding source=my_exchange destination=my_queue routing_key=my_routing_key
=>rabbitmqadmin declare binding source=imm-app-exc1 destination=imm-app-que1 routing_key=imm-app-rut1
  Now, bind the queue to the exchange:
  source=my_exchange: The exchange we created.
  destination=my_queue: The queue to bind.
  routing_key=my_routing_key: A routing key for message filtering.


Step 4: Send a Message
=>rabbitmqadmin publish routing_key=my_routing_key exchange=my_exchange payload="Hello RabbitMQ!"
=>rabbitmqadmin publish routing_key=imm-app-rut1 exchange=imm-app-exc1 payload="Hello RabbitMQ by imran"
  Now, send a message to the exchange:
  routing_key=my_routing_key: The routing key we bound the queue to.
  exchange=my_exchange: The exchange where the message will be published.
  payload="Hello RabbitMQ!": The message content.



Step 5: Consume the Message
=>rabbitmqadmin get queue=my_queue count=1 requeue=false
=>rabbitmqadmin get queue=imm-app-que1 count=1
  Finally, consume the message from the queue:
  queue=my_queue: The name of the queue to consume from.
  count=1: The number of messages to consume.
  requeue=false: Whether to requeue the message after consumption (set to false to discard it after consumption).







SimpleApp:
http://localhost:8081/rabbitmq/sent-emp?empName=emp1&empId=emp001
  
FanoutExchange:
http://localhost:8083/rabbitmq/fanout/producer?exchangeName=fanout-exchange&messageData=HelloWorldMsg
http://localhost:8084/rabbitmq/producer?empName=emp1&empId=emp001&salary=500000

DirectExchange:
http://localhost:8083/rabbitmq/direct/producer?exchangeName=direct-exchange&routingKey=finance&messageData=HelloWorldMSG






##RabbitMQ-doc
=====================================================
RabbitMQ is an open-source message broker software that facilitates communication between different services or applications. 
It follows the Advanced Message Queuing Protocol (AMQP).

In RabbitMQ, once a message is consumed and acknowledged, it is removed from the queue by default. 

Core Components:
  Broker: The RabbitMQ server that handles message delivery and manages queues.
  Producer: Applications that send messages to RabbitMQ, publishing them to exchanges.
  Consumer: Applications that receive messages from RabbitMQ queues to process them.
  Queue: Buffers that store messages until consumers retrieve them.
  Exchange: Routes messages to queues based on rules. Types include:
    Direct: Matches routing keys exactly.
    Fanout: Broadcasts to all queues.
    Topic: Matches patterns in routing keys.
    Headers: Uses headers for routing.
  Binding: Links between exchanges and queues, defining routing rules.
  Routing Key: Key used by producers to help exchanges route messages.
  Virtual Hosts (vhosts): Isolated environments within RabbitMQ for secure multi-tenancy.
  AMQP: The protocol RabbitMQ uses for message formatting and transfer.
  Channels: Lightweight connections over a single TCP connection to handle multiple tasks.
  Connections: Persistent TCP connections between producers/consumers and RabbitMQ.


RabbitMQ supports several exchange types:
  Direct Exchange: Routes messages based on a specified routing key.
  Fanout Exchange: Broadcasts messages to all queues bound to it.
  Topic Exchange: Allows flexible routing with wildcard patterns in routing keys.
  Headers Exchange: Routes based on message header attributes.
  Default Exchange (Direct): Uses queue names as routing keys.


RabbitMQ queue properties include:
  Name: Unique identifier.
  Durable: Survive broker restart.
  Auto-Delete: Delete when no consumers.
  Exclusive: Used by the creating connection.
  Arguments: Additional settings (e.g., TTL, max length).
  Message TTL: Maximum time a message can stay.
  Maximum Priority: Sets message priority.
  Maximum Length: Max number of messages.
  Overflow Behavior: What happens when max length is reached.
  Dead Letter Exchange/Key: For handling failed or expired messages.



Data Types:
RabbitMQ itself does not impose restrictions on the types of data you can send.
RabbitMQ does not impose any limits on the type of data you send; it simply handles the data as byte arrays.




RabbitMQ Exchange type:
------------------------------------------------
Direct Exchange (One-to-One Relationship):
  Like sending a letter to a specific person based on their address. 
  Each message is routed to a single queue based on an exact match between the message's routing key and the queue's binding key.

Fanout Exchange (One-to-Many Relationship):
  Like broadcasting a message on TV. Each message sent to a fanout exchange is delivered to all queues 
  bound to that exchange, regardless of any routing keys or message attributes.

Topic Exchange (One-to-Many Relationship with Conditions):
  Like sending emails to different groups based on their interests or topics. 
  Messages are routed to queues based on wildcard matches between routing keys and routing patterns specified by the queues. 
  One message may be delivered to multiple queues if they match the routing pattern.

Headers Exchange (One-to-Many Relationship based on Message Attributes):
  Like sorting mail based on various attributes or metadata attached to each letter. 
  Messages are routed to queues based on headers and their values instead of routing keys. 
  One message may be delivered to multiple queues if their header values match the specified criteria.



Message Flow in RabbitMQ:
  Production: A producer sends a message with a specific routing key to an exchange.
  Routing: The exchange evaluates the routing key and any associated binding rules to determine the target queue(s).
  Queue Storage: Messages are stored in the designated queue(s) until consumed.
  Consumption: Consumers fetch or receive messages from the queue(s) and process them.



Additional Features of RabbitMQ:
  Message Acknowledgment: Consumers acknowledge messages to RabbitMQ, confirming successful processing. 
  If a consumer fails before acknowledgment, the message is re-queued for another consumer.

  Dead Letter Exchange (DLX): Messages that cannot be processed (e.g., rejected or expired) 
  can be sent to a DLX for special handling.

  Delayed Messages: RabbitMQ supports delayed messaging, allowing messages to be held in a queue for a 
  specific period before being delivered to consumers.

  Transactional Messages: RabbitMQ can handle transactional message processing to ensure exactly-once delivery semantics.


Features of RabbitMQ:
Connectivity Options:
  RabbitMQ supports multiple messaging protocols and can be deployed in distributed/federated configurations to meet 
  high availability, scalability requirements.
Pluggable Architecture:
  RabbitMQ allows to choose a persistence mechanism and also provides options to customize security for 
  authentication and authorization as per the application needs.



RabbitMQ and Kafka Key Differences:
----------------------------------------
Message Model:
  RabbitMQ: Traditional queuing. Messages go from producers to queues, then to consumers. 
    Good for handling individual tasks and events.
    Kafka: Log-based. Messages are stored in partitions, allowing consumers to replay them. 
    Great for data streams and event history.

Message Storage:
  RabbitMQ: Short-term storage. Messages are removed after consumption.
  Kafka: Long-term storage. Messages are kept for a set time, so consumers can reread them.

Delivery Guarantees:
  RabbitMQ: At-most-once or at-least-once delivery.
  Kafka: Exactly-once processing for high accuracy.

Throughput:
  RabbitMQ: Low to moderate. Designed for real-time, quick responses.
  Kafka: Very high. Handles large volumes of data smoothly.

Use Cases:
  RabbitMQ: Task queues, real-time notifications, complex routing.
  Kafka: Data pipelines, analytics, event sourcing, log processing.

Summary:
RabbitMQ: Best for tasks and real-time applications with low to moderate message load.
Kafka: Ideal for large data streams and when replaying event history is needed.

RabbitMQ:
  Message Retention: RabbitMQ can keep messages as long as there is no TTL set or until they are consumed, \
  but it is optimized for short-term storage.
  Message Size: By default, RabbitMQ can handle messages up to 128 MB, though this can be adjusted. 
  For larger messages, external storage solutions are recommended.



at-most-once, at-least-once, and exactly-once:
  They define how many times a message can be delivered to a consumer and whether the message is guaranteed to 
  be processed without duplicates.

1. At-Most-Once Delivery
  Definition: The message is delivered zero or one time. If there’s a failure during delivery, the message might be lost, 
    and it will not be retried.
  Pros:
    Low overhead due to no retries.
  Cons:
    Potential message loss if a failure occurs.
  Use case: Use when it's acceptable to lose messages occasionally, and you don’t want to risk processing the 
    same message more than once (e.g., logging systems where missing a log entry is acceptable).

2. At-Least-Once Delivery
  Definition: The message is guaranteed to be delivered at least once, but it could be delivered multiple times 
    in case of failures and retries.
  Pros:
    Message guaranteed to be processed, even in case of failure.
  Cons:
    Potential duplicate messages. Consumers need to handle deduplication.
  Use case: Use when losing messages is not acceptable, but handling duplicates is manageable 
    (e.g., payment systems where you need to ensure that every payment is processed, even if it might be processed twice).

3. Exactly-Once Delivery
  Definition: The message is guaranteed to be delivered exactly once, no more and no less, even in the event of failures and retries.
  Pros:
    No message loss or duplicates.
  Cons:
    Higher overhead and complexity, as the system needs to track and ensure that messages are not duplicated.
  Use case: Use when both message loss and duplication are unacceptable (e.g., financial transactions, inventory 
    systems, where you need strict message integrity).





Key limitations and potential challenges:
  Lack of Exactly-Once Semantics:
    No Native Exactly-Once Delivery: RabbitMQ doesn’t have native support for exactly-once message 
    delivery semantics (like Kafka does). The best you can do is at-least-once delivery, meaning 
    a message might be delivered more than once in case of failure.

  If your system has very high throughput, large message sizes, or requires strict ordering and exactly-once delivery 
  guarantees, RabbitMQ may not be the best fit, and alternatives like Kafka or Apache Pulsar could be considered.



Two Spring RabbitMQ library:
  spring-boot-starter-amqp: is better suited for straightforward RabbitMQ integration with fine-grained 
  control over the messaging process.

  spring-cloud-starter-stream-rabbit: is more appropriate when you want a higher-level abstraction for 
  stream-based or event-driven systems, especially when you're working in a cloud-native or microservices architecture.








=================================================
##kafka | Message broker  | KAFKA                                        
================================================= 
MyUnntuKafkaLocation: 
cd /usr/local/kafka
sudo systemctl daemon-reload
sudo systemctl start zookeeper
sudo systemctl start kafka
sudo systemctl status zookeeper
sudo systemctl status kafka

For Update Server config Edit this fiele:
/usr/local/kafka/config/server.properties


Is kafka run:
=>ps -ef | grep kafka



Find kafka directory in hostmachine:
=>find / -name server.properties 2>/dev/null
=>sudo find / -type f -name "kafka-server-start.sh" 2>/dev/null
=>sudo find / -type f -name "kafka-topics.sh" 2>/dev/null




Describe Kafka Brokers
./kafka-broker-api-versions.sh --bootstrap-server localhost:9092


Describe Topic Configuration
./kafka-topics.sh --describe --topic topic_sms_notification --bootstrap-server localhost:9092




Docker:
-------------------------------------------------
docker pull apache/kafka:latest
docker run --name my-kafka -pd 9092:9092 apache/kafka:latest
docker exec -it 05e1154341ef bash
bash kafka-topics.sh --version



OfsetExpolar
kafkalytic-intelejidea-pluging

Install and Run KafKa:

$ tar -xzf kafka_2.13-3.6.1.tgz
$ cd kafka_2.13-3.6.1


Kafka with ZooKeeper
# Start the ZooKeeper service
$ bin/zookeeper-server-start.sh config/zookeeper.properties
# Start the Kafka broker service
$ bin/kafka-server-start.sh config/server.properties


SetUp Kafka SingleCluster
-------------------------------------------------
Start up the Zookeeper
=>./zookeeper-server-start.sh ../config/zookeeper.properties

Add the below properties in the server.properties
  listeners=PLAINTEXT://localhost:9092
  auto.create.topics.enable=false


Start up the Kafka Broker
=>./kafka-server-start.sh ../config/server.properties


Create a topic
=>./kafka-topics.sh --create --topic test-topic1 --bootstrap-server localhost:9092 --partitions 2 --replication-factor 1
=>./kafka-topics.sh --list  --bootstrap-server localhost:9092
=>./kafka-topics.sh --delete --topic test-topic1  --bootstrap-server localhost:9092

=>./kafka-topics.sh --bootstrap-server localhost:9092 --describe
=>./kafka-topics.sh --bootstrap-server localhost:9092 --describe --topic test-topic
Check topic and its partation


Create Console Producer
=>./kafka-console-producer.sh --broker-list localhost:9092 --topic test-topic1
=>./kafka-console-producer.sh --broker-list localhost:9092 --topic test-topic1 --property "partition=0"
=>./kafka-console-producer.sh --broker-list localhost:9092 --topic test-topic --property "key.separator=-" --property "parse.key=true"

Create Console Consumer
=>./kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test-topic --from-beginning
=>./kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test-topic1 --from-beginning   --partition 1
=>./kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test-topic1 --from-beginning   --partition 1 --offset 23
=>./kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test-topic --from-beginning -property "key.separator= - " --property "print.key=true"

If a consumer don not mentation any partition onlhy topic name then the consumer reveived all pertation message,
If consumer run with a partition number with topic then this consumer only reveived this mentationed  partition message.
SameAs:
If producer mentation partition number, its only gose to those partition, if not kafka defined a pertation number.







Consumer Groups:
=>./kafka-consumer-groups.sh --bootstrap-server localhost:9092 --all-groups --describe
Show all topic,pertation,and offset number

=>./kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test-topic1 --group my-consumer-group
=>./kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test-topic1  --from-beginning
=>./kafka-console-consumer.sh --bootstrap-server 172.16.8.132:9092 --topic topic_sms_notification  --from-beginning




Increse kafka partation:
kafka-topics.sh --describe --topic your_topic_name --bootstrap-server localhost:9092
Check the current partation of a topic.

kafka-topics.sh --alter --topic topic1 --partitions 4 --bootstrap-server localhost:9092




To set the message retention period in Kafka, you need to configure the retention settings either for the entire Kafka 
cluster or for specific topics.

Set Retention Period for an Existing Topic:
=>kafka-configs.sh --zookeeper localhost:2181 --entity-type topics --entity-name test-topic1 --alter --add-config retention.ms=604800000
=>kafka-topics.sh --describe --topic test-topic1 --zookeeper localhost:2181

Setting Default Retention Period for the Kafka Cluster:
=>sudo nano /usr/local/kafka/config/server.properties
log.retention.ms=604800000
sudo systemctl restart kafka

Retention by Size:
=>log.retention.bytes=1073741824  # 1 GB
=>kafka-configs.sh --zookeeper localhost:2181 --entity-type topics --entity-name test-topic1 --alter --add-config retention.bytes=1073741824









Steps to Execute Partition Reassignment
================================================= 
If Partation-replica not equal as active broker:
Like: I hava 3 broker kafka cluster but topic design as:


=>./kafka-broker-api-versions.sh --bootstrap-server 192.168.1.1:9092
Check active kafka broker:

=>./kafka-topics.sh --describe --topic topic_sms_notification --bootstrap-server 192.168.1.1:9092
Topic: topic_sms_notification	TopicId: W32-rGmiQaytdaH6bWxnhQ	PartitionCount: 10	ReplicationFactor: 3	Configs: min.insync.replicas=2
	Topic: topic_sms_notification	Partition: 0	Leader: 2	Replicas: 2,3,4	Isr: 2,3
	Topic: topic_sms_notification	Partition: 1	Leader: 3	Replicas: 3,4,5	Isr: 3

In this case 4,5 number broker not active in the cluster, so Now I have to reassignment it:

=>vi topics-to-move.json
{
  "version": 1,
  "topics": [
    {
      "topic": "topic_sms_notification"
    }
  ]
}


=>./kafka-reassign-partitions.sh --bootstrap-server 192.168.1.1:9092 --generate --topics-to-move-json-file topics-to-move.json --broker-list "1,2,3"
Generate the Reassignment JSON
this will output two json:
Current partition replica assignment and Proposed partition reassignment configuration

Create another json file proposed-reassignment.json with the json data of: Proposed partition reassignment configuration
then run it, and done !

=>./kafka-reassign-partitions.sh --bootstrap-server 192.168.1.1:9092--execute --reassignment-json-file proposed-reassignment.json








simple-Consumer:
-------------------------------------------------------
@Configuration
@EnableKafka
public class APIHUBKafkaConsumerConfig {

    @Value("${apihub-kafka.producing-trigger-list:topic_sms_notification}")
    private String consumingTgList;

    @Value("${apihub-kafka.bootstrap-servers:172.16.8.132:9092}")
    private List<String> apihubKafkaBootServers;

    @Value("${apihub-kafka.consumer.group-id:default-group}")
    private String groupId;

    @Bean
    public ConsumerFactory<String, APIHUBKafkaNotifyRecord> apihubConsumerFactory() {
        Map<String, Object> props = new HashMap<>();
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, apihubKafkaBootServers);
        props.put(ConsumerConfig.GROUP_ID_CONFIG, groupId);
        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest"); // or "latest"
        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false); // Manual offset commit
        props.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, 100);
        props.put(ConsumerConfig.HEARTBEAT_INTERVAL_MS_CONFIG, 3000);
        props.put(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG, 10000);
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, ErrorHandlingDeserializer.class);
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ErrorHandlingDeserializer.class);
        props.put(ErrorHandlingDeserializer.VALUE_DESERIALIZER_CLASS, JsonDeserializer.class.getName());
        props.put(JsonDeserializer.TRUSTED_PACKAGES, "*");
        return new DefaultKafkaConsumerFactory<>(
                props,
                new ErrorHandlingDeserializer<>(),
                new JsonDeserializer<>(APIHUBKafkaNotifyRecord.class)
        );
    }

    @Bean
    public ConcurrentKafkaListenerContainerFactory<String, APIHUBKafkaNotifyRecord> apihubKafkaListener() {
        ConcurrentKafkaListenerContainerFactory<String, APIHUBKafkaNotifyRecord> factory = new ConcurrentKafkaListenerContainerFactory<>();
        factory.setConsumerFactory(apihubConsumerFactory());
        factory.setConcurrency(3); // Adjust based on your requirements
        factory.getContainerProperties().setAckMode(ContainerProperties.AckMode.MANUAL); // Manual ack
        return factory;
    }
}


Manual Offset Commit
------------------------------------------------
Here's an example of how to handle manual offset commits in a Kafka listener:

java
Copy code
@KafkaListener(topics = "${apihub-kafka.producing-trigger-list:topic_sms_notification}", groupId = "${apihub-kafka.consumer.group-id:default-group}")
public void listen(APIHUBKafkaNotifyRecord record, Acknowledgment ack) {
    try {
        // Process the record
        // ...

        // Manually acknowledge the message
        ack.acknowledge();
    } catch (Exception e) {
        // Log and handle exceptions
        // Implement retry logic if needed
    }
}







Docker:
---------------------------------------------------------
gitClone: https://github.com/conduktor/kafka-stack-docker-compose
=>docker-compose -f zk-single-kafka-single.yml up -d
=>docker-compose -f zk-single-kafka-single.yml ps
=>docker-compose -f zk-single-kafka-single.yml stop


zk-single-kafka-single.yml:

version: '2.1'
services:
  zoo1:
    image: confluentinc/cp-zookeeper:7.3.2
    hostname: zoo1
    container_name: zoo1
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_SERVER_ID: 1
      ZOOKEEPER_SERVERS: zoo1:2888:3888

  kafka1:
    image: confluentinc/cp-kafka:7.3.2
    hostname: kafka1
    container_name: kafka1
    ports:
      - "9092:9092"
      - "29092:29092"
      - "9999:9999"
    environment:
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka1:19092,EXTERNAL://${DOCKER_HOST_IP:-127.0.0.1}:9092,DOCKER://host.docker.internal:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT,DOCKER:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_ZOOKEEPER_CONNECT: "zoo1:2181"
      KAFKA_BROKER_ID: 1
      KAFKA_LOG4J_LOGGERS: "kafka.controller=INFO,kafka.producer.async.DefaultEventHandler=INFO,state.change.logger=INFO"
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_JMX_PORT: 9999
      KAFKA_JMX_HOSTNAME: ${DOCKER_HOST_IP:-127.0.0.1}
      KAFKA_AUTHORIZER_CLASS_NAME: kafka.security.authorizer.AclAuthorizer
      KAFKA_ALLOW_EVERYONE_IF_NO_ACL_FOUND: "true"
    depends_on:
      - zoo1




Install Kafka In Ubuntu
===============================================
sudo wget https://downloads.apache.org/kafka/3.6.0/kafka_2.12-3.6.0.tgz
sudo tar xzf kafka_2.12-3.6.0.tgz
sudo mkdir -p /usr/local/kafka
sudo mv kafka_2.12-3.6.0 /usr/local/kafka



sudo nano /etc/systemd/system/kafka.service
sudo nano /etc/systemd/system/zookeeper.service


Create the systemd unit file for kafka service, and add this:
[Unit]
Description=Apache Zookeeper service
Documentation=http://zookeeper.apache.org
Requires=network.target remote-fs.target
After=network.target remote-fs.target

[Service]
Type=simple
ExecStart=/usr/local/kafka/bin/zookeeper-server-start.sh /usr/local/kafka/config/zookeeper.properties
ExecStop=/usr/local/kafka/bin/zookeeper-server-stop.sh
Restart=on-abnormal

[Install]
WantedBy=multi-user.target
-------------------------------------
[Unit]
Description=Apache Kafka Service
Documentation=http://kafka.apache.org/documentation.html
Requires=zookeeper.service

[Service]
Type=simple
Environment="JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64/jre"
ExecStart=/usr/local/kafka/bin/kafka-server-start.sh /usr/local/kafka/config/server.properties
ExecStop=/usr/local/kafka/bin/kafka-server-stop.sh

[Install]
WantedBy=multi-user.target


Then:
sudo systemctl daemon-reload
sudo systemctl start zookeeper
sudo systemctl start kafka
sudo systemctl status zookeeper
sudo systemctl status kafka

For Update Server config Edit this fiele:
/usr/local/kafka/config/server.properties





================================================= 
##kafka-doc | Main Concepts and Terminology
================================================= 
https://stackoverflow.com/questions/38024514/understanding-kafka-topics-and-partitions
SpringKafka:
https://gale.udemy.com/course/apache-kafka-for-developers-using-springboot/learn/lecture/37906902#overview
udTut-repo:
https://github.com/dilipsundarraj1/kafka-for-developers-using-spring-boot-v2/blob/main/SetUpKafkaDocker.md#set-up-broker-and-zookeeper





Events,records,message:
  An event records the fact that "something happened" in the world or in your business. 
  It is also called record or message. an event has a key, value, timestamp, and optional metadata headers. 



Topics:
  Events are organized and durably stored in topics. 
  A topic is similar to a folder in a filesystem, and the events are the files in that folder. 
  Events are durably stored in Kafka, they can be read as many times and by as many consumers as you want. 


Here's an example event:
  Event key: "Alice"
  Event value: "Made a payment of $200 to Bob"
  Event timestamp: "Jun. 25, 2020 at 2:06 p.m."

Topics in Kafka are always multi-producer and multi-subscriber: a topic can have zero, one, or many producers that write events to it, 
as well as zero, one, or many consumers that subscribe to these events
unlike traditional messaging systems, events are not deleted after consumption. 
Instead, you define for how long Kafka should retain your events through a per-topic configuration setting, 
after which old events will be discarded. 



Partition:
  Partitation is where message linve inside the topic.
  Each partition is independent of each other.

Each Partition is an Order, inmmutable sequence of record.
Each record is assigned a sequential number called offset.
Order is guaranteed only at the partition leve.

Topics are partitioned, meaning a topic is spread over a number of "buckets" located on different Kafka brokers. 
This distributed placement of your data is very important for scalability because it allows client applications to 
both read and write the data from/to many brokers at the same time. 
When a new event is published to a topic, it is actually appended to one of the topic's partitions. 
Events with the same event key (e.g., a customer or vehicle ID) are written to the same partition, 
and Kafka guarantees that any consumer of a given topic-partition will always read that partition's events in exactly the same 
order as they were written.

Each partition can be considered an ordered sequence of records. These partitions allow for parallel processing of messages 
within a topic.


Partition offset:
  Each partitioned message has a unique sequence id called as offset.
  Offsets only link to a single message within a single partition of a topic.There is no easy way to find the offset for a single message.

Partitation is where message linve inside the topic.
Each partition is independent of each other.

Each Partition is an Order, inmmutable sequence of record.
Each record is assigned a sequential number called offset.
Order is guaranteed only at the partition leve.




Kafka-Records:
  exist within Kafka topics. When data is produced to Kafka, it is encapsulated into records and appended to the 
  log (partition) of the respective topic. Each record consists of a key, a value, headers, timestamp, offset,
   and possibly other metadata. 


Record comprises several components:
  Key: An optional identifier for the record.
  Value: The main data payload.
  Headers: Optional metadata associated with the record.
  Timestamp: Indicates when the record was produced or ingested into Kafka.
  Offset: A unique identifier assigned to each record within a partition.
  Partition: Specifies the partition to which the record belongs.


To Unique identify a reocrd use (partition+Offset) composit key number. 




Consumer Groups: 
  They're logical groups of Kafka consumers. Each consumer in a group is assigned one or more partitions of a topic for processing. 
  With multiple consumers in the same group, they can work together to process all the partitions, enabling parallel processing.
Kafka keeps track of the last consumed message offset for each consumer group, so if you produce new messages, 
they will only be consumed if they have a higher offset than the last consumed message.

Consumer Groups How  Works:
  Assignment: When a consumer joins a group, Kafka assigns it one or more partitions to consume from. 
  The assignment is handled by the Kafka group coordinator.

  Rebalancing: When a consumer leaves the group (due to failure or scaling down), Kafka rebalances the remaining consumers 
  so that each partition is still covered.

  Offset Storage: Kafka stores the offset of the last consumed message for each partition in the group. 
  This can be stored in Kafka itself or in an external storage.




Producer:
By default, the producer doesn't care about partitioning. 
You have the option to use a customized partitioner to have a better control, but it's totally optional.


Consumer:
 Consumers join (or create if they're alone) a consumer group to share load. 
 No two consumers in the same group will ever receive the same message.

Since this is a queue with an offset for each partition, is it responsibility of the consumer to specify which messages 
it wants to read? Does it need to save its state?
Yes, consumers save an offset per topic per partition. This is totally handled by Kafka, no worries about it.


What happens when a message is deleted from the queue?
If a consumer ever request an offset not available for a partition on the brokers (for example, due to deletion), 
it enters an error mode, and ultimately reset itself for this partition to either the most recent or the oldest message available 
(depending on the auto.offset.reset configuration value), and continue working.






Message of Kafka retain inside in cluster after consumed, depending on retation prood.
Kafka Message are byte insede kafka broker, then producer/consumer serisized this.


For locket a specific message, required 3 thing:
- Topic Name
- Partition number
- Offset Number 


Consumer have three option to reload
- from-biginning
- latest
- specific offset



Kafka 4 main Component:
  Broker: Stores and manages data.Its a java process, can incress horizontally.
  ZooKeeper: Manages Kafka cluster's configuration and coordination.
  Producer: Sends data to Kafka. Communicate with broker with tcp. Producer create record with partiation number attributes.
  Consumer: Receives data from Kafka



Replication Factor: Replication factor factor make partation duplicate.
  If replication-factor=2 then every partation has two copy if three then the number of copy is 3.










=================================================
#     ELK Stack                                      
=================================================

=>elasticsearch.bat
=>kibana.bat


=>GET /_nodes

List of Index
=>GET /_cat/indices?v
  
Creae a index
=>PUT employee_indx
  
Add data to Index
=>POST employee_indx/_doc/1
  {
     "name":"MD IMRAN HOSSAIN", "gender":"Male",
     "age":"30", "city":"Dhaka"
  }

PUT /student_indx
{
  "settings": {
    "number_of_shards": 1
  },
  "mappings": {
    "properties": {
      "name": { "type": "text" },
      "age": { "type": "integer" },
      "gender": { "type": "text" }
    }
  }
}

 
Show Index date
  =>GET employee_indx/_doc/1
  =>POST /employee_indx/_search
  =>POST /employee_indx/_search?filter_path=hits.hits
  

Update
=>POST /employee_indx/_doc/1/_update
{
  "doc": {
  "name": "MD IMRAN HOSSAIN UPDATE"
  }
}
  
DeleteDate
  =>DELETE  employee_indx/_doc/11




=>PUT school_indx

=>POST school_indx/student/1
  {
     "name":"MD IMRAN HOSSAIN", 
     "gender":"Male",
     "age":"30"
  }
  



=================================================
#JWT | jwt                                   
================================================= 


You can create a signature using various algorithms:
  HMAC (HS256, HS384, HS512) are symmetric, meaning the same secret is used for both signing and verification. 
  RSA and ECDSA algorithms are asymmetric, meaning they use a pair of keys (public and private).
  

There are multiple algorithms available for creating a JWT signature.
The most common algorithms are HS256, RS256, and ES256.



Two type of encryption in jwt:
-(Symetric) Same key use for encryption and decription
-(Asymmetric) Differ key, private/public key



Here is how you can generate a JWT step-by-step:
-------------------------------------------------

1)Encode the Header:
{
  "alg": "HS256",
  "typ": "JWT"
}

Base64Url encoding of the above JSON: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9


2)Encode the Payload:
{
  "sub": "1234567890",
  "name": "John Doe",
  "iat": 1516239022
}
Base64Url encoding of the above JSON:eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiaWF0IjoxNTE2MjM5MDIyfQ


3)Create the Signature:
HMACSHA256(
  base64UrlEncode(header) + "." +
  base64UrlEncode(payload),
  secret)

Resulting signature:   SflKxwRJSMeKKF2QT4fwpMeJf36POk6yJV_adQssw5c


Combine all parts:
  eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiaWF0IjoxNTE2MjM5MDIyfQ.SflKxwRJSMeKKF2QT4fwpMeJf36POk6yJV_adQssw5c



RS256:
-------------------------------
RS256 (RSA Signature with SHA-256) uses asymmetric encryption with a private key to sign the JWT and a public key to validate it.

Step to create:
    Generate RSA key pair using OpenSSL or any other tool.
    Create the JWT using the private key.
    Validate the JWT using the corresponding public key.


Generating a Private Key
=>openssl genpkey -algorithm RSA -out private_key.pem -pkeyopt rsa_keygen_bits:2048

Generating a Public Key from the Private Key
=>openssl rsa -pubout -in private_key.pem -out public_key.pem





================================================
#OAuth | OAuth 2.0 | OAuth 2.1  | 
================================================


ClientCredentials:GrantType
================================================

curl --location 'http://localhost:9000/oauth2/token' \
--header 'Content-Type: application/x-www-form-urlencoded' \
--header 'Authorization: Basic b2lkYy1jbGllbnQ6c2VjcmV0' \
--data-urlencode 'grant_type=client_credentials' \
--data-urlencode 'scope=profile'

http://localhost:9000/oauth2/authorize?response_type=code&client_id=myclientid&redirect_uri=http://127.0.0.1:8080/login/oauth2/code/myclient&scope=profile





AuthorizationCode:GrantType
================================================
1) Request for code
GET
    http://localhost:9000/oauth2/authorize?
      response_type=code
      &client_id=myclientid
      &redirect_uri=http://127.0.0.1:8080/login/oauth2/code/myclient
      &scope=profile
OR
    http://localhost:9000/oauth2/authorize?response_type=code&client_id=myclientid&redirect_uri=http://localhost:8081/
    callback&scope=profile%20openid%20read


2)Response with code:

    http://127.0.0.1:8080/login/oauth2/code/myclient?code=
    0DfBufb2RNLkgE74lN3hEw92YuZ6IBnJXb9TrElJ_VW2EYY58gnYGhnOcN8QTf4blzeDI_Vog1pAkbDrTdfq4g0Vh-iAfh2IJXm_zVt70JAGn52eq5UWDXKYFju1GCWF



3) Exchange the Authorization Code for token

POST:
    curl --location 'http://localhost:9000/oauth2/token' \
    --header 'Content-Type: application/x-www-form-urlencoded' \
    --header 'Authorization: Basic bXljbGllbnRpZDpteWNsaWVudHNlYw==' \
    --header 'Cookie: JSESSIONID=28E4B4AA8867A380AC7B4873F6429708' \
    --data-urlencode 'grant_type=authorization_code' \
    --data-urlencode 'redirect_uri=http://127.0.0.1:8080/login/oauth2/code/myclient' \
    --data-urlencode 'code=0DfBufb2RNLkgE74lN3hEw92YuZ6IBnJXb9TrElJ_VW2EYY58gnYGhnOcN8QTf4blzeDI_Vog1pAkbDrTdfq4g0Vh-iAfh2IJXm_zVt70JAGn52eq5UWDXKYFju1GCWF' \
    --data-urlencode 'client_id=myclientid' \
    --data-urlencode 'client_secret=myclientsec'




OAuth2.1 | AuthorizationCode with PKCE
================================================
As OAuth2.0 additional:
First generates a random string to use for the state parameter. The client will need to store this to be used in the next step.

1) Request for code
GET
  http://localhost:9000/oauth2/authorize?response_type=code&client_id=spring-boot-client-app&redirect_uri=http://localhost:7070/
  callback&scope=read+profile


2. Get code and Verify the state parameter

Response:
  http://localhost:8080/callback?code=oECNrOihYaHL2z97nW74Gr2Xbq7cLzzM0W-ypjgel&state=245EY6gkVPScY3AH


The user was redirected back to the client, and you'll notice a few additional query parameters in the URL:

?state=245EY6gkVPScY3AH&code=oECNrOihYaHL2z97nW74Gr2Xbq7cLzzM0W-ypjgelR8Ql11PEACkloXJKFAdh2Hub06r2zt1Jxty-HhOZzhqVjV7
You need to first verify that the state parameter matches the value stored in this user's 
session so that you protect against CSRF attacks.

Depending on how you've stored the state parameter (in a cookie, session, or some other way), 
verify that it matches the state that you originally included in step 1. 
Previously, we had stored the state in a cookie for this demo.



3. Exchange the Authorization Code for Token
Now you're ready to exchange the authorization code for an access token.
The client builds a POST request to the token endpoint with the following parameters:

POST:WITH-PKCE
  curl --location 'http://localhost:9000/oauth2/token' \
  --header 'Content-Type: application/x-www-form-urlencoded' \
  --header 'Authorization: Basic c3ByaW5nLWJvb3QtY2xpZW50LWFwcDpzcHJpbmctYm9vdC1jbGllbnQtYXBwLXNlYw==' \
  --data-urlencode 'grant_type=authorization_code' \
  --data-urlencode 'redirect_uri=http://localhost:7070/callback' \
  --data-urlencode 'code=RmlSYNYPOzjHk4ROKZSeiX0IpVeOVu-nkGlP9D-OdS1R]oXqv3RFAU8Tom49Y8rd_hl98Ti' \
  --data-urlencode 'code_verifier=b0473b35778f41b987413c28db04b163'


Note that the client's credentials are included in the POST body in this example. 
Other authorization servers may require that the credentials are sent as a HTTP Basic Authentication header.








Doc:
================================================
Key Components of OAuth 2.0
  Resource Owner: The user who authorizes access to their data.

  Client (Application): The third-party app requesting access (e.g., a weather app asking for your location data).

  Resource Server: The server hosting the user’s data (e.g., Google’s servers).

  Authorization Server: Issues access tokens after the user grants permission (e.g., Google Authorization Server).


OAuth grant types:
    Authorization code
    Client credentials
    Device code
Legacy grant types:
    Password Grant (Resource Owner Password Credentials):
    Implicit grant


Tokens in OAuth 2.0
  Access Token:
  A temporary token that grants access to the user’s resources for a specific scope (e.g., read-only access to email).

  Refresh Token:
  A token that allows the client to request a new access token without requiring the user to log in again.


Scopes in OAuth 2.0
  Scopes define the level of access requested by the client application. For example:
    profile: Access to the user's public profile.
    email: Permission to view the user's email address.
    photos.readonly: Read-only access to the user’s photos.
    The user must approve these scopes during the authorization process.



Token type:
--------------------------------------------------
Access Token
  Used to access protected resources.

Refresh Token
  Used to obtain a new access token when the old one expires.

ID Token (in OpenID Connect)
  Contains user identity information (e.g., name, email).

Opaque Token (UUID Token)
  A random, non-readable token that requires server-side validation (often represented as a UUID).

Many modern implementations like Spring Authorization Server or Keycloak default to 
JWT access tokens because they reduce the need for server-side lookups.


ID Token is only issued as part of OpenID Connect (OIDC) flows, not in standard OAuth2 flows.
ID Tokens are only issued in OpenID Connect flows (like Authorization Code with OIDC).
Need to openid in client registry for id token (.scope("openid") // Required for ID Token).

Get token as:
  {
      "access_token": "eyJraWQiOi...<snipped>...",
      "refresh_token": "DixZHTK-PO..<UUID Token>",
      "scope": "openid",
      "id_token": "eyJhbGciOi...<OIDC ID Token JWT>...",
      "token_type": "Bearer",
      "expires_in": 299
  }




Choosing between UUID (opaque) tokens and JWT tokens depends on the security, use case, and architecture of your microservices. 

When to Use UUID vs JWT in Microservices?
  Use UUID (Opaque Tokens) if:
    Your system has sensitive data that you do not want to expose to clients.
    You can afford stateful token validation (centralized token store like Redis or database).
    You need fine-grained control over token revocation and session management (e.g., logouts or invalidating sessions immediately).

  Use JWT Tokens if:
    Your microservices are stateless and distributed, making central token validation impractical.
    You need high performance with minimal overhead (no need to query a token store).
    Tokens need to carry identity information or roles for quick authorization.




ClientCredentials:GrantType
================================================
A backend service (client) wants to access a protected API (resource server) without a user’s involvement, 
such as retrieving reports or monitoring data.


Step-by-Step Flow:
  The client sends a request to the authorization server with its client ID and client secret.
  The authorization server verifies the credentials and issues an access token.
  The client uses the access token to make requests to the resource server.




OAuth 2.1 
================================================
Proof Key for Code Exchange (PKCE) RFC 
[OAuth 2.1] does not introduce any new features to what already exists in the OAuth 2.0 specifications being replaced.
Adds security to the Authorization Code flow by preventing code interception attacks.


Removal of Deprecated Features:
  Implicit Flow: OAuth 2.1 removes the Implicit Flow, which was used in OAuth 2.0 for single-page applications (SPAs).
  Password Grant: The Resource Owner Password Credentials Grant (Password Grant) is removed in OAuth 2.1.

Mandatory Use of PKCE (Proof Key for Code Exchange):
  PKCE for All Clients: OAuth 2.1 mandates the use of 
  PKCE in the Authorization Code Flow for all clients, not just public clients (e.g., SPAs or mobile apps). 

Enhanced Security Requirements:
  Redirect URIs: OAuth 2.1 requires that redirect URIs must be pre-registered with the authorization server, 
  and wildcard redirect URIs are no longer allowed. 
  State Parameter Usage: The state parameter, used to prevent CSRF (Cross-Site Request Forgery) attacks, is strongly recommended in OAuth 2.1. 
  Token Binding






OAuth and OpenID Connect (OIDC):
  OpenID Connect (OIDC) is an open standard that runs on top of OAuth. While OAuth is used solely for authorization, 
  OIDC is used for authentication. 
  OIDC utilizes an additional token, called the ID token, that contains information about the user and their authentication status. 





PKCE Support in Spring Authorization Server
================================================
No  changes are required on your existing Spring Authorization Server to support PKCE.
Just ensure that your clients support PKCE by sending the code challenge and code verifier with the request.
The authorization server automatically handles PKCE verification, preventing the need for additional logic in your code.

How Spring Authorization Server Handles PKCE:
  The server automatically enforces PKCE when it detects the code_challenge parameter in the authorization request.
  During token exchange, it verifies the code_verifier against the earlier code_challenge.








Simple code backup:
------------------------------------------------
package com.madbarsoft;

import java.util.UUID;

import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.core.annotation.Order;
import org.springframework.http.MediaType;
import org.springframework.security.config.Customizer;
import org.springframework.security.config.annotation.web.builders.HttpSecurity;
import org.springframework.security.core.userdetails.User;
import org.springframework.security.core.userdetails.UserDetails;
import org.springframework.security.core.userdetails.UserDetailsService;
import org.springframework.security.crypto.bcrypt.BCryptPasswordEncoder;
import org.springframework.security.crypto.factory.PasswordEncoderFactories;
import org.springframework.security.crypto.password.PasswordEncoder;
import org.springframework.security.oauth2.core.AuthorizationGrantType;
import org.springframework.security.oauth2.server.authorization.client.InMemoryRegisteredClientRepository;
import org.springframework.security.oauth2.server.authorization.client.RegisteredClient;
import org.springframework.security.oauth2.server.authorization.client.RegisteredClientRepository;
import org.springframework.security.oauth2.server.authorization.config.annotation.web.configuration.OAuth2AuthorizationServerConfiguration;
import org.springframework.security.oauth2.server.authorization.config.annotation.web.configurers.OAuth2AuthorizationServerConfigurer;
import org.springframework.security.oauth2.server.authorization.settings.AuthorizationServerSettings;
import org.springframework.security.provisioning.InMemoryUserDetailsManager;
import org.springframework.security.web.SecurityFilterChain;
import org.springframework.security.web.authentication.LoginUrlAuthenticationEntryPoint;
import org.springframework.security.web.util.matcher.MediaTypeRequestMatcher;

@Configuration
public class AuthorizationServerConfig {
	
	
    @Bean
    @Order(1)
    public SecurityFilterChain authorizationServerSecurityFilterChain(HttpSecurity http) throws Exception {
    	
        OAuth2AuthorizationServerConfiguration.applyDefaultSecurity(http);
        
        http.getConfigurer(OAuth2AuthorizationServerConfigurer.class).oidc(Customizer.withDefaults()); // Enable OpenID Connect 1.0

        http.exceptionHandling(exceptions -> exceptions.defaultAuthenticationEntryPointFor(
                        new LoginUrlAuthenticationEntryPoint("/login"),
                        new MediaTypeRequestMatcher(MediaType.TEXT_HTML)
                )
        ).oauth2ResourceServer(resourceServer -> resourceServer.jwt(Customizer.withDefaults()));

        return http.build();
    }

    @Bean
    @Order(2)
    public SecurityFilterChain defaultSecurityFilterChain(HttpSecurity http) throws Exception {
        http.authorizeHttpRequests(authorize -> authorize
        		.anyRequest()
        		.authenticated())
            .formLogin(Customizer.withDefaults());
        return http.build();
    }
    

    @Bean
    public UserDetailsService userDetailsService() {
        PasswordEncoder encoder = PasswordEncoderFactories.createDelegatingPasswordEncoder();
        UserDetails userDetails = User.withUsername("imran")
                .password(encoder.encode("mypass"))
                .roles("USER")
                .build();
        return new InMemoryUserDetailsManager(userDetails);
    }

    
    
    
    @Bean
    public RegisteredClientRepository registeredClientRepository() {
        RegisteredClient registeredClient = RegisteredClient.withId(UUID.randomUUID().toString())
                .clientId("myclientid")
    			.clientSecret("{noop}myclientsec")
                .authorizationGrantType(AuthorizationGrantType.AUTHORIZATION_CODE)
                .authorizationGrantType(AuthorizationGrantType.REFRESH_TOKEN)
                .authorizationGrantType(AuthorizationGrantType.CLIENT_CREDENTIALS)
                .redirectUri("http://127.0.0.1:8080/login/oauth2/code/myclient")
                .scope("profile")
                .scope("read")
                .scope("write")
                .build();

        return new InMemoryRegisteredClientRepository(registeredClient);
    }
    
    
    @Bean
    public AuthorizationServerSettings authorizationServerSettings() {
        return AuthorizationServerSettings.builder().build();
    }




}



Key Difference: Social Login vs OIDC Servers
================================================
1. Social Login (GitHub, Google, LinkedIn) – User Info via Attributes Map:
   Social login providers (like GitHub, Google, LinkedIn) return user information as a map of attributes, 
   fetched via the User Info endpoint.

2. OIDC-compliant Authorization Servers (Keycloak, Spring Authorization Server) – JWT Token
   OIDC servers (like Keycloak or Spring Authorization Server) return user information as JWT tokens containing claims, 
   which eliminates the need for a separate User Info call.



Key Difference: Social Login vs OIDC Servers:
|=================================================================================================================================================|
| Aspect                    | Social Login (GitHub, Google, LinkedIn)             | OIDC-compliant Servers (Keycloak, Spring Auth Server)         |
|---------------------------|-----------------------------------------------------|------------------------------------------------------------   |
| User Info Format          | Map of attributes (JSON-like object)                | JWT (JSON Web Token) with standard/custom claims              |
| Authentication Mechanism  | OAuth2: Only provides user information              | OIDC: Provides both user info and identity via tokens (JWT)   |
|                           | (via the User Info endpoint)                        |                                                               |
| Token Type                | Usually access token for API access                 | ID token (JWT): Holds authenticated user claims               |
|                           | (not identity-bearing)                              |                                                               |
| Where User Info Is        | By calling User Info endpoint with access token     | Decoded directly from JWT token                               |
| Retrieved                 |                                                     | (ID token or access token)                                    |
| Standardized Claims       | No (varies by provider: GitHub, Google, LinkedIn)   | Yes (OIDC defines standard claims like `sub`, `name`,         |
|                           | all return different attributes)                    | `email`, etc.)                                                |
| Primary Purpose           | Social login and API access                         | Full authentication, authorization, and identity exchange     |
|-------------------------------------------------------------------------------------------------------------------------------------------------|





ID Token vs Access Token
================================================

ID Token:
  Used only for authentication (identifying the user).
  Passed between the client and the identity provider.

Access Token:
  Used to authorize access to APIs (e.g., getting user data from an API).
  Passed between the client and the resource server.

ID tokens are not sent to resource servers (APIs); they are used only between the client and the identity provider.




UUID Token:
  The UUID-based token was used as a reference token.
  It acted like a session key or ID that had no meaningful data embedded inside it.
  The actual token metadata (like user identity, roles, scopes, expiration) was stored in the authorization server’s database.
  When a client used the token, the server looked it up in the database to get the associated data.
  Non-Secure Tokens: UUIDs don’t carry any information; they are just unique strings.









================================================
##keycloak |   Keycloak                                          
================================================= 

http://localhost:8080/auth/realms/master/.well-known/openid-configuration
Get Configi Url:

http://localhost:7070/oauth2/authorization/github


http://localhost:8080/auth/realms/master/protocol/openid-connect/token

http://localhost:8080/realms/master
Get all url

mv keycloak-15.0.2 /opt/keycloak
tar -xvzf keycloak-15.0.2.tar.gz


CREATE DATABASE keycloak;
CREATE USER 'keycloak'@'%' IDENTIFIED BY 'your_password';
GRANT ALL PRIVILEGES ON keycloak.* TO 'keycloak'@'%';
FLUSH PRIVILEGES;



# conf/keycloak.conf

# Database configuration
db=mysql
db-url=jdbc:mysql://localhost:3306/keycloakdb
db-username=keycloak
db-password=your_password


Start:
-------------------------------------------------
sudo ./kc.sh start-dev




#Mysql server with keycloak
-------------------------------------------------

=>CREATE DATABASE keycloakdb CHARACTER SET utf8 COLLATE utf8_unicode_ci;
Create a database in MySQL with the utf8 character set


=>CREATE USER 'keycloakadmin'@'%' IDENTIFIED WITH mysql_native_password BY 'keycloak123';
Create a database user for Keycloak

=>GRANT ALL PRIVILEGES ON keycloakdb.* TO 'keycloakadmin'@'%';
--- Tell the server to reload the grant tables
--- by performing a flush privileges operation
=>FLUSH PRIVILEGES;

Grant all privileges


=>SHOW GLOBAL variables like "default_storage%"
Should output: default_storage_engine InnoDB

=>SET GLOBAL default_storage_engine = 'InnoDB';
If storage engine is not InnoDB, use this to set it



https://green.cloud/docs/how-to-install-configure-keycloak-on-ubuntu-20-04/
https://www.tutorialsbuddy.com/keycloak-mysql-setup
https://www.appsdeveloperblog.com/keycloak-configure-mysql-database/
https://sunitkatkar.blogspot.com/2020/07/setting-up-keycloak-10-with-mysql-8-on.html






=================================================
# kong | Gateway                                      
=================================================

KongWithkeykclok:https://www.jerney.io/secure-apis-kong-keycloak-1/

StartKongWithDocker:
--------------------------------------------------


Create a custom Docker network to allow the containers to discover and communicate with each other:
 docker network create kong-net


Start a PostgreSQL container:
 docker run -d --name kong-database \
  --network=kong-net \
  -p 5432:5432 \
  -e "POSTGRES_USER=kong" \
  -e "POSTGRES_DB=kong" \
  -e "POSTGRES_PASSWORD=kongpass" \
  postgres:13


Prepare the Kong database:
  docker run --rm --network=kong-net \
  -e "KONG_DATABASE=postgres" \
  -e "KONG_PG_HOST=kong-database" \
  -e "KONG_PG_PASSWORD=kongpass" \
  -e "KONG_PASSWORD=test" \
  kong/kong-gateway:3.6.0.0 kong migrations bootstrap



Run the following command to start a container with Kong Gateway:

docker run -d --name kong-gateway \
 --network=kong-net \
 -e "KONG_DATABASE=postgres" \
 -e "KONG_PG_HOST=kong-database" \
 -e "KONG_PG_USER=kong" \
 -e "KONG_PG_PASSWORD=kongpass" \
 -e "KONG_PROXY_ACCESS_LOG=/dev/stdout" \
 -e "KONG_ADMIN_ACCESS_LOG=/dev/stdout" \
 -e "KONG_PROXY_ERROR_LOG=/dev/stderr" \
 -e "KONG_ADMIN_ERROR_LOG=/dev/stderr" \
 -e "KONG_ADMIN_LISTEN=0.0.0.0:8001" \
 -e "KONG_ADMIN_GUI_URL=http://localhost:8002" \
 -e KONG_LICENSE_DATA \
 -p 8000:8000 \
 -p 8443:8443 \
 -p 8001:8001 \
 -p 8444:8444 \
 -p 8002:8002 \
 -p 8445:8445 \
 -p 8003:8003 \
 -p 8004:8004 \
 kong/kong-gateway:3.6.0.0



Verify your installation:
Access the /services endpoint using the Admin API:
 curl -i -X GET --url http://localhost:8001/services



Verify that Kong Manager is running by accessing it using the URL specified in KONG_ADMIN_GUI_URL:
 http://localhost:8002


Clean up containers
docker kill kong-gateway
docker kill kong-database
docker container rm kong-gateway
docker container rm kong-database
docker network rm kong-net



Service and Route:
------------------------------------------

Creating services
curl -i -s -X POST http://localhost:8001/services \
  --data name=example_service \
  --data url='http://httpbin.org'


CheckServicd
curl -X GET http://localhost:8001/services/serviceName

Listing services
curl -X GET http://localhost:8001/services



Managing routes:



Composer:
----------------------------------------

version: '3.4'

services:
  kong-database:
    image: postgres:13
    networks:
      - kong-net
    restart: unless-stopped
    ports:
      - 127.0.0.1:5432:5432
    environment:
      POSTGRES_USER: kong
      POSTGRES_PASSWORD: kongpass
      POSTGRES_DB: kong
      PGDATA: /var/lib/postgresql/data/pgdata
    volumes:
      - db-volume:/var/lib/postgresql/data/pgdata
  kong-gateway:
    image: kong/kong-gateway:3.6.0.0
    networks:
      - kong-net
    depends_on:
      - kong-database
    restart: unless-stopped
    ports:
      - 127.0.0.1:8000:8000
      - 127.0.0.1:8001:8001
      - 127.0.0.1:8002:8002
      - 127.0.0.1:8003:8003
      - 127.0.0.1:8004:8004
      - 127.0.0.1:8443:8443
      - 127.0.0.1:8444:8444
      - 127.0.0.1:8445:8445
    environment:
      KONG_DATABASE: postgres
      KONG_PG_HOST: kong-database
      KONG_PG_USER: kong
      KONG_PG_PASSWORD: kongpass
      KONG_PROXY_ACCESS_LOG: /dev/stdout
      KONG_ADMIN_ACCESS_LOG: /dev/stdout
      KONG_PROXY_ERROR_LOG: /dev/stderr
      KONG_ADMIN_ERROR_LOG: /dev/stderr
      KONG_ADMIN_LISTEN: 0.0.0.0:8001
      KONG_ADMIN_GUI_URL: http://localhost:8002
networks:
  kong-net:
    external: false
volumes:
  db-volume:




docker-compose up -d
docker run --rm --network=kong_kong-net  -e "KONG_DATABASE=postgres"  -e "KONG_PG_HOST=kong-database"  -e "KONG_PG_PASSWORD=kongpass"  -e "KONG_PASSWORD=test" kong/kong-gateway:3.6.0.0 kong migrations bootstrap










=================================================
#XML Requst/Response | xml | 
=================================================
 XML-RPC (Remote Procedure Call) is a remote procedure call protocol that uses XML to encode its calls and HTTP as a 
 transport mechanism. 
 It's simpler in structure compared to SOAP and doesn't have the extensive envelope structure associated with SOAP.








REST vs SOAP
-----------------------------------------------
REST vs SOAP are not really comparable. REST is an architectural style. SOAP is a message exchange format.

Let’s compare the popular implementations of REST and SOAP styles.

RESTful Sample Implementation : JSON over HTTP
SOAP Sample Implementation : XML over SOAP over HTTP



================================================= 
#SOAP SERVICE    | soap service | web-service                                     
================================================= 
SopWebService:
https://www.springboottutorial.com/creating-soap-web-service-with-spring-boot-web-services-starter
https://github.com/in28minutes/spring-boot-examples/tree/master/spring-boot-tutorial-soap-web-services


https://medium.com/@extio/developing-soap-web-services-with-spring-boot-a-comprehensive-guide-1d4f89bc3127

UdamyCourse for SOAP Service:
Master Java Web Services and REST API with Spring Boot






What is SOAP Web Services ?
	In short, a web service is a machine-to-machine, platform-independent service that allows communication over a network.
	SOAP is a messaging protocol. Messages (requests and responses) are XML documents over HTTP. 
	The XML contract is defined by the WSDL (Web Services Description Language). It provides a set of rules to define the messages, 
	bindings, operations, and location of the service.

Contract-First/Contract-Last Development Style OR Top-Down vs. Bottom-Up:
	There are two ways of building SOAP web services. We can go with a top-down approach or a bottom-up approach.
	In a top-down (contract-first) approach, a WSDL document is created, and the necessary Java classes are generated from the WSDL. 
	In a bottom-up (contract-last) approach, the Java classes are written, and the WSDL is generated from the Java classes.
Spring-WS only supports the contract-first development style.


WSDL: WSDL is used to define the structure of Request and Response xml.

WSDL will explain:
	What are the different services (operations) exposed by the server?
	How can a service (operation) be called? What url to use? (also called End Point).
	What should the structure of request xml?
	What should be the structure of response xml?


A SOAP service typically follows the SOAP message structure, which includes:
	<SOAP-Envelope>: The root element that encapsulates the entire SOAP message.
	<SOAP-Header> (optional): Contains header information such as authentication credentials or message routing details.
	<SOAP-Body>: Contains the actual payload or data being sent.
	<SOAP-Fault> (optional): Used to convey error information if a fault occurs during processing.



Create Soap Service:
-------------------------------------
Setp1: Define Request and Response xml 
setp2: Create .xsd file for validateion Request/response xml file
Step3: For generate Java Pojo class for req/response from as .xsd file add JAXB library in pom file



Simple Soap Request:
--------------------------------------

<soapenv:Envelope xmlns:soapenv="http://schemas.xmlsoap.org/soap/envelope/" 
xmlns:cal="http://imranmadbar.com/types/calculator">
	<soapenv:Header/>
	<soapenv:Body>
		<cal:SubtractionInput>
			<cal:number1>10</cal:number1>
			<cal:number2>4</cal:number2>
		</cal:SubtractionInput>
	</soapenv:Body>
</soapenv:Envelope>

<soapenv:Envelope xmlns:soapenv="http://schemas.xmlsoap.org/soap/envelope/" xmlns:inh="http://com.imranmadbr/">
   <soapenv:Header/>
   <soapenv:Body>
      <inh:opManager>
         <!--Optional:-->
         <version>2.0</version>
         <service>xxx</service>
         <method>xxx</method>
         <param>xxxxxx</param>
         <numberofparam>1</numberofparam>
         <userid>xxxxxxx</userid>
         <password>xxxxx</password>
         <failclause>xxxx</failclause>
      </inh:opManager>
   </soapenv:Body>
</soapenv:Envelope>



<soapenv:Envelope xmlns:soapenv="http://schemas.xmlsoap.org/soap/envelope/"
                  xmlns:us="http://imranmadbar.com/spring-boot-soap-service">
    <soapenv:Header/>
    <soapenv:Body>
        <us:getUserRequest>
            <us:name>imran</us:name>
        </us:getUserRequest>
    </soapenv:Body>
</soapenv:Envelope>
		
Simple Soap Response:
-------------------------------------
<SOAP-ENV:Envelope xmlns:SOAP-ENV="http://schemas.xmlsoap.org/soap/envelope/">
  <SOAP-ENV:Header/>
  <SOAP-ENV:Body>
	  <ns2:output xmlns:ns2="http://imranmadbar.com/types/calculator">
		  <ns2:result>6</ns2:result>
	  </ns2:output>
  </SOAP-ENV:Body>
</SOAP-ENV:Envelope>





Key Differences Between SOAP and REST Token Usage:
  Security Model
  SOAP: Provides message-level security, ensuring that the entire SOAP message, including the token, 
  can be encrypted, signed, and protected even across multiple intermediaries.
  REST: Primarily relies on transport-level security (HTTPS). Tokens are not typically encrypted within the message itself, 
  making REST services more dependent on the secure transport layer.
  
  Complexity
  SOAP: More complex, as it involves implementing WS-Security, managing XML-based tokens, and ensuring proper encryption 
  and signing of messages.
  REST: Simpler to implement, often using OAuth tokens and relying on HTTPS for securing tokens in transit.
  
  Flexibility
  SOAP: Supports a wide range of security features and token types, including SAML and custom tokens, allowing for 
  more tailored security implementations.
  REST: Generally focuses on a narrower set of token standards, like OAuth, which are easier to implement but might offer less flexibility.

Summary:
While both SOAP and REST can use tokens for authorization, SOAP offers more granular and robust security options 
through WS-Security, which provides message-level encryption, digital signatures, and other advanced security features. 
REST typically relies on HTTPS for transport-level security and uses simpler token mechanisms like OAuth.






================================================= 
#GraphQL | graphql
================================================= 
mygit-repourl: https://github.com/imrangthub/SPRING_FUNDAMENTAL/tree/master/spring-boot-graphql


Recommended to use post rquest mehtod for all apis: http://localhost:8181/book-app

RequestBoy:

Json:
{
  "query": "query bookDetails { bookById(id: \"book-2\") { id name pageCount author { id firstName lastName } } }"
}


qraphql-query:
query bookDetails {
  bookById(id: "book-2") {
    id
    name
    pageCount
    author {
      id
      firstName
      lastName
    }
  }
}


Doc:
GraphQL is a query language for APIs and a runtime for executing those queries against your data. 
It was developed by Facebook in 2012 and released to the public in 2015. It provides a more flexible and 
efficient way to interact with APIs compared to traditional RESTful APIs.

What is GraphQL?
Query Language: GraphQL allows clients to request exactly the data they need and nothing more, 
making it possible to avoid over-fetching or under-fetching of data.

Runtime: It runs on the server side and interprets the queries, fetching the necessary data from various 
data sources like databases or other APIs.

Type System: GraphQL APIs are strongly typed, meaning the schema defines all possible queries and the 
shape of the data that can be returned.








================================================= 
#gRPC
================================================= 
gRPC (gRPC Remote Procedure Call) is a high-performance, open-source framework developed by Google for building scalable and fast APIs. 
It allows clients and servers to communicate efficiently by using Remote Procedure Calls (RPC), 
where a client can directly call methods on a server as if it were a local object.




Summary
gRPC is a modern RPC framework that excels in performance, efficiency, and flexibility, especially for microservices architectures. 
It differs from REST and SOAP in terms of data format, transport protocol, communication style, and performance, 
with gRPC being the most performant and suitable for low-latency, high-throughput scenarios.

Use Cases:
  gRPC: Ideal for microservices, low-latency communication, and complex interactions like streaming.
  REST: Best suited for public APIs and scenarios where ease of use and human-readability are important.
  SOAP: Common in enterprise applications requiring high security and transactional reliability.









================================================= 
#camel | Apache Camel | camel
================================================= 


Simplap maven app
-------------------------------------------------
<dependency>
    <groupId>org.apache.camel</groupId>
    <artifactId>camel-core</artifactId>
    <version>3.20.0</version>
</dependency>
<dependency>
    <groupId>org.apache.camel</groupId>
    <artifactId>camel-file</artifactId>
    <version>3.20.0</version>
</dependency>



import org.apache.camel.CamelContext;
import org.apache.camel.builder.RouteBuilder;
import org.apache.camel.impl.DefaultCamelContext;

public class FileMoveExample {
    public static void main(String[] args) throws Exception {
        // Create Camel Context
        CamelContext context = new DefaultCamelContext();
        
        // Define the route
        context.addRoutes(new RouteBuilder() {
            @Override
            public void configure() throws Exception {
                // Route definition: Move files from 'input' directory to 'output'
                from("file:input")
                    .to("file:output");
            }
        });
        
        // Start the context
        context.start();
        
        // Keep the application running for a while to let the file processing happen
        Thread.sleep(5000);
        
        // Stop the context
        context.stop();
    }
}



from("file:input"): This sets up a route that watches the input directory for new files.
to("file:output"): Moves the files from input directory to output directory.


apihub@gzvldopenapi02 conf.d]$ /app/apihub/Nginx_LB/sbin/nginx -t
nginx: [emerg] unknown directive "status_zone" in /app/apihub/Nginx_LB/conf/conf.d/openapi_prod.conf:46
nginx: configuration file /app/apihub/Nginx_LB/conf/nginx.conf test failed
[apihub@gzvldopenapi02 conf.d]$ ps -aux | grep nginx
apihub      1813  0.0  0.0  45812  3348 ?        Ss   Jul04   0:01 nginx: master process /usr/sbin/nginx -c /app/apihub/Nginx_LB/conf/nginx.conf
