#################################################
#         MICROSERVICE-TECHSTACK                #
#################################################


=================================================
#                  Basic                                          
================================================= 






=================================================
#Spring-caching                                   
================================================= 
 In Spring, caching can be easily implemented using in-memory caching providers:
Generic
JCache (JSR-107) (EhCache 3, Hazelcast, Infinispan, and others)
EhCache 2.x
Hazelcast
Infinispan
Couchbase
Redis
Caffeine
Simple or the default ConcurrentMapCacheManager.


















https://www.itpanther.com/category/redis/

https://www.youtube.com/watch?v=tip2mgC6rwQ&list=PLVCgi5HZ0-YtWh-PD8LUCbdsG7oDIfGlD&index=13



Redis couse:
-------------------------------------------------
https://www.itpanther.com/installing-redis-on-linux-using-a-package-manager/









1. Visit https://www.itpanther.com/product/redis-virtual-lab/

2. Add product to cart

3. Move to Cart from the menu bar or https://www.itpanther.com/cart/

4. Apply coupon code: QEPKNXWJ

5. Checkout

6. Wait for an email containing your Redis environment details.



Remember:

- Each registered email ID can avail of up to 5 sessions.

- Each session grants access to the Lab environment for 1 hour.

- To start a new session, repeat these steps.



Happy Learning !!!



Regards,

Vikas Kumar Jha



https://www.youtube.com/playlist?list=PLVCgi5HZ0-YtWh-PD8LUCbdsG7oDIfGlD
=================================================
#redis | Redis                                     
================================================= 
=>sudo apt install redis-tools
=>redis-cli --version
Run a Redis Server using docker and install resis-toos for redis-cli in ubuntu

=>sudo systemctl status redis
=>sudo vi /etc/redis/redis.conf
Edit Config file, thne restart redis-service
=>sudo systemctl restart redis-server


=>redis-cli ping
=>redis-cli
=>redis-cli -p 8888

redis-server -v
redis-server --check-system


=>CONFIG GET "port"
127.0.0.1:8888> CONFIG GET *
Show currrent server configuration 

docker run -v /home/bs960/imranMadbar/myPROJECT/redisWorkshop/redisLocalVol:/usr/local/etc/redis -p8080:8080 -d  --name myredis redis redis-server /usr/local/etc/redis/redis.conf
=>docker run -p 6379:6379 --name my-redis1 -d redis
=>docker run -p 8888:6379 --name my-redis1 -d my-redis




=>set mykey myvalue
=>get mykey

=>keys *
=>dbsize
Show list of key

=>del keyName
Delete single key

=>flushall
Remove all data


=>redis-cli INFO MEMORY
Check Memory

=>redis-cli --latency-history
For Latency Check


=>CLIENT LIST
It is possible to verify the number of active connections using the CLIENT LIST command.



=>redis-cli --bigkeys
redis-cli -h <hostname> -p <port> -a <password> --bigkeys
For information about all key in redis (size, number of key etc)



#Redis HA 
=================================================

Master Node:
docker run -v /home/bs960/imranMadbar/myPROJECT/redisWorkshop/haV/master/config:/usr/local/etc/redis -v /home/bs960/imranMadbar/myPROJECT/redisWorkshop/haV/master/data:/data -p8080:8080 -d  --name redis_master redis redis-server /usr/local/etc/redis/redis.conf

Master: redis.conf file
port 8080
bind 0.0.0.0
protected-mode no
save 5 2
rdbcompression yes
dbfilename redis-master-dump.rdb
dir /data



Slave1
docker run -v /home/bs960/imranMadbar/myPROJECT/redisWorkshop/haV/slave/config:/usr/local/etc/redis -v /home/bs960/imranMadbar/myPROJECT/redisWorkshop/haV/slave/data:/data -p8181:8181 -d  --name redis_slave redis redis-server /usr/local/etc/redis/redis.conf

Slave: redis.conf file
port 8181
bind 0.0.0.0
protected-mode no
save 5 2
rdbcompression yes
dbfilename redis-slave-dump.rdb
dir /data
replicaof 192.168.217.145 8080




redis-cli -p 8080
redis-cli -p 8181
=>info replication




#Redis Sentinel 
=================================================

Create Doecker Network:
docker network create redis-net



Create masterNode
docker run --name redis-master -p 6379:6379 --network redis-net -d redis redis-server
docker run --name redis-master -p 6379:6379 --network redis-net -d redis redis-server --appendonly yes



Create Replica Node:
docker run --name redis-replica1 -p 6380:6379 --network redis-net -d redis redis-server --appendonly yes --slaveof redis-master 6379
docker run --name redis-replica2 -p 6381:6379 --network redis-net -d redis redis-server --appendonly yes --slaveof redis-master 6379

docker run --name redis-replica1 -p 6380:6379 --network redis-net -d redis redis-server --slaveof redis-master 6379
docker run --name redis-replica2 -p 6381:6379 --network redis-net -d redis redis-server --slaveof redis-master 6379



Login to Master slave, master allow write/read and slave allow only read(get).
redis-cli -p 6379

redis-cli -p 6380
redis-cli -p 6381




Redis Sentinel using Docker:
------------------------
Get IP address of all the containers:
docker inspect -f '{{.Name}} - {{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' $(docker ps -aq)



Create sentinel.conf file and provide master server details with master-node (container) IP.
echo "sentinel monitor mymaster 172.19.0.2 6379 2" > /home/bs960/imranMadbar/myPROJECT/redisWorkshop/redisSentinel/sntnl1/sentinel.conf
echo "sentinel monitor mymaster 172.19.0.2 6379 2" > /home/bs960/imranMadbar/myPROJECT/redisWorkshop/redisSentinel/sntnl2/sentinel.conf
echo "sentinel monitor mymaster 172.19.0.2 6379 2" > /home/bs960/imranMadbar/myPROJECT/redisWorkshop/redisSentinel/sntnl3/sentinel.conf



Create three docker containers for Sentinels:
docker run -d --name redis-sentinel_1 -p 26381:26379 --network redis-net -v /home/bs960/imranMadbar/myPROJECT/redisWorkshop/redisSentinel/sntnl1:/data redis redis-sentinel /data/sentinel.conf
docker run -d --name redis-sentinel_2 -p 26382:26379 --network redis-net -v /home/bs960/imranMadbar/myPROJECT/redisWorkshop/redisSentinel/sntnl2:/data redis redis-sentinel /data/sentinel.conf
docker run -d --name redis-sentinel_3 -p 26383:26379 --network redis-net -v /home/bs960/imranMadbar/myPROJECT/redisWorkshop/redisSentinel/sntnl3:/data redis redis-sentinel /data/sentinel.conf






=>redis-cli -p 26381
=>redis-cli -h my_host -p my_port -a my_password
=>redis-cli -h 172.16.7.161 -p 26379 -a pass
Login to sentinel


=>sentinel masters  
=>sentinel master mymaster
Info about all master

=>sentinel get-master-addr-by-name mymaster
Get current master and port


=>sentinel slaves mymaster
=>SENTINEL REPLICAS mymaster
Show connected replicas



docker run --name myng -d --network redis-net imranmadbar/nginx 
docker run --name mypy -d --network redis-net python sleep 99999
Run a simple client ng service, and install python

Install Python on DockerContainer for checkRedis
apt-get update
apt-get install python3
pip install redis
apt-get install nano -y



Get Master and Slave Info (in side from docker):
import redis
sentinel = redis.sentinel.Sentinel([('172.19.0.5', 26381),('localhost', 26382),('localhost', 26383)], socket_timeout=0.1)
master = sentinel.discover_master('mymaster')
print (master)
replicas = sentinel.discover_slaves('mymaster')
print(replicas)
OR
import redis
from redis.sentinel import Sentinel

# Define the list of Sentinel instances
sentinel_servers = [('172.19.0.5', 26379),
                    ('172.19.0.6', 26379),
                    ('172.19.0.7', 26379)]

# Define the service name
service_name = 'mymaster'

# Create a Sentinel object
sentinel = Sentinel(sentinel_servers, socket_timeout=0.1)

# Try to get the master
try:
    master = sentinel.master_for(service_name)
    master.ping()  # Test the connection
    print("Connected to the Redis master successfully.")
except redis.exceptions.ConnectionError as e:
    print(f"Failed to connect to the Redis master. Error: {e}")
except Exception as e:
    print(f"An error occurred: {e}")


WriteDatausing Sentanels
-----------------------------------------------------
import redis
from redis.sentinel import Sentinel

sentinel_servers = [('172.19.0.5', 26379),
                    ('172.19.0.6', 26379),
                    ('172.19.0.7', 26379)]
service_name = 'mymaster'

sentinel = Sentinel(sentinel_servers, socket_timeout=0.1)
master = sentinel.master_for(service_name)
replicas = sentinel.discover_slaves(service_name)

try:
    # Print current master and replicas
    print("Master:", master.connection_pool.get_master_address())

    print("Replicas:", replicas)

    # Attempt to ping master
    master.ping()
    print("Connected to the Redis master successfully.")

    # Ask for a key value and set it if provided
    key = input("Enter the key: ").strip()
    if key:
        value = input("Enter the value: ").strip()
        master.set(key, value)
        print(f"Key '{key}' set successfully.")
    else:
        print("No key provided. Exiting.")

    # Retrieve a key value if it exists
    key_to_retrieve = input("Enter the key to retrieve: ").strip()
    if key_to_retrieve:
        retrieved_value = master.get(key_to_retrieve)
        if retrieved_value:
            print(f"Retrieved value: {retrieved_value.decode('utf-8')}")
        else:
            print(f"No value found for '{key_to_retrieve}'")
    else:
        print("No key provided for retrieval.")

except redis.exceptions.ConnectionError as e:
    print(f"Failed to connect to the Redis master. Error: {e}")
except Exception as e:
    print(f"An error occurred: {e}")








==============================================
#Redis Performance Benchmarking | Tuning
==============================================


redis-benchmark -q -n 1000000
redis-benchmark -q -n 1000000 -p 8888
redis-benchmark -q -n 1000000 -h 10.128.0.2
redis-benchmark -q -n 100 -t get,set
  -q means run this in a quit mode and just display us the output.
  -n means the total number of queries to run during this test.


Tuning
---------------------------------------------------

RDB Persistence and Append Only File
If you are using the cluster mode of Redis then the RDB persistence and AOF is not required. So simply comment out these lines in redis.conf
sudo vim /etc/redis/redis.conf

# Comment out these lines
save 900 1
save 300 10
save 60 10000

rdbcompression no
rdbchecksum no
appendonly no



Memory Usage
Redis will use all of your available memory in the server unless this is configured. 
edit your redis.conf just like below:
	# Setting it to 16Gib
	maxmemory 17179869184
	
maxmemory 500mb
maxmemory-policy allkeys-lru


	
	
TCP-KeepAlive
Keepalive is a method to allow the same TCP connection for HTTP conversation instead of opening a new one with each new request.
## Editing default config file /etc/redis/redis.conf
# Update the value to 0
tcp-keepalive 0



TCP-backlog
you will need this to be higher if you have many connections. To do this, edit your redis config file
# TCP listen() backlog.
# In high requests-per-second environments you need an high backlog in order
# make sure to raise both the value of somaxconn and tcp_max_syn_backlog
tcp-backlog 65536


Set maxclients
The default is 10000 and if you have many connections you may need to go higher
# Once the limit is reached Redis will close all the new connections sending
# an error 'max number of clients reached'.
maxclients 10000

HostSide:
========
Max-Connection
sudo vim /etc/rc.local
# make sure this line is just before of exit 0.
sysctl -w net.core.somaxconn=65365


Overcommit Memory
Overcommit memory is a kernel parameter which checks if the memory is available or not.
If the overcommit memory value is 0 then there is a chance that your Redis will get OOM (Out of Memory) error.
echo 'vm.overcommit_memory = 1' >> /etc/sysctl.conf



Transparent Huge Page(THP)
but somehow it slows down the databases which are memory-based (for example — in the case of Redis). To overcome this issue you can disable THP.
sudo vim /etc/rc.local

# Add this line before exit 0
echo never > /sys/kernel/mm/transparent_hugepage/enabled























CreateClusterWay2
--------------------------------------------------
Create Doecker Network:
docker network create redis-cluster


There are 2 images that we need to pull first
docker pull redis
docker pull redislabs/redisinsight


mkdir -p {7000..7005}
mkdir 7000 7001 7002 7003 7004 7005
Redis recommends nodes with a minimum number of 6. So we will create those nodes 

Create a config file in parant directory:
vi redis.conf
bs960@BS-960:~/imranMadbar/myPROJECT/redisWorkshop/redisCluster$ cat redis.conf 
# redis.conf file
port 7000
cluster-enabled yes
cluster-config-file nodes.conf
cluster-node-timeout 5000
appendonly yes
bind 0.0.0.0
protected-mode no


Then create the minimal configuration redis.conf and copy all directory
for i in {7000..7005}; do cp redis.conf $i; done
Update redis.conf file port as directory name




Thne run this for each directory (redis instance, as node-1, node-1 ... etc.)
docker run -v `pwd`/7001:/redis --name node-1 -p 7001:7001 --network redis-cluster redis redis-server /redis/redis.conf
docker run -v `pwd`/7002:/redis --name node-2 -p 7002:7002 --network redis-cluster redis redis-server /redis/redis.conf
docker run -v `pwd`/7003:/redis --name node-3 -p 7003:7003 --network redis-cluster redis redis-server /redis/redis.conf

docker run -v `pwd`/7004:/redis --name node-4 -p 7004:7004 --network redis-cluster redis redis-server /redis/redis.conf
docker run -v `pwd`/7005:/redis --name node-5 -p 7005:7005 --network redis-cluster redis redis-server /redis/redis.conf
docker run -v `pwd`/7006:/redis --name node-6 -p 7006:7006 --network redis-cluster redis redis-server /redis/redis.conf



Now create the cluster:
docker exec -it node-1 bash
redis-cli -p 7001 --cluster create node-1:7001 node-2:7002 node-3:7003 node-4:7004 node-5:7005 node-6:7006 --cluster-replicas 1 --cluster-yes
Successfully Done if no error



For UI View:RedisInsight


 mkdir redisinsight
 docker run -v `pwd`/redisinsight:/db -p 8001:8001 --network redis-cluster redislabs/redis-insight


useCompose: docker-compose.yml
version: '3'
networks:
  redis-cluster-compose:
    driver: bridge

services:
  redis-node-1:
    image: redis:latest
    ports:
      - 7000:7000
    networks:
      - redis-cluster-compose
    hostname: redis-node-1
    volumes:
      - ./7000:/redis
    command: redis-server /redis/redis.conf
  redis-node-2:
    image: redis:latest
    ports:
      - 7001:7001
    networks:
      - redis-cluster-compose
    hostname: redis-node-2
    volumes:
      - ./7001:/redis
    command: redis-server /redis/redis.conf

  redis-node-3:
    image: redis:latest
    ports:
      - 7002:7002
    networks:
      - redis-cluster-compose
    hostname: redis-node-3
    volumes:
      - ./7002:/redis
    command: redis-server /redis/redis.conf

  redis-node-4:
    image: redis:latest
    ports:
      - 7003:7003
    networks:
      - redis-cluster-compose
    hostname: redis-node-4
    volumes:
      - ./7003:/redis
    command: redis-server /redis/redis.conf

  redis-node-5:
    image: redis:latest
    ports:
      - 7004:7004
    networks:
      - redis-cluster-compose
    hostname: redis-node-5
    volumes:
      - ./7004:/redis
    command: redis-server /redis/redis.conf
  
  redis-node-6:
    image: redis:latest
    ports:
      - 7005:7005
    networks:
      - redis-cluster-compose
    hostname: redis-node-6
    volumes:
      - ./7005:/redis
    command: redis-server /redis/redis.conf
  
  redis-cluster-creator:
    image: redis:latest
    ports:
      - 6999:6999
    networks:
      - redis-cluster-compose
    command: redis-cli -p 7000 --cluster create redis-node-1:7000 redis-node-2:7001 redis-node-3:7002 redis-node-4:7003 redis-node-5:7004 redis-node-6:7005 --cluster-replicas 1 --cluster-yes
    depends_on:
      - redis-node-1
      - redis-node-2
      - redis-node-3
      - redis-node-4
      - redis-node-5
      - redis-node-6
  
  redis-insight:
    image: redislabs/redisinsight
    ports:
      - 8001:8001
    networks:
      - redis-cluster-compose
    volumes:
      - ./redisinsight:/db
    depends_on:
      - redis-cluster-creator

























Doc
-------------------------
https://docs.spring.io/spring-data/redis/reference/redis/connection-modes.html
https://medium.com/@htyesilyurt/spring-boot-3-redis-sentinel-lettuce-client-and-docker-compose-for-high-availability-1f1e3c372a5a
https://www.youtube.com/watch?v=Pj8Q_9dovgY&list=PLq3uEqRnr_2HY6LMQsbvsK4btj51sWhBS&index=2


Spring Boot, both Lettuce and Jedis are popular choices for interacting with Redis, which is an open-source, in-memory data structure store. 

Lettuce:
  Asynchronous Support
  Thread Safety
  Connection Pooling
  Cluster Support

Jedis:
  Synchronous and Asynchronous Support:
  Connection Pooling:
  Maturity:

Choosing Between Lettuce and Jedis:
Performance:
Lettuce is often considered more performant than Jedis, especially in scenarios where high concurrency and asynchronous behavior are important.

Reactive Programming:
If your application is built using reactive programming paradigms (e.g., Spring WebFlux), Lettuce may be a more natural fit.
Ease of Use:

Jedis has a simpler API, and if you are looking for a straightforward and easy-to-use client, Jedis might be a good choice.






Caching Topology:
-Standalone         
 Application inMemory cashing

-Distributed
 Central case system

-Replication
 Chahe replicas every node

UdamyCourse:
https://gale.udemy.com/course/redis-latest/learn/lecture/21936116#overview


Example:
Assuming you have a Redis configuration as follows:

Master (Writer): 192.168.1.1:8080
Replicas (Readers):
192.168.3.1:8082
192.168.2.1:8082
Here's a configuration for a Spring Cloud microservice with 10 application instances:

java
Copy code
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.data.redis.connection.RedisStandaloneConfiguration;
import org.springframework.data.redis.connection.RedisStaticMasterReplicaConfiguration;
import org.springframework.data.redis.connection.lettuce.LettuceClientConfiguration;
import org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory;

@Configuration
public class RedisConfig {
    
    @Bean
    public LettuceClientConfiguration lettuceClientConfiguration() {
        return LettuceClientConfiguration.builder()
            .readFrom(ReadFrom.REPLICA_PREFERRED)
            .build();
    }
    
    @Bean
    public LettuceConnectionFactory lettuceConnectionFactory(LettuceClientConfiguration clientConfig) {
        RedisStandaloneConfiguration serverConfig = new RedisStandaloneConfiguration("192.168.1.1", 8080);
        RedisStaticMasterReplicaConfiguration staticMasterReplicaConfig = new RedisStaticMasterReplicaConfiguration(
            serverConfig,
            Arrays.asList(
                new RedisNode("192.168.3.1", 8082),
                new RedisNode("192.168.2.1", 8082)
            )
        );
        
        return new LettuceConnectionFactory(staticMasterReplicaConfig, clientConfig);
    }
}


























=================================================
#Event-driven Architecture
=================================================

Event-driven Architecture alternate of loss cupling or temp-cuplinc(Synchronous communitation)

Two even-driven model/technology:
  1) Pub/sub model
  2) Event Streming model

1)Pub/sub model: Once an event is dreceived, its cannot be replayed, which means new 
  subscribers join later will not have acccess paset event.

2)Event Streming model: this model events are writeen to a log in a sequential manner.
Producer publish events as they occur, and these events are stored in a well-ordered fashion
consumer have the ability to read from any part of the event stream.
One advantage of this model is thet events cna be replayed, allowing clients to join at 
any time and received all past events.





=================================================
#    Message broker  | RabbitMQ                                        
================================================= 
The AMQP operates on the principales of exchanges and queues for all queue operation.

RabitMQ design overview (4 actors):                              
  publisher(producer) --> exchange ----(routes)--> queue --> subscriber(consumer)


Installing and Run RabbitMQ: 
-------------------------------------------------
Docker:
docker run -it --rm --name rabbitmq -p 5672:5672 -p 15672:15672 rabbitmq:3.12-management

OR
=>rabbitmq-server start
Start rabbitMQ Server, Open RabbitMQ command prompt

=>localhost:15672
guest/guest



Example: Geting Start with Simple Queue
------------------------------------------------
Creating a Quequ:
Queues and Streams->add a new queque->add Queue

Publish to Quequ:
Click to queue->	my-test-queue1 -> Puhlish Message
Now it will show Messsage(Ready)

Received Msg from Quequ:
Click Get Message from Quew





SimpleApp:
http://localhost:8081/rabbitmq/sent-emp?empName=emp1&empId=emp001

              
FanoutExchange:
http://localhost:8083/rabbitmq/fanout/producer?exchangeName=fanout-exchange&messageData=HelloWorldMsg
http://localhost:8084/rabbitmq/producer?empName=emp1&empId=emp001&salary=500000

DirectExchange:
http://localhost:8083/rabbitmq/direct/producer?exchangeName=direct-exchange&routingKey=finance&messageData=HelloWorldMSG






Tach Info
-------------------------------------------------


Features of RabbitMQ:
Connectivity Options:
  RabbitMQ supports multiple messaging protocols and can be deployed in distributed/federated configurations to meet high availability, scalability requirements.
Pluggable Architecture:
  RabbitMQ allows to choose a persistence mechanism and also provides options to customize security for authentication and authorization as per the application needs.


MQ-Types of Messaging

Point to Point
  In this type of communication, the broker sends messages to only one consumer, while the other consumers will wait till they get 
the messages from the broker. No consumer will get the same message.

If there are no consumers, the Broker will hold the messages till it gets a consumer (Queue based communication ). 
If there is more than one consumer, they may get the next message but they won’t get the same message as the other consumer.


Publish/Subscribe
  In this type of communication, the Broker sends same copy of messages to all the active consumers. 
  This type of communication is also known as Topic based communication where broker sends same message to all active 
  consumer who has subscribed for particular Topic. 


RabbitMQ supports several exchange types:
  Direct Exchange: Routes messages based on a specified routing key.
  Fanout Exchange: Broadcasts messages to all queues bound to it.
  Topic Exchange: Allows flexible routing with wildcard patterns in routing keys.
  Headers Exchange: Routes based on message header attributes.
  Default Exchange (Direct): Uses queue names as routing keys.


RabbitMQ queue properties include:
  Name: Unique identifier.
  Durable: Survive broker restart.
  Auto-Delete: Delete when no consumers.
  Exclusive: Used by the creating connection.
  Arguments: Additional settings (e.g., TTL, max length).
  Message TTL: Maximum time a message can stay.
  Maximum Priority: Sets message priority.
  Maximum Length: Max number of messages.
  Overflow Behavior: What happens when max length is reached.
  Dead Letter Exchange/Key: For handling failed or expired messages.



RabbitMQ Exchange type:
------------------------------------------------
Direct Exchange (One-to-One Relationship):
  Like sending a letter to a specific person based on their address. Each message is routed to a single queue based on an exact match between the message's routing key and the queue's binding key.

Fanout Exchange (One-to-Many Relationship):
  Like broadcasting a message on TV. Each message sent to a fanout exchange is delivered to all queues bound to that exchange, regardless of any routing keys or message attributes.

Topic Exchange (One-to-Many Relationship with Conditions):
  Like sending emails to different groups based on their interests or topics. Messages are routed to queues based on wildcard matches between routing keys and routing patterns specified by the queues. One message may be delivered to multiple queues if they match the routing pattern.

Headers Exchange (One-to-Many Relationship based on Message Attributes):
  Like sorting mail based on various attributes or metadata attached to each letter. Messages are routed to queues based on headers and their values instead of routing keys. One message may be delivered to multiple queues if their header values match the specified criteria.



=================================================
#kafka | Message broker  | KAFKA                                        
================================================= 
OfsetExpolar
kafkalytic-intelejidea-pluging

Install and Run KafKa:

$ tar -xzf kafka_2.13-3.6.1.tgz
$ cd kafka_2.13-3.6.1


Kafka with ZooKeeper
# Start the ZooKeeper service
$ bin/zookeeper-server-start.sh config/zookeeper.properties
# Start the Kafka broker service
$ bin/kafka-server-start.sh config/server.properties


$ bin/kafka-topics.sh --create --topic quickstart-events --bootstrap-server localhost:9092
CREATE A TOPIC TO STORE YOUR EVENTS

=>cd /opt/apache-kafka/kafka3_6/bin



SetUp Kafka SingleCluster
-------------------------------------------------
Start up the Zookeeper
=>./zookeeper-server-start.sh ../config/zookeeper.properties

Add the below properties in the server.properties
  listeners=PLAINTEXT://localhost:9092
  auto.create.topics.enable=false


Start up the Kafka Broker
=>./kafka-server-start.sh ../config/server.properties


Create a topic
=>./kafka-topics.sh --create --topic test-topic --bootstrap-server localhost:9092 --partitions 4 --replication-factor 1
=>./kafka-topics.sh --list  --bootstrap-server localhost:9092


=>./kafka-topics.sh --bootstrap-server localhost:9092 --describe
=>./kafka-topics.sh --bootstrap-server localhost:9092 --describe --topic test-topic



Create Console Producer
=>./kafka-console-producer.sh --broker-list localhost:9092 --topic test-topic
=>./kafka-console-producer.sh --broker-list localhost:9092 --topic test-topic --property "key.separator=-" --property "parse.key=true"

Create Console Consumer
=>./kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test-topic --from-beginning
=>./kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test-topic --from-beginning -property "key.separator= - " --property "print.key=true"




Consumer Groups:
=>./kafka-consumer-groups.sh --bootstrap-server localhost:9092 --list



Docker:
---------------------------------------------------------
gitClone: https://github.com/conduktor/kafka-stack-docker-compose
=>docker-compose -f zk-single-kafka-single.yml up -d
=>docker-compose -f zk-single-kafka-single.yml ps
=>docker-compose -f zk-single-kafka-single.yml stop


zk-single-kafka-single.yml:

version: '2.1'
services:
  zoo1:
    image: confluentinc/cp-zookeeper:7.3.2
    hostname: zoo1
    container_name: zoo1
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_SERVER_ID: 1
      ZOOKEEPER_SERVERS: zoo1:2888:3888

  kafka1:
    image: confluentinc/cp-kafka:7.3.2
    hostname: kafka1
    container_name: kafka1
    ports:
      - "9092:9092"
      - "29092:29092"
      - "9999:9999"
    environment:
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka1:19092,EXTERNAL://${DOCKER_HOST_IP:-127.0.0.1}:9092,DOCKER://host.docker.internal:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT,DOCKER:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_ZOOKEEPER_CONNECT: "zoo1:2181"
      KAFKA_BROKER_ID: 1
      KAFKA_LOG4J_LOGGERS: "kafka.controller=INFO,kafka.producer.async.DefaultEventHandler=INFO,state.change.logger=INFO"
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_JMX_PORT: 9999
      KAFKA_JMX_HOSTNAME: ${DOCKER_HOST_IP:-127.0.0.1}
      KAFKA_AUTHORIZER_CLASS_NAME: kafka.security.authorizer.AclAuthorizer
      KAFKA_ALLOW_EVERYONE_IF_NO_ACL_FOUND: "true"
    depends_on:
      - zoo1






=>docker exec -it kafka1 /bin/bash
=>kafka-topics --version


=>kafka-topics --create --topic test-topic --bootstrap-server localhost:9092 --partitions 4 --replication-factor 1
=>kafka-topics --list --bootstrap-server localhost:9092

=>kafka-topics --bootstrap-server localhost:9092 --describe


=>kafka-console-producer --broker-list localhost:9092 --topic test-topic
Console Producer

=>kafka-console-consumer --bootstrap-server localhost:9092 --topic test-topic --from-beginning
console consumer


=>kafka-consumer-groups --bootstrap-server localhost:9092 --list
consumer group


=>kafka-topics --bootstrap-server localhost:9092 --delete --topic '__consumer_offsets-.*'














Kafka Info Doc:
---------------------------------------------------------
SpringKafka:
https://gale.udemy.com/course/apache-kafka-for-developers-using-springboot/learn/lecture/37906902#overview
udTut-repo:
https://github.com/dilipsundarraj1/kafka-for-developers-using-spring-boot-v2/blob/main/SetUpKafkaDocker.md#set-up-broker-and-zookeeper


Main Concepts and Terminology
Events,records,message:
  An event records the fact that "something happened" in the world or in your business. 
  It is also called record or message in the documentation. When you read or write data to Kafka, you do this in the form of events. 
  Conceptually, an event has a key, value, timestamp, and optional metadata headers. 

Topics:
  Events are organized and durably stored in topics. 
  Very simplified, a topic is similar to a folder in a filesystem, and the events are the files in that folder. 

Here's an example event:
  Event key: "Alice"
  Event value: "Made a payment of $200 to Bob"
  Event timestamp: "Jun. 25, 2020 at 2:06 p.m."

Topics in Kafka are always multi-producer and multi-subscriber: a topic can have zero, one, or many producers that write events to it, 
as well as zero, one, or many consumers that subscribe to these events
unlike traditional messaging systems, events are not deleted after consumption. 
Instead, you define for how long Kafka should retain your events through a per-topic configuration setting, 
after which old events will be discarded. 

Partition:
  Topics are partitioned, meaning a topic is spread over a number of "buckets" located on different Kafka brokers. 
  This distributed placement of your data is very important for scalability because it allows client applications to 
  both read and write the data from/to many brokers at the same time. 
  When a new event is published to a topic, it is actually appended to one of the topic's partitions. 
  Events with the same event key (e.g., a customer or vehicle ID) are written to the same partition, 
  and Kafka guarantees that any consumer of a given topic-partition will always read that partition's events in exactly the same 
  order as they were written.


Partition offset:
  Each partitioned message has a unique sequence id called as offset.








Kafka:
  Kafka is a distributed event streaming platform that lets you read, write, store, and process events 
  (also called records or messages in the documentation) across many machines.

  These events are organized and stored in topics. 
  A topic is similar to a folder in a filesystem, and the events are the files in that folder.
So before you can write your first events, you must create a topic.



A Kafka client communicates with the Kafka brokers via the network for writing (or reading) events. 
Once received, the brokers will store the events in a durable and fault-tolerant manner for as long as you need—even forever.


Events are durably stored in Kafka, they can be read as many times and by as many consumers as you want. 







Zookeeper:
 Manage the kafka broker, broker registred with zookeeper.

Topic/Partation/Offset:
  Kafka tpoic as database table.
  Each topic will create with one or more partiation.

  Partitation is where message linve inside the topic.
  Each partition is independent of each other.

  Each Partition is an Order, inmmutable sequence of record.
  Each record is assigned a sequential number called offset.
  Order is guaranteed only at the partition leve.
All the records are persisted in acommit log in the files system where is kafka intalled.


Message of Kafka retain inside in cluster after consumed, depending on retation prood.
Kafka Message are byte insede kafka broker, then producer/consumer serisized this.


For locket a specific message, required 3 thing:
- Topic Name
- Partition number
- Offset Number 


Consumer have three option to reload
- from-biginning
- latest
- specific offset



Kafka 4 main Component:
  Broker: Stores and manages data.Its a java process, can incress horizontally.
  ZooKeeper: Manages Kafka cluster's configuration and coordination.
  Producer: Sends data to Kafka. Communicate with broker with tcp. Producer create record with partiation number attributes.
  Consumer: Receives data from Kafka


Kafka-Records:
  exist within Kafka topics. When data is produced to Kafka, it is encapsulated into records and appended to the 
  log (partition) of the respective topic. Each record consists of a key, a value, headers, timestamp, offset,
   and possibly other metadata. 


Record comprises several components:

  Key: An optional identifier for the record.
  Value: The main data payload.
  Headers: Optional metadata associated with the record.
  Timestamp: Indicates when the record was produced or ingested into Kafka.
  Offset: A unique identifier assigned to each record within a partition.
  Partition: Specifies the partition to which the record belongs.


To Unique identify a reocrd use (partition+Offset) composit key number. 




Replication Factor: Replication factor factor make partation duplicate.
  If replication-factor=2 then every partation has two copy if three then the number of copy is 3.





Example:
=================================================
Producer and Consume the Messages
Let's going to the container by running the below command.
docker exec -it kafka1 bash
Create a Kafka topic using the kafka-topics command.
kafka1:19092 refers to the KAFKA_ADVERTISED_LISTENERS in the docker-compose.yml file.
kafka-topics --bootstrap-server kafka1:19092 \
             --create \
             --topic test-topic \
             --replication-factor 1 --partitions 1
Produce Messages to the topic.
docker exec --interactive --tty kafka1  \
kafka-console-producer --bootstrap-server kafka1:19092 \
                       --topic test-topic
Consume Messages from the topic.
docker exec --interactive --tty kafka1  \
kafka-console-consumer --bootstrap-server kafka1:19092 \
                       --topic test-topic \
                       --from-beginning
Producer and Consume the Messages With Key and Value
Produce Messages with Key and Value to the topic.
docker exec --interactive --tty kafka1  \
kafka-console-producer --bootstrap-server kafka1:19092 \
                       --topic test-topic \
                       --property "key.separator=-" --property "parse.key=true"
Consuming messages with Key and Value from a topic.
docker exec --interactive --tty kafka1  \
kafka-console-consumer --bootstrap-server kafka1:19092 \
                       --topic test-topic \
                       --from-beginning \
                       --property "key.separator= - " --property "print.key=true"
Consume Messages using Consumer Groups
docker exec --interactive --tty kafka1  \
kafka-console-consumer --bootstrap-server kafka1:19092 \
                       --topic test-topic --group console-consumer-41911\
                       --property "key.separator= - " --property "print.key=true"
Example Messages:
a-abc
b-bus
Consume Messages With Headers
docker exec --interactive --tty kafka1  \
kafka-console-consumer --bootstrap-server kafka1:19092 \
                       --topic library-events.DLT \
                       --property "print.headers=true" --property "print.timestamp=true" 
Example Messages:
a-abc
b-bus
Set up a Kafka Cluster with 3 brokers
Run this command and this will spin up a kafka cluster with 3 brokers.
docker-compose -f docker-compose-multi-broker.yml up
Create topic with the replication factor as 3
docker exec --interactive --tty kafka1  \
kafka-topics --bootstrap-server kafka1:19092 \
             --create \
             --topic test-topic \
             --replication-factor 3 --partitions 3
Produce Messages to the topic.
docker exec --interactive --tty kafka1  \
kafka-console-producer --bootstrap-server localhost:9092,kafka2:19093,kafka3:19094 \
                       --topic test-topic
Consume Messages from the topic.
docker exec --interactive --tty kafka1  \
kafka-console-consumer --bootstrap-server localhost:9092,kafka2:19093,kafka3:19094 \
                       --topic test-topic \
                       --from-beginning
Log files in Multi Kafka Cluster
Log files will be created for each partition in each of the broker instance of the Kafka cluster.
Login to the container kafka1.
docker exec -it kafka1 bash
Login to the container kafka2.
docker exec -it kafka2 bash
Shutdown the kafka cluster
docker-compose -f docker-compose-multi-broker.yml down
Setting up min.insync.replica
Topic - test-topic
docker exec --interactive --tty kafka1  \
kafka-configs  --bootstrap-server localhost:9092 --entity-type topics --entity-name test-topic \
--alter --add-config min.insync.replicas=2
Topic - library-events
docker exec --interactive --tty kafka1  \
kafka-configs  --bootstrap-server localhost:9092 --entity-type topics --entity-name library-events \
--alter --add-config min.insync.replicas=2
Advanced Kafka Commands
List the topics in a cluster
docker exec --interactive --tty kafka1  \
kafka-topics --bootstrap-server kafka1:19092 --list

Describe topic
Command to describe all the Kafka topics.
docker exec --interactive --tty kafka1  \
kafka-topics --bootstrap-server kafka1:19092 --describe
Command to describe a specific Kafka topic.
docker exec --interactive --tty kafka1  \
kafka-topics --bootstrap-server kafka1:19092 --describe \
--topic test-topic
Alter topic Partitions
docker exec --interactive --tty kafka1  \
kafka-topics --bootstrap-server kafka1:19092 \
--alter --topic test-topic --partitions 40
How to view consumer groups
docker exec --interactive --tty kafka1  \
kafka-consumer-groups --bootstrap-server kafka1:19092 --list
Consumer Groups and their Offset
docker exec --interactive --tty kafka1  \
kafka-consumer-groups --bootstrap-server kafka1:19092 \
--describe --group console-consumer-41911
Log file and related config
Log into the container.
docker exec -it kafka1 bash
The config file is present in the below path.
/etc/kafka/server.properties
The log file is present in the below path.
/var/lib/kafka/data/
How to view the commit log?
docker exec --interactive --tty kafka1  \
kafka-run-class kafka.tools.DumpLogSegments \
--deep-iteration \
--files /var/lib/kafka/data/test-topic-0/00000000000000000000.log





=================================================
#     ELK Stack                                      
=================================================

=>elasticsearch.bat
=>kibana.bat


=>GET /_nodes

List of Index
=>GET /_cat/indices?v
  
Creae a index
=>PUT employee_indx
  
Add data to Index
=>POST employee_indx/_doc/1
  {
     "name":"MD IMRAN HOSSAIN", "gender":"Male",
     "age":"30", "city":"Dhaka"
  }

PUT /student_indx
{
  "settings": {
    "number_of_shards": 1
  },
  "mappings": {
    "properties": {
      "name": { "type": "text" },
      "age": { "type": "integer" },
      "gender": { "type": "text" }
    }
  }
}

 
Show Index date
  =>GET employee_indx/_doc/1
  =>POST /employee_indx/_search
  =>POST /employee_indx/_search?filter_path=hits.hits
  

Update
=>POST /employee_indx/_doc/1/_update
{
  "doc": {
  "name": "MD IMRAN HOSSAIN UPDATE"
  }
}
  
DeleteDate
  =>DELETE  employee_indx/_doc/11




=>PUT school_indx

=>POST school_indx/student/1
  {
     "name":"MD IMRAN HOSSAIN", 
     "gender":"Male",
     "age":"30"
  }
  



=================================================
#JWT | jwt                                   
================================================= 
https://www.youtube.com/watch?v=BQwKZ6zfyk0&list=PLJq-63ZRPdBt-RFGwsJO9Pv6A8ZwYHua9

VerifyJWT:
https://dinochiesa.github.io/jwt/

Two type of encryption in jwt:
- (Symetric) Same key use for encryption and decription
-(Asymmetric) Differ key, private/public key


generate a valid key pair using OpenSSL:
=>openssl genpkey -algorithm RSA -out private_key.pem
=>openssl rsa -pubout -in private_key.pem -out public_key.pem

ExamplePython:
-------------------------------------------------
import jwt

# Generate a JWT token using an asymmetric private key
private_key = """
-----BEGIN RSA PRIVATE KEY-----
MIIEowIBAAKCAQEA1AmT5s8T1yPRX9YeCFBqA8T6r3XrI9ilfz3EiKXsReR7H1Vi
... (your private key) ...
-----END RSA PRIVATE KEY-----
"""

payload = {
    "user_id": 123,
    "username": "example_user",
    "role": "admin",
}

# Sign the JWT token with the private key
token = jwt.encode(payload, private_key, algorithm="RS256")

print("JWT Token:")
print(token)

# Now, let's assume you have the public key for verification
public_key = """
-----BEGIN PUBLIC KEY-----
MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA1AmT5s8T1yPRX9YeCFBqA
... (your public key) ...
-----END PUBLIC KEY-----
"""

try:
    # Verify the JWT token using the public key
    decoded_payload = jwt.decode(token, public_key, algorithms=["RS256"])
    print("\nDecoded Payload:")
    print(decoded_payload)
except jwt.ExpiredSignatureError:
    print("\nJWT has expired.")
except jwt.InvalidTokenError:
    print("\nJWT is invalid or tampered with.")






=================================================
#  keycloak                                          
================================================= 

https://www.tutorialsbuddy.com/keycloak-mysql-setup
https://www.appsdeveloperblog.com/keycloak-configure-mysql-database/
https://sunitkatkar.blogspot.com/2020/07/setting-up-keycloak-10-with-mysql-8-on.html


#Mysql server with keycloak
-------------------------------------------------

=>CREATE DATABASE keycloakdb CHARACTER SET utf8 COLLATE utf8_unicode_ci;
Create a database in MySQL with the utf8 character set


=>CREATE USER 'keycloakadmin'@'%' IDENTIFIED WITH mysql_native_password BY 'keycloak123';
Create a database user for Keycloak

=>GRANT ALL PRIVILEGES ON keycloakdb.* TO 'keycloakadmin'@'%';
--- Tell the server to reload the grant tables
--- by performing a flush privileges operation
=>FLUSH PRIVILEGES;

Grant all privileges


=>SHOW GLOBAL variables like "default_storage%"
Should output: default_storage_engine InnoDB

=>SET GLOBAL default_storage_engine = 'InnoDB';
If storage engine is not InnoDB, use this to set it




=================================================
# kong | Gateway                                      
=================================================

KongWithkeykclok:https://www.jerney.io/secure-apis-kong-keycloak-1/

StartKongWithDocker:
--------------------------------------------------


Create a custom Docker network to allow the containers to discover and communicate with each other:
 docker network create kong-net


Start a PostgreSQL container:
 docker run -d --name kong-database \
  --network=kong-net \
  -p 5432:5432 \
  -e "POSTGRES_USER=kong" \
  -e "POSTGRES_DB=kong" \
  -e "POSTGRES_PASSWORD=kongpass" \
  postgres:13


Prepare the Kong database:
  docker run --rm --network=kong-net \
  -e "KONG_DATABASE=postgres" \
  -e "KONG_PG_HOST=kong-database" \
  -e "KONG_PG_PASSWORD=kongpass" \
  -e "KONG_PASSWORD=test" \
  kong/kong-gateway:3.6.0.0 kong migrations bootstrap



Run the following command to start a container with Kong Gateway:

docker run -d --name kong-gateway \
 --network=kong-net \
 -e "KONG_DATABASE=postgres" \
 -e "KONG_PG_HOST=kong-database" \
 -e "KONG_PG_USER=kong" \
 -e "KONG_PG_PASSWORD=kongpass" \
 -e "KONG_PROXY_ACCESS_LOG=/dev/stdout" \
 -e "KONG_ADMIN_ACCESS_LOG=/dev/stdout" \
 -e "KONG_PROXY_ERROR_LOG=/dev/stderr" \
 -e "KONG_ADMIN_ERROR_LOG=/dev/stderr" \
 -e "KONG_ADMIN_LISTEN=0.0.0.0:8001" \
 -e "KONG_ADMIN_GUI_URL=http://localhost:8002" \
 -e KONG_LICENSE_DATA \
 -p 8000:8000 \
 -p 8443:8443 \
 -p 8001:8001 \
 -p 8444:8444 \
 -p 8002:8002 \
 -p 8445:8445 \
 -p 8003:8003 \
 -p 8004:8004 \
 kong/kong-gateway:3.6.0.0



Verify your installation:
Access the /services endpoint using the Admin API:
 curl -i -X GET --url http://localhost:8001/services



Verify that Kong Manager is running by accessing it using the URL specified in KONG_ADMIN_GUI_URL:
 http://localhost:8002


Clean up containers
docker kill kong-gateway
docker kill kong-database
docker container rm kong-gateway
docker container rm kong-database
docker network rm kong-net



Service and Route:
------------------------------------------

Creating services
curl -i -s -X POST http://localhost:8001/services \
  --data name=example_service \
  --data url='http://httpbin.org'


CheckServicd
curl -X GET http://localhost:8001/services/serviceName

Listing services
curl -X GET http://localhost:8001/services



Managing routes:



Composer:
----------------------------------------

version: '3.4'

services:
  kong-database:
    image: postgres:13
    networks:
      - kong-net
    restart: unless-stopped
    ports:
      - 127.0.0.1:5432:5432
    environment:
      POSTGRES_USER: kong
      POSTGRES_PASSWORD: kongpass
      POSTGRES_DB: kong
      PGDATA: /var/lib/postgresql/data/pgdata
    volumes:
      - db-volume:/var/lib/postgresql/data/pgdata
  kong-gateway:
    image: kong/kong-gateway:3.6.0.0
    networks:
      - kong-net
    depends_on:
      - kong-database
    restart: unless-stopped
    ports:
      - 127.0.0.1:8000:8000
      - 127.0.0.1:8001:8001
      - 127.0.0.1:8002:8002
      - 127.0.0.1:8003:8003
      - 127.0.0.1:8004:8004
      - 127.0.0.1:8443:8443
      - 127.0.0.1:8444:8444
      - 127.0.0.1:8445:8445
    environment:
      KONG_DATABASE: postgres
      KONG_PG_HOST: kong-database
      KONG_PG_USER: kong
      KONG_PG_PASSWORD: kongpass
      KONG_PROXY_ACCESS_LOG: /dev/stdout
      KONG_ADMIN_ACCESS_LOG: /dev/stdout
      KONG_PROXY_ERROR_LOG: /dev/stderr
      KONG_ADMIN_ERROR_LOG: /dev/stderr
      KONG_ADMIN_LISTEN: 0.0.0.0:8001
      KONG_ADMIN_GUI_URL: http://localhost:8002
networks:
  kong-net:
    external: false
volumes:
  db-volume:




docker-compose up -d
docker run --rm --network=kong_kong-net  -e "KONG_DATABASE=postgres"  -e "KONG_PG_HOST=kong-database"  -e "KONG_PG_PASSWORD=kongpass"  -e "KONG_PASSWORD=test" kong/kong-gateway:3.6.0.0 kong migrations bootstrap










=================================================
#XML Requst/Response | xml | 
=================================================
 XML-RPC (Remote Procedure Call) is a remote procedure call protocol that uses XML to encode its calls and HTTP as a 
 transport mechanism. 
 It's simpler in structure compared to SOAP and doesn't have the extensive envelope structure associated with SOAP.








REST vs SOAP
-----------------------------------------------
REST vs SOAP are not really comparable. REST is an architectural style. SOAP is a message exchange format.

Let’s compare the popular implementations of REST and SOAP styles.

RESTful Sample Implementation : JSON over HTTP
SOAP Sample Implementation : XML over SOAP over HTTP



================================================= 
#SOAP SERVICE    | soap service | web-service                                     
================================================= 
SopWebService:
https://www.springboottutorial.com/creating-soap-web-service-with-spring-boot-web-services-starter
https://github.com/in28minutes/spring-boot-examples/tree/master/spring-boot-tutorial-soap-web-services


https://medium.com/@extio/developing-soap-web-services-with-spring-boot-a-comprehensive-guide-1d4f89bc3127

UdamyCourse for SOAP Service:
Master Java Web Services and REST API with Spring Boot






What is SOAP Web Services ?
	In short, a web service is a machine-to-machine, platform-independent service that allows communication over a network.
	SOAP is a messaging protocol. Messages (requests and responses) are XML documents over HTTP. 
	The XML contract is defined by the WSDL (Web Services Description Language). It provides a set of rules to define the messages, 
	bindings, operations, and location of the service.

Contract-First/Contract-Last Development Style OR Top-Down vs. Bottom-Up:
	There are two possible approaches when creating a web service: Contract-Last and Contract-First. 
	When we use a contract-last approach, we start with the Java code and generate the web service contract (WSDL) from the classes. 
	When using contract-first, we start with the WSDL contract, from which we generate the Java classes.
Spring-WS only supports the contract-first development style.

	There are two ways of building SOAP web services. We can go with a top-down approach or a bottom-up approach.
	In a top-down (contract-first) approach, a WSDL document is created, and the necessary Java classes are generated from the WSDL. 
	In a bottom-up (contract-last) approach, the Java classes are written, and the WSDL is generated from the Java classes.


WSDL
WSDL is used to define the structure of Request and the Structure of Response xml.

WSDL will explain:
	What are the different services (operations) exposed by the server?
	How can a service (operation) be called? What url to use? (also called End Point).
	What should the structure of request xml?
	What should be the structure of response xml?


A SOAP service typically follows the SOAP message structure, which includes:
	<SOAP-Envelope>: The root element that encapsulates the entire SOAP message.
	<SOAP-Header> (optional): Contains header information such as authentication credentials or message routing details.
	<SOAP-Body>: Contains the actual payload or data being sent.
	<SOAP-Fault> (optional): Used to convey error information if a fault occurs during processing.






Create Soap Service:
-------------------------------------
Setp1: Define Request Response xml 
setp2: Create .xsd file for validateion Request/response xml file
Step3: For generate Java Pojo class for req/response from as .xsd file add JAXB library in pom file



Simple Soap Request:
--------------------------------------

<soapenv:Envelope xmlns:soapenv="http://schemas.xmlsoap.org/soap/envelope/" 
xmlns:cal="http://imranmadbar.com/types/calculator">
	<soapenv:Header/>
	<soapenv:Body>
		<cal:SubtractionInput>
			<cal:number1>10</cal:number1>
			<cal:number2>4</cal:number2>
		</cal:SubtractionInput>
	</soapenv:Body>
</soapenv:Envelope>

<soapenv:Envelope xmlns:soapenv="http://schemas.xmlsoap.org/soap/envelope/" xmlns:inh="http://com.imranmadbr/">
   <soapenv:Header/>
   <soapenv:Body>
      <inh:opManager>
         <!--Optional:-->
         <version>2.0</version>
         <service>xxx</service>
         <method>xxx</method>
         <param>xxxxxx</param>
         <numberofparam>1</numberofparam>
         <userid>xxxxxxx</userid>
         <password>xxxxx</password>
         <failclause>xxxx</failclause>
      </inh:opManager>
   </soapenv:Body>
</soapenv:Envelope>



<soapenv:Envelope xmlns:soapenv="http://schemas.xmlsoap.org/soap/envelope/"
                  xmlns:us="http://imranmadbar.com/spring-boot-soap-service">
    <soapenv:Header/>
    <soapenv:Body>
        <us:getUserRequest>
            <us:name>imran</us:name>
        </us:getUserRequest>
    </soapenv:Body>
</soapenv:Envelope>
		
Simple Soap Response:
-------------------------------------

<SOAP-ENV:Envelope xmlns:SOAP-ENV="http://schemas.xmlsoap.org/soap/envelope/">
  <SOAP-ENV:Header/>
  <SOAP-ENV:Body>
	  <ns2:output xmlns:ns2="http://imranmadbar.com/types/calculator">
		  <ns2:result>6</ns2:result>
	  </ns2:output>
  </SOAP-ENV:Body>
</SOAP-ENV:Envelope>










