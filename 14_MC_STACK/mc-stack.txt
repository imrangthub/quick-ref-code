#################################################
#         MICROSERVICE-TECHSTACK                #
#################################################


=================================================
#                  Basic                                          
================================================= 






=================================================
#Spring-caching                                   
================================================= 
 In Spring, caching can be easily implemented using in-memory caching providers:
Generic
JCache (JSR-107) (EhCache 3, Hazelcast, Infinispan, and others)
EhCache 2.x
Hazelcast
Infinispan
Couchbase
Redis
Caffeine
Simple or the default ConcurrentMapCacheManager.






=================================================
#redis | Redis                                     
================================================= 

=>docker run -p 8888:6379 --name my-redis1 -d redis
=>sudo apt install redis-tools
=>redis-cli --version
Run a Redis Server using docker and install resis-toos for redis-cli in ubuntu

=>sudo systemctl status redis
=>sudo vi /etc/redis/redis.conf
Edit Config file
=>sudo systemctl restart redis-server


=>redis-cli ping
=>redis-cli
=>redis-cli -p 8888



=>set mykey myvalue
=>get mykey

=>keys *
=>dbsize
Show list of key

=>del keyName
Delete single key

=>flushall
Remove all data





#Redis Sentinel 
=================================================
=>redis-cli
=>redis-cli -p 8888

=>redis-cli -h my_host -p my_port -a my_password
=>redis-cli -h 172.16.7.161 -p 26379 -a pass


=>sentinel masters  
=>sentinel get-master-addr-by-name mymaster
Info about all master

=>SENTINEL REPLICAS mymaster
Show connected replicas



=>info replication
Show Master and Slave info


sentinel master mymaster
sentinel slaves mymaster






Doc
-------------------------
https://docs.spring.io/spring-data/redis/reference/redis/connection-modes.html
https://medium.com/@htyesilyurt/spring-boot-3-redis-sentinel-lettuce-client-and-docker-compose-for-high-availability-1f1e3c372a5a
https://www.youtube.com/watch?v=Pj8Q_9dovgY&list=PLq3uEqRnr_2HY6LMQsbvsK4btj51sWhBS&index=2


Spring Boot, both Lettuce and Jedis are popular choices for interacting with Redis, which is an open-source, in-memory data structure store. 

Lettuce:
  Asynchronous Support
  Thread Safety
  Connection Pooling
  Cluster Support

Jedis:
  Synchronous and Asynchronous Support:
  Connection Pooling:
  Maturity:

Choosing Between Lettuce and Jedis:
Performance:
Lettuce is often considered more performant than Jedis, especially in scenarios where high concurrency and asynchronous behavior are important.

Reactive Programming:
If your application is built using reactive programming paradigms (e.g., Spring WebFlux), Lettuce may be a more natural fit.
Ease of Use:

Jedis has a simpler API, and if you are looking for a straightforward and easy-to-use client, Jedis might be a good choice.






Caching Topology:
-Standalone         
 Application inMemory cashing

-Distributed
 Central case system

-Replication
 Chahe replicas every node

UdamyCourse:
https://gale.udemy.com/course/redis-latest/learn/lecture/21936116#overview


Example:
Assuming you have a Redis configuration as follows:

Master (Writer): 192.168.1.1:8080
Replicas (Readers):
192.168.3.1:8082
192.168.2.1:8082
Here's a configuration for a Spring Cloud microservice with 10 application instances:

java
Copy code
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.data.redis.connection.RedisStandaloneConfiguration;
import org.springframework.data.redis.connection.RedisStaticMasterReplicaConfiguration;
import org.springframework.data.redis.connection.lettuce.LettuceClientConfiguration;
import org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory;

@Configuration
public class RedisConfig {
    
    @Bean
    public LettuceClientConfiguration lettuceClientConfiguration() {
        return LettuceClientConfiguration.builder()
            .readFrom(ReadFrom.REPLICA_PREFERRED)
            .build();
    }
    
    @Bean
    public LettuceConnectionFactory lettuceConnectionFactory(LettuceClientConfiguration clientConfig) {
        RedisStandaloneConfiguration serverConfig = new RedisStandaloneConfiguration("192.168.1.1", 8080);
        RedisStaticMasterReplicaConfiguration staticMasterReplicaConfig = new RedisStaticMasterReplicaConfiguration(
            serverConfig,
            Arrays.asList(
                new RedisNode("192.168.3.1", 8082),
                new RedisNode("192.168.2.1", 8082)
            )
        );
        
        return new LettuceConnectionFactory(staticMasterReplicaConfig, clientConfig);
    }
}



=================================================
#Event-driven Architecture
=================================================

Event-driven Architecture alternate of loss cupling or temp-cuplinc(Synchronous communitation)

Two even-driven model/technology:
  1) Pub/sub model
  2) Event Streming model

1)Pub/sub model: Once an event is dreceived, its cannot be replayed, which means new 
  subscribers join later will not have acccess paset event.

2)Event Streming model: this model events are writeen to a log in a sequential manner.
Producer publish events as they occur, and these events are stored in a well-ordered fashion
consumer have the ability to read from any part of the event stream.
One advantage of this model is thet events cna be replayed, allowing clients to join at 
any time and received all past events.





=================================================
#    Message broker  | RabbitMQ                                        
================================================= 
The AMQP operates on the principales of exchanges and queues for all queue operation.

RabitMQ design overview (4 actors):                              
  publisher(producer) --> exchange ----(routes)--> queue --> subscriber(consumer)


Installing and Run RabbitMQ: 
-------------------------------------------------
Docker:
docker run -it --rm --name rabbitmq -p 5672:5672 -p 15672:15672 rabbitmq:3.12-management

OR
=>rabbitmq-server start
Start rabbitMQ Server, Open RabbitMQ command prompt

=>localhost:15672
guest/guest



Example: Geting Start with Simple Queue
------------------------------------------------
Creating a Quequ:
Queues and Streams->add a new queque->add Queue

Publish to Quequ:
Click to queue->	my-test-queue1 -> Puhlish Message
Now it will show Messsage(Ready)

Received Msg from Quequ:
Click Get Message from Quew





SimpleApp:
http://localhost:8081/rabbitmq/sent-emp?empName=emp1&empId=emp001

              
FanoutExchange:
http://localhost:8083/rabbitmq/fanout/producer?exchangeName=fanout-exchange&messageData=HelloWorldMsg
http://localhost:8084/rabbitmq/producer?empName=emp1&empId=emp001&salary=500000

DirectExchange:
http://localhost:8083/rabbitmq/direct/producer?exchangeName=direct-exchange&routingKey=finance&messageData=HelloWorldMSG






Tach Info
-------------------------------------------------


Features of RabbitMQ:
Connectivity Options:
  RabbitMQ supports multiple messaging protocols and can be deployed in distributed/federated configurations to meet high availability, scalability requirements.
Pluggable Architecture:
  RabbitMQ allows to choose a persistence mechanism and also provides options to customize security for authentication and authorization as per the application needs.


MQ-Types of Messaging

Point to Point
  In this type of communication, the broker sends messages to only one consumer, while the other consumers will wait till they get 
the messages from the broker. No consumer will get the same message.

If there are no consumers, the Broker will hold the messages till it gets a consumer (Queue based communication ). 
If there is more than one consumer, they may get the next message but they wonâ€™t get the same message as the other consumer.


Publish/Subscribe
  In this type of communication, the Broker sends same copy of messages to all the active consumers. 
  This type of communication is also known as Topic based communication where broker sends same message to all active 
  consumer who has subscribed for particular Topic. 


RabbitMQ supports several exchange types:
  Direct Exchange: Routes messages based on a specified routing key.
  Fanout Exchange: Broadcasts messages to all queues bound to it.
  Topic Exchange: Allows flexible routing with wildcard patterns in routing keys.
  Headers Exchange: Routes based on message header attributes.
  Default Exchange (Direct): Uses queue names as routing keys.


RabbitMQ queue properties include:
  Name: Unique identifier.
  Durable: Survive broker restart.
  Auto-Delete: Delete when no consumers.
  Exclusive: Used by the creating connection.
  Arguments: Additional settings (e.g., TTL, max length).
  Message TTL: Maximum time a message can stay.
  Maximum Priority: Sets message priority.
  Maximum Length: Max number of messages.
  Overflow Behavior: What happens when max length is reached.
  Dead Letter Exchange/Key: For handling failed or expired messages.



RabbitMQ Exchange type:
------------------------------------------------
Direct Exchange (One-to-One Relationship):
  Like sending a letter to a specific person based on their address. Each message is routed to a single queue based on an exact match between the message's routing key and the queue's binding key.

Fanout Exchange (One-to-Many Relationship):
  Like broadcasting a message on TV. Each message sent to a fanout exchange is delivered to all queues bound to that exchange, regardless of any routing keys or message attributes.

Topic Exchange (One-to-Many Relationship with Conditions):
  Like sending emails to different groups based on their interests or topics. Messages are routed to queues based on wildcard matches between routing keys and routing patterns specified by the queues. One message may be delivered to multiple queues if they match the routing pattern.

Headers Exchange (One-to-Many Relationship based on Message Attributes):
  Like sorting mail based on various attributes or metadata attached to each letter. Messages are routed to queues based on headers and their values instead of routing keys. One message may be delivered to multiple queues if their header values match the specified criteria.



=================================================
#kafka | Message broker  | KAFKA                                        
================================================= 
kafkalytic-intelejidea-pluging

Install and Run KafKa:

$ tar -xzf kafka_2.13-3.6.1.tgz
$ cd kafka_2.13-3.6.1


Kafka with ZooKeeper
# Start the ZooKeeper service
$ bin/zookeeper-server-start.sh config/zookeeper.properties
# Start the Kafka broker service
$ bin/kafka-server-start.sh config/server.properties


$ bin/kafka-topics.sh --create --topic quickstart-events --bootstrap-server localhost:9092
CREATE A TOPIC TO STORE YOUR EVENTS

=>cd /opt/apache-kafka/kafka3_6/bin



SetUp Kafka SingleCluster
-------------------------------------------------
Start up the Zookeeper.
=>./zookeeper-server-start.sh ../config/zookeeper.properties

Add the below properties in the server.properties
  listeners=PLAINTEXT://localhost:9092
  auto.create.topics.enable=false


Start up the Kafka Broker
=>./kafka-server-start.sh ../config/server.properties


Create a topic
=>./kafka-topics.sh --create --topic test-topic --bootstrap-server localhost:9092 --partitions 4 --replication-factor 1
=>./kafka-topics.sh --list  --bootstrap-server localhost:9092


=>./kafka-topics.sh --bootstrap-server localhost:9092 --describe
=>./kafka-topics.sh --bootstrap-server localhost:9092 --describe --topic test-topic



Create Console Producer
=>./kafka-console-producer.sh --broker-list localhost:9092 --topic test-topic
=>./kafka-console-producer.sh --broker-list localhost:9092 --topic test-topic --property "key.separator=-" --property "parse.key=true"

Create Console Consumer
=>./kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test-topic --from-beginning
=>./kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test-topic --from-beginning -property "key.separator= - " --property "print.key=true"




Consumer Groups:
=>./kafka-consumer-groups.sh --bootstrap-server localhost:9092 --list



Docker:
---------------------------------------------------------
gitClone: https://github.com/conduktor/kafka-stack-docker-compose
=>docker-compose -f zk-single-kafka-single.yml up -d
=>docker-compose -f zk-single-kafka-single.yml ps
=>docker-compose -f zk-single-kafka-single.yml stop


zk-single-kafka-single.yml:

version: '2.1'
services:
  zoo1:
    image: confluentinc/cp-zookeeper:7.3.2
    hostname: zoo1
    container_name: zoo1
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_SERVER_ID: 1
      ZOOKEEPER_SERVERS: zoo1:2888:3888

  kafka1:
    image: confluentinc/cp-kafka:7.3.2
    hostname: kafka1
    container_name: kafka1
    ports:
      - "9092:9092"
      - "29092:29092"
      - "9999:9999"
    environment:
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka1:19092,EXTERNAL://${DOCKER_HOST_IP:-127.0.0.1}:9092,DOCKER://host.docker.internal:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT,DOCKER:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_ZOOKEEPER_CONNECT: "zoo1:2181"
      KAFKA_BROKER_ID: 1
      KAFKA_LOG4J_LOGGERS: "kafka.controller=INFO,kafka.producer.async.DefaultEventHandler=INFO,state.change.logger=INFO"
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_JMX_PORT: 9999
      KAFKA_JMX_HOSTNAME: ${DOCKER_HOST_IP:-127.0.0.1}
      KAFKA_AUTHORIZER_CLASS_NAME: kafka.security.authorizer.AclAuthorizer
      KAFKA_ALLOW_EVERYONE_IF_NO_ACL_FOUND: "true"
    depends_on:
      - zoo1






=>docker exec -it kafka1 /bin/bash
=>kafka-topics --version


=>kafka-topics --create --topic test-topic --bootstrap-server localhost:9092 --partitions 4 --replication-factor 1
=>kafka-topics --list --bootstrap-server localhost:9092

=>kafka-topics --bootstrap-server localhost:9092 --describe


=>kafka-console-producer --broker-list localhost:9092 --topic test-topic
Console Producer

=>kafka-console-consumer --bootstrap-server localhost:9092 --topic test-topic --from-beginning
console consumer


=>kafka-consumer-groups --bootstrap-server localhost:9092 --list
consumer group


=>kafka-topics --bootstrap-server localhost:9092 --delete --topic '__consumer_offsets-.*'














Kafka Info Doc:
---------------------------------------------------------

Kafka 4mainComponent:
  Broker: Stores and manages data.Its a java process, can incress horizontally.
  ZooKeeper: Manages Kafka cluster's configuration and coordination.
  Producer: Sends data to Kafka. Communicate with broker with tcp. Producer create record with partiation number attributes.
  Consumer: Receives data from Kafka


Kafka-Records:
  exist within Kafka topics. When data is produced to Kafka, it is encapsulated into records and appended to the 
  log (partition) of the respective topic. Each record consists of a key, a value, headers, timestamp, offset,
   and possibly other metadata. 


Record comprises several components:

  Key: An optional identifier for the record.
  Value: The main data payload.
  Headers: Optional metadata associated with the record.
  Timestamp: Indicates when the record was produced or ingested into Kafka.
  Offset: A unique identifier assigned to each record within a partition.
  Partition: Specifies the partition to which the record belongs.


To Unique identify a reocrd use (partition+Offset) composit key number. 




Replication Factor: Replication factor factor make partation duplicate.
  If replication-factor=2 then every partation has two copy if three then the number of copy is 3.














Zookeeper:
 Manage the kafka broker, broker registred with zookeeper.

Topic/Partation/Offset:
  Kafka tpoic as database table.
  Each topic will create with one or more partiation.

  Partitation is where message linve inside the topic.
  Each partition is independent of each other.

  Each Partition is an Order, inmmutable sequence of record.
  Each record is assigned a sequential number called offset.
  Order is guaranteed only at the partition leve.
All the records are persisted in acommit log in the files system where is kafka intalled.


Message of Kafka retain inside in cluster after consumed, depending on retation prood.
Kafka Message are byte insede kafka broker, then producer/consumer serisized this.


For locket a specific message, required 3 thing:
- Topic Name
- Partition number
- Offset Number 


Consumer have three option to reload
- from-biginning
- latest
- specific offset





SpringKafka:
https://gale.udemy.com/course/apache-kafka-for-developers-using-springboot/learn/lecture/37906902#overview





udTut-repo:https://github.com/dilipsundarraj1/kafka-for-developers-using-spring-boot-v2/blob/main/SetUpKafkaDocker.md#set-up-broker-and-zookeeper

Producer and Consume the Messages
Let's going to the container by running the below command.
docker exec -it kafka1 bash
Create a Kafka topic using the kafka-topics command.
kafka1:19092 refers to the KAFKA_ADVERTISED_LISTENERS in the docker-compose.yml file.
kafka-topics --bootstrap-server kafka1:19092 \
             --create \
             --topic test-topic \
             --replication-factor 1 --partitions 1
Produce Messages to the topic.
docker exec --interactive --tty kafka1  \
kafka-console-producer --bootstrap-server kafka1:19092 \
                       --topic test-topic
Consume Messages from the topic.
docker exec --interactive --tty kafka1  \
kafka-console-consumer --bootstrap-server kafka1:19092 \
                       --topic test-topic \
                       --from-beginning
Producer and Consume the Messages With Key and Value
Produce Messages with Key and Value to the topic.
docker exec --interactive --tty kafka1  \
kafka-console-producer --bootstrap-server kafka1:19092 \
                       --topic test-topic \
                       --property "key.separator=-" --property "parse.key=true"
Consuming messages with Key and Value from a topic.
docker exec --interactive --tty kafka1  \
kafka-console-consumer --bootstrap-server kafka1:19092 \
                       --topic test-topic \
                       --from-beginning \
                       --property "key.separator= - " --property "print.key=true"
Consume Messages using Consumer Groups
docker exec --interactive --tty kafka1  \
kafka-console-consumer --bootstrap-server kafka1:19092 \
                       --topic test-topic --group console-consumer-41911\
                       --property "key.separator= - " --property "print.key=true"
Example Messages:
a-abc
b-bus
Consume Messages With Headers
docker exec --interactive --tty kafka1  \
kafka-console-consumer --bootstrap-server kafka1:19092 \
                       --topic library-events.DLT \
                       --property "print.headers=true" --property "print.timestamp=true" 
Example Messages:
a-abc
b-bus
Set up a Kafka Cluster with 3 brokers
Run this command and this will spin up a kafka cluster with 3 brokers.
docker-compose -f docker-compose-multi-broker.yml up
Create topic with the replication factor as 3
docker exec --interactive --tty kafka1  \
kafka-topics --bootstrap-server kafka1:19092 \
             --create \
             --topic test-topic \
             --replication-factor 3 --partitions 3
Produce Messages to the topic.
docker exec --interactive --tty kafka1  \
kafka-console-producer --bootstrap-server localhost:9092,kafka2:19093,kafka3:19094 \
                       --topic test-topic
Consume Messages from the topic.
docker exec --interactive --tty kafka1  \
kafka-console-consumer --bootstrap-server localhost:9092,kafka2:19093,kafka3:19094 \
                       --topic test-topic \
                       --from-beginning
Log files in Multi Kafka Cluster
Log files will be created for each partition in each of the broker instance of the Kafka cluster.
Login to the container kafka1.
docker exec -it kafka1 bash
Login to the container kafka2.
docker exec -it kafka2 bash
Shutdown the kafka cluster
docker-compose -f docker-compose-multi-broker.yml down
Setting up min.insync.replica
Topic - test-topic
docker exec --interactive --tty kafka1  \
kafka-configs  --bootstrap-server localhost:9092 --entity-type topics --entity-name test-topic \
--alter --add-config min.insync.replicas=2
Topic - library-events
docker exec --interactive --tty kafka1  \
kafka-configs  --bootstrap-server localhost:9092 --entity-type topics --entity-name library-events \
--alter --add-config min.insync.replicas=2
Advanced Kafka Commands
List the topics in a cluster
docker exec --interactive --tty kafka1  \
kafka-topics --bootstrap-server kafka1:19092 --list

Describe topic
Command to describe all the Kafka topics.
docker exec --interactive --tty kafka1  \
kafka-topics --bootstrap-server kafka1:19092 --describe
Command to describe a specific Kafka topic.
docker exec --interactive --tty kafka1  \
kafka-topics --bootstrap-server kafka1:19092 --describe \
--topic test-topic
Alter topic Partitions
docker exec --interactive --tty kafka1  \
kafka-topics --bootstrap-server kafka1:19092 \
--alter --topic test-topic --partitions 40
How to view consumer groups
docker exec --interactive --tty kafka1  \
kafka-consumer-groups --bootstrap-server kafka1:19092 --list
Consumer Groups and their Offset
docker exec --interactive --tty kafka1  \
kafka-consumer-groups --bootstrap-server kafka1:19092 \
--describe --group console-consumer-41911
Log file and related config
Log into the container.
docker exec -it kafka1 bash
The config file is present in the below path.
/etc/kafka/server.properties
The log file is present in the below path.
/var/lib/kafka/data/
How to view the commit log?
docker exec --interactive --tty kafka1  \
kafka-run-class kafka.tools.DumpLogSegments \
--deep-iteration \
--files /var/lib/kafka/data/test-topic-0/00000000000000000000.log





=================================================
#     ELK Stack                                      
=================================================

=>elasticsearch.bat
=>kibana.bat


=>GET /_nodes

List of Index
=>GET /_cat/indices?v
  
Creae a index
=>PUT employee_indx
  
Add data to Index
=>POST employee_indx/_doc/1
  {
     "name":"MD IMRAN HOSSAIN", "gender":"Male",
     "age":"30", "city":"Dhaka"
  }

PUT /student_indx
{
  "settings": {
    "number_of_shards": 1
  },
  "mappings": {
    "properties": {
      "name": { "type": "text" },
      "age": { "type": "integer" },
      "gender": { "type": "text" }
    }
  }
}

 
Show Index date
  =>GET employee_indx/_doc/1
  =>POST /employee_indx/_search
  =>POST /employee_indx/_search?filter_path=hits.hits
  

Update
=>POST /employee_indx/_doc/1/_update
{
  "doc": {
  "name": "MD IMRAN HOSSAIN UPDATE"
  }
}
  
DeleteDate
  =>DELETE  employee_indx/_doc/11




=>PUT school_indx

=>POST school_indx/student/1
  {
     "name":"MD IMRAN HOSSAIN", 
     "gender":"Male",
     "age":"30"
  }
  



=================================================
#JWT | jwt                                   
================================================= 
https://www.youtube.com/watch?v=BQwKZ6zfyk0&list=PLJq-63ZRPdBt-RFGwsJO9Pv6A8ZwYHua9

VerifyJWT:
https://dinochiesa.github.io/jwt/

Two type of encryption in jwt:
- (Symetric) Same key use for encryption and decription
-(Asymmetric) Differ key, private/public key


generate a valid key pair using OpenSSL:
=>openssl genpkey -algorithm RSA -out private_key.pem
=>openssl rsa -pubout -in private_key.pem -out public_key.pem

ExamplePython:
-------------------------------------------------
import jwt

# Generate a JWT token using an asymmetric private key
private_key = """
-----BEGIN RSA PRIVATE KEY-----
MIIEowIBAAKCAQEA1AmT5s8T1yPRX9YeCFBqA8T6r3XrI9ilfz3EiKXsReR7H1Vi
... (your private key) ...
-----END RSA PRIVATE KEY-----
"""

payload = {
    "user_id": 123,
    "username": "example_user",
    "role": "admin",
}

# Sign the JWT token with the private key
token = jwt.encode(payload, private_key, algorithm="RS256")

print("JWT Token:")
print(token)

# Now, let's assume you have the public key for verification
public_key = """
-----BEGIN PUBLIC KEY-----
MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA1AmT5s8T1yPRX9YeCFBqA
... (your public key) ...
-----END PUBLIC KEY-----
"""

try:
    # Verify the JWT token using the public key
    decoded_payload = jwt.decode(token, public_key, algorithms=["RS256"])
    print("\nDecoded Payload:")
    print(decoded_payload)
except jwt.ExpiredSignatureError:
    print("\nJWT has expired.")
except jwt.InvalidTokenError:
    print("\nJWT is invalid or tampered with.")






=================================================
#  keycloak                                          
================================================= 

https://www.tutorialsbuddy.com/keycloak-mysql-setup
https://www.appsdeveloperblog.com/keycloak-configure-mysql-database/
https://sunitkatkar.blogspot.com/2020/07/setting-up-keycloak-10-with-mysql-8-on.html


#Mysql server with keycloak
-------------------------------------------------

=>CREATE DATABASE keycloakdb CHARACTER SET utf8 COLLATE utf8_unicode_ci;
Create a database in MySQL with the utf8 character set


=>CREATE USER 'keycloakadmin'@'%' IDENTIFIED WITH mysql_native_password BY 'keycloak123';
Create a database user for Keycloak

=>GRANT ALL PRIVILEGES ON keycloakdb.* TO 'keycloakadmin'@'%';
--- Tell the server to reload the grant tables
--- by performing a flush privileges operation
=>FLUSH PRIVILEGES;

Grant all privileges


=>SHOW GLOBAL variables like "default_storage%"
Should output: default_storage_engine InnoDB

=>SET GLOBAL default_storage_engine = 'InnoDB';
If storage engine is not InnoDB, use this to set it



=================================================
#  SPRING SOAP SERVICE                                          
================================================= 

http://localhost:8080/user-soap-service/user.wsdl
http://localhost:8080/imranmadbar/ws/calculatorDemo.wsdl



Simple Soap Request:
--------------------------------------

<soapenv:Envelope xmlns:soapenv="http://schemas.xmlsoap.org/soap/envelope/" 
xmlns:cal="http://imranmadbar.com/types/calculator">
	<soapenv:Header/>
	<soapenv:Body>
		<cal:SubtractionInput>
			<cal:number1>10</cal:number1>
			<cal:number2>4</cal:number2>
		</cal:SubtractionInput>
	</soapenv:Body>
</soapenv:Envelope>

<soapenv:Envelope xmlns:soapenv="http://schemas.xmlsoap.org/soap/envelope/" xmlns:inh="http://com.imranmadbr/">
   <soapenv:Header/>
   <soapenv:Body>
      <inh:opManager>
         <!--Optional:-->
         <version>2.0</version>
         <service>xxx</service>
         <method>xxx</method>
         <param>xxxxxx</param>
         <numberofparam>1</numberofparam>
         <userid>xxxxxxx</userid>
         <password>xxxxx</password>
         <failclause>xxxx</failclause>
      </inh:opManager>
   </soapenv:Body>
</soapenv:Envelope>



<soapenv:Envelope xmlns:soapenv="http://schemas.xmlsoap.org/soap/envelope/"
                  xmlns:us="http://imranmadbar.com/spring-boot-soap-service">
    <soapenv:Header/>
    <soapenv:Body>
        <us:getUserRequest>
            <us:name>imran</us:name>
        </us:getUserRequest>
    </soapenv:Body>
</soapenv:Envelope>
		
Simple Soap Response:
-------------------------------------

<SOAP-ENV:Envelope xmlns:SOAP-ENV="http://schemas.xmlsoap.org/soap/envelope/">
  <SOAP-ENV:Header/>
  <SOAP-ENV:Body>
	  <ns2:output xmlns:ns2="http://imranmadbar.com/types/calculator">
		  <ns2:result>6</ns2:result>
	  </ns2:output>
  </SOAP-ENV:Body>
</SOAP-ENV:Envelope>