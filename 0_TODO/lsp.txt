#################################################
#                 L-SP                          #
#################################################
https://kodekloud.com/lessons/challenge-1-3/
https://kodekloud.com/lessons/certified-kubernetes-application-developer-mock-exam-series/


alias k='kubectl'
alias cc='clear'
alias kk='kubectl get'
alias ke='kubectl edit'
alias kkk='kubectl delete'
alias kc='kubectl create'
alias kd='kubectl describe'


Ing:
===========================
Deployment multiple label issue for service.
2)Waht is emptyDir() in volues

https://www.youtube.com/watch?v=ZwC2NWWUDL0

ps -ef | grep kube-apiserver | grep admission-plugins
kubectl exec -it kube-apiserver-controlplane -n kube-system -- kube-apiserver -h | grep 'enable-admission-plugins'

privileged mode
LimitRange

KCH:Skip-Q

18
info_outline
Question
SECTION: SERVICES AND NETWORKING


For this question, please set the context to cluster3 by running:


kubectl config use-context cluster3


We have deployed several applications in the ns-ckad17-svcn namespace that are exposed inside the cluster via ClusterIP.


Your task is to create a LoadBalancer type service that will serve traffic to the applications based on its labels. Create the resources as follows:
Service lb1-ckad17-svcn for serving traffic at port 31890 to pods with labels "exam=ckad, criteria=location".

Service lb2-ckad17-svcn for serving traffic at port 31891 to pods with labels "exam=ckad, criteria=cpu-high".
info_outline
Solution
To create the loadbalancer for the pods with the specified lables, first we need to find the pods with the mentioned lables.
To get pods with labels "exam=ckad, criteria=location"
kubectl -n ns-ckad17-svcn get pod -l exam=ckad,criteria=location
-----
NAME               READY   STATUS    RESTARTS   AGE
geo-location-app   1/1     Running   0          10m
Similarly to get pods with labels "exam=ckad,criteria=cpu-high".
kubectl -n ns-ckad17-svcn get pod -l exam=ckad,criteria=cpu-high
-----
NAME           READY   STATUS    RESTARTS   AGE
cpu-load-app   1/1     Running   0          11m
Now we know which pods use the labels, we can create the LoadBalancer type service using the imperative command.

kubectl -n ns-ckad17-svcn expose pod geo-location-app --type=LoadBalancer --name=lb1-ckad17-svcn
Similarly, create the another service.

kubectl -n ns-ckad17-svcn expose pod cpu-load-app --type=LoadBalancer --name=lb2-ckad17-svcn

Once the services are created, you can edit the services to use the correct nodePorts as per the question using kubectl -n ns-ckad17-svcn edit svc lb2-ckad17-svcn.



16
info_outline
Question
SECTION: APPLICATION ENVIRONMENT, CONFIGURATION and SECURITY


For this question, please set the context to cluster3 by running:


kubectl config use-context cluster3


Define a Kubernetes custom resource definition (CRD) for a new resource kind called Foo (plural form - foos) in the samplecontroller.k8s.io group.

This CRD should have a version of v1alpha1 with a schema that includes two properties as given below:


deploymentName (a string type) and replicas (an integer type with minimum value of 1 and maximum value of 5).



It should also include a status subresource which enables retrieving and updating the status of Foo object, including the availableReplicas property, which is an integer type.
The Foo resource should be namespace scoped.


Note: We have provided a template /root/foo-crd-aecs.yaml for your ease. There are few issues with it so please make sure to incorporate the above requirements before deploying on cluster.

info_outline
Solution
student-node ~ ➜  kubectl config use-context cluster3
Switched to context "cluster3".

student-node ~ ➜  vim foo-crd-aecs.yaml

student-node ~ ➜  cat foo-crd-aecs.yaml 
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: foos.samplecontroller.k8s.io
  annotations:
    "api-approved.kubernetes.io": "unapproved, experimental-only"
spec:
  group: samplecontroller.k8s.io
  scope: Namespaced
  names:
    kind: Foo
    plural: foos
  versions:
    - name: v1alpha1
      served: true
      storage: true
      schema:
        # schema used for validation
        openAPIV3Schema:
          type: object
          properties:
            spec:
              # Spec for schema goes here !
              type: object
              properties:
                deploymentName:
                  type: string
                replicas:
                  type: integer
                  minimum: 1
                  maximum: 5
            status:
              type: object
              properties:
                availableReplicas:
                  type: integer
      # subresources for the custom resource
      subresources:
        # enables the status subresource
        status: {}

student-node ~ ➜  kubectl apply -f foo-crd-aecs.yaml
customresourcedefinition.apiextensions.



14
info_outline
Question
SECTION: APPLICATION ENVIRONMENT, CONFIGURATION and SECURITY


For this question, please set the context to cluster1 by running:


kubectl config use-context cluster1


Create a Kubernetes Pod named ckad16-memory-aecs, with a container named ckad16-memory-ctr-aecs running the polinux/stress image, and configure it to use the following specifications:


Command: stress
Arguments: ["--vm", "1", "--vm-bytes", "15M", "--vm-hang", "1"]
Requested memory: 10Mi
Memory limit: 20Mi
info_outline
Solution
student-node ~ ➜  kubectl config use-context cluster1
Switched to context "cluster1".

student-node ~ ➜  cat << EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: ckad16-memory-aecs
spec:
  containers:
  - name: ckad16-memory-ctr-aecs
    image: polinux/stress
    resources:
      requests:
        memory: "10Mi"
      limits:
        memory: "20Mi"
    command: ["stress"]
    args: ["--vm", "1", "--vm-bytes", "15M", "--vm-hang", "1"]
EOF

pod/ckad16-memory-aecs created



11
info_outline
Question
SECTION: APPLICATION OBSERVABILITY AND MAINTENANCE

For this question, please set the context to cluster2 by running:


kubectl config use-context cluster2



A manifest file located at root/ckad-flash89.yaml on cluster2-controlplane. Which can be used to create a multi-containers pod. There are issues with the manifest file, preventing resource creation. Identify the errors, fix them and create resource.


You can access controlplane by ssh cluster2-controlplane if required.

info_outline
Solution
Set context to cluster2 and SSH to cluster2-controlplane.
use the kubectl create command we will see following error.

kubectl create -f ckad-flash89.yaml
error: resource mapping not found for name: "ckad-flash-89" namespace: "" from "ckad-flash89.yaml": no matches for kind "Pod" in version "V1"
ensure CRDs are installed first
about error shows us there is something wrong with apiVersion. So change it v1 and try again. and check status.

kubectl get pods
NAME            READY   STATUS             RESTARTS      AGE
ckad-flash89-aom   1/2     CrashLoopBackOff   3 (39s ago)   93s
Now check for reason using

kubectl describe pod ckad-flash89-aom
we will see that there is problem with nginx container

open yaml file and check in spec -> nginx container you can see error with mountPath --> mountPath: "/var/log" change it to mountPath: /var/log/nginx and apply changes.







7
info_outline
Question
SECTION: APPLICATION DEPLOYMENT


For this question, please set the context to cluster3 by running:


kubectl config use-context cluster3


In this task, we have to create two identical environments that are running different versions of the application. The team decided to use the Blue/green deployment method to deploy a total of 10 application pods which can mitigate common risks such as downtime and rollback capability.

Also, we have to route traffic in such a way that 30% of the traffic is sent to the green-apd environment and the rest is sent to the blue-apd environment. All the development processes will happen on cluster 3 because it has enough resources for scalability and utility consumption.


Specification details for creating a blue-apd deployment are listed below: -

The name of the deployment is blue-apd.
Use the label type-one: blue.
Use the image kodekloud/webapp-color:v1.
Add labels to the pod type-one: blue and version: v1.

Specification details for creating a green-apd deployment are listed below: -

The name of the deployment is green-apd.
Use the label type-two: green.
Use the image kodekloud/webapp-color:v2.
Add labels to the pod type-two: green and version: v1.

We have to create a service called route-apd-svc for these deployments. Details are here: -

The name of the service is route-apd-svc.
Use the correct service type to access the application from outside the cluster and application should listen on port 8080.
Use the selector label version: v1.

NOTE: - We do not need to increase replicas for the deployments, and all the resources should be created in the default namespace.




You can check the status of the application from the terminal by running the curl command with the following syntax:

curl http://cluster3-controlplane:NODE-PORT



You can SSH into the cluster3 using ssh cluster3-controlplane command.

info_outline
Solution
Run the following command to change the context: -

kubectl config use-context cluster3


In this task, we will use the kubectl command. Here are the steps: -



Use the kubectl create command to create a deployment manifest file as follows: -

kubectl create deployment blue-apd --image=kodekloud/webapp-color:v1 --dry-run=client -o yaml > <FILE-NAME-1>.yaml


Do the same for the other deployment and service.


kubectl create deployment green-apd --image=kodekloud/webapp-color:v2 --dry-run=client -o yaml > <FILE-NAME-2>.yaml



kubectl create service nodeport route-apd-svc --tcp=8080:8080 --dry-run=client -oyaml > <FILE-NAME-3>.yaml


Open the file with any text editor such as vi or nano and make the changes as per given in the specifications. It should look like this: -

---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    type-one: blue
  name: blue-apd
spec:
  replicas: 7
  selector:
    matchLabels:
      type-one: blue
      version: v1
  template:
    metadata:
      labels:
        version: v1
        type-one: blue
    spec:
      containers:
        - image: kodekloud/webapp-color:v1
          name: blue-apd


We will deploy a total of 10 application pods. Also, we have to route 70% traffic to blue-apd and 30% traffic to the green-apd deployment according to the task description.

Since the service distributes traffic to all pods equally, we have to set the replica count 7 to the blue-apd deployment so that the given service will send ~70% traffic to the deployment pods.



green-apd deployment should look like this: -


---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    type-two: green
  name: green-apd
spec:
  replicas: 3
  selector:
    matchLabels:
      type-two: green
      version: v1
  template:
    metadata:
      labels:
        type-two: green
        version: v1
    spec:
      containers:
        - image: kodekloud/webapp-color:v2
          name: green-apd


route-apd-svc service should look like this: -


---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: route-apd-svc
  name: route-apd-svc
spec:
  type: NodePort
  ports:
    - port: 8080
      protocol: TCP
      targetPort: 8080
  selector:
    version: v1
Now, create a deployment and service by using the kubectl create -f command: -

kubectl create -f <FILE-NAME-1>.yaml -f <FILE-NAME-2>.yaml -f <FILE-NAME-3>.yaml









=================================================
Helm:
=================================================

=>helm repo ls 
List the helm repositories.

=>helm ls -A
Deployed Helm charts releases list, Here lists all the releases of all the namespaces.


Official Helm Stable Charts: https://charts.helm.sh/stable
Prometheus Helm chart repository: https://prometheus-community.github.io/helm-charts
Rancher's Helm chart repository: https://releases.rancher.com/server-charts

=>helm repo add repoName https://charts.bitnami.com/bitnami
=>helm repo ls 
Add the repostiory to Helm.
It allows us to browse and install charts from the new repository using the Helm package manager.


=>helm search repo nginx
=>helm search hub nginx
Search Helm charts, Its return last lates version. for more

=>helm search repo bitnami/joomla -l | head -n10

When you run "helm search repo nginx", it will query the repositories you have added to your Helm configuration 
When you run "helm search hub nginx", it queries the Helm Hub and returns any matching charts related to nginx 
that are available on the Helm Hub. The Helm Hub is a public repository and can be accessed by anyone.


=>helm search repo polar | grep nginx
=>helm install nginx-server polar/nginx 
=>helm  install jom13 bitnami/joomla  --version=13.3.19
Search and Inatall for the nginx chart in a polar chart repository



cd /root/
=>helm lint ./newVersion
Validate the helm chart by using the helm lint command


=>helm uninstall oldVersioApp -n default
=>helm install -myNewApp ./new-version
=>helm install --generate-name ./new-version
Install/Uninstall application


=>helm search repo lvm-crystal-apd/nginx -l | head -n30
The helm search command searches for all the available charts in a specific Helm chart repository. 


=>helm upgrade nging ofc/nginx-ingress --version=1.41.1
=>helm upgrade lvm-crystal-apd lvm-crystal-apd/nginx -n crystal-apd-ns --version=13.2.12 --set replicaCount=2
Upgrade the helm chart and increase the replica count.

=>helm ls -n crystal-apd-ns
Look under the CHART column for the chart version for varify.


=>helm repo update myHelmChart
Now, update the helm repository with the following command: -
The above command updates the local cache of available charts from the configured chart repositories.









 1 apiVersion: v1
  2 kind: Pod
  3 metadata:
  4   creationTimestamp: null
  5   labels:
  6     run: dos-containers-pod
  7   name: dos-containers-pod
  8   namespace: ckad-multi-containers
  9 spec:
 10   containers:
 11   - image: nginx:1.17
 12     name: alpha
 13     env:
 14       - name: ROLE
 15         value: "SERVER"
 16   - image: busybox:1.28
 17     name: beta
 18     command: ["sh", "-c",'echo "Hello multi-containers"']
 19     resources: {}
 20   restartPolicy: Always
 21 status: {}


apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: dos-containers-pod
  name: dos-containers-pod
  namespace: ckad-multi-containers
spec:
  containers:
  - image: nginx:1.17
    name: dos-containers-pod
    env:
      - name: ROLE
        value: "SERVER"
  - image: busybox:1.28
    name: beta
    command: ["sh", "-c",'echo "Hello multi-containers"']
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}


apiVersion: v1
kind: Pod
metadata:
  name: tres-containers-pod
  namespace: ckad-multi-containers
spec:
  containers:
  - name: primero
    image: busybox:1.28
    env:
    - name: ORDER
      value: FIRST
  - name: segundo
    image: nginx:1.17
    ports:
    - containerPort: 8080
  - name: tercero
    image: busybox:1.31.1
    env:
    - name: ORDER
      value: THIRD
    command: ["/bin/sh", "-c", "tail -f /dev/null"]




apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: redis-access
  namespace: default
spec:
  podSelector:
    matchLabels:
      access: redis
  policyTypes:
    - Ingress
  ingress:
    - from:
        - podSelector:
            matchLabels:
              app: redis
      ports:
        - protocol: TCP
          port: 6379

To create deployment:

kubectl create deployment redis --image=redis:alpine --replicas=1
To expose the deployment using ClusterIP:

kubectl expose deployment redis --name=redis --port=6379 --target-port=6379
To create ingress rule:

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: redis-access
  namespace: default
spec:
  podSelector:
    matchLabels:
       app: redis
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          access: redis
    ports:
     - protocol: TCP
       port: 6379

       
               
spec:
  containers:
  - name: simple-webapp
    image: kodekloud/event-simulator
    env:
    - name: OVERRIDE_USER
      value: USER99394
    volumeMount:
    - mountPath:
        path: /log/app.log
        name: app-log
    volumes:
    - name: app-log
      hostPath:
        path: /opt/outputs/e-com-1123.logs
        type: FileOrCreate
        

cat fsvc.yaml 
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: null
  labels:
    app: fsvc
  name: fsvc
spec:
  ports:
  - name: 80-80
    port: 80
    protocol: TCP
    targetPort: 80
    nodePort: 30080
  selector:
    env: prod
  type: NodePort
status:
  loadBalancer: {}

controlplane ~ ➜  cat pod1.yaml 
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    env: prod
  name: ng
spec:
  containers:
  - image: nginx
    name: ng
    resources: {}
    volumeMounts:
    - name: pod-vol
      mountPath: /usr/share/nginx/html
  volumes:
  - name: pod-vol
    hostPath:
      path: /root/imran/
      type: DirectoryOrCreate
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}





https://brain2life.hashnode.dev/how-to-set-helpful-aliases-for-kubernetes-commands-in-ubuntu-2004

Faster delete pod
=>k delete po ubuntu-sleeper --force

CKAD-feedback


Assignment:
===============================================
Create configmap
Set user role rolebinding and get pod


CKAD-EXAM-CHAPTER
================================================

-CoreConcept
 Done, concept and patise test.

-Configuration
    ##Encrypting the Secret data in Rest
    -------------------------------------------------


-Observability
-POD Design
-Service&Networking
-State Persistence
-Update:2021-sep 

-Adtional Practice
-LightLab
-MockExam



If you have decided to write this exam, the following points will help you:
a. Complete the KodeKloud CKAD course and do the hands-on exercises properly.
b. Use internet sources to solve as many questions as you can.
c. If you are CKA certified, focus only on the delta part.
d. After purchasing the exam, use killer.sh to practice questions and familiarize yourself with the exam environment. Take these questions seriously and try to solve them, or use the provided solutions to help you understand.
e. On the exam day, stay calm. The exam is not as difficult as people make it out to be, and the provided time is sufficient. Try to solve the questions you know first in the first 90 minutes, then use the remaining 30 minutes to tackle the more difficult questions. Do not get stuck on any question as it will consume your time.
f. Use a high-spec laptop, as using a lower-spec laptop like an i5 can cause performance issues during the exam.
g. The exam environment is known to be poor, with lags and hangs, but have patience. You will have sufficient time, so don't try to rush as the system may not work properly.



ConfigMap:
----------------------------------
We can Edit configmap in a single command.
=>kubectl edit configmap my-configmap


Set complate env with value:
apiVersion: v1
kind: Pod
metadata:
  name: my-app
spec:
  containers:
  - image: nginx
    name: my-app
    env:
     - name: TYPE
       value: web application
     - name: COLOR
       value: blue
  restartPolicy: Always


Set env value from configmap:
apiVersion: v1
kind: Pod
metadata:
  name: my-app2
spec:
  containers:
  - image: nginx
    name: my-app
    env:
     - name: COLOR
       valueFrom:
          configMapKeyRef:
             name: webapp-config-map
             key: APP_COLOR
  restartPolicy: Always


A single env now set value from configmap:

=>kubectl exec my-app -- env
  APP_COLOR=darkblue




Set complate env from configmap:

apiVersion: v1
kind: Pod
metadata:
  name: my-app3
spec:
  containers:
  - image: nginx
    name: my-app
    envFrom:
      - configMapRef:
          name: webapp-config-map
  restartPolicy: Always

When it done all env abailable in this pod as list:
=>kubectl exec my-app3 -- env
  APP_COLOR=darkblue
  APP_OTHER=disregard


Volume mount from configmap:

apiVersion: v1
kind: Pod
metadata:
  name: webapp-green
  labels:
    name: webapp-green
spec:
  containers:
    - name: simple-webapp
      image: kodekloud/webapp-color
      args: ["--color", "green"]
      volumeMounts:
        - name: my-app-volume
          mountPath: /path/to/config
  volumes:
    - name: my-app-volume
      configMap:
        name: webapp-config-map

This way this data place on you pod location, and user by app inside pod.
=>k exec webapp-green -- ls /path/to/config
APP_COLOR
APP_OTHER

=>k exec webapp-green -- cat /path/to/config/APP_COLOR
darkblue


