#################################################
#                 L-SP                          #
#################################################
https://kodekloud.com/lessons/challenge-1-3/
https://kodekloud.com/lessons/certified-kubernetes-application-developer-mock-exam-series/

https://dev.mysql.com/downloads/installer/

tar zxvf backups.tgz

alias k='kubectl'
alias cc='clear'
alias kk='kubectl get'
alias ke='kubectl edit'
alias kkk='kubectl delete'
alias kc='kubectl create'
alias kd='kubectl describe'
alias ka='kubectl apply --force'
dr="--dry-run=client -oyaml"

Document

Certified Kubernetes Application Developer (CKAD) 

PV/PVC
https://kubernetes.io/docs/concepts/storage/persistent-volumes/


Workload/cronjob
https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/


NetPol/Service
https://kubernetes.io/docs/concepts/services-networking/network-policies/


ReesourcesManagement
https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/


ReqourceQusta
https://kubernetes.io/docs/concepts/policy/resource-quotas/



CRD
https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/




containerD
Askto build image using ctl and push image to docker repo

FilterSome Pod with short and redirect result

JsonPath
SAVE OUTPI  :
=>k get po | sudo tree them/logfile.log







=====================================
kilerInv:
=====================================

=>kubectl -n default describe pod pod1 | grep -i status:
=>kubectl -n default get pod pod1 -o jsonpath="{.status.phase}"


=>k run ng2 --image=nginx --dry-run=client -oyaml -- /bin/sh -c 'sleep 999'
=>k run ng2 --image=nginx --dry-run=client -oyaml --command -- /bin/sh -c 'sleep 999'

=>k -n neptune create job neb-new-job --image=busybox:1.31.0 $do > /opt/course/3/job.yaml -- sh -c "sleep 2 && echo
done"



=>k run pod6 --image=busybox:1.31.0 $do --command -- sh -c "touch /tmp/ready && sleep 1d" > 6.yaml

=>k -n saturn get pod -o yaml | grep my-happy-shop -A10
Change the Namespace to neptune, also remove the 
status: section, volume, volumeMount and the nodeName


=>k -n neptune rollout history deploy api-new-c32
=>k -n neptune get deploy,pod | grep api-new-c32
=>k -n neptune describe pod api-new-c32-7d64747c87-zh648 | grep -i error
=>k -n neptune describe pod api-new-c32-7d64747c87-zh648 | grep -i image
=>k -n neptune rollout undo deploy api-new-c32


=>k -n pluto expose pod project-plt-6cc-api --name project-plt-6cc-svc --port 3333 --target-port 80

=>k run tmp3 --restart=Never --rm --image=nginx:alpine -i -- curl ng-svc:8080
=>k run tmp3 --restart=Never --rm --image=nginx:alpine -i -- curl ng-svc.ns1.svc.local:8080
k run --rm tmp --restart=Never --image=nginx:alpine -i -- curl ng-svc1.ns1.svc.cluster.local:8080

=>k run tmp --restart=Never --rm --image=nginx:alpine -i -- curl http://project-plt-6cc-svc.pluto:3333
Check byTempod
=>k -n pluto logs project-plt-6cc-api > /opt/course/10/service_test.log



# build container stage 1
FROM docker.io/library/golang:1.15.15-alpine3.14
WORKDIR /src
COPY . .
RUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o bin/app .
# app container stage 2
FROM docker.io/library/alpine:3.12.4
COPY --from=0 /src/bin/app app
# CHANGE NEXT LINE
ENV SUN_CIPHER_ID=5b9c1065-e39d-4a43-a04a-e59bcea3e03f
CMD ["./app"]

=>sudo docker build -t registry.killer.sh:5000/sun-cipher:latest -t registry.killer.sh:5000/sun-cipher:v1-docker .
=>sudo docker push registry.killer.sh:5000/sun-cipher:latest
=>sudo docker push registry.killer.sh:5000/sun-cipher:v1-docker

➜ cd /opt/course/11/image
➜ podman build -t registry.killer.sh:5000/sun-cipher:v1-podman .
=>podman push registry.killer.sh:5000/sun-cipher:v1-podman
➜ podman run -d --name sun-cipher registry.killer.sh:5000/sun-cipher:v1-podman

➜ podman ps
➜ podman ps > /opt/course/11/containers
➜ podman logs sun-cipher
➜ podman logs sun-cipher > /opt/course/11/logs



➜ k -n earth get pv,pvc
➜ k -n earth describe pod project-earthflower-d6887f7c5-pn5wv | grep -A2 Mounts:



apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
name: moon-retain
provisioner: moon-retainer
reclaimPolicy: Retain
➜ k -n moon describe pvc moon-pvc-126


➜ k -n moon exec secret-handler -- env | grep SECRET1
➜ k -n moon exec secret-handler -- find /tmp/secret2
➜ k -n moon exec secret-handler -- cat /tmp/secret2/key



➜k -n moon describe pod web-moon-847496c686-2rzj4
=>k -n moon create configmap configmap-web-moon-html --from-file=index.html=/opt/course/15/web-moon.html
=>k run tmp --restart=Never --rm -i --image=nginx:alpine -- curl 10.44.0.78
=>k -n moon describe pod web-moon-c77655cc-dc8v4 | grep -A2 Mounts:


- name: logger-con # add
image: busybox:1.31.0 # add
command: ["sh", "-c", "tail -f /var/log/cleaner/cleaner.log"] # add
volumeMounts: # add
- name: logs # add
mountPath: /var/log/cleaner 




initContainers: # initContainer start
- name: init-con
image: busybox:1.31.0
command: ['sh', '-c', 'echo "check this out!" > /tmp/web-content/index.html']
volumeMounts:
- name: web-content
mountPath: /tmp/web-content # initContainer end
➜ k run tmp --restart=Never --rm -i --image=nginx:alpine -- curl 10.0.0.67



➜ k -n mars get all
➜ k -n mars run tmp --restart=Never --rm -i --image=nginx:alpine -- curl -m 5 manager-api-svc:4444
=>k -n mars run tmp --restart=Never --rm -i --image=nginx:alpine -- curl -m 5 10.0.1.14
➜ k -n mars describe service manager-api-svc
➜ k -n mars get ep
➜ k -n mars run tmp --restart=Never --rm -i --image=nginx:alpine -- curl -m 5 manager-api-svc:4444



➜ k -n jupiter run tmp --restart=Never --rm -i --image=nginx:alpine -- curl -m 5 jupiter-crew-svc:8080
➜ k -n jupiter run tmp --restart=Never --rm -i --image=nginx:alpine -- curl -m 5 jupiter-crew-svc:8080
➜ curl 192.168.100.11:30100



➜ k -n venus run tmp --restart=Never --rm -i --image=busybox -i -- wget -O- frontend:80
➜ k -n venus run tmp --restart=Never --rm --image=busybox -i -- wget -O- api:2222

Test using: wget www.google.com and wget api:2222
➜ k -n venus exec frontend-789cbdc677-c9v8h -- wget -O- www.google.com
➜ k -n venus exec frontend-789cbdc677-c9v8h -- wget -O- api:2222

# 20_np1.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
name: np1
namespace: venus
spec:
podSelector:
matchLabels:
id: frontend # label of the pods this policy should be applied on
policyTypes:
- Egress # we only want to control egress
egress:
- to: # 1st egress rule
  - podSelector: # allow egress only to pods with api label
  matchLabels:
  id: api
- ports: # 2nd egress rule
  - port: 53 # allow DNS UDP
    protocol: UDP
  - port: 53 # allow DNS TCP
    protocol: TCP

Now test
➜ k -n venus exec frontend-789cbdc677-c9v8h -- wget -O- www.google.de
➜ k -n venus exec frontend-789cbdc677-c9v8h -- wget -O- -T 5 www.google.de:80
➜ k -n venus exec frontend-789cbdc677-c9v8h -- wget -O- api:2222




➜ k -n sun get pod --show-labels
=>k -n sun get pod -l type=runner
k label -h # help
=>k -n sun label pod -l type=runner protected=true # run for label runner
=>k -n sun label pod -l type=worker protected=true # run for label worker
=>k -n sun annotate pod -l protected=true protected="do not delete this pod"
=>k -n sun get pod -l protected=true -o yaml | grep -A 8 metadata:


➜ k run tmp --restart=Never --rm -i --image=nginx:alpine -- curl -m 5 sun-srv.sun:9999





HelmReppUpdate:
=>helm repo list
=>helm repo update
=>helm -n mercury upgrade internal-issue-report-apiv2 bitnami/nginx

=>helm rollback
Undo upgrade

=>helm show values bitnami/apache # will show a long list of all possible value-settings
=>helm -n mercury install internal-issue-report-apache bitnami/apache --set replicaCount=2
=>k -n mercury get deploy internal-issue-report-apache

pending-upgrade
=>helm -n mercury ls -a
=>helm -n mercury uninstall internal-issue-report-daniel
=>helm -n mercury delete internal-issue-report-daniel


Question:
----------------------------------------
There seems to be a broken release, stuck in pending-install state. Find it and delete it
=>helm list --namespace mercury --pending
=>helm delete -n mercury reponanme



Q
Team Moonpie, which has the Namespace moon, needs more storage. Create a new PersistentVolumeClaim named moon-pvc-126 in that namespace. This claim should use a new StorageClass moon-retain with the provisioner set to moon-retainer and the reclaimPolicy set to Retain. The claim should request storage of 3Gi, an accessMode of ReadWriteOnce and should use the new StorageClass.
The provisioner moon-retainer will be created by another team, so it's expected that the PVC will not boot yet. Confirm this by writing the log message from the PVC into file /opt/course/13/pvc-126-reason.



Ing:
===========================
https://killer.sh/
https://killercoda.com/


endpoint service
Read about projected volumes
hpa


https://kodekloud.com/topic/faq-what-is-the-rewrite-target-option/


NeedExplor:
-------------------------------------------------
=>$ kubectl delete all --all

=>kubectl label po kubia-manual creation_method=manual
=>kubectl label po kubia-manual-v2 env=debug --overwrite


List the pods again to see the updated labels:
$ kubectl get po -L creation_method,env


And those that don’t have the env label:
$ kubectl get po -l '!env'

creation_method!=manual to select pods with the creation_method label with
any value other than manual



Exec to service:
$ kubectl exec kubia-3inly env

=>k get po -n ckad17-nqoss-aecs --output=custom-columns="NAME":.metadata.name,"QOS":.status.qosClass>qos_status_aecs
=>kubectl get pods -A -o=custom-columns='POD_NAME:metadata.name,IP_ADDR:status.podIP' --sort-by=.status.podIP > /root/pod_ips_ckad02_svcn


DaemonSet
=>kubectl get daemonset
Demonset every node can include NodeAffinity



KCH:Skip-Q
=================================================

  envFrom:
      - secretRef:
          name: ckad01-db-scrt-aecs

Dirrentent 
command:
      - /bin/sh
      - -c
      - echo Initialize application environment! && sleep 10

      - echo 'Initialize application environment!'; sleep 10


We have deployed some pods in the namespaces ckad-alpha and ckad-beta.

You need to create a NetworkPolicy named ns-netpol-ckad that will restrict all Pods in Namespace ckad-alpha to only have outgoing traffic to Pods in Namespace ckad-beta . Ingress traffic should not be affected.


However, the NetworkPolicy you create should allow egress traffic on port 53 TCP and UDP.





Service and Endpoints
              
Create a new pod with image nginx and name ckad-probe-aom and configure the pod with 
livenessProbe with command ls and set initialDelaySeconds to 5 .

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: ckad-probe-aom
  name: ckad-probe-aom
spec:
  containers:
  - image: nginx
    name: ckad-probe-aom
    resources: {}
    livenessProbe:
      exec:
        command:
           - ls
      initialDelaySeconds: 5
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}

imparagein comm for this



Nginx mountPath:

apiVersion: v1
kind: Pod
metadata:
  name: ckad-flash89-aom
spec:
  containers:
    - name: nginx
      image: nginx
      volumeMounts:
        - name: flash-logs
          mountPath: /var/log/nginx
    - name: busybox
      image: busybox
      command:
        - bin/sh
        - -c
        - sleep 10000
      volumeMounts:
        - name: flash-logs
          mountPath: /usr/src
  volumes:
    - name: flash-logs
      emptyDir: {}






gCommand
===========================
=>kubectl run ckad-probe-aom --image=nginx --restart=Always --labels=run=ckad-probe-aom

=>sudo apt-get update && apt-get install iputils-ping && sudo apt install net-tools
=>kubectl create quota my-quota --hard=cpu=1,memory=1G,pods=2,services=3,replicationcontrollers=2,resourcequotas=1,secrets=5,persistentvolumeclaims=10



=>kubectl create cronjob learning-every-minute  -n ckad-job --image=busybox:1.28 --schedule="* * * * *" -- 'I am practicing for CKAD certification'
=>kubectl create cronjob my-alarm -n ckad-job               --image=busybox:1.28 --schedule="0 0 * * 0" -- date
=>kubectl create cronjob simple-python-job -n ckad-job      --image=python       --schedule="*/30 * * * *" -- 'ps –eaf'
=>kubectl create cronjob learning-every-minute -n ckad-job --restart=OnFailure --image=busybox:1.28 --schedule="*/1 * * * *" -- echo 'I am practicing for CKAD certification'


=>k create clusterrole healthz-access --verb=get,post --non-resource-url=/healthz,/healthz/*



=>scp ../media/* root@node01:/web
Copey file to another

=>ps -ef | grep kube-apiserver | grep admission-plugins
=>kubectl exec -it kube-apiserver-controlplane -n kube-system -- kube-apiserver -h | grep 'enable-admission-plugins'

=>k create cronjob simple-python-job -n ckad-job --image=python --schedule="*/30 * * * *" -- 'ps –eaf'


=>kubectl create ingress world -n world --class=nginx --rule="world.universe.mine/europe*=europe:80" --rule="world.universe.mine/asia*=asia:80" 
=>kubectl create ingress ingress-ckad09-svcn -n critical-space  --rule="/pay=pay-service:8282
=>kubectl create ingress ingress-resource-svcn \
  --namespace app-space \
  --rule='/wear'='wear-service:8080' \
  --rule='/watch'='video-service:8080' \
  --annotation='nginx.ingress.kubernetes.io/rewrite-target=/' \
  --annotation='nginx.ingress.kubernetes.io/ssl-redirect=false' \
  --class=nginx


 annotations:
    "api-approved.kubernetes.io": "unapproved, experimental-only"



=>kubectl --namespace=ckad17-nqoss-aecs get pod --output=custom-columns="NAME:.metadata.name,QOS:.status.qosClass"
NAME                QOS
ckad17-qos-aecs-1   BestEffort
ckad17-qos-aecs-2   Guaranteed
=>kubectl --namespace=ckad17-nqoss-aecs get pod --output=custom-columns="NAME:.metadata.name,QOS:.status.qosClass" > /root/qos_status_aecs

Perform the connectivity test using 
=>kubectl  exec testpod -- curl backend-ckad-svcn

=>helm upgrade lvm-crystal-apd -n crystal-apd-ns lvm-crystal-apd/nginx --version=13.2.30 --set replicaCount=2

SAVE OUTPI  :
=>k get po | sudo tree them/logfile.log



=================================================
Helm:
=================================================

=>helm repo ls 
List the helm repositories.

=>helm ls -A
Deployed Helm charts releases list, Here lists all the releases of all the namespaces.


Official Helm Stable Charts: https://charts.helm.sh/stable
Prometheus Helm chart repository: https://prometheus-community.github.io/helm-charts
Rancher's Helm chart repository: https://releases.rancher.com/server-charts

=>helm repo add repoName https://charts.bitnami.com/bitnami
=>helm repo ls 
Add the repostiory to Helm.
It allows us to browse and install charts from the new repository using the Helm package manager.


=>helm search repo nginx
=>helm search hub nginx
Search Helm charts, Its return last lates version. for more

=>helm search repo bitnami/joomla -l | head -n10

When you run "helm search repo nginx", it will query the repositories you have added to your Helm configuration 
When you run "helm search hub nginx", it queries the Helm Hub and returns any matching charts related to nginx 
that are available on the Helm Hub. The Helm Hub is a public repository and can be accessed by anyone.


=>helm search repo polar | grep nginx
=>helm install nginx-server polar/nginx 
=>helm  install jom13 bitnami/joomla  --version=13.3.19
Search and Inatall for the nginx chart in a polar chart repository



cd /root/
=>helm lint ./newVersion
Validate the helm chart by using the helm lint command


=>helm uninstall oldVersioApp -n default
=>helm install -myNewApp ./new-version
=>helm install --generate-name ./new-version
Install/Uninstall application


=>helm search repo lvm-crystal-apd/nginx -l | head -n30
The helm search command searches for all the available charts in a specific Helm chart repository. 


=>helm upgrade nging ofc/nginx-ingress --version=1.41.1
=>helm upgrade lvm-crystal-apd lvm-crystal-apd/nginx -n crystal-apd-ns --version=13.2.12 --set replicaCount=2
Upgrade the helm chart and increase the replica count.

=>helm ls -n crystal-apd-ns
Look under the CHART column for the chart version for varify.


=>helm repo update myHelmChart
Now, update the helm repository with the following command: -
The above command updates the local cache of available charts from the configured chart repositories.




https://brain2life.hashnode.dev/how-to-set-helpful-aliases-for-kubernetes-commands-in-ubuntu-2004






If you have decided to write this exam, the following points will help you:
a. Complete the KodeKloud CKAD course and do the hands-on exercises properly.
b. Use internet sources to solve as many questions as you can.
c. If you are CKA certified, focus only on the delta part.
d. After purchasing the exam, use killer.sh to practice questions and familiarize yourself with the exam environment. 
   Take these questions seriously and try to solve them, or use the provided solutions to help you understand.
e. On the exam day, stay calm. The exam is not as difficult as people make it out to be, and the provided time is sufficient. 
   Try to solve the questions you know first in the first 90 minutes, 
   then use the remaining 30 minutes to tackle the more difficult questions. 
   Do not get stuck on any question as it will consume your time.
f. Use a high-spec laptop, as using a lower-spec laptop like an i5 can cause performance issues during the exam.
g. The exam environment is known to be poor, with lags and hangs, but have patience. 
   You will have sufficient time, so don't try to rush as the system may not work properly.






Example:
==============================================================================
cat stf.yaml 
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: redis-cluster
spec:
  selector:
    matchLabels:
      app: redis-cluster
  serviceName: "nginx"
  replicas: 6 # by default is 1
  template:
    metadata:
      labels:
        app: redis-cluster
    spec:
      terminationGracePeriodSeconds: 10
      volumes:
        - name: conf
          configMap:
            name: redis-cluster-configmap
            defaultMode: 0755
      containers:
      - name: redis
        command: ["/conf/update-node.sh", "redis-server", "/conf/redis.conf"]
        image: redis:5.0.1-alpine
        env:
          - name: POD_IP
            valueFrom: 
              fieldRef:
                fieldPath: status.podIP
        ports:
        - name: client
          containerPort: 6379
        - containerPort: 16379
          name: gossip
        volumeMounts:
        - name: data
          mountPath: /data
          readOnly: false
        - name: conf
          mountPath: /conf
          readOnly: false
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: "mystc"
      resources:
        requests:
          storage: 1Gi







controlplane ~ ➜  kubectl config view
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: DATA+OMITTED
    server: https://controlplane:6443
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: kubernetes-admin
  name: kubernetes-admin@kubernetes
current-context: kubernetes-admin@kubernetes
kind: Config
preferences: {}
users:
- name: kubernetes-admin
  user:
    client-certificate-data: DATA+OMITTED
    client-key-data: DATA+OMITTED

controlplane ~ ➜  cat /etc/kubernetes/manifests/kube-apiserver.yaml 
apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 127.0.0.1:6443
  creationTimestamp: null
  labels:
    component: kube-apiserver
    tier: control-plane
  name: kube-apiserver
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-apiserver
    - --advertise-address=127.0.0.1
    - --allow-privileged=true
    - --authorization-mode=Node,RBAC
    - --client-ca-file=/etc/kubernetes/pki/ca-authority.crt
    - --enable-admission-plugins=NodeRestriction
    - --enable-bootstrap-token-auth=true
    - --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
    - --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
    - --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
    - --etcd-servers=https://127.0.0.1:2379
    - --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
    - --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
    - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
    - --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
    - --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
    - --requestheader-allowed-names=front-proxy-client
    - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
    - --requestheader-extra-headers-prefix=X-Remote-Extra-
    - --requestheader-group-headers=X-Remote-Group
    - --requestheader-username-headers=X-Remote-User
    - --secure-port=6443
    - --service-account-issuer=https://kubernetes.default.svc.cluster.local
    - --service-account-key-file=/etc/kubernetes/pki/sa.pub
    - --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
    - --service-cluster-ip-range=10.96.0.0/12
    - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
    - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
    image: registry.k8s.io/kube-apiserver:v1.27.0
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 8
      httpGet:
        host: 127.0.0.1
        path: /livez
        port: 6443
        scheme: HTTPS
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    name: kube-apiserver
    readinessProbe:
      failureThreshold: 3
      httpGet:
        host: 127.0.0.1
        path: /readyz
        port: 6443
        scheme: HTTPS
      periodSeconds: 1
      timeoutSeconds: 15
    resources:
      requests:
        cpu: 250m
    startupProbe:
      failureThreshold: 24
      httpGet:
        host: 127.0.0.1
        path: /livez
        port: 6443
        scheme: HTTPS
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    volumeMounts:
    - mountPath: /etc/ssl/certs
      name: ca-certs
      readOnly: true
    - mountPath: /etc/ca-certificates
      name: etc-ca-certificates
      readOnly: true
    - mountPath: /etc/kubernetes/pki
      name: k8s-certs
      readOnly: true
    - mountPath: /usr/local/share/ca-certificates
      name: usr-local-share-ca-certificates
      readOnly: true
    - mountPath: /usr/share/ca-certificates
      name: usr-share-ca-certificates
      readOnly: true
  hostNetwork: true
  priority: 2000001000
  priorityClassName: system-node-critical
  securityContext:
    seccompProfile:
      type: RuntimeDefault
  volumes:
  - hostPath:
      path: /etc/ssl/certs
      type: DirectoryOrCreate
    name: ca-certs
  - hostPath:
      path: /etc/ca-certificates
      type: DirectoryOrCreate
    name: etc-ca-certificates
  - hostPath:
      path: /etc/kubernetes/pki
      type: DirectoryOrCreate
    name: k8s-certs
  - hostPath:
      path: /usr/local/share/ca-certificates
      type: DirectoryOrCreate
    name: usr-local-share-ca-certificates
  - hostPath:
      path: /usr/share/ca-certificates
      type: DirectoryOrCreate
    name: usr-share-ca-certificates
status: {}



