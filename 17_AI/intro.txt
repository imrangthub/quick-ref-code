#################################################
#                AI                            #
#################################################





=================================================
##AI-RoadMap
=================================================

RoadMap:Data Science/Analytics/ML/AI
-------------------------------------------------
1. Python (Basic to Advanced)
2. Git/GitHub 
3. SQL / Pandas / NumPy / Matplotlib / Seaborn / Plotly ⭘
4. Excel/PowerBi/Tableau/ Statistics/ BigQuery
5. Sklearn/ Statistics & Mathematics
6. TensorFlow/PyTorch/Keras/OpenCV/ Mathematics
7. Build a Strong Network on LinkedIn
8. Hadoop/Spark/AWS/Google Cloud

RoadMap:Generative AI
-------------------------------------------------
☐ Machine Learning (ML): AI মডেলের ভিত্তি।  
☐ Deep Learning (DL): নিউরাল নেটওয়ার্ক ব্যবহার করে জটিল ডেটা থেকে শেখা।  
☐ Generative Models: GANs (Generative Adversarial Networks), VAEs (Variational Autoencoders), এবং Transformers।  

ForGetAI
-------------------------------------------------
ভেক্টর ডাটাবেস (Vector Database)
হাগিং ফেস (Hugging Face)
ল্যাংচেইন (LangChain)


Python Roadmap
-------------------------------------------------
https://docs.python.org/3/tutorial/index.html

Python Programming Concepts 
Basic Syntax and Structure 
Object-Oriented Programming (OOP) 
Control Flow 
Working with Data 
Functions 
Advanced Concepts 
Data Structures & Algorithms 
Libraries and Frameworks 
Modules and Packages 
Testing and Debugging 
File Handling 
Version Control 
Error Handling 
Best Practices 





=================================================
##project
=================================================


Fake News Identify Project:
=================================================
https://github.com/rashakil-ds/10-ML-Projects-for-Resume
https://www.youtube.com/watch?v=IGOCtD4HFFQ&list=PLKdU0fuY4OFcot0zyVbM1-zKf_eCUK4zQ&index=132


code-base
-------------------------------------------------
Step 1: Import necessary libraries

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB, BernoulliNB
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, roc_curve, plot_roc_curve, classification_report
from wordcloud import WordCloud
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import plotly.express as px
import string
import warnings
warnings.filterwarnings('ignore')



Step 2: Load the dataset

data = pd.read_csv('WELFake_Dataset.csv')
data.head()

data.tail()

Dataset contains four columns: Serial number (starting from 0); Title (about the text news); and Label (0 = spam and 1 = real).

data['text'][0]       # Label 1, Real News

data['text'][72131]   # Label 0, Spam News



Step 3: Handle null values and separate features from labels

data.fillna(' ', inplace=True)
features = data[['title', 'text']]
labels = data['label']

data.isnull().sum()

label_Status = data['label'].value_counts()
transactions = label_Status.index
quantity = label_Status.values

figure = px.pie(data,
                values=quantity,
                names=transactions, hole =.60,
                title="Spam & Ham Status")

figure.show()



Step 4: Create word clouds

def plot_wordcloud(text, title):
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)
    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.title(title)

fake_text = ' '.join(data[data['label'] == 0]['text'])
real_text = ' '.join(data[data['label'] == 1]['text'])

plot_wordcloud(fake_text, 'Fake News')
plt.show()

plot_wordcloud(real_text, 'Genuine News')
plt.show()




Step 5: Text preprocessing

text_column = 'text'
label_column = 'label'

stopword = set(stopwords.words('english'))
stopword

def preprocess_text(text):
    # Remove punctuation
    remove_punc = [char for char in text if char not in string.punctuation]
    clean_words = ''.join(remove_punc) # char joining

    #Remove stopwords
    text = [word for word in clean_words.split() if word.lower() not in stopword] # stopword = stopwords.words('english')
    return text

data[text_column] = data[text_column].apply(preprocess_text)

data[text_column] 



Step 6: Lemmatization

lemmatizer = WordNetLemmatizer()

def lemmatize_text(text):
    lemmatized_text = ' '.join([lemmatizer.lemmatize(word) for word in text])
    return lemmatized_text

data[text_column] = data[text_column].apply(lemmatize_text)

data[text_column]



Step 7: TF-IDF Vectorization

vectorizer = TfidfVectorizer()
x = vectorizer.fit_transform(data[text_column])
y = data[label_column]



Step 8: Split dataset into training and testing sets

xtrain, xtest, ytrain, ytest = train_test_split(x, labels, test_size=0.3, random_state=42)



Step 09: Evaluate Naive Bayes models

def evaluate_model(model, xtest, ytest):
    y_pred = model.predict(xtest)
    accuracy = accuracy_score(ytest, y_pred)
    cm = confusion_matrix(ytest, y_pred)
    prob = model.predict_proba(xtest)[:, 1]
    roc_auc = roc_auc_score(ytest, prob)
    fpr, tpr, thresholds = roc_curve(ytest, prob)
    precision, recall, _ = precision_recall_curve(ytest, prob)
    pr_auc = auc(recall, precision)
    return {
        'Accuracy': accuracy,
        'Confusion Matrix': cm,
    }




Step 10: Build Naive Bayes models

Multinomial Naive Bayes

mnb_model = MultinomialNB(alpha=1.0, fit_prior=True, class_prior=None)
mnb_model.fit(xtrain, ytrain)


from sklearn.metrics import precision_recall_curve, auc

nb_results = evaluate_model(mnb_model, xtest, ytest)
nb_results




Evaluate Manually

cm = confusion_matrix(ytest, mnb_model.predict(xtest))
cm
array([[9711,  902],
       [1770, 9258]], dtype=int64)
Python

# Create a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Predicted 0', 'Predicted 1'],
            yticklabels=['Actual 0', 'Actual 1'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
Python

print(classification_report(ytest, mnb_model.predict(xtest)))
Python

plot_roc_curve(mnb_model, xtest, ytest)
# plt.plot([0, 1], [0, 1], 'k--') # This line seems to be missing in the image but is often included for reference in ROC curve plots.




Step 11: Apply Logistic Regression

lr_model = LogisticRegression(max_iter=1000)
lr_model.fit(xtrain, ytrain)





Step 14: Input random text and make a prediction

models = [nb_model, bnb_model, lr_model]

random_text = input()
preprocessed_text = preprocess_text(random_text) # remove punctuation
lemmatized_text = lemmatize_text(preprocessed_text) # text scaling
text_vector = vectorizer.transform([' '.join(lemmatized_text)])

preprocessed_text

lemmatized_text

text_vector

text_vector.toarray()



for model in models:
    prediction = model.predict(text_vector)
    print(f"Model: {type(model).__name__}")
    print("Prediction:", prediction)
    print('\n')




Step 16: Save Good ML Model for Deployment

import pickle

# Define a file path where you want to save the model
model_file_path = 'logistic_regression_model.pkl'

# Save the model to the file
with open(model_file_path, 'wb') as model_file:
    pickle.dump(lr_model, model_file)













