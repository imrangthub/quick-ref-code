#################################################
#                DOCKER-ENG                           #
#################################################


=================================================
##ForDockerCourse:
=================================================



##1) Introduction
=================================================

00:00-to-00:40mm
================
StartngPeech:
    Hey Guys, Welcome to this tutorial on Docker.
    If you are new to Docker, and want to learn Docker from scratch you’re in the right place.

    In this tutorial, I'll explain Docker from the very beginning.
    What is docker, how this docker thing works, why it’s so popular, and definitely TheEasyWay!

    Each topic will be explained with hands-on exercises to make it clear and practical.
    We are gonna use different programming languages, such as  Java, Python, PHP javaScript for practice projects. 
    This will be more interesting and gain a clear understanding of 
    how docker deals with various programming languages.

    and finally, we will deploy a Three Tier application including frontend, backend and Database using docker.
    to get a clear picture.

    Really,
    By the end, you’ll have a solid understanding of Docker and be ready to start using Docker in 
    real-world projects.
    So, grab your coffee, get comfortable, and let’s dive into the world of Docker!


00:40mm-to-00:50mm
==================
AboutDocuemnt:
    In this tutorial, I am gonna use this Document to organize the topic-resource, uses-commands, diagrams, screenshots, etc etc.
    and I will share it with you.
    The link of this Document will be available in the video description and first comment.
    And I will also share the GitHub link of the project and source code that will be used in this tutorial.
    


00:50mm-to-1:05mm
=================
You don’t have to be a programmer to learn Docker.
Anyone can learn Docker without coding skills.
actually, docker focuses on deployment and system management, not application development.



1:05mm-to-2:40mm
================
Introducing Tutorial Topics

Introduction
Just course Intro.


Docker Installation:
Here we Learn how to install Docker on various platforms, including Windows, macOS, and Linux.
Set it up step-by-step to ensuring Docker runs smoothly on your system and get started quickly.

Getting Started with Docker
Run our first container and we will explore some basic commands. 
a HelloWorld thing of docker.
Get familiar with Docker’s key parts like images and containers.

Docker in Depth
Understand how Docker works behind the scenes. 
Learn about its components, engine, internal-architecture, and how it makes apps run faster.

Docker Images and Containers
a deep dive into Docker images and containers.
Learn how to create, manage, and optimize Docker images and containers for efficient application deployment.
Find out what images and containers are and how they’re different. 
Create, manage, and explore them in detail.

Storage and Volumes
Explore Docker's storage options for data persistence.
Learn about different types of volumes and mounting to manage container data effectively.

Networking and Security
Discover Docker's networking capabilities, type of network. 
Learn about how to create, manage Docker's network and 
how containers talk to each other and stay secure using this docker network.

Simple Example Project
Work on several simple hands-on projects using different programming languages 
and run them as Docker containers. including frontend backend and database.
This will reinforce key concepts and give you practical Docker experience.


Docker Compose
Set up and run multiple containers with one file using Docker Compose. 
Learn how to run multiple DockerContainer using a single command.
Like frontend backend and database all those will run using only a single command.
Waht is Simplify your workflows with YAML-based configuration files and Manage everything easily.

Container Orchestration
Understand the need for container orchestration and explore tools like Kubernetes and Docker Swarm. 
Learn how to manage and scale containerized applications in production environments.


2:40mm-to-3:06
================
Which for This Course
    This course is for DevOps professionals, app developers, software engineers, IT managers, 
    and system architects. 
    It’s also perfect for anyone interested in learning Docker.





##2) Docker Installation In Windows | 10:44mm
=================================================
00:00 -to- end

Hey Guys: 
In this tutorial, we are gonna see how to install Docker on Windows.
It's pretty simple and state-forward.
Official Docker documentation provides installation guides for various operating systems.
Like linux, windows Mac etc.
We will follow the recommended steps to install Docker on Windows.

Need to know one thing before we proceed.
Docker was two editions:
    Docker Community Edition (CE) – Free and suitable for learning and development.
    Docker Enterprise Edition (EE) – Includes additional features for enterprise use.

Now subscription base. you cna find more details on officeal site.
Let’s go for installation.


First, let's start by downloading the Docker .exe file.

Download is in progress, Let’s check the system requirements to ensure compatibility.
Also we need to verify the hypervisor settings before installation.
If you're using a hypervisor, select that option and uncheck the other one before the installation.

Now, let's start the installation process. 
Follow the on-screen instructions to complete the setup.

Once the installation is done, restart your PC.
After rebooting, You get the Docker app from the Start Menu.

To confirm that Docker is installed correctly, 
open Command Prompt or PowerShell and check the Docker version. 
It will make sure our docker installed perfectlty.

In the next video, we'll cover Docker installation on Linux.

If you have an issue installing docker in your current machine 
like security or office-regulation then don't worry.
There is a way: Play with Docker, It's a web-based platform for practicing Docker.
It will give you free access Linux host with docker and you can practice on it.
every session of three hours.

Stay tuned as we explore Docker’s menu functionalities and how to use them effectively in the upcoming video!


Before we get started, I recommend creating an account on hub.docker.com.
This is where we can find and upload Docker images, including official images provided by Docker.
Now, let's run a simple Hello World container.

First, open the Docker Desktop app.
Since we haven’t run any containers yet, you’ll see that there are no images or containers available.
We can also verify this using the command line—both should be empty.

Next, let's pull the Hello World image from Docker Hub.
To do this, we run the docker pull hello-world command.
This command fetches the image from Docker Hub and downloads it to our local system.

Once the download is complete, we should see the image listed in Docker Desktop.

Now, let's execute the docker run hello-world command to run the container.
If everything is set up correctly, we should see a success message in the terminal.

Finally, let’s check the Containers section in Docker Desktop and verify the running container using PowerShell.

In the next video, we’ll explore more Docker commands and features. 
Stay tuned!





##LinuxInstallation | 8:44 sec
===========================================

"In this video, we’re going to install Docker on a Linux machine.

There are two ways to install Docker on Linux:

Using Docker Desktop
Installing just the Docker Engine (without the desktop interface)
The official documentation provides multiple installation methods for different Linux distributions.
However, the easiest way is to use a simple installation script.
First, we download the script.
Before running it, we can perform a dry run to check for any potential issues.

Once everything looks good, we proceed by running the script to install Docker.
Since installation steps may vary across Linux distributions, 
always refer to the official documentation for details.

After installation is complete, let's verify that Docker is installed by checking its version.
If we encounter any permission issues, we need to grant the necessary permissions to our user.

Next, we check the list of images and containers, at this point, both should be empty.
Now, let’s pull and run a simple Hello World container to confirm that Docker is working correctly.

That’s it for this installation guide!
In the next video, we’ll explore more Docker commands and best practices. Stay tuned!"**


"In this video, we’ll explore Docker’s online platform— Play with Docker.

First, go to the Play with Docker website.
To use this platform, you’ll need a Docker account.
Log in with your Docker credentials to start a three-hour session where you 
can practice running containers online.

Once logged in, let’s create an instance and run a simple Hello World Docker image.
This is perfect for learning and testing Docker commands.
You can even run web applications here with port access.

Now, let’s try running an NGINX web server on this platform.
We’ll map port 8080 on our instance to the default port 80 of NGINX.

Don’t worry too much about the command right now, we’ll cover everything in detail in a later video.
For now, just know that we’re using Docker’s official NGINX image.

You can check the official documentation to learn more about this image and how to run it.

Once the application is running, click on the 8080 port link.
You should see the default NGINX welcome page!

This works just like running Docker on a local machine, 
making it a great environment for learning and experimentation.

In the next video, we’ll dive into Docker’s basic commands and how to use them effectively. 
Stay tuned!"**




##3) Getting Started with Dockers | 30:44 sec
===================================================

1:00mm
=========
HeyGuys:
"In today’s video, we’ll go over some of the most frequently used Docker commands.

You can run these commands using the Docker command line on Windows, Linux, or Mac.
Docker Desktop also provides a visual interface, 
but in real-world professional projects, developers primarily use the command line.

So, it’s best to get comfortable with the command-line mode for better efficiency and flexibility.

Let’s dive in and explore these essential Docker commands!"**



2:00 mm
=====================
"Let's start with our first Docker command: 
=>docker info
This command displays detailed information about the current Docker setup, including:
    The installed Docker version
    System information, such as CPU and memory usage
    Default network drivers
    Docker volumes

Since this is a brand-new installation, we’ll see 0 containers and 0 images listed.

Now, let’s move on to the next command: 
=>docker ps
This command is used to list running containers.
Since we haven't started any containers yet, nothing will be displayed.
We can also check this visually in Docker Desktop, and it should be empty.

Next, let’s pull an image from Docker Hub, we’ll start with the simple Hello World image.
To do this, we run:
docker pull hello-world
Once the image is pulled, we can verify it using the command:
docker image ls
This will display a list of all downloaded images, including hello-world.

You can also check this in Docker Desktop, it should now show the pulled image.
Now lets run it.

That’s it for now! In the next part, we’ll run a container and explore more Docker commands."**

3:40
===========
"Now, let’s run an NGINX container with port mapping on port 8080.

To do this, we use the following command:
=>docker run -p 8080:80 nginx

The docker run command performs two actions at the same time:
Pulls the image from Docker Hub if it's not already downloaded.
Runs the container using that image.
If the NGINX image isn’t already on your system, Docker will first pull it.
Once the download is complete, you can verify that the image is now available by running:
docker image ls
You’ll see the NGINX image listed both in the command line and in Docker Desktop under the Images section.

However, you won’t see anything in the Containers section yet, 
because we haven't actually started the container.

In the next step, we’ll run the container and check its status. Stay tuned!"**



3:05mm
==============
"Now, let’s run the NGINX container and check its status in the command line.
To start the container, we use:
=>docker run -d -p 8080:80 nginx
The -d flag runs the container in detached mode (in the background).
The -p 8080:80 maps port 8080 on our local machine to port 80 inside the container.

Checking the Running Container
To see the running container, use:
=>docker ps

This command displays:
Container ID – A unique identifier for the container
Name – An auto-generated name for the container
Command – The default command used to run the container
Now that the container is running, if we open http://localhost:8080 in a browser, 
we should see the NGINX default home page.

Custom Container Names
By default, Docker assigns random names to containers.
If you want to set a custom name, you can use the --name option:
=>docker run -d -p 8080:80 --name my-ng1 nginx
We’ll go over this in more detail later.

If we run:
=>docker ps
It only shows running containers.
But what if we want to see all containers, including those that have stopped?
docker ps -a
This command lists both running and stopped containers.
From here, we can also remove containers if needed.
We’ll cover container management in detail in the next video.

Stay tuned!"**



8:40mm
=================
Now, let's see how to stop a running container using the command line.
To stop a container, we need its Container ID or Name.
First, list running containers:
docker ps
Copy the Container ID or Name, then stop it with:
docker stop <container_id_or_name>
For example:
docker stop my-nginx
Now, let’s check if the container is still there:
docker ps -a
This lists all containers, including stopped ones.

Removing a Container
To delete a stopped container, use:
docker rm <container_id_or_name>
You don’t need to type the full ID—just a few characters are enough.
Now, check again:
docker ps -a
You’ll see no containers left.
However, the Docker images are still there.

An image is like a class in programming—it's a read-only template.
A container is like an object, a running instance of an image.
You can run multiple containers from the same image.

Now, let’s run a new container from the NGINX image and give it a custom name:
docker run -d -p 8080:80 --name ng-cont1 nginx
-d → Runs the container in detached mode (background).
-p 8080:80 → Maps port 8080 on the host to port 80 in the container.
--name ng-cont1 → Assigns a custom name instead of a random one.
Check the running container:
docker ps
Running Another Container from the Same Image
Now, let’s start another container using the same image:

docker run -d -p 8081:80 --name ng-cont2 nginx

If you try to use a name that already exists, Docker will show an error.
If the port is already in use, you must choose a different one.
If you get an error, you can either:
    Delete the previous container:
    docker rm ng-cont1
    OR Use a different container name, like:
docker run -d -p 8082:80 --name ng-cont3 nginx

Now, both ng-cont2 and ng-cont3 are running.
Check this using docker ps.




16:40
=======================
here is a interesting fact i am going to sared: 

Let’s look at a real-world Problem:
Imagine you need multiple NGINX instances on your machine:so..
Without Docker (Traditional Way)
    You would have to manually copy multiple versions of the NGINX software.
    Keep them in different locations to avoid conflicts.
    Update the nginx.conf files separately for each instance.
    Manage them individually, which is complex and time-consuming.

With Docker (The Magic Solution):
    Using Docker, we skip all that complexity!
You can run multiple NGINX instances with just one simple command: 
What we just did.

docker run -d -p 8080:80 --name nginx1 nginx
docker run -d -p 8081:80 --name nginx2 nginx
    ✔ No need to copy the software multiple times
    ✔ No manual configuration conflicts
    ✔ No dependency issues with the host machine
✔ Easy to start, stop, and manage everything with commands

This is the true power of Docker!

It allows us to run multiple apps efficiently, without worrying about underlying host dependencies.
Now, you can deploy and manage multiple services effortlessly—all with just a few Docker commands.
That’s why Docker is a game changer in software deployment!



17:50
=====================
Docker containers run in an isolated environment, meaning:
✔ Each container works independently and doesn’t affect the others.
✔ Containers are isolated from the host machine as well.

As long as the host machine has Docker Engine running, multiple containers 
(like NGINX or others) can operate without interfering with each other.

In this video, we won’t dive deep into Docker's internal architecture just yet, but don’t worry!
We’ll explore how Docker manages containers in more detail in the Docker In-Depth Section.


Right now, we’ll keep our focus on the basic Docker commands to get you 
comfortable with docker.




18:40
===========================
Docker Exec Command: Interact with Running Containers
The docker exec command allows you to enter a running container and execute operations inside it. 
This is super helpful for tasks like:
✔ Installing packages
✔ Updating files
✔ Modifying configurations (e.g., updating NGINX pages)

How Does It Work?
    To interact with a container, we use the -it flag, which stands for:
    i: Interactive mode
    t: Terminal, so we get a terminal session inside the container

We’re going to change the default NGINX page running on port 8282.

Enter the container using the docker exec command.

We can see the container is running with a minimal Linux distribution, 
so some packages (like vi or nano) might be missing.
We’ll use basic commands like cat and echo to interact with the container.

Enter the running container using the docker exec -it <container_id> bash command.
Inside the container, we’ll run the command cat /etc/os-release to check the Linux distribution it’s running.

Next, we will update the default NGINX homepage (located in the default directory) 
using the echo command (because text editors like vi aren’t available).

Exit the current container running on port 8282, and enter the container running on port 8181.

Check the container ID, and use it to enter the new container with docker exec -it <container_id> bash.
Update the NGINX page by running echo 'Welcome to My Custom NGINX Page' > /usr/share/nginx/html/index.html 
to change the homepage content.
Results:
Once you’ve updated the page, you can verify it by checking the contents of the index.html 
file again with the cat command.
If you refresh your browser, you’ll see the updated NGINX homepage! 🌐


25:50
===============
Now we explore How to Check Container Logs
When a container is running an application, it often produces logs. 
You can easily view these logs using the docker logs command. 
This is useful to monitor the application's behavior or debug issues.

Find the Container ID – First, check the container ID by running the command:
docker ps
Use the Docker Logs Command – Once you have the container ID, you can view the logs with:
docker logs <container_id>

What’s Next?
We’ll explore more about viewing and managing container logs in detail in the container section. 
But for now, just know this basic log-checking command!
Stay tuned for more advanced log monitoring techniques!


26:35
=====================
Lest see docker netowrk now:

docker network ls
This will show you a list of the networks available. 
As we haven’t created any custom networks yet, you’ll see the default ones.

What is a Docker Network?
Docker networks allow containers to communicate with each other. 
Containers that are connected to the same network can easily interact with one another, 
which is essential for multi-container applications.

We will dive deeper into Docker networks in a dedicated section, so don't worry if it’s not clear yet!


28:10
========================
Docker Volumes:
We can view the existing volumes using the following command:
docker volume ls
Since we haven't created any volumes yet, this command won’t show any volumes at the moment.

dockr login/logout To push images to Docker Hub (Docker's cloud registry), you need to log in first:

Prune Unused Images:
You can remove unused images using the following command to clean up your system:

Building Docker Images:
The docker build command is used to create custom images from a Dockerfile. 
We'll cover this in more detail in the Images and Containers section.


Pushing Images to Docker Hub:
To upload an image to Docker Hub, you use the docker push command. Make sure you're logged in first:

In the next video, we'll explore Docker's internal architecture in greater detail, 
including how Docker manages containers, images, networks, and volumes.






##4) Docker in Depth 
====================================

00:00-to-1:50
=========================
Hey guys! Welcome back to the channel. In today's video, we're diving deep into Docker!

We'll break down Docker’s internal architecture, covering:
    Docker Architecture – How everything fits together
    Docker Components – The key building blocks
    Namespaces – How Docker isolates processes
    Control Groups (cGroups) – How resource limits work

By the end of this section, you’ll have a solid understanding of how Docker works under the hood!
Let’s get started! 


First up—Docker Architecture!
Docker actually works in a client-server model.

When we run a Docker command using the CLI, the CLI acts as the client, and the Docker Engine is the server.
Here’s how it works:
🔹 We enter a command in the CLI (Client)
🔹 The Docker Engine (Server) receives the command, executes it, and returns a response
🔹 The response is displayed on the CLI (our black screen)

But that’s not all!
The Docker Dashboard is also a client.
It fetches data from the Docker Engine and displays it in a graphical way.
So, whether it’s the CLI or Dashboard, everything talks to the Docker Engine behind the scenes! 




1:50-to-3:00
======================
We can use the Docker Client locally or from any remote CLI to communicate with the Docker Engine, no difference! 




3:00-to-3:00
=========================
Docker Components:
Docker has 5 key components:

🔹 Docker Daemon – Creates containers, manages volumes, and handles execution
🔹 Docker Client – Sends commands to the Docker Engine (server)
🔹 Docker Image – A read-only blueprint for creating containers
🔹 Container – A running instance of a Docker image (we’ll dive deeper in later videos)
🔹 Docker Registry – A repository for storing and sharing Docker images

Docker Hub & Private Registries
 Docker Hub is a public registry with official images.
 Companies can also set up private registries for internal use.

Here’s my Docker Hub account, where I’ve pushed some custom images.
You can also push and share your own images!

Explore Docker Hub, it has great documentation for each image.
For example, the MySQL official image page provides all the details on how to use it! 




09:10-to-17:00 
===========================
Now we’re diving into two fundamental concepts in containerization: Namespaces and cGroups.
These are key technologies that make containers possible, and understanding them will give you a deeper insight into 
how containerized applications work. 
So, let’s get started!


What is a Namespace?
Imagine you have multiple applications running on the same system, but each one needs to think it’s the only application running. 
That’s where Namespaces come in!

A Namespace in Linux is like a virtual wall around a process or a container. 
It isolates resources, so one process doesn’t interfere with another.

Here’s how it works:
    When you launch a container, it runs in its own Namespace.
    It gets a unique process ID (PID) and doesn’t see processes running in other Namespaces.
    This isolation allows multiple containers to run from the same software image without interfering with each other.
    Think of it as each container having its own little sandbox, where it operates independently without affecting other 
    processes on the system.



What is a cGroup?
Now that we know how containers stay isolated, let’s talk about cGroups or Control Groups.
A cGroup is responsible for managing resources like CPU, memory, and disk I/O for a container.

Here’s why it’s important:
    Without cGroups, one container could use too much CPU or memory, affecting the entire system.
    With cGroups, you can limit and prioritize resource usage for each container.
    This ensures fair distribution of system resources and prevents one container from overwhelming the server.
    Think of cGroups as a traffic controller that decides how much CPU, memory, and other resources each container gets.


Real-World Example
Imagine a host machine where we allocate 50% of resources for Application A and 50% for Application B.
With cGroups, we can define how much CPU, memory, and other system resources each application is allowed to use.

If we don’t set resource limits, then when Application A experiences high traffic, it may consume most of the available resources.
As a result, Application B and other applications on the same host might not get enough resources, leading to performance issues.

By using cGroups, we ensure that Application A cannot take more than its allocated quota,
allowing fair resource distribution and preventing any single application from overwhelming the system.


How do Namespaces and cGroups Work Together?
    Together, Namespaces and cGroups provide:
    Isolation (via Namespaces)
    Resource Control (via cGroups)
This combination allows multiple containers to run efficiently on the same machine while ensuring stability and performance.






17:00 -to - End
===========================
This is an overall idea of Docker architecture and components—how Docker actually works.
We won’t spend too much time here, just a high-level view of Docker’s internal components.

When we run a Docker command using the CLI, Docker’s internal server receives the command
and instructs the Docker engine to run the specified image as a container.
If the container is not already present in Docker, it pulls the image from the registry and then runs it.

In the next video, we’ll explore image and container operations—how to build an image,
run it as a container, create and modify custom images, and upload them to Docker Hub.


That’s it for today’s video! If you found this useful, don’t forget to like, subscribe, and hit the notification bell 
for more tech insights. See you in the next video!






##5) Docker Image and Containers
======================================

00-3:00
=============
Hey guys, in this tutorial, we’re going to explore image and container operations.

Image
An image is an executable package that includes everything needed to run a specific application, like Nginx.
We can download pre-built images for Java, MySQL, Redis, etc., or create our own custom images.

An image is essentially a template for containers.
For example, we can create two Java app containers from a single Java image
or three Redis app containers from a single Redis image.
This shows how we can reuse images to create multiple containers as needed.


Docker Hub Registry
Now, let’s take a look at Docker Hub.
Here, we can find a vast collection of official and custom images.
We can also upload our own images, which we’ll do in just a moment.

Public images can be stored in a public registry like Docker Hub,
but if a company needs a private registry, they can set one up and share images with their team to run applications securely.



3:00--
====================
Now, let’s explore some frequently used Docker image commands.
Listing Docker Images
dokcer image ls
This command lists all images available on your current Docker host.

Repository: The name of the image repository.
Tag: This represents the image version. Let’s check Docker Hub to explore tags.
For example, let’s search for the Ubuntu image.
We can see different tags listed—each representing a specific release version.


Check a image, ubuty.
see the show ubutn image tag this is actually  thsi is the release version.

If no tag is specified, Docker will pull the latest tag by default:
However, the latest tag does not always mean the most recent version.
For example, if we run:docker run ubuntu:latest

It may run Ubuntu 22.04, even though the latest release might be 23.xx.
The latest tag is just a label assigned to a specific version, not necessarily the newest release.
You can check the official Docker Hub image documentation to verify the correct tags.


Now, let’s check one of my custom images, which I built and released with different versions as tags.
In real-world projects, this is how you manage application release versions using tags.

When running docker image ls, we see:
Image ID: A unique auto-generated identifier for the image.
Created Date: When the image was built.
Size: The disk space used by the image.




10:20
=====================
Now, let's explore base images, image tags, and how to build an image.

1. What is a Base Image?
A base image is the parent image for all official and custom images.
Every Docker image is created from another image called a base image.

2. Understanding Dockerfile
Next, let's explore Dockerfile—a file that defines what an image is and how it works.
Docker images follow a layered architecture, and the Dockerfile shows how an image is built layer by layer.

3. Exploring a Dockerfile
Let’s check the Nginx Dockerfile from Docker Hub.
In the first line, we see the FROM directive: FROM debian:bullseye-slim

This defines the base image used to create the Nginx image.
Every image has a parent image from which it is derived.

Now, let’s explore another image—MySQL.
Opening its Dockerfile, we see: FROM debian:bookworm

This means the MySQL image is built on top of Debian (Bookworm) as its base image.

4. How Images Are Built
All images are created on top of another image, forming a chain of dependencies.
The FROM directive in a Dockerfile always points to the base image used to build a new image or application.





13:20
=====================
Now, let's see some examples of custom Docker images.

1. Choosing a Base Image for Custom Images
Just like official images, custom images also require a base image, which we define using the FROM directive in a Dockerfile.

But do we choose a random base image? No!
We need to select a base image that is relevant to our custom image.

For example:
If we are building a Java application, we should choose a JDK image with a specific version:
FROM openjdk:17

This base image already includes Java, so we don’t need to install it manually.
On the other hand, if we choose a Ubuntu image:
FROM ubuntu:22.04
Then, we must manually install JDK and configure our Java application.
2. Choosing the Right Base Image
We should always pick a base image that:
✔ Matches the type of application we are building.
✔ Meets the system requirements of our app.
✔ Aligns with business logic and optimizes performance.

Choosing the correct base image helps reduce setup effort, improves efficiency, and keeps our images lightweight.




14:10
==========================

Now, let’s explore the Scratch image and the FROM scratch tag.

1. What is the Scratch Image?
The Scratch image is the minimal base image used in Docker.
It is essentially an empty image—no operating system, no packages—just the bare minimum.

Every Docker image is built layer by layer, starting from the Scratch image.

2. Exploring MySQL Docker Image
Let’s explore the MySQL Docker image.
Looking at its Dockerfile, we see it is built on top of the Oracle Linux image.
This means the Oracle Linux image is the base image for MySQL.

Now, let's check the MySQL image tag, which is 8-slim.
When we open this tag, we find that it is still built on the Scratch image at its core.

3. How Images are Built Layer by Layer
All images are built on top of another image.
For instance:

The MySQL image starts with Oracle Linux.
Then, it builds on top of that, adding necessary components.
Finally, it adds the MySQL application.
At the very bottom of every image, there is Scratch—the most minimal layer.
On top of Scratch, we install packages, add files, and run commands to build up the image layer by layer.

This is how Docker images are constructed—step by step, layer by layer.




If you don't want to use ready-made images like the official Nginx image, you can also build your own image layer by layer.

1. Creating Your Own Nginx Image
For example, let’s say we want to create an Nginx image using a Linux image as the base.
The process is similar to setting up a traditional host.

On a physical machine, we install a Linux OS and then manually install packages like Nginx.
In Docker, we can start with a Linux base image and manually install Nginx, making it ready to use.
This way, we can customize the image to suit our needs.

2. Scratch Image and Layering
When you start with the Scratch image (the bare minimum), you can install any server or package you need.
Each line in the Dockerfile represents a layer of the image.

Each layer is added step-by-step, building up the image, just like you would on a traditional system.
You can read more about Scratch and layering in its documentation.





23:00-end
===============
Let’s take a closer look at the details of a Dockerfile.

1. Understanding a Dockerfile
A Dockerfile is a combination of instructions and arguments that define how an image is built.

Here’s an example of a Dockerfile:

It starts with a base image like Ubuntu.
Then, it updates the package list.
Next, it installs a package like Nginx.
It also defines the default port for the container.
Finally, it includes a run command to start the application.
2. Building an Image from the Dockerfile
Once you’ve written the Dockerfile, you can build your image using the following command:
docker build -t image-name:tag .
This will create a Docker image with the specified name and tag based on the instructions in the Dockerfile.



Now let’s see how we can build a Docker image from a Dockerfile in practice.
1. Preparing the Index File
First, I need an index.html file, so I’ll create it in the same location as the Dockerfile.

2. Checking Current Images
Let’s check the current list of images on my Docker host.
To do this, run the command:
docker image ls
3. Building the Image
Now it’s time to build our first image!
Run the following docker build command:
docker build -t image-name:tag .
I’ll also add a screenshot of this process in our document.

4. Verifying the Build
Once the build is complete, let’s verify it.
Use the docker image ls command again to check the list of images.
You should see our newly created image in the list.


Now, it's time to run our newly built image as a container.
1. Running the Container
Once the image is built, we can run it using the docker run command.
After running it, let's check the container.

2. Checking in the Browser
We can open a browser and verify the container is running.
This was the default Nginx image earlier. Now we’re running our custom Nginx, and you should see the custom default page.

3. Inspecting the Image
If we want to see more details about the image, we can use the docker inspect command.
This will give us a deep dive into the image's metadata.

4. Saving the Image
Now, let’s say we want to share this image as a physical file.
We can save it as a .tar file using the docker save command:
docker save -o myimage.tar image-name:tag
This generates a .tar file that we can share with others.

5. Docker Inspect on Container
Just like images, we can also inspect containers.
To inspect a container, we can use docker inspect with the container ID to get detailed information about the container’s configuration and status.

6. Viewing Containers
To see all running containers, use the docker ps command.
For all containers (including stopped ones), use:
docker ps -a






=================================================
##6) Storage and Volumes
=================================================
In this tutorial, we will be discussing Docker Storage and Volumes.

By default, all files created inside a container are stored on a writable container layer.

The Problem with the Default System:
Data Persistence: The data doesn’t persist when the container no longer exists.
Tightly Coupled to the Host Machine: The container’s data is tightly bound to the host’s file system, making it difficult to 
manage independently.
Reduced Performance: Storing data in the container itself can affect performance, especially when containers are 
constantly created and removed.

Next, we will explore how volumes can solve these problems and make data management more efficient in Docker.




3:00
==================
To solve the data persistence issue, Docker provides two primary ways of managing and persisting data in containers: Volumes and Bind-mounts.

You can find more detailed information on these in Docker’s official documentation.

These are the two main storage systems in Docker:

1. Volumes:
Volumes are stored inside Docker and can only be accessed by Docker containers, not by the host processes.
Volumes are the recommended approach when you want to keep container data secure and isolated from the host machine.
Docker suggests using volumes as the best practice for managing container data.

2. Bind-mounts:
With bind-mounts, a file or directory is managed on the host machine, and non-Docker processes can modify it.
Bind-mounts are useful when you need to perform analysis or allow other host machine processes to access data 
generated by the container, such as logs or reports.
Use bind-mounts when you want to interact with container-generated data outside the container or if other processes on the host
 need access to it.

3. tmpfs:
tmpfs allows Docker to store data in the host machine's memory instead of the disk. 
It’s the fastest and most secure option for temporary data storage.
tmpfs is ideal when you need high-performance, non-persistent storage that doesn’t require saving data after the container stops.






9:20mm
============================
Now, let's start with the basic Docker volume commands and practice with volumes.

Example: Using Volume with an Application Server (Nginx)
Let's start with Nginx and see how we can use volumes with an application server.

Run the application:

My application is running now. Let’s modify it to update the default Nginx page.
Update the Default Nginx Page:

We need to exec into the container to update its welcome HTML page.
Use docker exec -it <container_name> /bin/bash to enter the container.
Inside the container, vi is not available, so I’ll use the echo command to update the page.
echo "Welcome to My Custom Nginx Page!" > /usr/share/nginx/html/index.html
Once we update the page, we refresh the browser, and we can see the update reflected on the page.
What happens when the container crashes or stops?

Manually stop the container and restart it.

Observation:

This time, the default Nginx welcome page shows up again, and we lose the update we made just a few minutes ago.
This happens because the data was stored inside the container's filesystem. When the container stops or crashes, the data inside is lost. This is the default behavior of Docker containers.
To persist the changes made to the Nginx page, we can use Docker volumes.

We’ll see how Docker volumes work in the next steps.


16:00
============================
Now, let's fix the data loss issue by using Docker Volumes:

Steps to Fix Data Loss with Docker Volume
Check for Existing Volumes:

First, let’s check the existing volumes by running:
docker volume ls
As we haven’t created a volume yet, the list will be empty.
Create a Volume:

Now, let’s create a Docker volume using the following command:
docker volume create my_volume
Verify the Volume:

Once the volume is created, run docker volume ls again to see the volume we just created:
docker volume ls
You should see my_volume in the list of volumes.
Run Nginx with the Created Volume:

Now, let’s run the Nginx container and mount the created volume to it. Use the following command to do this:
docker run -d -p 80:80 -v my_volume:/usr/share/nginx/html nginx
Here, -v my_volume:/usr/share/nginx/html tells Docker to mount the my_volume to the /usr/share/nginx/html directory inside the container.
Check the Home Page:

Now, you can check the Nginx homepage in your browser. It should display the default Nginx welcome page.
Update the Nginx HTML Page Again:

Let’s update the index.html page again, but this time the changes will be persisted even if the container is restarted.

Exec into the container to update the HTML page:
docker exec -it <container_name> /bin/bash
Update the index.html file using echo:
echo "Welcome to My Custom Nginx Page!" > /usr/share/nginx/html/index.html
Verify the Update:

You can verify the update by using the cat command to see the contents of the index.html file:
cat /usr/share/nginx/html/index.html
Once the update is made, refresh your browser, and you should see the updated page as expected.
Persist Data with Volume:

If the container stops or is restarted, the changes to the index.html page will persist because the data is stored in the Docker volume (my_volume).
Docker Desktop - View the Volume and Container:
In Docker Desktop, you can also see the container and the volume being used.
Now we have successfully solved the data loss issue by using Docker volumes.





Let's assume the application crashed, and you stopped and deleted both the container and the image. In Docker Desktop, you can see that there is no container or image listed.
You can also verify this by running the following command:
docker ps -a  # to list all containers
docker images  # to list all images
Volume Remains Intact:

Even if the container or image is deleted, the volume will still persist. You can check the volume by running:
docker volume ls
The volume we created earlier (my_volume) will still be listed.
Re-run the Application with the Same Volume:

Now, let's pull the Nginx image again and run a new container with the same volume. You can run the following command:
docker run -d -p 80:80 -v my_volume:/usr/share/nginx/html nginx
Access the Updated index.html:

Once the container is up and running again, you can go to the browser and check if the previously updated index.html page is displayed (the one we updated before the crash).
The volume ensures that the data is preserved even after container restarts or deletions.
Run Nginx Without the Volume to See the Default Data:
Run Nginx Without Mapping the Volume:

If you run a new Nginx container without mapping the volume, it will show the default Nginx page because it is not using the previous volume that stored the updated index.html file.
docker run -d -p 80:80 nginx
Result:

Since we didn’t use the volume (my_volume) this time, the container is using the default Nginx data, and we won't see the updated index.html page.
Conclusion:
Docker Volumes ensure that data is persistent even after the container is deleted or crashes. By reattaching the volume to a new container, you can recover the previous state (e.g., the updated index.html).
Without the volume, Docker containers will use their default data and lose any changes made previously.




27:30
==================================

[Intro]

Hey everyone! Welcome back to the channel!

In this video, we're going to dive into Bind Mounting in Docker, and also take a quick look at Tmpfs, another mounting mechanism.

If you’ve been using Docker, you might have heard of mounting volumes, but bind mounting has a key difference. Let’s break it down and explore how it works.

[Section 1: What is Bind Mounting?]

So, what exactly is Bind Mounting?

With Bind Mounting, a file or directory on the host machine is mounted directly into a Docker container.
This allows you to share files and data between the host machine and the container.
It’s super useful when you need to update or manage files on your container from your local machine directly.

[Section 2: Syntax of Bind Mounting]

Here’s the syntax for bind mounting:

You can use the -v flag for a quick bind mount or the more explicit --mount option.
Let’s take a look at both approaches:

For Linux and Windows systems, the command looks like this:

Using -v (short version):
docker run -d -v /path/on/host:/path/in/container nginx
Or, the more explicit version using --mount:
docker run -d --mount type=bind,source=/path/on/host,target=/path/in/container nginx
The --mount approach is the recommended method because it’s more explicit and easier to understand.

[Section 3: Trying Bind Mounting in Action]

Let’s see Bind Mounting in action!

We’re going to start by cleaning up our previous container:
docker rm -f <container_id>
Next, we’ll update our HTML file on the host machine and use bind mounting to reflect those changes directly into the Docker container.

Here’s how we do it:

Update the index.html file on our host machine.
Let’s add a simple change, like modifying the welcome text.

Run the Docker container with the bind mount:
docker run -d -v /path/to/host/index.html:/usr/share/nginx/html/index.html nginx
What we’ve done here is mount the file from our host machine to the container.

[Section 4: Verifying the Changes]

Now, let’s open our browser and refresh the page.

As you can see, we’ve got the expected update! The change we made to the file on the host machine was reflected directly inside the container.

That’s the magic of bind mounting!

Now, let’s update the index.html file once again on the host machine.

We’ll make a small tweak to the content, save it, and refresh the browser again.
And boom, we see our new update immediately in the container as well!

[Section 5: Key Difference from Volume Mounting]

So, the key difference between bind mounting and volume mounting is that with bind mounting, we’re directly sharing and modifying a file or directory from the host machine to the container.

This allows for real-time updates from the host, which is perfect when you need to change application files frequently or need external processes to access the files being used by the container.

[Section 6: Tmpfs Mounting]

Now, let’s briefly touch on Tmpfs, another mounting option in Docker.

Tmpfs allows you to store data in memory instead of on disk. It’s a volatile storage solution, meaning that the data won’t persist if the container stops or crashes.

Here’s the basic syntax for Tmpfs mounting:
docker run -d --mount type=tmpfs,target=/path/in/container nginx
Tmpfs is generally rarely used and can be handy in specific scenarios where performance is critical, and the data doesn’t need to persist.

[Outro]
So, that’s an overview of Bind Mounting and Tmpfs in Docker.

Bind mounting lets you share files between the host machine and the container, making it easy to update files in real-time.
Tmpfs provides temporary storage in memory for situations where you don’t need data to persist.
Feel free to explore both and let me know if you have any questions in the comments!

If you found this tutorial helpful, don’t forget to like, subscribe, and hit the bell icon for more Docker tips and tricks. Thanks for watching!

See you in the next video!




34:30
=============================
 we'll explore Docker Volumes and how they can be used to persist data, specifically using a MySQL database as a real-world example.

In this lecture, we'll cover why Docker containers lose data by default and how volumes can help solve this problem by storing data outside of the container's filesystem.

[Section 1: Understanding the Problem - Data Loss in Containers]

By default, Docker containers are ephemeral, which means that any data inside a container is lost once the container stops or is deleted. This behavior works great for temporary data, but it’s a huge problem when you need to store persistent data, such as a database.

Let’s take a simple MySQL container as an example. If we run a MySQL database inside a container and insert some data, once we stop or remove the container, all the data inside that container is gone. So, if your MySQL container crashes or you delete it for some reason, you lose all the database records.

This is a problem if you want to store important data like customer records, transactions, or any type of persistent database information.

[Section 2: The Solution - Docker Volumes]

The solution to this problem is Docker volumes. A Docker volume is a special kind of storage that is managed by Docker itself, separate from the container’s filesystem. Volumes allow you to persist data even when the container stops, crashes, or is deleted.

Instead of storing data inside the container, Docker volumes are stored on the host system. This means that even if a container is removed, the volume persists, keeping your data safe.

[Section 3: How Docker Volumes Work]

When you use volumes with a container, you mount the volume to a specific path inside the container. For example, in the case of a MySQL container, you would mount a volume to the path where MySQL stores its data (/var/lib/mysql). This way, MySQL’s data is saved in the volume instead of inside the container.

Once a volume is created and linked to a container, you can stop, remove, and recreate the container without worrying about losing your data. The volume remains intact, and the next time the container starts, it will have access to the same data.

[Section 4: Example Scenario - MySQL with Docker Volumes]

Imagine you are running a MySQL container, and you insert some data into the database. Without a volume, this data would be lost if the container is removed. But with a volume, the data is safely stored in the volume and can be reused even after the container is deleted and recreated.

To make this work, you create a Docker volume and mount it to the container’s data storage directory. Now, any changes you make to the database, like adding or modifying records, are stored in the volume, ensuring that the data persists.

When the container is stopped or deleted, you simply run a new container, mount the same volume, and the database will continue with the previous data intact.

[Section 5: The Benefits of Docker Volumes]

The benefits of using Docker volumes include:

Persistence: Data survives even after the container is deleted or crashes.
Isolation: Volumes are separate from the container’s filesystem, reducing the risk of accidental data loss.
Portability: You can easily back up or migrate volumes across different environments or machines.
Data Sharing: Volumes allow multiple containers to share data without worrying about data duplication or file conflicts.
[Section 6: Comparing Volumes with Other Methods (Bind Mounts, tmpfs)]

There are other ways to manage data in Docker, such as bind mounts and tmpfs. However, volumes are typically the most reliable and efficient way to persist data.

Bind Mounts: These link a specific file or directory from the host system to the container. While they are useful for development or when you need access to host data inside a container, they are not as secure or portable as volumes.
tmpfs: This is an in-memory filesystem that is mounted inside the container, and data is lost when the container stops. It’s the fastest but not suitable for persistent data.
Volumes are recommended because they offer persistence, security, and flexibility.

[Outro]

In summary, Docker volumes provide a solution to the problem of data loss when containers are removed or crash. By using volumes, you can ensure that your database and other important data are safe and persistent.

I hope this lecture helped you understand how Docker volumes work and why they are so important. If you found this information useful, be sure to like, subscribe, and click the bell icon for more tutorials.

Thanks for watching, and I’ll see you in the next video!








we'll explore how to persist MySQL data using Docker Volumes and Bind Mounts. 
We'll cover how each method works and how you can keep your data intact even after container deletion.

[Section 1: Docker Volumes with MySQL]

Create a Volume: First, create a volume called mysql-data to store MySQL data. You can check if it’s created by listing the volumes with a simple command.

Run MySQL with Volume: Now, run a MySQL container while mounting the mysql-data volume. Once it’s running, log into the database, create a table, and insert some data.

Verify Data Insertion: Retrieve the data by running a SELECT query to ensure that everything is in place.

Volume Persistence: Docker volumes keep the data separate from the container, meaning if the container is deleted or crashes, the data is safe. You can see the volume details in Docker Desktop.

Delete and Restart the Container: Now, stop and delete the container. Afterward, restart the container with the same volume attached.

Verify Data After Restart: Use the same methods to check that your data is still intact after restarting the container. This proves that the data has been successfully persisted with Docker volumes.

[Section 2: Bind Mounts with MySQL]

Use Bind Mount: Next, let’s use a Bind Mount. Choose a directory on your host machine and bind mount it to the MySQL container.

Run MySQL with Bind Mount: Run the container with the host directory mounted to the container. This way, the container directly interacts with the files on the host machine.

Insert Data: Just like before, insert data into the MySQL container.

Verify Data: After inserting data, check the host machine’s directory. You should see the MySQL data being written directly to that path.

Delete and Restart the Container: Stop and delete the container again, and then run the container with the same bind mount to verify data persistence.

Check Data After Restart: After restarting the container, check the data to ensure it remains intact. This confirms that the data is properly persisted with bind mounting.

[Conclusion]

Today, we’ve seen how Docker Volumes and Bind Mounts help us persist data in containers. While volumes are typically the preferred method for most cases due to better separation of concerns, bind mounts give you direct control over the host machine’s file system. Both methods ensure your data is safe even when containers are deleted or restarted.

Thank you for watching! Don’t forget to like, subscribe, and click the bell for more tutorials!

This streamlined version should help you focus on the key concepts and steps for your viewers.






=================================================
##7) Networking and Security
=================================================
[Intro]

Welcome to today’s tutorial on Docker Networking and Security. We'll dive into the concept of Docker networking, its importance, and how it supports multi-tier applications and security.

[Section 1: What is Docker Networking and Why Is It Important?]

What is Docker Networking? Docker networking allows containers to communicate with each other. This becomes crucial when building complex applications, especially multi-tier applications, where different services need to interact. For example, an app server needs to communicate with a database server or other services.

Why Do We Need Docker Networking? Docker networking helps maintain secure and isolated communication between containers. In real-world applications, where different components of the application (like the frontend, backend, and database) run in different environments, Docker ensures they can securely talk to each other.

Security Considerations: Networking in Docker also plays a major role in security. For example, you might want the database to only be accessible by the app server and not by the public network. Docker networking lets you define these boundaries, keeping your services secure while ensuring they can still communicate effectively.

[Section 2: Three-Tier Application Example]

Three-Tier Application: Let’s consider a three-tier application:

The frontend app is deployed in a public network.
The app server is deployed in a secure network.
The database server is placed in another secure network with restricted access, allowing only the app server to communicate with it.
Docker’s Role in Security and Networking: Docker networking gives you the ability to manage and segregate these different tiers, controlling which services can access each other. This helps ensure that, for example, the database can’t be accessed from the public network, providing an additional layer of security.

[Section 3: Basic Docker Network Commands]

Default Network: Docker provides a default network called the bridge network. When you run a container and don't specify a network, it will automatically connect to this bridge network.

Viewing Networks: You can view Docker’s networks using the docker network ls command. This shows all available networks, including the default ones.

Inspecting Containers: To get more details on how a container is connected to a network, use the docker inspect command. 
This command will show you detailed information, including the network settings and which network a container is using.


[Conclusion]

Docker networking is a fundamental part of building scalable and secure applications. 
It enables communication between containers while providing mechanisms to isolate and secure different components of an application. 
Whether you're building a multi-tier app or securing your database, Docker networking helps you manage both.



9:30:
=================
[Intro]

Welcome back! In today’s tutorial, we’ll be creating our first Docker network. We’ll also explore different network drivers and how they work with Docker containers.

[Section 1: Creating Your First Docker Network]

Creating a Network: To create a Docker network, you simply use the docker network create command. By default, if you don’t specify a network driver, Docker will create the network using the bridge driver.

What is a Network Driver? A network driver is a way Docker manages the networking configuration for containers. It determines how containers can communicate with each other and the host machine. In this tutorial, we'll start by exploring the bridge network driver.

[Section 2: Exploring the Bridge Network Driver]

Bridge Network Details: The bridge network is Docker’s default network driver. When you create a container without specifying a network, it automatically connects to this bridge network.

Inspecting the Network: After creating the network, we can inspect it using the docker network inspect <network_name> command. This will show the details of the network, such as the subnet, gateway, and the containers connected to it.

Custom Subnet: When you create a network, Docker automatically assigns a subnet for it. However, you can specify a custom subnet if needed. This allows you to define a specific range of IPs that your containers can use, providing better control over your network configuration.

Unique IPs for Containers: Containers connected to the bridge network are assigned a unique IP address from the network's subnet. This ensures that each container can be individually addressed within the network.

[Section 3: Host and None Network Drivers]

Host Driver: The host driver connects the container directly to the host machine’s network. This means the container will share the same network namespace as the host machine, and you can access it using the host's IP address. The container won’t have its own IP; it uses the host machine’s IP.

None Driver: The none driver essentially isolates the container from any network. Containers using this driver have no network connection at all and can’t communicate with other containers or the host machine.

[Conclusion]

That’s it for today’s tutorial! Now you know how to create Docker networks and the different network drivers available. We’ve covered the bridge, host, and none drivers, and how each of them affects container communication.








14:30
===================
[Section 1: Viewing Container IPs]

Checking a Container’s IP: To see a container’s IP from the inside, you can use the docker exec command. This allows us to enter the container and check the IP address it has been assigned.

Communicating Between Containers: If both containers are on the same network, they can communicate with each other using their IP addresses.

[Section 2: Creating and Using a New Network]

Creating a New Network: Now, let's create a new network and run containers within this network. When running a container or service with a specific network, we need to specify that network using the docker run command.

Running Containers on the New Network: Let’s run a new container, for example, a backend app container, connected to our newly created network. Now, both the backend app and the database container will be in the same network.





20:00
==============
[Section 3: Testing Communication Between Containers]

Testing the Network Connection: Now that both containers are on the same network, we can test the connection between them using curl commands. This ensures that the containers can communicate properly with each other.

Deleting and Recreating Containers: Next, let’s delete the backend container and run it again, but this time, connect it to the same network as the database container. This way, the backend container and the database container will be able to communicate again.

Verifying Communication: After the containers are recreated and connected to the same network, we can use docker exec to check if they can reach each other. The communication should now be successful.



29:00
============================
[Section 4: Host Network and Overlay Networks]

Host Network: The host network is rarely used, but it allows a container to share the network namespace with the host machine. 
In this mode, the container does not have its own network stack, and it uses the host’s network for communication.

Overlay Network: The overlay network is used for clustering multiple Docker nodes, such as in a Docker Swarm. 
The overlay network enables communication between containers on different Docker nodes within the cluster. 
It ensures that containers across multiple machines can communicate as if they were on the same network.




32:30:
=====================
[Section 5: Creating a Custom Network with a Subnet]

Creating a Custom Subnet: In previous steps, we used Docker's default subnet. Now, let's create a network with a custom subnet. This allows us to define our own range of IP addresses and gateway for the network.

Inspecting the Network: After creating the custom network with a specified subnet, we can inspect it to view the network details, such as the assigned subnet, IP range, and gateway.

[Section 6: Running Containers on a Custom Subnet]

Running a Container on the Custom Subnet: When we run a container on the custom subnet, it will be assigned an IP address from the defined subnet.

Verifying the Container IP: Let’s run the container and check its IP using the docker inspect command. The container should receive an IP address from our custom subnet, and we’ll confirm it by inspecting the container’s network settings.

[Conclusion]

That wraps up this tutorial on Docker networks. We’ve learned about bridge, host, and overlay networks, how to create custom networks with subnets, and how containers can communicate with each other. Docker networking is essential for organizing your containers and managing communication between them.

Thanks for watching! If you found this tutorial helpful, please like, comment, and subscribe for more Docker tips and tutorials.

This script explains Docker networks, creating custom subnets, and testing container communication in a 
clear and structured way for your audience.











=================================================
##8) Simple Example Project
=================================================

This is well be handon tutorial where we going to roun bunch or project using docker.
and Here is the project source gitHub link, you cna get the source code from here also ths docuemn.
What is i use on this tutorial for help.
You cna make a git colne or may download directly.


So let see how we cna dow git clone and directly zip download.


1:30
==============
In very frisrt we now going to run a php projectr using docker.
Lets see how we cna do it.

In thsi project drectory there is a only one file index.php to macke it simple.
Just like a HelloWorld project.

Lets see the docker file how it looks like:
Here jsut a from tag for get php runtime enviroment, then a copy comand wht is copy file from this location to a docker directory
and Finay expose a port.

4:10
=====================
Now we goitng to build our php project image with the command docker build image.
Now we run this image with docker run comand, lets Runn the command.

Go to browser and check 8080 port.
Yes we get it ! we get our expected resutl.

In this case here only a single file and defanntly your project comw with many file, dont same projectss.


7:40
==========================
We know that a multip tier like Three tier applicaotn run frontend backend and databse differently.

So we going to run same a three tier application, with frontend, badk end and databse.
and All together will run entier application. 

In this case frontend have to communticaton with backend and backeend havd both with database and fronend also.
So we need a netwrok to resolve tis issue.

First check the what network we already have.

Befor we goitn to create our application I have need clear a issue with defautl netwrok.
the issue defautl bridege nework dosnot come with internal dns resolver, until is create a custome one.

Lets check it our in protically to run tow contienr.
Here we can do make communitcation using ip address but not by contienr name, what is need intenral dns resolver.


When we create a custome network it come with docker internal dns resolver.
and this way we cna use contaienr anem as dns to communticton instead of ip address.


Then we call other serive by its name like this instead of IP address.

yes on the document say this that defautl and usier-define network difference.




19:20
=====================
So we start with create a user-define netwrok with custome subnet, lets run a create command.
Run it
Take a screenshot and add it in the docuemtn.
Run done, so now for testing perpose I amd going to run againt two nginx contienr we this ntwerok.
My new conteinr  is ng3 and ng4.

Lets instpect then ntwork to chck the current status.
so oru ng3 and ng4 contienr runnign with this newtork.


so lets echk the is it possible to make communtit contienr to contienr using dns.
First lets check using ip againg and then dns.
Now usign dns, and yes grate ! we get it we can able to conenction with dns.


So wha we get here, for dns resoluation we have to use user-define network not defautl one.


We go to out three tier application, befor the we ging to clean and remove oru current all docker contienr.


Lwts echk what oru three tier applicaotn will looks like, here good show one.




26:10
=====================
First we goiny to run our database server as a container as:
This is then comand you cna copy it from thsi docuemtn and run it.
You cna also chedk the docker hub mysql image ducument ehat is infor need to run this mysql image as a server.


mysql service run done !
Do mysql contienr exec and run mysql command with user credentials.

Chek database, use one, her we use simpledatabase.

then create a table, here is table creation mysql command, and run it.
So now our mysql database server ready to use !




30:10
==========================
Now we goitn to run a python project with this database.
So check then sourc code of python file.

Nothenr fency hee we create database connection in this connecotn we pass hostname with enviornment.

we just going run a simple select query to keep it simple.

Lets ceck thsi Dockerfile now,

Fistr as ausual a from tage for python.
crate a directory to get a copy of source and keep it in thsi directory indise of a contienr.
and Expose the port.
Finally execut python run commnd.



36:20
==================
Lets build a image ro python applicaton wth docker build command.
Run doen ! check this image with image ls command age we get it !
Now the Image is ready.

To run it wiht mysql database the comamn is as:
here this is contienr naem this is network name, port mapping, and mysql server information here and finally image name.
Run it, run done lets check it on browser 8080 port.

Yes ! we get it ! we get the data from mysqlserver.
lets leave a screenshot from here to doucment.


40:10
===========================
Now we gong work king with a java applicaotn. I am gonna use a java spring boot project.

for run a java applicatoon we have to first compali and build a jar file first, here i am usring maven.
This is my appliaotn .pom file.
now i amd run the jar build command.
yes the build pross is success.
So the location of har is here showing, and my applicaotn ready to run.


Lets now create a dokcerfile for this java applicaton.
Same we crate befor python image i am doing same here.

See thsi commadn:
    from a jdk
    create a directory 
    copy file and exppsoe the port.
    then finally run it.

Hwre we see how to maek database credentails. i have to go thsi way for docker contienr perpose.


lets run thei build comand. and tis done.
Check thed image.
Now run this java applicaton with this ntwrok and mysql server as:
run it.
Oh! we get port conflicts with pthon app. lets stop sna delete previour ptyhon app.
and run tis ageitn, run done, taking a screenshot and add it on document.



lets eheck the application on browser 8080 port. 
and yest we get it! data from mysql server.


here another this conteinr logges, how we can check contienr logs.
usti contienr lgos, 
we can also see live log using -f.

ok now our mysqldatabse and backeend applicaothn is ready now.
For frontend ther we gojtn to run a Node applicaton.



53:30
=====================
Lets run a nodeja applicaton as a fronend app.

lets echk the appliaton source code.
Nother faccy here here we provie oru backne url and make a http reqeust and get resposne.
then we map this respons data wtih a html file.


Lets build a image, ok check the dockerfile first.

same as befor: a form tag, copy file, 
We use diffetne commadn in diffente docker file.


Lwts run image build command.
taking a screenshot and keep it in the document.

In this soruce code the app will run on 3000 port.


Now time to run the fronedn applicaton wtih backend server.
ok run done !

now check it on the browder 3000 port.

we can now flow teh three step for three tier application.
here data base is mysql, middletier is java app and node app is a fron end.

yes we get oru data in browser wth beautiful way.


Now lets add some data on the database manually.

Refresh the frowser and we get new addes data !















9) Docker Compose
============================================
Docker composer is tools to run multiple project at a time.

Till now we run osine service using differetn differetn command 
so that for a three tier applicaton like database backend frontedn we run saperately .

With docker composer we are able to this type system where we need to run multiple application 
we can run thoses services wtih in one command using docker composer ths is the main benefits of docker composer.


In wendows docker composwer come with docker desktop no need to install saperately but linux may need.

3:00
==============
To ensure you machine have docker composer to check it use: docker compose version command.
Thne you get it.


Lets go ferther, check how looks a docker composer file, its pretty differnt from dockerfile.
Lets check it out.
So, its say as: 
In the file we first a secton is version, this is then composer version.
then services, after network and volume section as you needed in you services.

Lets see a simple three tier application model wher a front service, back end service and database wit in a netwrok.
Lets how looks like this docker-compose.yaml file 


In thsi file we can mentation dockerfile,  may directly dockr image or may pull form any git repository.
After that we define a bridge network and volume.

So now we going to run our first dockr composer file.
I already cratre a file, look here i define a forntend appliaotn,
I am not insede this directory where is docker-composer.yaml file and project source code.


Here a tag called depend on to mentation ependency servie to otuer services like frontend depend on backend
and backend depend on database serice.
Here we also mentaitn a voluem for database and netwrok.


Remember our command line have to open inside from serices parant dirctory where the composer file exists.
So time to run composer, lets run it.
Then command is docker-compose up
alsoe we cna run from different location usng -f.

Ok Its runnign....
Okey we get it our serices running done, we can see a blick or containers here in docker desktop.


Now i we see how to stop the apps or down it, for thst we use docker compoe downe.


16:50
=============
Lets make a change in our source code and see how first  we can able to run oru entier system using this composer tools.
Lets update the service.

udpate doen, lets run it ageing.
agaign stop it, one good thing here when we run docker-compose down this command
first stop the applyplication and then remove thsi contianers.


19:40
======================
For testing perpose where no need to build image like qa secton just run the app and check the applicaton.
in thsi case we can remove the image build section.

so for thst lets create a another docker-compose file and run it.


we can see whow many contienr run by this composer we cna see it using docker compose ps, also we cna see logs as well as.


Reamember to run docker compose up need a docker-compose.yaml file woth our it thsi command are not work.







##10) Container Orchestration
========================================
In this video we talking about contier orchestration, what is actually  this contaoer orchestration.

Container orchestration is a big tools for managing and deploying containerized applications. 
It helps automate the deployment, scaling, and management of containerized applications. 


The orchestration used for: Use container orchestration to automate and manage tasks such as:
Provisioning and deployment
Configuration and scheduling 
Resource allocation
Scaling or removing containers based on balancing workloads across your infrastructure
Load balancing and traffic routing 
Keeping interactions between containers secure.



One exmple is show requirment of orchestration is servie scaling, here is a example.

Like you have a applicaton like a echormach site wher maay regualr customer/visitor need 100 tps.
But on any festival tiam or a black friday when visitor come 2x from regular time.
in thsi case we need to scale the application capacity to handel thsi extra reqeust.
this case need auto scaling this the one thisn a orchestration do smartly !



















