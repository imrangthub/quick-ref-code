
#################################################
#              K8S                             #
#################################################

systemctl --user start docker-desktop

=>kubectl api-resources

=>kubectl version
=>kubectl version --short 
=>kubectl get nodes




=================================================
#General                                
=================================================

=>=>kubectl exec webapp-color -- env
=>kubectl exec -it ubuntu-sleeper -- whoami
=>kubectl exec -it webapp-color -- sh


=>kubectl get pods
=>kubectl get pods -o wide
=>kubectl get -o json pod prodName

=>kubectl explain pod
=>kubectl explain deployment

=kubectl logs myPod

=>kubectl run --help
=>kubectl run myng --image=nginx --dry-run=client -o yaml>labelpod.yaml
=>kubectl create deployment --image=nginx nginx --dry-run=client -o yaml
=>kubectl run ng3 --image=nginx -- /bin/sh -c "while true; do echo  $(date); sleep 1; done "

=>k delete po myng


docker tag currentImageName newImageName
=>docker tag imranmadbar/nginx ng-debug
Image from Image

=>kubectl  run myng --imranmadbar/nginx
DockerContRun:
=>docker container run --name myng -dp 8080:80 imranmadbar/nginx


Byuild image and push
-------------------------------------------------
docker run -d -p 5000:5000 --restart=always --name registry registry
docker build . -t localhost:5000/openapi-customer-information
docker push localhost:5000/openapi-customer-information


k8 default loging file 
---------------------------------------
/var/log/pods/*$1/*.log
/var/log/pods/apihub-microservice_octopus-sms-6bff58b4cb-gjh48_53ed786a-4c06-46aa-828d-7c6997fb3060/octopus-sms/0.log

Log Storage: 
In Kubernetes, logs are stored on the node where the pod is running, typically under /var/log/ in a structured directory format.


Each container within a pod streams its output to a log file, which Kubernetes manages. By default, this log file is stored at /var/log/pods/<namespace>_<pod-name>/<container-name>/<timestamp>.log on the node where the pod is running.




=================================================
Service:
=================================================

apiVersion: v1
kind: Service
metadata:
  name: webapp-service
  namespace: default
spec:
  ports:
  - nodePort: 30080
    port: 8080
    targetPort: 8080
  selector:
    name: simple-webapp
  type: NodePort


=>kubectl create service nodeport mysvc4 --tcp=8080:80 --node-port=30040
=>kubectl create service nodeport mysvc4 --tcp=8080:80 --node-port=30040 --dry-run=client -o yaml>mysvc.yaml



ExposeService:
=>k expose pod jekyll --name=jekyll --type=NodePort --port=8080 --target-port=4000 -n development
=>k expose pod myng --name=ng-svc --type=NodePort --port=8080 --target-port=80 






=================================================
k8 dns | DNS
=================================================
sudo apt update
sudo apt install net-tools
sudo apt install dnsutils



pod->CoreDNS->hostDns->targetIP
pod->CoreDNS->hostDns->NAT->hostEath0->hostIP->externalServer
k8-dns-resolution-flow


=>sudo cat /etc/resolv.conf
=>sudo resolvectl status
Check hostmachine dns server


=>nslookup example.com
=>nslookup example.com 172.16.6.141
to find ip of dns example.com dns




Troubleshooting DNS in Kubernetes
-------------------------------------------------

=>kubectl get pods -n kube-system -l k8s-app=kube-dns
CoreDNS Pod Status: Ensure that the CoreDNS pods are running properly:

=>kubectl logs -n kube-system coredns-565d847f94-g6c5t
CoreDNS Logs: Check CoreDNS logs for any errors or misconfigurations:

=>kubectl exec -it <pod-name> -- cat /etc/resolv.conf
Pod DNS Configuration: Check if the pod‚Äôs /etc/resolv.conf is properly configured to use the CoreDNS service:





add internal-dns-server for enteir cluster:
---------------------------------------------------
=>kubectl -n kube-system get configmap coredns -o yaml
=>kubectl -n kube-system edit configmap coredns


.:53 {
    errors
    health
    kubernetes cluster.local in-addr.arpa ip6.arpa {
        pods insecure
        fallthrough in-addr.arpa ip6.arpa
        ttl 30
    }
    forward . /etc/resolv.conf
    cache 30
    loop
    reload
    loadbalance
}

internal.company.com:53 {
    forward . 192.168.1.100
}
OR
.:53 {
    errors
    health
    kubernetes cluster.local in-addr.arpa ip6.arpa {
        pods insecure
        fallthrough in-addr.arpa ip6.arpa
        ttl 30
    }
    forward . 192.168.1.100
    cache 30
    loop
    reload
    loadbalance
}


For deployment a specific deployment or pod:
      restartPolicy: Always
      dnsPolicy: None
      dnsConfig:
        nameservers:
          - 192.168.1.1
          - 192.168.1.2
          - 192.168.1.3
        searches:
          - my-microservice.svc.cluster.local
          - svc.cluster.local
          - cluster.local
        options:
          - name: ndots
            value: '5'
          - name: edns0
      imagePullSecrets:
        - name: regcred

for pod:
apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod
spec:
  containers:
  - name: nginx
    image: nginx
  dnsPolicy: None
  dnsConfig:
    nameservers:
      - 192.168.1.1    # Custom DNS server 1
      - 192.168.1.2     # Custom DNS server 2
      - 192.168.1.3     # Custom DNS server 3
    searches:
      - my-microservice.svc.cluster.local
      - svc.cluster.local
      - cluster.local
    options:
      - name: ndots
        value: '5'
      - name: edns0





Key Impacts of dnsPolicy: None:
  Kubernetes Service Discovery Disabled:
    With dnsPolicy: None, the pod will not use Kubernetes' internal DNS system to resolve services or pod names.
    This means the pod will not be able to resolve Kubernetes services or other pods by their 
    DNS names (e.g., my-service.svc.cluster.local).

  Custom DNS Resolution:
    Instead of using the internal Kubernetes DNS (CoreDNS), the pod will use the custom DNS configuration you specify under dnsConfig.
    You are responsible for providing external DNS servers (as you did with the IP addresses in the nameservers field), 
    and these servers will be queried for DNS resolution.

  No Access to Cluster DNS:
    Since the pod will not use the Kubernetes-provided DNS, it won't have access to typical DNS entries 
    like my-service.default.svc.cluster.local or any other service discovery mechanisms provided by Kubernetes.



Use both:
      dnsPolicy: ClusterFirst
      dnsConfig:
        nameservers:
          - 192.168.1.1
          - 192.168.1.2
        searches:
          - apihub-microservice.svc.cluster.local
          - svc.cluster.local
          - cluster.local
        options:
          - name: ndots
            value: '5'
          - name: edns0





Doc:
=================================================
How CoreDNS Works in Kubernetes:
  CoreDNS runs as a Kubernetes service and is typically deployed as pods in the kube-system namespace.
  It watches the Kubernetes API for new or removed services and pods, and dynamically updates DNS records for them.
  CoreDNS listens on a ClusterIP, and pods are configured to use this as their DNS server (via the /etc/resolv.conf file inside each pod).


Kubernetes DNS Architecture:
  CoreDNS Pods: CoreDNS is deployed as a set of pods, managed by a deployment in the kube-system namespace. These pods run on multiple nodes to ensure high availability.
  ClusterIP Service: CoreDNS exposes a ClusterIP service inside the Kubernetes cluster. Each pod in the cluster uses this service as its DNS resolver.
  Kubelet Configuration: The kubelet on each node automatically configures the DNS settings for each pod, pointing to the CoreDNS service.



Network Flow Summary of POD:
  Pod's Request:        Pod sends a request to access google.com.
  DNS Resolution:       CoreDNS handles the DNS request and resolves the IP address.
  Routing through Node: The pod‚Äôs internal IP address is translated to the node‚Äôs external IP via NAT.
  Internet Access:      The packet exits the node and goes through the internet gateway (router or modem) to reach the external destination.
  Response to Node:     The external server responds, and the traffic is routed back to the node‚Äôs external IP.
  Reverse NAT:          The node translates the external response to the pod's internal IP and forwards it to the pod.










Main roles of Kubernetes (K8s) Master and Worker Nodes:
==========================================================
üëë Master Node (Control Plane) ‚Äì Main Role
  Responsible for managing and controlling the cluster.

  üîß Core Components and Responsibilities:
  kube-apiserver: Central access point for all API requests (cluster communication hub).

  etcd: Stores the entire cluster configuration and state.

  kube-scheduler: Assigns pods to worker nodes based on resources, affinity, etc.

  kube-controller-manager: Maintains desired cluster state (replicas, nodes, etc.).

  cloud-controller-manager (optional): Integrates with cloud provider APIs.

üëâ Master node makes decisions and orchestrates everything.



üß± Worker Node ‚Äì Main Role
  Responsible for running the application workloads (i.e., your containers).

  üîß Core Components and Responsibilities:
  kubelet: Communicates with master, manages pods on the node.

  kube-proxy: Handles network routing and load balancing for services.

  Container Runtime (e.g., containerd, Docker): Runs containers.

üëâ Worker node does the actual work of running your applications.







üèóÔ∏è Multiple Master Nodes ‚Äì Purpose & Role
  Used for High Availability (HA) of the control plane.

  ‚úÖ Benefits:
  No single point of failure.

  Cluster remains operational if one master fails.

  üîÅ How it works:
  All masters run the same components (kube-apiserver, etcd, etc.).

  Leader election is used for controllers and scheduler (only one leader active at a time).

  A load balancer distributes API requests to available masters.

  etcd is usually run in a replicated cluster (3 or 5 nodes recommended).

üß± Multiple Worker Nodes ‚Äì Purpose & Role
  Used for scalability and fault tolerance of application workloads.

  ‚úÖ Benefits:
  Run more pods/containers by distributing load.

  If one worker fails, pods can be rescheduled on other workers (if replication is configured).

  Helps in rolling updates, blue/green deployments, etc.



‚öôÔ∏è How it works:
  All workers register with the control plane.

  The kube-scheduler assigns pods based on available resources across workers.

  kubelet on each worker ensures pods run as expected.

  üîó Combined Multi-Master & Multi-Worker Cluster
  Highly available and scalable architecture.

  Typical in production-grade Kubernetes clusters.

  Common setup:

  3 master nodes (with 3 etcd members)

  3+ worker nodes

  1 external load balancer in front of masters








üß† Why Odd Number of Master Nodes is Recommended
etcd (and other consensus systems like Raft or Paxos) need a majority (quorum) of nodes to agree before making any change.






üìå Why Even Numbers Are Not Efficient
Even though 4 nodes can tolerate 1 failure (same as 3), it requires more nodes (3 instead of 2) to make decisions.

So:

More overhead

No gain in fault tolerance

Wasted resources

‚úÖ Best Practice:
Use an odd number of etcd (master) nodes: 3, 5, or 7
These give you maximum fault tolerance for the minimum number of nodes.

üõë What If You Use an Even Number (e.g., 2, 4, 6)?
Risk of deadlock or no quorum if one node goes down

Cluster might become unavailable

Not efficient in terms of fault tolerance per node





üî¢ Quorum = (N / 2) + 1, where N = total nodes in the cluster.
----------------------------------------------------------------------
| Total Nodes (N) | Quorum Needed | Fault Tolerance | Recommended? |
|-----------------|---------------|-----------------|--------------|
| 2               | 2             | 0 nodes         | No           |
| 3               | 2             | 1 node          | Yes          |
| 4               | 3             | 1 node          | No           |
| 5               | 3             | 2 nodes         | Yes          |
| 6               | 4             | 2 nodes         | No           |
| 7               | 4             | 3 nodes         | Yes          |
----------------------------------------------------------------------





üß† What is the Split-Brain Problem?
Split-brain happens when a distributed system (like etcd or Kubernetes masters) splits into two or more parts (called partitions) due to a network failure or communication issue.

Each partition thinks it's the only active group, and they try to make decisions independently, leading to:

‚ùå Conflicting updates
‚ùå Data inconsistency
‚ùå Corruption of cluster state

What can happen in a split-brain scenario:
  Master1 might think it‚Äôs still the leader and continue making updates to etcd.

  Master2 and Master3 elect a new leader and also make updates.

  Now you have two versions of truth, which breaks the consistency.
  
How Kubernetes Prevents Split-Brain (via etcd & Raft):
  etcd uses the Raft consensus algorithm, which requires a quorum (majority) to make decisions.

  For 3 nodes: quorum = (3 / 2) + 1 = 2

  If only 1 node is isolated, it cannot act alone ‚Äî it refuses to serve until it rejoins a majority.

  This prevents split-brain by ensuring that only the majority partition can act as the cluster.






Safe Rule of Thumb (Max Failure Tolerance):
---------------------------------------------------
| etcd Members | Quorum Needed | Max Nodes You Can Lose |
|--------------|---------------|------------------------|
| 3            | 2             | 1                      |
| 5            | 3             | 2                      |
| 7            | 4             | 3                      |
| 9            | 5             | 4                      |
---------------------------------------------------
Once you lose half or more, you're below quorum ‚Äî cluster fails.
‚úÖ Summary:
üíÄ 50% or more master nodes down = etcd breaks quorum = control plane failure
üõ†Ô∏è To survive failures, always plan quorum with odd number of etcd nodes





SetUp
-------------------------------------------------
Install and set up the kubectl tool: ‚Äì

https://kubernetes.io/docs/tasks/tools/

Install Minikube: ‚Äì

https://minikube.sigs.k8s.io/docs/start/

Install VirtualBox: ‚Äì

https://www.virtualbox.org/wiki/Downloads

https://www.virtualbox.org/wiki/Linux_Downloads

Minikube Tutorial: ‚Äì

https://kubernetes.io/docs/tutorials/hello-minikube/

If the minikube installation has been done on the macOS, then to access the URL on the local browser, we need to do a few steps to get the service URL to work. Those steps are covered on this documentation page: ‚Äì

https://minikube.sigs.k8s.io/docs/handbook/accessing/#using-minikube-service-with-tunnel








Install Minikube:
====================================
curl -LO https://github.com/kubernetes/minikube/releases/latest/download/minikube-linux-amd64
sudo install minikube-linux-amd64 /usr/local/bin/minikube && rm minikube-linux-amd64



minikube status
minikube kubectl -- get po -A
alias kubectl="minikube kubectl --"
minikube dashboard


RemoveMiniKube
------------------------- 
minikube stop
minikube delete
minikube delete --all






