
=================================================
            CKA                                #         
================================================= 











=================================================
##CoreConcept         
================================================= 

Pod
-------------------------------------------------


##Solution:01 Configuration
-------------------------------------------------
1)Which nodes are these pods placed on?
=>kubectl get pod -o wide

2)What does the READY column in the output of the kubectl get pods command indicate?
runing container/total container.

3)Create a new pod with the name redis and with the image redis123.
=>kubectl run redis --image=redis123 --dry-run=client -o yaml

4)Now change the image on this pod to redis.
=>kubectl edit pod redis 

Edit current running pod, like change image
5) kubectl edit pod myng
OR
6) Edit then pod defanation yaml file and run again
=>Kubectl apply -f mypod.yaml


Generate another pod defenation yaml file from current pod
=>>kubectl get pod mypod -o yaml
=>kubectl get pod mypod -o yaml> pod.yaml


Remember, you CANNOT edit specifications of an existing POD other than the below.
spec.containers[*].image
spec.initContainers[*].image
spec.activeDeadlineSeconds
spec.tolerations

For edit other file generate another yaml file from current pod status and update it, delete perivious pod and run new one.
=>kubectl get pod mypod -o yaml> pod.yaml
=>kubectl apply -f pod.yaml





##ReplicaSet
-------------------------------------------------
Simple ReplicaSet:
apiVersion:apps/v1
kind: ReplicaSet
metadata:
  name: replicaset-1
spec:
  replicas: 2
  selector:
    matchLabels:
      tier: frontend
  template:
    metadata:
      labels:
        tier: frontend
    spec:
      containers:
      - name: nginx
        image: nginx


For finding any issu in defination yaml file, first run the yaml
=>kubectl create -f myreplicaset.yaml
And now check the error message



##Scale ReplicaSet
-------------------------------------------------
1) Update number of replica(replicas:5) in yaml file and run
=>kubectl replace -f myreplica.yaml
OR
2) Run
=>kubectl scale --replicas=5 -f myreplica.yaml
OR
3) Run with replical type(replicaset) and name (myreplicaset) 
=>kubectl scale --replicas=5 replicaset myreplicaset
2 and 3 Not made change the original defination file

Check ReplicaSet info like: uses image, etc
=>kubectl describe rs new-replica-set

Check the pod status of ReplicaSet
=>kubectl get rs new-replica-set


If need to Update ReplicaSet like Image change, then its does not automatically re-create the  pod.
First you can eidt current rs
=>kubectl edit rs new-replica-set 
Then delete existing pod
=>kubectl delete pod --all
and RS will create new pod with new Image
OR
Delete old rs and create again


=>kubectl get rs myreplicaset1 -o yaml
=>kubectl get rs myreplicaset1 -o yaml>myrs2.yaml



##Solution:02 ReplicaSet
-------------------------------------------------
1)How many ReplicaSets exist on the system?
=>kubectl get replicaset

2)What is the image used to create the pods in the new-replica-set?
=>kubectl describe rs new-replica-set


3)Why do you think the PODs are not ready in replicaset?
=>kubectl get rs
=>kubectl get pods

=>kubectl get events --field-selector involvedObject.name=podName
OR
=>kubectl logs podName

4)Delete the two newly created ReplicaSets - replicaset-1 and replicaset-2
=>kubectl delete rs replicaset-1 replicaset-2


5)Fix the original replica set new-replica-set to use the correct busybox image.
Either delete and recreate the ReplicaSet or Update the existing ReplicaSet and then delete all PODs, so new ones with the correct image will be created.

=>kubectl get rs
=>kubectl edit rs new-replica-set
=>k delete pod --all


6)Scale the ReplicaSet to 5 PODs.
Use kubectl scale command or edit the replicaset using kubectl edit replicaset.
=>kubectl scale replicaset new-replica-set --replicas=5
OR
=>kubectl edit rs new-replica-set
Update new value in:
spec:
  replicas: 2
  selector:




##Deployment
-------------------------------------------------
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deployment-1
spec:
  replicas: 2
  selector:
    matchLabels:
      name: busybox-pod
  template:
    metadata:
      labels:
        name: busybox-pod
    spec:
      containers:
      - name: busybox-container
        image: busybox
        command:
        - sh
        - "-c"
        - echo Hello Kubernetes! && sleep 3600

With Deployments you can easily edit any field/property of the POD template. 

=>kubectl create deployment --help
=>kubectl create deployment httpd-frontend --image=httpd:2.4-alpine --replicas=3 --dry-run=client -o yaml>mydeployment1.yaml


Generate a yaml file from current deployment
=>kubectl get deployment mydeployment -o yaml
=>kubectl get deployment httpd-frontend -o yaml>mydeployment2.yaml



##Solution:03 Deployment
-------------------------------------------------

1)What is the image used to create the pods in the new deployment?
=>kubectl describe deploy frontend-deployment 

2)Why do you think the deployment is not ready?

=>kubectl get deployment
=>kubectl describe deployment
=>kubectl get pod
=>kubectl get events --field-selector involvedObject.name=podName

3)Create a new Deployment with the below attributes using your own deployment definition file.
Name: httpd-frontend;
Replicas: 3;
Image: httpd:2.4-alpine
Name: httpd-frontend
Replicas: 3
Image: httpd:2.4-alpine

=>kubectl create deployment -h
=>kubectl create deployment httpd-frontend --image=httpd:2.4-alpine --replicas=3 --dry-run=client -o yaml>mydep.yaml





##NameSpace
-------------------------------------------------
=>kubectl get all

=>kubectl get namespace
=>kubectl get pod --namespace=research
=>kubectl get pod -n=research
=>kubectl run redis --image=redis --namespace=research


=>kubectl get pod -A
=>kubectl get pod --all-namespaces
Show all pod with namespace

=>kubectl get pods -n=marketing
=>kubectl get svc -n=marketing
Show service of a specific namespace





##Solution:04 NameSpace
-------------------------------------------------

1)How many Namespaces exist on the system?
=>kubectl get ns

2)How many pods exist in the research namespace?
=>kubectl get pod -n research

3)Create a POD in the finance namespace.
Name: redis
Image Name: redis
=>kubectl run redis --image=redis --namespace finance

4)Which namespace has the blue pod in it?
=>kubectl get pods --all-namespaces | grep blue


5)What DNS name should the Blue application use to access the database db-service in the dev namespace?
You can try it in the web application UI. Use port 6379.

The FQDN consists of the <service-name>.<namespace>.svc.cluster.local format.
like:my-service.my-namespace.svc.cluster.local




##Solution:05 CORE CONCEPTS, PRACTICE TEST SERVICES
-------------------------------------------------

1)How many Services exist on the system?
=>kubectl get svc

2)What is the targetPort configured on the kubernetes service?
=>kubectl describe svc kubernetes 


3)How many labels are configured on the kubernetes service?
=>kubectl describe svc kubernetes 

5)Create a new service to access the web application using the service-definition-1.yaml file.
Name: webapp-service
Type: NodePort
targetPort: 8080
port: 8080
nodePort: 30080
selector:
  name: simple-webapp

apiVersion: v1
kind: Service
metadata:
  name: webapp-service
  namespace: default
spec:
  ports:
  - nodePort: 30080
    port: 8080
    targetPort: 8080
  selector:
    name: simple-webapp
  type: NodePort



Creating Kubernetes objects imperatively
-------------------------------------------------
There are two main ways to manage Kubernetes objects: imperative (with kubectl commands) and 
declarative (by writing manifests and then using kubectl apply).


=>kubectl run --help
=>kubectl run redis --image=redis:alpine --labels="tier=db"
=>get pod --show-labels


Create a service redis-service to expose the redis application within the cluster on port 6379.
Use imperative commands.
=>kubectl expose pod redis --port=6379 --name=redis-service
=>kubectl describe svc redis-service
OR
=>kubectl create service --help
=>kubectl create service clusterip --help
=>kubectl create service clusterip redis-service --tcp=6379:6379


expose:
=>kubectl expose --help
"expose" command user for expose existing pod for expose.
=>kubectl expose pod redis --port=6379 --name=redis-service
OR 
Create pod and service at a same time
=>kubectl run httpd --image=httpd:alpine --port=80 --expose=true
This command create pod as well as service with same name with labels and bind service with pod at a time




##Solution:06 CORE CONCEPTS, IMPERATIVE COMMANDS
-------------------------------------------------

1)Deploy a pod named nginx-pod using the nginx:alpine image.
Name: nginx-pod
Image: nginx:alpine

=>kubectl run nginx-pod --image=nginx:alpine

2)Deploy a redis pod using the redis:alpine image with the labels set to tier=db.
=>kubectl run --help
=>kubectl run redis --image=redis:alpine --labels="tier=db"
=>get pod --show-labels


3)Create a service redis-service to expose the redis application within the cluster on port 6379.
Use imperative commands.
Service: redis-service
Port: 6379
Type: ClusterIP

=>kubectl expose pod redis --port=6379 --name=redis-service
=>kubectl describe svc redis-service
OR
=>kubectl create service --help
=>kubectl create service clusterip --help
=>kubectl create service clusterip redis-service --tcp=6379:6379


4)Create a deployment named webapp using the image kodekloud/webapp-color with 3 replicas.
Try to use imperative commands only. Do not create definition files.
Name: webapp
Image: kodekloud/webapp-color
Replicas: 3

=>kubectl create deployment --help
=>kubectl create deployment webapp --image=kodekloud/webapp-color --replicas=3


5)Create name space and Create a new deployment called redis-deploy in the dev-ns namespace with the redis image. It should have 2
=>kubectl run custom-nginx --image=nginx --port=8080
=>kubectl create namespace dev-ns
=>kubectl create deployment redis-deploy --image=redis --replicas=2 -n=dev-ns


6)Create a pod called httpd using the image httpd:alpine in the default namespace. Next, create a service of type ClusterIP by the same name (httpd). The target port for the service should be 80.

=>kubectl run httpd --image=httpd:alpine --port=80

=>kubectl create service clusterip --help
=>kubectl create service clusterip httpd --tcp=5678:80

=>kubectl get pod --show-labels

=>kubectl edit pod httpd
Update the lavel as service



##solution2-schedulling     
================================================= 


##SCHEDULING, PRACTICE TEST MANUAL SCHEDULING
-------------------------------------------------

1)Why is the POD in a pending state?
Inspect the environment for various kubernetes control plane components.
because nod schedular found

=>k get pod -o yaml | scheduler
   schedulerName: default-scheduler

=>k describe pod nginx


2)Manually schedule the pod on node01.
Delete and recreate the POD if necessary.
add this:
  nodeName: node01

apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  nodeName: node01
  containers:
  -  image: nginx
     name: nginx

=>kubectl get pod -o wide
Now check the NODE for schedule node name





##SCHEDULING, PRACTICE TEST LABELS AND SELECTORS
-------------------------------------------------

1)How many objects are in the prod environment including PODs, ReplicaSets and any other objects?
=>kubectl get all --show-labels | grep prod




##SCHEDULING, PRACTICE TEST – NODE AFFINITY
-------------------------------------------------
1)How many Labels exist on node node01?
=>kubectl describe node node01
Check Labels section


2)Apply a label color=blue to node node01
=>kubectl label node node01 color=blue

3)Which nodes can the pods for the blue deployment be placed on?
Make sure to check taints on both nodes!

=>kubectl describe node node01
=>kubectl describe node | grep Taints
=>kubectl get pod -o wide


4)Set Node Affinity to the deployment to place the pods on node01 only.


Name: blue
Replicas: 3
Image: nginx
NodeAffinity: requiredDuringSchedulingIgnoredDuringExecution
Key: color
value: blue

apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: blue
  name: blue
spec:
  replicas: 3
  selector:
    matchLabels:
      app: blue
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: blue
    spec:  
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: color
                operator: In
                values:
                - blue
      containers:
      - image: nginx
        name: nginx
        resources: {}
status: {}


5)Create a new deployment named red with the nginx image and 2 replicas, and ensure it gets placed on the controlplane node only.

Use the label key - node-role.kubernetes.io/control-plane - which is already set on the controlplane node.

Name: red
Replicas: 2
Image: nginx
NodeAffinity: requiredDuringSchedulingIgnoredDuringExecution
Key: node-role.kubernetes.io/control-plane
Use the right operator

apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: red
  name: red
spec:
  replicas: 2
  selector:
    matchLabels:
      app: red
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: red
    spec:  
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: node-role.kubernetes.io/control-plane
                operator: Exists
      containers:
      - image: nginx
        name: nginx
        resources: {}
status: {}



##SCHEDULING, PRACTICE TEST RESOURCE LIMITS
-------------------------------------------------
1)A pod called rabbit is deployed. Identify the CPU requirements set on the Pod
=>kubectl describe pod myPod

2)Another pod called elephant has been deployed in the default namespace. It fails to get to a running state. Inspect this pod and identify the Reason why it is not running.
=>kubectl describe pod elephant
check 
   Last State:     Terminated
      Reason:       OOMKilled
The status OOMKilled indicates that it is failing because the pod ran out of memory. 
Identify the memory limit set on the POD.






##SCHEDULING, PRACTICE TEST DAEMONSETS
-------------------------------------------------
1)How many DaemonSets are created in the cluster in all namespaces?
Check all namespaces
=>kubectl get daemonsets -A


2)On how many nodes are the pods scheduled by the DaemonSet kube-proxy?
=>kubectl get daemonsets -A
=>kubectl describe daemonset kube-flannel-ds -n kube-flannel

checkout  
Containers:
   kube-flannel:
    Image:      docker.io/rancher/mirrored-flannelcni-flannel:v0.19.2


3)Deploy a DaemonSet for FluentD Logging.

Use the given specifications.

Name: elasticsearch
Namespace: kube-system
Image: registry.k8s.io/fluentd-elasticsearch:1.20

apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: elasticsearch
  namespace: kube-system
spec:
  selector:
    matchLabels:
      app: fluentd-elasticsearch
  template:
    metadata:
      labels:
        app: fluentd-elasticsearch
    spec:
      containers:
        - name: fluentd
          image: registry.k8s.io/fluentd-elasticsearch:1.20





##SCHEDULING, PRACTICE TEST – STATIC PODS
-------------------------------------------------
1)kubectl get pods --field-selector spec.nodeName!= -A | grep -c "nodeName:"
=>kubectl get pods --field-selector spec.nodeName!= -A | grep -c "controlplane"
kubectl get pods --field-selector spec.nodeName!= -A | grep -c "node01"


2)Which of the below components is NOT deployed as a static POD?
=>kubectl get pods --field-selector spec.nodeName!= -A -o=custom-columns=POD:metadata.name,NODE:spec.nodeName

3)What is the path of the directory holding the static pod definition files?
/etc/kubernetes/manifests

4)Create a static pod named static-busybox that uses the busybox image and the command sleep 1000

Name: static-busybox
Image: busybox

=>cd /etc/kubernetes/manifests
=>vi mystatipoc.yaml

apiVersion: v1
kind: Pod
metadata:
  name: static-busybox
spec:
  containers:
  - name: busybox
    image: busybox
    command:
      - sleep
      - "1000"

Kubernetes will automatically detect the new static pod and start running it.

=>kubectl get pod


5)Edit the image on the static pod to use with neiw image
=>kubectl edit pod mystaticpod





##SCHEDULING, PRACTICE TEST MULTIPLE SCHEDULERS
-------------------------------------------------
1)What is the name of the POD that deploys the default kubernetes scheduler in this environment?
=>kubectl get pods -n kube-system -l component=kube-scheduler -o jsonpath='{.items[0].metadata.name}'


2)What is the image used to deploy the kubernetes scheduler?
Inspect the kubernetes scheduler pod and identify the image
=>kubectl describe pod -n kube-system | grep scheduler


3)Checkout the following Kubernetes objects:

ServiceAccount: my-scheduler (kube-system namespace)
ClusterRoleBinding: my-scheduler-as-kube-scheduler
ClusterRoleBinding: my-scheduler-as-volume-scheduler

Run the command: kubectl get serviceaccount -n kube-system and kubectl get clusterrolebinding





##SCHEDULING, PRACTICE TEST – TAINTS AND TOLERATIONS
-------------------------------------------------
1)Do any taints exist on node01 node?

=>kubectl describe node node01 | grep -i taints
=>kubectl get node node01 -o=jsonpath='{.spec.taints}'

2)Create a taint on node01 with key of spray, value of mortein and effect of NoSchedule

Key = spray
Value = mortein
Effect = NoSchedule

=>kubectl taint nodes node01 spray=mortein:NoSchedule

3)Why do you think the pod is in a pending state?
=>kubectl describe pod mosquito



4)Create another pod named bee with the nginx image, which has a toleration set to the taint mortein.
Image name: nginx
Key: spray
Value: mortein
Effect: NoSchedule
Status: Running


apiVersion: v1
kind: Pod
metadata:
  name: bee
spec:
  containers:
    - name: nginx
      image: nginx
  tolerations:
    - key: spray
      operator: Equal
      value: mortein
      effect: NoSchedule



5)Remove the taint on controlplane, which currently has the taint effect of NoSchedule.
=>kubectl taint nodes controlplane node-role.kubernetes.io/control-plane:NoSchedule-








=================================================
##Login & Monitoring         
================================================= 




##OGGING & MONITORING, PRACTICE TEST MONITOR CLUST
-------------------------------------------------
1)Let us deploy metrics-server to monitor the PODs and Nodes. Pull the git repository for the deployment files.
Run: git clone https://github.com/kodekloudhub/kubernetes-metrics-server.git

=>kubectl create -f . 
=>kubectl top node

2)Identify the node that consumes the most CPU(cores).
=>kubectl top nodes --sort-by=cpu


3)Identify the node that consumes the most Memory(bytes).
=>kubectl top nodes --sort-by=memory


4)Identify the POD that consumes the most Memory(bytes)/CPU.
=>kubectl top pods --sort-by=memory
=>kubectl top pods --all-namespaces --sort-by=memory

=>kubectl top pods --all-namespaces --sort-by=cpu


5)A user - USER5 - has expressed concerns accessing the application. Identify the cause of the issue.
Inspect the logs of the POD
=>kubectl logs webapp-1




=================================================
##Application Lifecycle Management
=================================================


##APPLICATION LIFECYCLE MANAGEMENT, PRACTICE TEST ROLLING UPDATES AND ROLLBACKS 
--------------------------------------------------
1)Run the script named curl-test.sh to send multiple requests to test the web application. Take a note of the output.

for i in {1..35}; do
   kubectl exec --namespace=kube-public curl -- sh -c 'test=`wget -qO- -T 2  http://webapp-service.default.svc.cluster.local:8080/info 2>&1` && echo "$test OK" || echo "Failed"';
   echo ""
dones


2)Inspect the deployment and identify the current strategy
=>kubectl get deploy
=>kubectl describe deploy myhdeploy

3)Let us try that. Upgrade the application by setting the image on the deployment to kodekloud/webapp-color:v2
Do not delete and re-create the deployment. Only set the new image name for the existing deployment.

Deployment Name: frontend
Deployment Image: kodekloud/webapp-color:v2

=>kubectl edit deploy frontend


5)Up to how many PODs can be down for upgrade at a time
Consider the current strategy settings and number of PODs - 4
=>kubectl describe deploy frontend
Check 
RollingUpdateStrategy:  25% max unavailable, 25% max surge


6)Change the deployment strategy to Recreate
Delete and re-create the deployment if necessary. Only update the strategy type for the existing deployment.

Deployment Name: frontend
Deployment Image: kodekloud/webapp-color:v2
Strategy: Recreate

Edit thei file, delete old deployment and run new file





##Practice Test Commands and Arguments
----------------------------------------------------------
1)What is the command used to run the pod ubuntu-sleeper?
=>kubectl get pod -o yaml


2)Create a pod with the ubuntu image to run a container to sleep for 5000 seconds. Modify the file ubuntu-sleeper-2.yaml
apiVersion: v1 
kind: Pod 
metadata:
  name: ubuntu-sleeper-2 
spec:
  containers:
  - name: ubuntu
    image: ubuntu
    command: ["sleep"]
    args: ["5000"]

apiVersion: v1
kind: Pod 
metadata:
  name: ubuntu-sleeper-3
spec:
  containers:
  - name: ubuntu
    image: ubuntu
    command:
      - "sleep"
      - "1200"

Dockerfile1
FROM python:3.6-alpine
RUN pip install flask
COPY . /opt/
EXPOSE 8080
WORKDIR /opt
ENTRYPOINT ["python", "app.py"]

Dockerfile2
FROM python:3.6-alpine
RUN pip install flask
COPY . /opt/
EXPOSE 8080
WORKDIR /opt
ENTRYPOINT ["python", "app.py"]
CMD ["--color", "red"]



3)Create a pod with the given specifications. By default it displays a blue background. Set the given command line arguments to change it to green.
Pod Name: webapp-green
Image: kodekloud/webapp-color
Command line arguments: --color=green

apiVersion: v1 
kind: Pod 
metadata:
  name: webapp-green
  labels:
      name: webapp-green 
spec:
  containers:
  - name: simple-webapp
    image: kodekloud/webapp-color
    command: ["--color","green"]




##Practice Test Env
----------------------------------------------------------
1)What is the environment variable name set on the container in the pod?

=>kubectl exec webapp-color -- env

2)How many ConfigMaps exists in the default namespace?
=>k get configmap
=>kubectl describe configmap db-config


3)Create a new ConfigMap for the webapp-color POD. Use the spec given below.

ConfigMap Name: webapp-config-map
Data: APP_COLOR=darkblue
Data: APP_OTHER=disregard

apiVersion: v1
kind: ConfigMap
metadata:
  name: webapp-config-map
data:
  APP_COLOR: darkblue
  APP_OTHER: disregard

=>kubectl create configmap webapp-config-map --from-literal=APP_COLOR=darkblue --from-literal=APP_OTHER=disregard

4)Update the environment variable on the POD to use only the APP_COLOR key from the newly created ConfigMap.
Note: Delete and recreate the POD. Only make the necessary changes. Do not modify the name of the Pod.

Pod Name: webapp-color
ConfigMap Name: webapp-config-map

apiVersion: v1
kind: Pod
metadata:
  name: webapp-pod
spec:
  containers:
    - name: webapp-container
      image: your-webapp-image
      env:
        - name: APP_COLOR
          valueFrom:
            configMapKeyRef:
              name: webapp-config-map
              key: APP_COLOR










##Practice Test secrets
----------------------------------------------------------

=>kubectl get secrets

1)How many secrets are defined in the dashboard-token secret?
=>kubectl describe secrets dashboard-token

2)What is the type of the dashboard-token secret?
=>kubectl describe secrets dashboard-token


3)Which of the following is not a secret data defined in dashboard-token secret?
Type


4)We are going to deploy an application with the below architecture
We have already deployed the required pods and services. Check out the pods and services created. Check out the web application using the Webapp MySQL link above your terminal, next to the Quiz Portal Link.

https://prnt.sc/uABsPLI-MODK


The reason the application is failed is because we have not created the secrets yet. Create a new secret named db-secret with the data given below.


Secret Name: db-secret
Secret 1: DB_Host=sql01
Secret 2: DB_User=root
Secret 3: DB_Password=password123

apiVersion: v1
kind: Secret
metadata:
  name: db-secret
type: Opaque
data:
  DB_Host: c3FsMDE=
  DB_User: cm9vdA==
  DB_Password: cGFzc3dvcmQxMjM=


Configure webapp-pod to load environment variables from the newly created secret.
Delete and recreate the pod if required.

Pod name: webapp-pod
Image name: kodekloud/simple-webapp-mysql
Env From: Secret=db-secret

=>kubectl edit pod webapp
add this like:
spec:
  containers:
  - image: kodekloud/simple-webapp-mysql
    envFrom:
        - secretRef:
            name: db-secret



##Practice Test MultiContainer
----------------------------------------------------------
1)Create a multi-container pod with 2 containers.
Use the spec given below.
If the pod goes into the crashloopbackoff then add the command sleep 1000 in the lemon container.

Name: yellow
Container 1 Name: lemon
Container 1 Image: busybox
Container 2 Name: gold
Container 2 Image: redis

apiVersion: v1
kind: Pod
metadata:
  name: yellow
spec:
  restartPolicy: OnFailure
  containers:
    - name: lemon
      image: busybox
      command: ["sleep", "1000"]
    - name: gold
      image: redis


2)We have deployed an application logging stack in the elastic-stack namespace. Inspect it.
https://prnt.sc/ar4DoZncsZIQ


Once the pod is in a ready state, inspect the Kibana UI using the link above your terminal. There shouldn't be any logs for now.
We will configure a sidecar container for the application to send logs to Elastic Search.
NOTE: It can take a couple of minutes for the Kibana UI to be ready after the Kibana pod is ready.
You can inspect the Kibana logs by running:

=>kubectl -n elastic-stack logs kibana

Inspect the app pod and identify the number of containers in it.
It is deployed in the elastic-stack namespace.

=>k get pod -n elastic-stack


The application outputs logs to the file /log/app.log. View the logs and try to identify the user having issues with Login.
Inspect the log file inside the pod.
=>k exec app -n elastic-stack -- cat /log/app.log


3)Edit the pod to add a sidecar container to send logs to Elastic Search. Mount the log volume to the sidecar container.
Only add a new container. Do not modify anything else. Use the spec provided below.
Note: State persistence concepts are discussed in detail later in this course. For now please make use of the below documentation link for updating the concerning pod.
https://kubernetes.io/docs/tasks/access-application-cluster/communicate-containers-same-pod-shared-volume/

Name: app
Container Name: sidecar
Container Image: kodekloud/filebeat-configured
Volume Mount: log-volume
Mount Path: /var/log/event-simulator/
Existing Container Name: app
Existing Container Image: kodekloud/event-simulator

=>kubectl edit pod -n elastic-stack

- image: kodekloud/filebeat-configured
  name: sidecar
  volumeMounts:
    - mountPath: /var/log/event-simulator/
      name: log-volume



##Practice Test Init Container
----------------------------------------------------------
1)Identify the pod that has an initContainer configured.

=>kubectl describe pod

2)Update the pod red to use an initContainer that uses the busybox image and sleeps for 20 seconds

Delete and re-create the pod if necessary. But make sure no other configurations change.
Pod: red
initContainer Configured Correctly


3)A new application orange is deployed. There is something wrong with it. Identify and fix the issue.
Once fixed, wait for the application to run before checking solution.




=================================================
##Cluster Maintenance            
================================================= 



##Practice Test OS Upgradation
-------------------------------------------------
1)We need to take node01 out for maintenance. Empty the node of all applications and mark it unschedulable.

Node node01 Unschedulable
Pods evicted from node01

=>kubectl cordon node01
=>kubectl drain node01 --ignore-daemonsets

The maintenance tasks have been completed. Configure the node node01 to be schedulable again.

=>kubectl uncordon node01
Now Node01 is Schedulable


##Practice Test Cluster Upgraztation
-------------------------------------------------

1)How many nodes can host workloads in this cluster?
Inspect the applications and taints set on the nodes.

=>kubectl describe node | grep Taints:





=================================================
##Security            
================================================= 



##PracticeTest View Certificate Detatails
-------------------------------------------------

=>ls /etc/kubernetes/manifests/
=>ls /etc/kubernetes/pki/
All Certifict with++ file defination here

1)Identify the certificate file used for the kube-api server.

 By default, this file is named kube-apiserver.yaml and is typically located in the /etc/kubernetes/ directory.
=>sudo find /etc/kubernetes -name kube-apiserver.yaml
=>cat /etc/kubernetes/manifests/kube-apiserver.yaml | grep tls-cert-file


2)The kube-api server stopped again! Check it out. Inspect the kube-api server logs and identify the root cause and fix the issue.
Run crictl ps -a command to identify the kube-api server container. Run crictl logs container-id command to view the logs.





##PracticeTest Certificate API
-------------------------------------------------













=================================================
##Storage            
================================================= 



##PracticeTest PERSISTENT VOLUME CLAIMS
-------------------------------------------------

=>kubectl exec webapp -- cat /log/app.log

1)Configure a volume to store these logs at /var/log/webapp on the host.

Use the spec provided below.
Name: webapp
Image Name: kodekloud/event-simulator
Volume HostPath: /var/log/webapp
Volume Mount: /log


apiVersion: v1
kind: Pod
metadata:
  name: webapp
spec:
  containers:
    - name: webapp-container
      image: kodekloud/event-simulator
      volumeMounts:
        - name: log-volume
          mountPath: /log
  volumes:
    - name: log-volume
      hostPath:
        path: /var/log/webapp

=>kubectl apply -f pod2.yaml


2)Create a Persistent Volume with the given specification.

Volume Name: pv-log
Storage: 100Mi
Access Modes: ReadWriteMany
Host Path: /pv/log
Reclaim Policy: Retain


apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-log
spec:
  capacity:
    storage: 100Mi
  accessModes:
    - ReadWriteMany
  hostPath:
    path: /pv/log
  persistentVolumeReclaimPolicy: Retain


=>kubectl get pv

2)Let us claim some of that storage for our application. Create a Persistent Volume Claim with the given specification.

Volume Name: claim-log-1
Storage Request: 50Mi
Access Modes: ReadWriteOnce


apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: claim-log-1
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 50Mi
  volumeName: pv-log

=>kubectl get pvc


3)Why is the claim not bound to the available Persistent Volume?
Access Mode missmatch



4)Update the Access Mode on the claim to bind it to the PV.

Delete and recreate the claim-log-1.

Volume Name: claim-log-1
Storage Request: 50Mi
PVol: pv-log
Status: Bound

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: claim-log-1
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 50Mi
  volumeName: pv-log
status:
  phase: Bound



5)Update the webapp pod to use the persistent volume claim as its storage.


Replace hostPath configured earlier with the newly created PersistentVolumeClaim.


Name: webapp
Image Name: kodekloud/event-simulator
Volume: PersistentVolumeClaim=claim-log-1
Volume Mount: /log

apiVersion: v1
kind: Pod
metadata:
  name: webapp
spec:
  containers:
    - name: webapp-container
      image: kodekloud/event-simulator
      volumeMounts:
        - name: log-volume
          mountPath: /log
  volumes:
    - name: log-volume
      persistentVolumeClaim:
        claimName: claim-log-1


6)What would happen to the PV if the PVC was destroyed?
PV is not deleted but not available





##PracticeTest – STORAGE CLASS
-------------------------------------------------

=>kubectl get storageclass | sc

1)What is the name of the Storage Class that does not support dynamic volume provisioning?

glusterfs-sc
nfs-sc
local-storage
portWorx-Storageclass

The local-storage Storage Class is an example of a Storage Class that does not support dynamic volume provisioning. I


2)Let's fix that. Create a new PersistentVolumeClaim by the name of local-pvc that should bind to the volume local-pv.


Inspect the pv local-pv for the specs.

PVC: local-pvc
Correct Access Mode?
Correct StorageClass Used?
PVC requests volume size = 500Mi?

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: local-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 500Mi
  volumeName: local-pv
  storageClassName: local-storage



3)Create a new pod called nginx with the image nginx:alpine. The Pod should make use of the PVC local-pvc and mount the volume at the path /var/www/html.
The PV local-pv should in a bound state.


apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
    - name: nginx-container
      image: nginx:alpine
      volumeMounts:
        - name: html-volume
          mountPath: /var/www/html
  volumes:
    - name: html-volume
      persistentVolumeClaim:
        claimName: local-pvc



4)Create a new Storage Class called delayed-volume-sc that makes use of the below specs:

provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer




=================================================
##Netorking            
================================================= 



##PracticeTest  EXPLORE ENVIRONMENT
-------------------------------------------------

1)What is the Internal IP address of the controlplane node in this cluster?
=>kubectl get node -o wide


2)What is the network interface configured for cluster connectivity on the controlplane node?
node-to-node communication
=>ifconfig
Check the IP

3)We use Containerd as our container runtime. What is the interface/bridge created by Containerd on the controlplane node?
cni0
=>ip link show

4)If you were to ping google from the controlplane node, which route does it take?
What is the IP address of the Default Gateway?

=>ip route | grep default



5)What is the port the kube-scheduler is listening on in the controlplane node?
=>netstat -tuln

6)Notice that ETCD is listening on two ports. Which of these have more client connections established?
2350
2379
6443
2380

Based on the provided options, the two ports typically associated with ETCD are:
Port 2379: This is the default port for ETCD client communication.
Port 2380: This is the default port for ETCD peer-to-peer communication.


##PracticeTest  CNI
-------------------------------------------------

1)Inspect the kubelet service and identify the container runtime endpoint value is set for Kubernetes.
=>tr \\0 ' ' < /proc/"$(pgrep kubelet)"/cmdline

2)What is the path configured with all binaries of CNI supported plugins?
=>ls /opt/cni/bin/




##PracticeTest  Deploy Network Solution | NETWORKING WEAVE
-------------------------------------------------
In this practice test we will install weave-net POD networking solution to the cluster. Let us first inspect the setup.


1)What is the Networking Solution used by this cluster?
=>ls /etc/cni/net.d/
=>kubectl get pods -n kube-system


2)How many weave agents/peers are deployed in this cluster?
=>kubectl get pods -n kube-system -l name=weave-net -o wide --no-headers | wc -l

3)Identify the name of the bridge network/interface created by weave on each node.
=>ifconfig


4)What is the default gateway configured on the PODs scheduled on node01?
Try scheduling a pod on node01 and check ip route output

apiVersion: v1
kind: Pod
metadata:
  name: test-pod
spec:
  nodeName: node01
  containers:
  - name: busybox
    image: busybox
    command:
    - sleep
    - "3600"

=>kubectl exec -it test-pod -- sh
=>ip route






##PracticeTest SERVICE NETWORKING
-------------------------------------------------
1)What network range are the nodes in the cluster part of?
=>kubectl get node -o wide

2)what is pod cider rang?
=>kubectl cluster-info dump | grep -oP '(?<=--cluster-cidr=)[\d./]+'

3)What is the IP Range configured for the services within the cluster?
=>kubectl cluster-info dump | grep -oP '(?<=--service-cluster-ip-range=)[\d./]+'

4)How many kube-proxy pods are deployed in this cluster?
=>k get pod -A | grep proxy

5)How does this Kubernetes cluster ensure that a kube-proxy pod runs on all nodes in the cluster?
Inspect the kube-proxy pods and try to identify how they are deployed.

By default, Kubernetes ensures that a kube-proxy pod runs on all nodes in the cluster through the use of a DaemonSet.





##PracticeTest  COREDNS IN KUBERNETES
-------------------------------------------------

1)Identify the DNS solution implemented in this cluster.
=>kubectl get endpoints -A
=>kubectl get endpoints kube-dns --namespace=kube-system

2)How many pods of the DNS server are deployed?
=>kubectl get pods -n kube-system -l k8s-app=kube-dns --no-headers | wc -l

3)What is the name of the service created for accessing CoreDNS?
=>kubectl get service -n kube-system -l k8s-app=kube-dns -o jsonpath='{.items[0].metadata.name}'


4)What is the IP of the CoreDNS server that should be configured on PODs to resolve services?
=>kubectl get svc -n kube-system -l k8s-app=kube-dns -o jsonpath='{.items[0].spec.clusterIP}'

5)kubectl get svc -n kube-system -l k8s-app=kube-dns -o jsonpath='{.items[0].spec.clusterIP}'
=>/etc/coredns/Corefile

6)How is the Corefile passed into the CoreDNS POD?
The Corefile is typically passed into the CoreDNS pod by configuring it as a ConfigMap object in Kubernetes. 

7)What is the name of the ConfigMap object created for Corefile?
coredns
=>kubectl get configmap -n kube-system -l k8s-app=kube-dns -o jsonpath='{.items[0].metadata.name}'

8)What is the root domain/zone configured for this kubernetes cluster?
cluster.local is the root domain or zone configured for the Kubernetes cluster.


9)Which of the names CANNOT be used to access the HR service from the test pod?

10)Which of the below name can be used to access the payroll service from the test application?


11)Which of the below name CANNOT be used to access the payroll service from the test application?

12)We just deployed a web server - webapp - that accesses a database mysql - server. However the web server is failing to connect to the database server. Troubleshoot and fix the issue.

They could be in different namespaces. First locate the applications. The web server interface can be seen by clicking the tab Web Server at the top of your terminal.

Web Server: webapp
Uses the right DB_Host name



12)From the hr pod nslookup the mysql service and redirect the output to a file /root/CKA/nslookup.out




##PracticeTest CKA – INGRESS NETWORKIN
-------------------------------------------------

1)We have deployed Ingress Controller, resources and applications. Explore the setup.
=>k get ingress -A

2)What is the name of the Ingress Resource?
=>k get ingress -A

3)What is the Host configured on the Ingress Resource?
The host entry defines the domain name that users use to reach the application like www.google.com

4)What backend is the /wear path on the Ingress configured with?
=>k describe ingress ingress-wear-watch -n app-space

5)You are requested to change the URLs at which the applications are made available.
Make the video application available at /stream.

Ingress: ingress-wear-watch
Path: /stream
Backend Service: video-service
Backend Service Port: 8080

6)You are requested to add a new path to your ingress to make the food delivery application available to your customers.


Make the new application available at /eat.
Ingress: ingress-wear-watch
Path: /eat
Backend Service: food-service
Backend Service Port: 8080




=================================================
##Install            
================================================= 



##PracticeTest Install
-------------------------------------------------
1)Install the kubeadm and kubelet packages on the controlplane and node01.

Use the exact version of 1.26.0-00 for both.
kubeadm installed on controlplane ?
kubelet installed on controlplane?
Kubeadm installed on worker node01 ?
Kubelet installed on worker node01 ?


2)What is the version of kubelet installed?
=>kubelet --version


3)Initialize Control Plane Node (Master Node). Use the following options:

apiserver-advertise-address - Use the IP address allocated to eth0 on the controlplane node
apiserver-cert-extra-sans - Set it to controlplane
pod-network-cidr - Set to 10.244.0.0/16
Once done, set up the default kubeconfig file and wait for node to be part of the cluster.


Controlplane node initialized



4)Install a Network Plugin. As a default, we will go with flannel
Refer to the official documentation for the procedure.

Network Plugin deployed?







=================================================
##TROUBLESHOOTING
================================================= 

         
##PracticeTest  Application failure
-------------------------------------------------

1)Troubleshooting Test 1: A simple 2 tier application is deployed in the alpha namespace. It must display a green web page on success. Click on the App tab at the top of your terminal to view your application. It is currently failed. Troubleshoot and fix the issue.
Stick to the given architecture. Use the same names and port numbers as given in the below architecture diagram. Feel free to edit, delete or recreate objects as necessary.

Architecture Image:
https://prnt.sc/NxnO9ORob0tT

Solution:
The service name used for the MySQL Pod is incorrect. According to the Architecture diagram, it should be mysql-service.
To fix this, first delete the current service: kubectl -n alpha delete svc mysql
Then create a new service with the following YAML file (or use imperative command):

---
apiVersion: v1
kind: Service
metadata:
  name: mysql-service
  namespace: alpha
spec:
    ports:
    - port: 3306
      targetPort: 3306
    selector:
      name: mysql


Create the new service: kubectl create -f <service.yaml>


2)Troubleshooting Test 2: The same 2 tier application is deployed in the beta namespace. It must display a green web page on success. Click on the App tab at the top of your terminal to view your application. It is currently failed. Troubleshoot and fix the issue.
Stick to the given architecture. Use the same names and port numbers as given in the below architecture diagram. Feel free to edit, delete or recreate objects as necessary.

ArchitectureImage:
https://prnt.sc/45eaA8moRPg-

Issue:In the beta namespace, check the ports configured on the services.

Solution:
If you inspect the mysql-service in the beta namespace, you will notice that the targetPort used to create this service is incorrect.
Compare this to the Architecture diagram and change it to 3306. Update the mysql-service as per the below YAML:

apiVersion: v1
kind: Service
metadata:
  name: mysql-service
  namespace: beta
spec:
    ports:
    - port: 3306
      targetPort: 3306
    selector:
      name: mysql




3)Troubleshooting Test 3: The same 2 tier application is deployed in the gamma namespace. It must display a green web page on success. Click on the App tab at the top of your terminal to view your application. It is currently failed or unresponsive. Troubleshoot and fix the issue.
Stick to the given architecture. Use the same names and port numbers as given in the below architecture diagram. Feel free to edit, delete or recreate objects as necessary.

ArchitectureImage:
https://prnt.sc/45eaA8moRPg-

Issue:Inspect the selector used by the mysql-service. Is it correct?

Solution:
If you inspect the mysql-service, you will see that that the selector used does not match the label on the mysql pod.

Service:

root@controlplane:~# kubectl -n gamma describe svc mysql-service | grep -i selector
Selector:          name=sql00001
root@controlplane:~#
Pod:

root@controlplane:~# kubectl -n gamma describe pod mysql | grep -i label   
Labels:       name=mysql
root@controlplane:~#
As you can see the selector used is name=sql001 whereas it should be name=mysql.
Update the mysql-service to use the correct selector as per the below YAML:

---
apiVersion: v1
kind: Service
metadata:
  name: mysql-service
  namespace: gamma
spec:
    ports:
    - port: 3306
      targetPort: 3306
    selector:
      name: mysql



4)Troubleshooting Test 4: The same 2 tier application is deployed in the delta namespace. It must display a green web page on success. Click on the App tab at the top of your terminal to view your application. It is currently failed. Troubleshoot and fix the issue.
Stick to the given architecture. Use the same names and port numbers as given in the below architecture diagram. Feel free to edit, delete or recreate objects as necessary.

ArchitectureImage:
https://prnt.sc/22a4uBitG4LT

Issue:Are the environment variables used by the deployment correct?


Try accessing the web application from the browser using the tab called app. You will notice that it cannot connect to the MySQL database:
Environment Variables: DB_Host=mysql-service; DB_Database=Not Set; DB_User=sql-user; DB_Password=paswrd; 1045 (28000): Access denied for user 'sql-user'@'10.244.1.9' (using password: YES)


According to the architecture diagram, the DB_User should be root but it is set to sql-user in the webapp-mysql deployment.
Use the command kubectl -n delta edit deployments.apps webapp-mysql and update the environment variable as follows:

spec:
      containers:
      - env:
        - name: DB_Host
          value: mysql-service
        - name: DB_User
          value: root
        - name: DB_Password
          value: paswrd


This will recreate the pod and you should then be able to access the application.


5)Troubleshooting Test 5: The same 2 tier application is deployed in the epsilon namespace. It must display a green web page on success. Click on the App tab at the top of your terminal to view your application. It is currently failed. Troubleshoot and fix the issue.
Stick to the given architecture. Use the same names and port numbers as given in the below architecture diagram. Feel free to edit, delete or recreate objects as necessary.

ArchitectureImage:
https://prnt.sc/haJDWtZ_iwKN

Issue:
Ensure the right password is configured for the database on the MySQL pod and the Web application.

Solution:
If you inspect the environment variable called MYSQL_ROOT_PASSWORD, you will notice that the value is incorrect as compared to the architecture diagram: -


root@controlplane:~# kubectl -n epsilon describe pod mysql  | grep MYSQL_ROOT_PASSWORD 
      MYSQL_ROOT_PASSWORD:  passwooooorrddd
root@controlplane:~#


Correct this by deleting and recreating the mysql pod with the correct environment variable as follows: -

spec:
  containers:
  - env:
    - name: MYSQL_ROOT_PASSWORD
      value: paswrd


Also edit the webapp-mysql deployment and make sure that the DB_User environment variable is set to root as follows: -

spec:
    containers:
    - env:
      - name: DB_Host
        value: mysql-service
      - name: DB_User
        value: root
      - name: DB_Password
        value: paswrd


Once the objects are recreated, and you should be able to access the application.



6)Troubleshooting Test 6: The same 2 tier application is deployed in the zeta namespace. It must display a green web page on success. Click on the App tab at the top of your terminal to view your application. It is currently failed. Troubleshoot and fix the issue.
Stick to the given architecture. Use the same names and port numbers as given in the below architecture diagram. Feel free to edit, delete or recreate objects as necessary.

ArchitectureImage:
https://prnt.sc/o7VazUYzU9gS

Issue:
Inspect the ports used by the web-service is it correct? Are the environment variable used as per the architecture diagram?

Solution:
There are a few things wrong in this setup:

1. If you inspect the web-service, you will see that the nodePort used is incorrect.
This service should be exposed on port 30081 and NOT 30088.

root@controlplane:~# kubectl -n zeta get svc web-service 
NAME          TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
web-service   NodePort   10.102.190.212   <none>        8080:30088/TCP   3m1s
root@controlplane:~#


To correct this, delete the service and recreate it using the below YAML file:

---
apiVersion: v1
kind: Service
metadata:
  name: web-service
  namespace: zeta
spec:
  ports:
  - nodePort: 30081
    port: 8080
    targetPort: 8080
  selector:
    name: webapp-mysql
  type: NodePort


2. Also edit the webapp-mysql deployment and make sure that the DB_User environment variable is set to root as follows: -

spec:
    containers:
    - env:
      - name: DB_Host
        value: mysql-service
      - name: DB_User
        value: root
      - name: DB_Password
        value: paswrd


3. The DB_Password used by the mysql pod is incorrect. Delete the current pod and recreate with the correct environment variable as per the snippet below: -

spec:
  containers:
  - env:
    - name: MYSQL_ROOT_PASSWORD
      value: paswrd


Once the objects are recreated, and you should be able to access the application.












=================================================
##Other Topics            
================================================= 



##PracticeTest Practice JSON PATH
-------------------------------------------------



##PracticeTest advanced-kubectl-commands
-------------------------------------------------

1)Get the list of nodes in JSON format and store it in a file at /opt/outputs/nodes.json.
=>kubectl get nodes -o json>/opt/outputs/nodes.json

2)Get the details of the node node01 in json format and store it in the file /opt/outputs/node01.json.
=>kubectl get node node01 -o json>/opt/outputs/node01.json


3)Use JSON PATH query to fetch node names and store them in /opt/outputs/node_names.txt.
Remember the file should only have node names.
=>kubectl get nodes -o jsonpath='{.items[*].metadata.name}'>/opt/outputs/node_names.txt



4)Use JSON PATH query to retrieve the osImages of all the nodes and store it in a file /opt/outputs/nodes_os.txt.
The osImages are under the nodeInfo section under status of each node.
=>kubectl get nodes -o jsonpath='{.items[*].status.nodeInfo.osImage}' > /opt/outputs/nodes_os.txt


5)A kube-config file is present at /root/my-kube-config. Get the user names from it and store it in a file /opt/outputs/users.txt.
Use the command kubectl config view --kubeconfig=/root/my-kube-config to view the custom kube-config.

=>kubectl config view --kubeconfig=my-kube-config -o jsonpath="{.users[*].name}" > /opt/outputs/users.txt


6)A set of Persistent Volumes are available. Sort them based on their capacity and store the result in the file /opt/outputs/storage-capacity-sorted.txt.
=>kubectl get pv --sort-by=.spec.capacity.storage > /opt/outputs/storage-capacity-sorted.txt


7)That was good, but we don't need all the extra details. Retrieve just the first 2 columns of output and store it in /opt/outputs/pv-and-capacity-sorted.txt.
The columns should be named NAME and CAPACITY. Use the custom-columns option and remember, it should still be sorted as in the previous question.
=>kubectl get pv --sort-by=.spec.capacity.storage -o=custom-columns=NAME:.metadata.name,CAPACITY:.spec.capacity.storage > /opt/outputs/pv-and-capacity-sorted.txt



8)Use a JSON PATH query to identify the context configured for the aws-user in the my-kube-config context file and store the result in /opt/outputs/aws-context-name.
=>kubectl config view --kubeconfig=my-kube-config -o jsonpath="{.contexts[?(@.context.user=='aws-user')].name}" > /opt/outputs/aws-context-name
































