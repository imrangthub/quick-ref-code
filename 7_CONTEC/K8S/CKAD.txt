#################################################
#          CKAD EXAM-PREPARATION                #
#################################################


https://killer.sh/ckad
https://blog.getambassador.io/ckad-cka-exam-tips-from-10-people-who-passed-the-exam-24132b1f1cc5
10 people commat for CKAD

CKAD Exam Artical:
https://blog.getambassador.io/how-to-pass-the-ckad-exam-f8ca180ee853


DCUBE20 25% disc
https://devopscube.com/ckad-exam-study-guide/
Good site for exam  slabus

CNCF Certification
Certified Kubernetes Application Developer: http://kub.to/dev

Candidate Handbook: https://www.cncf.io/certification/candidate-handbook

Exam Tips: https://docs.linuxfoundation.org/tc-docs/certification/tips-cka-and-ckad

Keep the code – DEVOPS15 – handy while registering for the CKA or CKAD exams at Linux Foundation to get a 15% discount.




Certified Kubernetes Application Developer: https://www.cncf.io/certification/ckad/
Candidate Handbook: https://www.cncf.io/certification/candidate-handbook
Exam Tips: https://docs.linuxfoundation.org/tc-docs/certification/tips-cka-and-ckad
https://www.youtube.com/watch?v=wtKef83kmUA&list=PL0hSJrxggIQoKLETBSmgbbvE4FO_eEgoB

https://docs.linuxfoundation.org/tc-docs/certification/faq-cka-ckad-cks
FAQ

lAB
https://kodekloud.com/lessons/recap-core-concepts/



=>sudo apt-get update && apt-get install iputils-ping && sudo apt install net-tools
=================================================
#General                                
=================================================

=>kubectl api-resources

=>kubectl version
=>kubectl version --short 
=>kubectl get nodes

=>kubectl get pods
=>kubectl get pods -o wide
=>kubectl get -o json pod prodName

=>kubectl explain pod
=>kubectl explain deployment

=kubectl logs myPod

=>kubectl run --help
=>kubectl run nginx --image=nginx --dry-run=client
=>kubectl run nginx --image=nginx --dry-run=client -o yaml
=>kubectl run myng --image=nginx --dry-run=client -o yaml>labelpod.yaml
=>kubectl create deployment --image=nginx nginx --dry-run=client -o yaml
=>kubectl run ng3 --image=nginx -- /bin/sh -c "while true; do echo  $(date); sleep 1; done "

=>kubectl run pod1 --image=imranmadbar/nginx && sleep 2 && kubectl exec -it pod1 -- bash
=>echo -e "Welcome to nginx! \nHost Name: $(hostname -f)\nHost IP: $(hostname -i)">/usr/share/nginx/html/index.html

=>kubectl run mynginx --image=nginx
=>kubectl run  bu1 --image=busybox -- sh  -c "hostname -i"
=>kubectl run logpod --image=busybox -- sh -c "ping google.com"
=>kubectl run bx2 --image=busybox -- sh -c "while true; do echo $(date)>>/var/1.log; sleep 1; done"

=>kubectl run -it ubuntu1 --image=ubuntu --restart=Never -- bash -ec "apt update; apt install mysql-server; bash"
          =while true; do echo "infinity"; sleep 1; done

=>kubectl exec -it ub1 -- bash
=>kubectl exec -it mynginx -- bash
=>kubectl exec -it mynginx -- ls -l


=>kubectl apply -f sample.yaml
=>kubectl delete -f sample.yaml
=>kubectl delete pod my-pod1 

=>kubectl describe pod mynginx


@@@KodeKloud
=================================================
##CoreConcept         
================================================= 

Two type Node: 
-------------------------------------------------
- Master
       It has install kube-apiserver and this is  make it master node.
       Master also containe: etcd, controller, scheduler etc.
- Worker
       Install container runtime or rkt or cri-o etc.
       Its contain kubelet component what is response master node kube-apiserver.


kubectl CLI: 
      Its a commandline utility, user to deploy manage application on kubernetes cluster.
      Get the cluster information, status the other node etc.
      Using command like:
        =>kubectl run hellow-world-app
        =>kubectl cluster-info
        =>kubectl get nodes


When instal kubernetes, install this component:
-------------------------------------------------
- API Server
  API Server act as a frontend of kubernetes, user, devece, terminal interface etc.

- etcd
  Key-value prayers storage, Store all data for managing cluster.

- Scheduler
  Scheduler Responsibel for distribute work to across multiple node in cluster.
  Like createing container and assign this to approvaid node.

- Controller
  Controller is the brain behind the workstation, its notice when node,container,endpoint gose down, 
  and make decision bring new one. and find right node for right pod and other component.

- ContainerRuntime
  ContainerRuntime is the underlying software for running container(Docker, containerD).

- kubelet
  Kubelet is an agent present on each worker node, make sure the container running on node as aspected.



Docker vs ContainerD 
-------------------------------------------------
Kubernetes->Docker was titly couple at first time, then kubernetes get popularity
and Other vendor want to join like rkt etc. 
This time kubernetes make an interface for CRI(Container runtime Interface) and now 
every vendor can join if it mentain OCI(Open Container Initiative=imagespace,runtimespace).

Docker was before OCI this why it not flow the OCI, to help this use a tool called dockershim with kubernetes.

Docker component (CLI,api, volumes,auth,security,build) working with containerD
and ContainerD support OCI so, from kubernetes v1.24 remove dockershim, only working with containerD.

ContainerD - CLI = ctr 
ctr come with containerD not friendly only support limited feature for debuging purpose.
Like:
=>ctr image pull docker.io/library/redis
=>ctr run docker.io/library/redis redis

A alternetis of ctr is nerdctl, its provide docker like cli for containerD.
Its support newest feature in containerD, lazy pulling, dokcer composer, namespace in kubernetes etc.

NERDCTL command like docker:
=>nerdctl run --name redis redis
=>nerdctl run --name mywebserver -p8181:80 -d nginx

CLI - CRICTL
crictl provides a CLI for CRI compatable container runtime, install separately.
User to inspect, debug container runtimes work accross dirrerent runtime and mantaine develop by kubernetes.

kubernetes->CRI->crictl->| rkt, containerD etc.
=>crictl pull nginx
=>crictl images
=>crictl ps -a
=>crictl logs 567890987654456789
=>crictl pod





##POD
-------------------------------------------------
Kubernetes use yaml defination file as inpur to kubernetes.

All defination file flow same strecture, all file 4 top/root lavel field, this field are required.

apiVersion: v1               | Object version
kind: Pod                    | Object type       
metadata:                    | Data about the object
spac:                        | Object spacefication secrion, provide aditional iinfo about object.


apiVersion: v1               | Object version
kind: Pod                    | Object type       
metadata:                    | Data about the object
  name: my-pod
  labels:
    app: myapp
    type: back-end
spac:                        | Object spacefication secrion, provide aditional iinfo about object.

Wen a pod is start it accessable within from of its host machine, for out side user need to add 
Another component of kubernatis called Service.




##Solution:01 | Pod
-------------------------------------------------
=>kubectl run nginx --image=nginx

What is the image used to create the new pods?
=>kubectl describe pod newpods-j44dw | grep -i image

Show pod IP, NODe
=>k get pod -o wide

How many container part of the webapp pod and What images are used in the new webapp pod?
=>kubectl describe pod webapp
Check the container section,and look at the images used.


Why do you think the container agentx in pod webapp is in error?
=>kubectl describe pod webapp 
and look under the events section, An image by that name does not exist in DockerHub.



1)Which nodes are these pods placed on?
=>kubectl get pod -o wide

2)What does the READY column in the output of the kubectl get pods command indicate?
runing container/total container.

3)Create a new pod with the name redis and with the image redis123.
=>kubectl run redis --image=redis123 --dry-run=client -o yaml
=>kubectl run redis --image=redis123 --dry-run=client -o yaml > redis-definition.yaml

4)Now change the image on this pod to redis.
=>kubectl edit pod redis 
For Edit/Change pod image is easy done by =>kubectl edit command with in a single command.
No need to take other step.


Edit current running pod, like change image
5) kubectl edit pod myng
OR
6) Edit then pod defanation yaml file and run again
=>Kubectl apply -f mypod.yaml


Generate another pod defenation yaml file from current pod
=>>kubectl get pod mypod -o yaml
=>kubectl get pod mypod -o yaml> pod.yaml


Remember, you CANNOT edit specifications of an existing POD other than the below.
spec.containers[*].image
spec.initContainers[*].image
spec.activeDeadlineSeconds
spec.tolerations

For edit other file generate another yaml file from current pod status and update it, delete perivious pod and run new one.
=>kubectl get pod mypod -o yaml> pod.yaml
=>kubectl apply -f pod.yaml





##ReplicaSet
-------------------------------------------------
RsVersion:  apiVersion: apps/v1

Replica controller and ReplicaSet are same purpose, controller is older and ReplicaSet are newar.
ReplicaSet make certain number of pod allows running.

Event you need only a single pod you cand user replicaSet, it keep allow one podn runing in your system.

One of mazor Diffeetnt of ReplicaSet and RsController is selector property, what is optional in RsController.
Using this selector ReplicaSet select the pod on its own area.



Simple ReplicaSet:
apiVersion:apps/v1
kind: ReplicaSet
metadata:
  name: replicaset-1
spec:
  replicas: 2
  selector:
    matchLabels:
      tier: frontend
  template:
    metadata:
      labels:
        tier: frontend
    spec:
      containers:
      - name: nginx
        image: nginx


For finding any issu in defination yaml file, first run the yaml
=>kubectl create -f myreplicaset.yaml
And now check the error message


BasicCommand:
=>kubectl create -f rsfilw.yaml
=>kubectl get rs
=>kubectl delete rs myrsx
=>kubectl replace -f myrs.yaml
=>kubectl scale -replicas=5 -f myrsfile.yaml




##Update/Scale ReplicaSet
-------------------------------------------------
1) Update number of replica(replicas:5) in yaml file and run
=>kubectl replace -f myreplica.yaml
OR
2) Run
=>kubectl scale --replicas=5 -f myreplica.yaml
OR
3) Run with replical type(replicaset) and name (myreplicaset) 
=>kubectl scale --replicas=5 replicaset myreplicaset
2 and 3 Not made change the original defination file

There are other approce to Update replicaset count.this is advance.

Check ReplicaSet info like: uses image, etc
=>kubectl describe rs new-replica-set

Check the pod status of ReplicaSet
=>kubectl get rs new-replica-set


If need to Update ReplicaSet like Image change, then its does not automatically re-create the  pod.
First you can eidt current rs
=>kubectl edit rs new-replica-set 
Then delete existing pod
=>kubectl delete pod --all
and RS will create new pod with new Image
OR
Delete old rs and create again


=>kubectl get rs myreplicaset1 -o yaml
=>kubectl get rs myreplicaset1 -o yaml>myrs2.yaml



##Solution:02 | ReplicaSet
-------------------------------------------------
1)How many ReplicaSets exist on the system?
=>kubectl get replicaset

2)What is the image used to create the pods in the new-replica-set?
=>kubectl describe rs new-replica-set


3)Why do you think the PODs are not ready in replicaset?
=>kubectl get rs
=>kubectl get pods

=>kubectl get events --field-selector involvedObject.name=podName
=>kubectl get events | grep objName
OR
=>kubectl logs podName
=>kubectl describe po new-replica-set-pcn6t

4)Delete the two newly created ReplicaSets - replicaset-1 and replicaset-2
=>kubectl delete rs replicaset-1 replicaset-2


5)Fix the original replica set new-replica-set to use the correct busybox image.
Either delete and recreate the ReplicaSet or Update the existing ReplicaSet and then delete all PODs, so new ones with the correct image will be created.

=>kubectl get rs
=>kubectl edit rs new-replica-set
=>k delete pod --all

Edit/Change Image name from Rs is done only one command kubectl edit rs rsName, but need to delete previously created pod.
Because old pod running on with old image, for effact update image, need to delte all pod created by this rs,
If delte all pod using =>kubectl delte pod --all command, thend this rs will create new pod wfith update image.




6)Scale the ReplicaSet to 5 PODs.
Use kubectl scale command or edit the replicaset using kubectl edit replicaset.
=>kubectl scale replicaset new-replica-set --replicas=5
OR
=>kubectl edit rs new-replica-set
Update new value in:
spec:
  replicas: 2
  selector:



##Deployment
-------------------------------------------------
Deployment defination file as ReplicaSet just kind willbe Deployment.
Deployment mainly provide different type approce for pod Update like: Recreate, RollingUpdate.


=>kubectl create -f mydeploy.yaml
Its create three Object:

=>k get deploy
=>k get rs
=>k get pods
OR
=>k get all

apiVersion: apps/v1
kind: Deployment
metadata:
  name: deployment-1
spec:
  replicas: 2
  selector:
    matchLabels:
      name: busybox-pod
  template:
    metadata:
      labels:
        name: busybox-pod
    spec:
      containers:
      - name: busybox-container
        image: busybox
        command:
        - sh
        - "-c"
        - echo Hello Kubernetes! && sleep 3600

With Deployments you can easily edit any field/property of the POD template. 

=>kubectl create deployment --help
=>kubectl create deployment httpd-frontend --image=httpd:2.4-alpine --replicas=3 --dry-run=client -o yaml>mydeployment1.yaml


Generate a yaml file from current deployment
=>kubectl get deployment mydeployment -o yaml
=>kubectl get deployment httpd-frontend -o yaml>mydeployment2.yaml



##Solution:03 Deployment
-------------------------------------------------

1)What is the image used to create the pods in the new deployment?
=>kubectl describe deploy frontend-deployment 

2)Why do you think the deployment is not ready?

=>kubectl get deployment
=>kubectl describe deployment
=>kubectl get pod
=>kubectl get events --field-selector involvedObject.name=podName

3)Create a new Deployment with the below attributes using your own deployment definition file.
Name: httpd-frontend;
Replicas: 3;
Image: httpd:2.4-alpine
Name: httpd-frontend
Replicas: 3
Image: httpd:2.4-alpine

=>kubectl create deployment -h
=>kubectl create deployment httpd-frontend --image=httpd:2.4-alpine --replicas=3 --dry-run=client -o yaml>mydep.yaml





##NameSpace
-------------------------------------------------
Call one service to other service in different namespace:
app-service call db-service in DEV namespace:
  app-service.connect("db-service.dev.svc.cluster.local")
      
         db-service     .dev        .svc       .cluster.local
Here:    serviceName    Namespace   Service    Default Domain name in k8      v            

Switch Namespace:
=>kubectl config set-context $(kubectl config current-contest) --namespace=dev
=>kubectl get pods --all-namespace

=>kubectl get all

=>kubectl get namespace
=>kubectl get pod --namespace=research
=>kubectl get pod -n=research
=>kubectl run redis --image=redis -n=research


=>kubectl get pod -A
=>kubectl get pod --all-namespaces
Show all pod with namespace

=>kubectl get pods -n=marketing
=>kubectl get svc -n=marketing
Show service of a specific namespace

We can set Resource Quota for specifice namespace.
like:
apiVersion: v1
kind: ResourceQuota
metadata: 
  name: compute-quota
  namespace: dev
spac:
  hard:
    prods: "10"
    request.cpu: "4"
    request.memory: "5Gi"
    limits.cpu: "10"
    limits.memory: "10Gi"




##Solution:04 NameSpace
-------------------------------------------------

=>k get namespace --no-headers | wc -l

=>k get pod -A
Get all pod in all namespacw


1)How many Namespaces exist on the system?
=>kubectl get ns

2)How many pods exist in the research namespace?
=>kubectl get pod -n research

3)Create a POD in the finance namespace.
Name: redis
Image Name: redis
=>kubectl run redis --image=redis --namespace finance

4)Which namespace has the blue pod in it?
=>kubectl get pods --all-namespaces | grep blue


5)What DNS name should the Blue application use to access the database db-service in the dev namespace?
You can try it in the web application UI. Use port 6379.

The FQDN consists of the <service-name>.<namespace>.svc.cluster.local format.
like:my-service.my-namespace.svc.cluster.local
=>db-service.dev.svc.cluster.local




Creating Kubernetes objects imperatively
-------------------------------------------------
There are two main ways to manage Kubernetes objects: 
  imperative (with kubectl commands) and 
  declarative (by writing manifests and then using kubectl apply).


=>kubectl run --help
=>kubectl run redis --image=redis:alpine --labels="tier=db"
=>get pod --show-labels


Create a service redis-service to expose the redis application within the cluster on port 6379.
Use imperative commands.
=>kubectl expose pod redis --port=6379 --name=redis-service
=>kubectl describe svc redis-service
OR
=>kubectl create service --help
=>kubectl create service clusterip --help
=>kubectl create service clusterip redis-service --tcp=6379:6379


expose:
=>kubectl expose --help
"expose" command user for expose existing pod for expose.
=>kubectl expose pod redis --port=6379 --name=redis-service
OR 
Create pod and service at a same time
=>kubectl run httpd --image=httpd:alpine --port=80 --expose=true
This command create pod as well as service with same name with labels and bind service with pod at a time




##Solution:06 | IMPERATIVE COMMANDS
-------------------------------------------------



1)Deploy a pod named nginx-pod using the nginx:alpine image.
Name: nginx-pod
Image: nginx:alpine

=>kubectl run nginx-pod --image=nginx:alpine

2)Deploy a redis pod using the redis:alpine image with the labels set to tier=db.
=>kubectl run --help
=>kubectl run redis --image=redis:alpine --labels="tier=db"
=>get pod --show-labels


3)Create a service redis-service to expose the redis application within the cluster on port 6379.
Use imperative commands.
Service: redis-service
Port: 6379
Type: ClusterIP

=>k expose pod -h
=>kubectl expose pod redis --port=6379 --name=redis-service
=>kubectl describe svc redis-service
OR
=>kubectl create service --help
=>kubectl create service clusterip --help
=>kubectl create service clusterip redis-service --tcp=6379:6379


4)Create a deployment named webapp using the image kodekloud/webapp-color with 3 replicas.
Try to use imperative commands only. Do not create definition files.
Name: webapp
Image: kodekloud/webapp-color
Replicas: 3

=>kubectl create deployment --help
=>kubectl create deployment webapp --image=kodekloud/webapp-color --replicas=3


5)Create name space and Create a new deployment called redis-deploy in the dev-ns namespace with the redis image. It should have 2
=>kubectl run custom-nginx --image=nginx --port=8080
=>kubectl create namespace dev-ns
=>kubectl create deployment redis-deploy --image=redis --replicas=2 -n=dev-ns


6)Create a pod called httpd using the image httpd:alpine in the default namespace. 
Next, create a service of type ClusterIP by the same name (httpd). The target port for the service should be 80.

=>kubectl run httpd --image=httpd:alpine --port=80 --expose
OR
=>kubectl run httpd --image=httpd:alpine --port=80
=>kubectl expose pod httpd --port=80 --name=httpd
OR
=>kubectl create service clusterip --help
=>kubectl create service clusterip httpd --tcp=5678:80

=>kubectl get pod --show-labels
=>kubectl edit pod httpd
Update the lavel as service





=================================================
##Configuration         
================================================= 


##Command and Args in Docker 
-------------------------------------------------
Container unlike vM its a specific task, when finish the task its tarminated automatically.
Container remain unitl a process or task inside it running on it. 
If a web service in container stop or cursh container are exists.


NGINX Image:
-------------------------------------------------
# Pull the minimal Ubuntu image
FROM ubuntu

# Install Nginx
RUN apt-get -y update && apt-get -y install nginx

# Copy the Nginx config
COPY default /etc/nginx/sites-available/default

# Expose the port for access
EXPOSE 80/tcp

# Run the Nginx server
CMD ["/usr/sbin/nginx", "-g", "daemon off;"]



UBUNTU Image:
-------------------------------------------------
# Use the official Ubuntu base image
FROM ubuntu:latest

# Update package lists and upgrade existing packages
RUN apt-get update && apt-get upgrade -y

# Your additional instructions here (install packages, configure settings, etc.)

# Clean up the package cache to reduce image size
RUN apt-get clean && rm -rf /var/lib/apt/lists/*

# Set the default command when running the container
CMD ["bash"]



This Nginx image and Ubuntu image has tow dirrerent CMD command:

  Nginx start a process in side container using this command 
  CMD ["/usr/sbin/nginx", "-g", "daemon off;"]

  Whenre Ubuntu container run this bash Command
  CMD ["bash"]



Bash is not a actual process, its a shell for listing inpur from tarminal.
Expect to receiving actuall process command, if not find then it exist.

By default docker not add a termional when a container run, and bash not found any terminal and it exited.

So, how prevent it: 

FirstWay:
Override the default command like this:
=>docker run ubuntu sleep 5

SecondWay:
Create your won image with a sleep command or other process start with the command.

From Ubuntu
CMD sleep 2


We can provide this command different way:

AsBash:
CMD:
command parameters

CMD ["command", "parameter1"]

OR
AsJSONArraty Formate:
CMD sleep 2
CMD ["sleep","5"]

Remember: when cmd will be JSON formate, First element of array have to be executable.
And Command and Arg not mixed as, 
CMD ["sleep 5"]
command and param should be different.

If this way i build by image like, and run it, it will be 5 sec. No need parse arge on run time.
From Ubuntu
CMD sleep 2

=>docker build -t my-ubuntu .
=>docker run my-ubuntu
Now if we wanto change the number of sleep thne we cand do it:(again overrides the command)
=>docker run my-ununtu sleep 10

But what about we cand only pass the number:
=>docker run my-ubunty 100

For this ENTRYPOINT Come into picture:

FROM Ubuntu
ENTRYPOINT["sleep"]

Now if run this:
=>docker run my-ubunty 100

100 will append on this sleep command like this: sleep 100

Thas the different between CMD and ENTRYPOINT. CMD replace, ENTRYPOINT append.

Now for default value of ENTRYPOINT:
FROM Ubuntu
ENTRYPOINT["sleep"]
CMD["5"]

Now if we pass param then it will override cmd default or it run with defafutl value
=>docker run my-ubunty | will be 5 second sleep
=>docker run my-ubunty 10 | will be 10 second sleep

Remembar: for combine both ENTRYPOINT and CMD must use JSON formate.
Then:
If you need to pass the ENTRYPOINT then:
=>docker run --entrypoint ping my-ubuntu google.com -T
This will override the docker file entry point value





##Command and Args in Kubernetes | Pod
-------------------------------------------------
A simeple Pod Defination file willbe using privious Docker image:
apiVersion: v1
kind: Pod
metadata:
  name: my-ubuntu-pod
spec:
  containers:
    - name: my-ununtu-cont
      image: my-ununtu
      args: ["10"]


If we want to Override the command form Pod file, it will be like this:
apiVersion: v1
kind: Pod
metadata:
  name: my-ubuntu-pod
spec:
  containers:
    - name: my-ununtu-cont
      image: my-ununtu
      command: ["ping"]
      args: ["google.com"]


Kubernetes "command" override Docker "ENTRYPOINT"
kubernetes "args"    override Docker "CMD"




=>kubectl run ub1 --image=ubuntu sleep 4000
If we pass value on inparageve way, then its reveive in pod as Args.

This is with args:
apiVersion: v1 
kind: Pod 
metadata:
  name: ub3 
spec:
  containers:
  - name: ubuntu
    image: ubuntu
    args:
       - "sleep"
       - "5000"
OR
apiVersion: v1
kind: Pod 
metadata:
  name: ub4
spec:
  containers:
  - name: ubuntu
    image: ubuntu
    args: ["sleep","5000"]


This is with command:
apiVersion: v1 
kind: Pod 
metadata:
  name: ub3 
spec:
  containers:
  - name: ubuntu
    image: ubuntu
    command:
       - "sleep"
       - "5000"
OR
apiVersion: v1
kind: Pod 
metadata:
  name: ub5
spec:
  containers:
  - name: ubuntu
    image: ubuntu
    command: ["sleep","5000"]



## Solution1: Pod command and Args
-------------------------------------------------
=>kubectl run ub1 --image=ubuntu sleep 4000

1)What is the command used to run the pod ubuntu-sleeper?
=>kubectl get pod ubntu-sleeper -o yaml
=>kubectl describe pod ubuntu-sleeper

2)Create a pod with the ubuntu image to run a container to sleep for 5000 seconds.
apiVersion: v1 
kind: Pod 
metadata:
  name: ubuntu-sleeper-2 
spec:
  containers:
  - name: ubuntu
    image: ubuntu
    command:  ["sleep"]
    args: ["5000"]

apiVersion: v1 
kind: Pod 
metadata:
  name: ubuntu-sleeper-2 
spec:
  containers:
  - name: ubuntu
    image: ubuntu
    args: ["sleep","5000"]

apiVersion: v1 
kind: Pod 
metadata:
  name: pod3 
spec:
  containers:
  - name: ubuntu
    image: ubuntu
    args:
      - "sleep"
      - "1200"

apiVersion: v1 
kind: Pod 
metadata:
  name: ubuntu-sleeper-3 
spec:
  containers:
  - name: ubuntu
    image: ubuntu
    command:
      - "sleep"
      - "1200"

apiVersion: v1 
kind: Pod 
metadata:
  name: webapp-green
  labels:
      name: webapp-green 
spec:
  containers:
  - name: simple-webapp
    image: kodekloud/webapp-color
    command: ["python", "app.py"]
    args: ["--color", "pink"]


SimpleDockerFile:
  FROM python:3.6-alpine
  RUN pip install flask
  COPY . /opt/
  EXPOSE 8080
  WORKDIR /opt
  ENTRYPOINT ["python", "app.py"]
  CMD ["--color", "red"]

FinalRunningCommandWillBe:
=>python app.py --color red



cat Dockerfile 
FROM python:3.6-alpine
RUN pip install flask
COPY . /opt/
EXPOSE 8080
WORKDIR /opt
ENTRYPOINT ["python", "app.py"]
CMD ["--color", "red"]

cat webapp-color-pod.yaml 
  apiVersion: v1 
  kind: Pod 
  metadata:
    name: webapp-green
    labels:
        name: webapp-green 
  spec:
    containers:
    - name: simple-webapp
      image: kodekloud/webapp-color
      command: ["--color","green"]

FinalRunningCommandWillBe:
=>--color green


cat Dockerfile 
  FROM python:3.6-alpine
  RUN pip install flask
  COPY . /opt/
  EXPOSE 8080
  WORKDIR /opt
  ENTRYPOINT ["python", "app.py"]
  CMD ["--color", "red"]

cat webapp-color-pod-2.yaml 
apiVersion: v1 
kind: Pod 
metadata:
  name: webapp-green
  labels:
      name: webapp-green 
spec:
  containers:
  - name: simple-webapp
    image: kodekloud/webapp-color
    command: ["python", "app.py"]
    args: ["--color", "pink"]

FinalRunningCommandWillBe:
=>python app.py --color pink




2)Update pod ubuntu-sleeper-3 to sleep for 2000 seconds.
=>kubectl get pod ubuntu-sleeper-3 -o yaml>newpod.yaml
Modify the newpod.yaml file and run again
OR
=>kubectl replace --force -f newpod.yaml
It will delete existing pod and create new one
OR
=>kubectl apply --force -f /tmp/kubectl-edit-219235037.yaml



Dockerfile with ENTRYPOINT and cmd
FROM python:3.6-alpine
RUN pip install flask
COPY . /opt/
EXPOSE 8080
WORKDIR /opt
ENTRYPOINT ["python", "app.py"]
CMD ["--color", "red"]

apiVersion: v1 
kind: Pod 
metadata:
  name: webapp-green
  labels:
      name: webapp-green 
spec:
  containers:
  - name: simple-webapp
    image: kodekloud/webapp-color
    command: ["python", "app.py"]
    args: ["--color", "pink"]


=>kubectl run webapp-green2 --image=kodekloud/webapp-color --command -- python app.py  -- --color=green

3)Create a pod with the given specifications. By default it displays a blue background. 
Set the given command line arguments to change it to green.
=>kubectl run webapp-green --image=kodekloud/webapp-color -- --color=green
OR
apiVersion: v1 
kind: Pod 
metadata:
  name: webapp-green
  labels:
      name: webapp-green 
spec:
  containers:
  - name: simple-webapp
    image: kodekloud/webapp-color
    args: ["--color", "green"]





##Environment Varibale | evn
-------------------------------------------------
Set Env varibale in Docker:
=>docker run -e APP_COLOR=green my-colorwebapp

Set Env varibale in Kubernetes pod:
apiVersion: v1 
kind: Pod 
metadata:
  name: webapp-green
  labels:
      name: webapp-green 
spec:
  containers:
  - name: simple-webapp
    image: kodekloud/webapp-color
    args: ["--color", "green"]
    
    env:
      - name: APP_COLOR
        value: green


In pod we can set env from different source:
env: 
  - name: APP_COLOR
    value: green
OR
env: 
  - name: APP_COLOR
    valueFrom:
      configMapKeyRef:
OR
env: 
  - name: APP_COLOR
    valueFrom:
      secretKeyRef:
  



 
##ConfigMap
-------------------------------------------------
When we have loat of evn data it is deficult to manage  those variable data.This is why ConfigMap.
For this create a ConfigMap using keyvalue payers and interact in pod:



Create a configmap both two way: Imparative and Diclarative:

=>kubectl create configmap --help
=>kubectl create configmap my-configmap --from-literal=key1=val1 --from-literal=key2=val2
Or from a keyvalue file  

Declarative:
apiVersion: v1
kind: ConfigMap
metadata: 
  name: my-configmap
data:
  APP_COLOR: blue
  APP_TYPE: prod

=>kubectl get configmap
=>kubectl describe configmap my-configmap



apiVersion: v1 
kind: Pod 
metadata:
  name: webapp-green
  labels:
      name: webapp-green 
spec:
  containers:
  - name: simple-webapp
    image: kodekloud/webapp-color
    args: ["--color", "green"]

    envFrom:
      - configMapRef:
          name: my-configmap



We can ingect the configmap: as evn, as a single value for a confifmap, or whole data at-once as a volume:
ENV: 
envFrom:
  - configMapRef:
    name: myapp-configmap

SINGLE ENV:
env:
  - name:
    valueFrom: 
      configMapKeyRef:
        name: my-configmap
        key: APP_COLOR

OR as VOLUME:
volume:
  - name: my-app-volume
    configMap:
      name: my-configmap
    






##Solution02 | ConfigMap
-------------------------------------------------
What is the environment variable name set on the container in the pod?
=>kubectl describe pod webapp-color
=>kubectl get pod webapp-color -o yaml
=>kubectl exec webapp-color -- env


1)Update the environment variable on the POD to display a green background.
Note: Delete and recreate the POD. 
=>kubectl get pod webapp-color -o yaml>newpod.yaml
=>kubectl replace --force -f newpod.yaml


Identify the database host from the config map db-config?
=>kubectl describe configmap db-config


2)Create a new ConfigMap for the webapp-color POD. Use the spec given below?
  ConfigMap Name: webapp-config-map
  Data: APP_COLOR=darkblue
  Data: APP_OTHER=disregard

=>kubectl create configmap --help
=>kubectl create configmap my-config --from-literal=key1=config1 --from-literal=key2=config2

=>kubectl create configmap webapp-config-map --from-literal=APP_COLOR=darkblue --from-literal=APP_OTHER=disregard
OR
apiVersion: v1
data:
  APP_COLOR: darkblue
  APP_OTHER: disregard
kind: ConfigMap
metadata:
  creationTimestamp: null
  name: webapp-config-map

=>kubectl get cm

We can Edit configmap any falue with a single command using kubectl edit command.
=>kubectl edit configmap webapp-config-map 



3)Update the environment variable on the POD to use only the APP_COLOR key from the newly created ConfigMap.
Note: Delete and recreate the POD.
  Pod Name: webapp-color
  ConfigMap Name: webapp-config-map

=>k get po -o yaml>newpod.yaml
=>vi ewpod.yaml
add this:
  spec:
    containers:
    - env:
      - name: APP_COLOR
        valueFrom:
            configMapKeyRef:
              name: webapp-config-map
              key: APP_COLOR

=>kubectl replace --force -f newpod.yaml





##Secret
-------------------------------------------------
There are two step to use a secret: Create a secret, Ingect it on pod.

For pass data to a pod we can use configmap, but configmap data are plane text.
For database credentials and this type sensitive data should not keep as planetex formate this why come secret in kubernetes.
Secret data are encoded

We can create secret both way Imparative and Declarative.

=>kubectl create secret --help
=>kubectl create secret generic --help
=>kubectl create secret generic my-secret --from-literal=key1=supersecret --from-literal=key2=topsecret
OR
apiVersion: v1
kind: Secret
metadata:
  name: my-secret
data:
  DB_HOSTNAME: erewr4234234sdf=
  DB_USER: sfsfsfsfsfs==
  DB_PP: sfsdff2342

You must proved data in base64 encoded formate, for encode in linux:
=>echo -n "mysql" | base64
Decode:
=>echo -n "3df32==" | base64 --decode


When you create secret by imparative command no need to providce secret as encoded, plan tex is ok.
Onther hand Declarative way not allow plantext data, need base64 endoded.



=>kubectl get secret
=>kubectl get secret db-secret -o yaml
=>echo "sfsdfrweewer" | base64 --decode

=>kubectl describe secret dashboard-token 
Show How many secret data on a secret, What is the type of the dashboard-token secret.



For createing secret, secret data can be collect from fa file usig from file method.
the file carry data as keyvalue payers and myfile.properties extension.


We can Edit Secret using only a single command eidt:
=>k edit secret db-secret 






##Solution3: | Secret
-------------------------------------------------
How many secrets are defined in the dashboard-token secret?
=>kubectl describe secrets dashboard-token 
and look at the data field.
There are three secrets - ca.crt, namespace and token.


Which of the following is not a secret data defined in dashboard-token secret?
=>kubectl describe secrets dashboard-token 
and look at the data field.
There are three secrets - ca.crt, namespace and token. type is not a secret data.



We are going to deploy an application with the below architecture
We have already deployed the required pods and services. Check out the pods and services created. 
Check out the web application using the Webapp MySQL link above your terminal, next to the Quiz Portal Link.
https://prnt.sc/hAc-9XtgJ4eX

The reason the application is failed is because we have not created the secrets yet. 
Create a new secret named db-secret with the data given below.
You may follow any one of the methods discussed in lecture to create the secret.

Secret Name: db-secret
Secret 1: DB_Host=sql01
Secret 2: DB_User=root
Secret 3: DB_Password=password123



1)Create a new secret named db-secret with the data given below.
  Secret Name: db-secret
  Secret 1: DB_Host=sql01
  Secret 2: DB_User=root
  Secret 3: DB_Password=password123

=>kubectl create secret --help
=>kubectl create secret generic --help
=>kubectl create secret generic my-secret --from-literal=key1=supersecret --from-literal=key2=topsecret
=>kubectl create secret generic db-secret --from-literal=DB_Host=sql01 --from-literal=DB_User=root --from-literal=DB_Password=password123

=>kubectl get secret db-secret -o yaml
=>kubectl describe secret db-secret


=>kubectl describe pod webapp-pod
Show Is the pod has any secret

2)Configure webapp-pod to load environment variables from the newly created secret.
Delete and recreate the pod if required.

Pod name: webapp-pod
Image name: kodekloud/simple-webapp-mysql
Env From: Secret=db-secret


envFrom:
 - secretRef:
    name: db-secret

Add this under container like this:
spec:
  containers:
  - image: kodekloud/simple-webapp-mysql
    imagePullPolicy: Always
    name: webapp
    resources: {}
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    envFrom:
      - secretRef:
          name: db-secret


=>kubectl edit pod webapp-pod
=>kubectl replace --force -f /tmp/kubectl-edit-3803810208.yaml





##Encrypting the Secret data in Rest
-------------------------------------------------








##Security in Docker
-------------------------------------------------
Unlike VM Container and host shard the same OS kernel. Host has namespace and container has it s namespace,.

Docker container run its own namespace, and it not see other namespace process or host presecc.
This why when we run container ps command like this:
=>ps aux
Its only show one process runing on it. 
This is call Process Isolation.

Docker User:
Container has root user its like this root user of host, By default container run under root prvelise.
If you dont want to run container with default user you can pass user id on run command like this:
=>docker run --user=1000 ubuntu sleep 1000
OR
We cand set this user on docker file:
FROM ubuntu 
USER 1000
CMD ["sleep 1000"]

Docker container root user are not powerful as host root user, It can not has parmission as host root user.
Docer user Linux Capability to implement this.
Using  Linux Capability(dap-add/cap-drop) you cand add and remove capability power of docker container root user.
=>docker run --user=1000 ubuntu sleep 5
=>docker run --cap-add MAC_ADMIN ubuntu sleep 5
=>docker run --cap-drop KILL ubuntu sleep 5

You and run docker container with all privileged enable using priviledge flag
=>docker run --privileged ubuntu




##Kubernetes Security Contexts
-------------------------------------------------

As docker container we can add security on container lavel or pod lavel
When add it on pod level, it is for all container in a pod. 
If add security on container lavels then its is override thsi pod lavel security.

Add Security on Pod Level:
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  securityContext:
    runAsUser: 1000
  
  containers:
    - name: ubuntu
      image: ubuntu
      command: ["sleep", "1000"]

Add Security on container Level:
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
    - name: ubuntu
      image: ubuntu
      command: ["sleep", "1000"]
        securityContext:
          runAsUser: 1000

          capabilities:
            add: ["MAC_ADMIN"]
  


##Solution4: | Security Contexts
-------------------------------------------------
1)What is the user used to execute the sleep process within the ubuntu-sleeper pod?
=>kubectl exec -it ubuntu-sleeper -- whoami

2)Edit the pod ubuntu-sleeper to run the sleep process with user ID 1010.

Pod Name: ubuntu-sleeper
Image Name: ubuntu
SecurityContext: User 1010

add this under containers: section
  securityContext:
    runAsUser:1010

Like this:
spec:
  containers:
  - command:
    - sleep
    - "4800"
    image: ubuntu
    securityContext:
      runAsUser: 1010

=>kubectl replace --force -f /tmp/kubectl-edit-1907445851.yaml


Pod and container lavel SecurityContext:

A Pod definition file named multi-pod.yaml is given.
The pod is created with multiple containers and security contexts defined at the Pod and Container level.

apiVersion: v1
kind: Pod
metadata:
  name: multi-pod
spec:
  securityContext:
    runAsUser: 1001
  containers:
  -  image: ubuntu
     name: web
     command: ["sleep", "5000"]
     securityContext:
      runAsUser: 1002
  -  image: ubuntu
     name: sidecar
     command: ["sleep", "5000"]

With what user are the processes in the web container started?
The User ID defined in the securityContext of the container overrides the User ID in the POD. 1002

With what user are the processes in the sidecar container started?
The pod is created with multiple containers and security contexts defined at the Pod and Container level.1001





3)Update pod ubuntu-sleeper to run as Root user and with the SYS_TIME capability.
Pod Name: ubuntu-sleeper
Image Name: ubuntu
SecurityContext: Capability SYS_TIME

=>kubectl edit pod ubuntu-sleeper
add this under  SecurityContext:
capabilities:
  add: ["SYS_TIME"]

=>kubectl replace --force -f /tmp/kubectl-edit-2505242802.yaml


4)Now update the pod to also make use of the NET_ADMIN capability.

Pod Name: ubuntu-sleeper
Image Name: ubuntu
SecurityContext: Capability SYS_TIME
SecurityContext: Capability NET_ADMIN
Add this under containers section:
    securityContext:
      capabilities:
        add: ["SYS_TIME", "NET_ADMIN"]

Security Contest Update/Edit not done by one edit command, need to take another step from temp file run.




##Resource Requirements | Resource Limits
-------------------------------------------------
1 cpu = 1 Core,  1 AWS vCPU, 1GCP Core, 1 Hyperthread

cup 0.1 = 100Mi

Memory:
1G = Gigabyte = 1000 made
1k = kilobyte = 1000 bytes

1Gi = Gibibyte - 1024 mi
1Ki = Kibibyte - 1024 byte

A Docker container has no by default set limit to consume host resource, it can consume as much as needed.
Kubernetes by default set limit to container if you not set to.
Default value is:
CPU = 1vCPU, Momory= 512Mi

When you set resource limit on pod and If a pod try to cross the limit:
  In CPU kubernets throttle the uses of cpu, but not limit make throttle on memory.
  If a pod continouslly try to use more memory then pod will terminated.




##Solution6: | Resource Requirements
-------------------------------------------------
When a pod is created the containers are assigned a default CPU request of .5 and 
memory of 256Mi". 
For the POD to pick up those defaults you must have first set those as default values for request and limit by creating a LimitRange in that namespace.

apiVersion: v1
kind: LimitRange
metadata:
  name: mem-limit-range
spec:
  limits:
  - default:
      memory: 512Mi
    defaultRequest:
      memory: 256Mi
    type: Container

apiVersion: v1
kind: LimitRange
metadata:
  name: cpu-limit-range
spec:
  limits:
  - default:
      cpu: 1
    defaultRequest:
      cpu: 0.5
    type: Container 

1)A pod called rabbit is deployed. Identify the CPU requirements set on the Pod?
=>kubectl describe pod rabbit
Check:    
Requests:
      cpu:        1

2)Another pod called elephant has been deployed in the default namespace. It fails to get to a running state. Inspect this pod and identify the Reason why it is not running.

=>kubectl describe pod elephant

Check this section:
    Last State:     Terminated
      Reason:       OOMKilled
      Exit Code:    1
The status OOMKilled indicates that it is failing because the pod ran out of memory. Identify the memory limit set on the POD.


The elephant pod runs a process that consumes 15Mi of memory. Increase the limit of the elephant pod to 20Mi.
apiVersion: v1
kind: Pod
metadata:
  name: elephant
  namespace: default
spec:
  containers:
  - args:
    - --vm
    - "1"
    - --vm-bytes
    - 15M
    - --vm-hang
    - "1"
    command:
    - stress
    image: polinux/stress
    name: mem-stress
    resources:
      limits:
        memory: 20Mi
      requests:
        memory: 5Mi


3)The status OOMKilled indicates that it is failing because the pod ran out of memory. Identify the memory limit set on the POD.

=>kubectl describe pod elephant
    Limits:
      memory:  10Mi

4)The elephant pod runs a process that consumes 15Mi of memory. Increase the limit of the elephant pod to 20Mi.

=>kubectl get pod -o yaml elephant >newpod.yaml
=>kubectl delete pod elephant 
=>vi newpod.yaml 
Update the memory limit from 10 20
=>kubectl apply -f newpod.yaml 




##Service Accounts
-------------------------------------------------
CKAD EXAM: Only know how to woring with service Account.


Service Accounts for Other security purpose like: Authorization, Authentication, Role etc.
In CKAD only need to how work with service account.

Totally Two type fo Account in Kubernetes: 
  UserAccout: User human. Like a admin user do adminastatiov task and a developer deploy pod.
  ServiceAccoutn: Use by Machine. Like: a Jenken app need to deploy a pod in kubernetes cluster.



=>kubectl create serviceaccount my-serviceAccount
=>kubectl get serviceaccount
When a service account create a Token automatically created, what is for extrrnal authencation for kubernetes cluster.

When a service acc created its create:
  aServiceAccObject
  aToken
  aSecretWithToken
Then secretObjet link with service account.

To view the token run:
=>kunbectl describe serviceaccount my-serviceaccount
Chekout and token:
 Tokens:
=>kubectrl describe secret my-sec-token-kbbdm
Check:
 token:
This token can be use Bearer token on rest call api.
Like any curl request this token can be user as Authorization header Bearar token.

then secret object link with service account.
You can use this token to access outside of cluster.
If the app in same cluster then kubernetes make it simple to mounting the secret inside ths pod.

When create a pod by default default secret mounted inside all pod. to see this:
=>kubect describe pod my-pod



If the thard-party app placed in the same kubernetes cluster then no need to user this token as bearer.
This token can be mounted inside the pod.


Every name space has its own default service account.
It has a Secrect Object and a Token accoucated with it.
When a pod created auto assocated this servieaccount to the pod and mounted token as volume mount 
in this well know place of the pod.

For the show where the file mounted in pod:
=>kubectl describe pod mypod
Check Mounts:
=>kubectl exec -it my-pod -- ls /var/run/secrets/kubernatis.is/serviceaccount
It show three file inside it, its one is token.

If requird custome service accoutn add field of serviceAccountName

apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
    -  image: ubuntu
      name: web
      command: ["sleep", "5000"]
  serviceAccountName: my-serviceaccount

Remembar: You cna not edit the service account for a existing pod, You must delete and recreated the pod.
For Assigniing auto service account user:

apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
    -  image: ubuntu
      name: web
      command: ["sleep", "5000"]
  automountServiceAccountToken: false



 
1.22 to 1.24 Update of ServieAcc:
---------------------------------------------------

From kubernetes v1.22 not automatically mounted. token has no time limit.
From kubernetes v1.24 not auto create a toke and secret.
  Need to create secre token expectedly.
  First create s Service account then associated to Secre and token.





##Solution5: | Service Accounts
-------------------------------------------------
=>k get sa
=>kubectl get serviceaccount


1)What is the secret token used by the default service account?
=>kubectl describe sa default

2)Which account does the Dashboard application use to query the Kubernetes API?
Check the error:
pods is forbidden: User "system:serviceaccount:default:default" cannot list resource "pods" in API group "" in the namespace "default"
Its default account identify by "system:serviceaccount:default:default".
OR
=>kubectl logs podName
Chck pod event section:


3)Inspect the Dashboard Application POD and identify the Service Account mounted on it.
Identify the service account name of the pod:

=>kubectl get pod mypod -o yaml
and Find this:
serviceAccount: default

4)what location is the ServiceAccount credentials available within the pod?
=>kubectl describe pod web-dashboard-65b9cf6cbb-vpkkz 
Now check the  Mounts: properties

5)Create a new ServiceAccount named dashboard-sa
=>kubectl create sa dashboard-sa


6)We just added additional permissions for the newly created dashboard-sa account using RBAC.

pod-reader-role.yaml
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: default
  name: pod-reader
rules:
- apiGroups:
  - ''
  resources:
  - pods
  verbs:
  - get
  - watch
  - list

dashboard-sa-role-binding.yaml 
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: read-pods
  namespace: default
subjects:
- kind: ServiceAccount
  name: dashboard-sa # Name is case sensitive
  namespace: default
roleRef:
  kind: Role #this must be Role or ClusterRole
  name: pod-reader # this must match the name of the Role or ClusterRole you wish to bind to
  apiGroup: rbac.authorization.k8s.io

=>kubectl get role
=>kubectl get rolebinding


7)Enter the access token in the UI of the dashboard application. Click Load Dashboard button to load Dashboard

Create an authorization token for the newly created service account, copy the generated token and paste it into the token field of the UI.
To do this, run kubectl create token dashboard-sa for the dashboard-sa service account, copy the token and paste it in the UI.

=>kubectl create token dashboard-sa
OR
=>kubectl get sa
=>kubectl describe sa dashboard-sa
=>kubectl describe secret dashboard-sa-token-rdqw9


8)You shouldn't have to copy and paste the token each time. The Dashboard application is programmed to read token from the secret mount location. However currently, the default service account is mounted. Update the deployment to use the newly created ServiceAccount
Edit the deployment to change ServiceAccount from default to dashboard-sa

=>k get deployment
=>kubectl get deployment -o yaml>mydeployment.yaml
=>kubectl delete deploy web-dashboard 

=>vi mydeployment.yaml
Add serviceAccountName: dashboard-sa under templated spec
OR
=>kubectl edit deploy web-dashboard
Add this serviceAccount in pod space

    template:
      metadata:
        creationTimestamp: null
        labels:
          name: web-dashboard
      spec:
        serviceAccountName: dashboard-sa
        containers:





##Taints and Tolerations
-------------------------------------------------
=>kubectl taint nodes nodeName key=value:taint-effect
Three type of taint-effect:
 - NoSchedule               | No pod will- be deploy
 - PreferNoSchedule         | Try to nod pod deploy
 - NoExecute                | No New pod will be deploy, existing pod will evect if not can tolarent


Tain = Node
Toleration = Pod

=>kubectl describe node controlplane | grep Taints


=>kuhectl taint nodes node1 app=blue:NoSchedule
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
    - name: my-ngcontainer
      image: nginx

    tolerations:
      - key: "app"
        operator: "Equal"
        value: "blue"
        effect: "NoSchedule"




#Solution7: | Taints and Tolerations
-------------------------------------------------


1)How many nodes exist on the system, Do any taints exist on node01 node?
=>kubectl get node
=>kubectl describe node node01 | grep -i taints


2)Create a taint on node01 with key of spray, value of mortein and effect of NoSchedule
Key = spray
Value = mortein
Effect = NoSchedule

=>kubectl taint --help
=>kubectl taint nodes node01 spray=mortein:NoSchedule
=>kubectl describe node node01 | grep Taints


3) Create a pod name mosquito, Why pod on pending state ?
=>kubectl describe pod mosquito

4)Create another pod named bee with the nginx image, which has a toleration set to the taint mortein.

Image name: nginx
Key: spray
Value: mortein
Effect: NoSchedule
Status: Running

=>kubectl run bee --image=nginx --dry-run=client -o yaml>bee.yaml
=>vi bee.yaml

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: bee
  name: bee
spec:
  containers:
  - image: nginx
    name: bee
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
  tolerations:
    - key: "spray"
      value: "mortein"
      effect: "NoSchedule"
      operator: "Equal"
status: {}

=>k apply -f bee.yaml 


5)Notice the bee pod was scheduled on node node01 despite the taint?
=>kuebctl get pod -o wide

6)Remove the taint on controlplane, which currently has the taint effect of NoSchedule.
=>kubectl describe node controlplane
Check:
Taints:             node-role.kubernetes.io/control-plane:NoSchedule

Remove it:
=>kubectl taint node controlplane node-role.kubernetes.io/control-plane:NoSchedule-






##Node Selectors| Affinity
-------------------------------------------------
To deploy specific pod to specific Node we cand user Node Selector:
Lavel the node and add this to pod as Node selector

NodeAffinity user for complex node select operation:

=>kubectl label nodes node01 size=learge
Labeling a node

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: bee
  name: bee
spec:
  containers:
  - image: nginx
    name: bee
  nodeSelector:
    size: Large

=>kuebctl apply -f newpod.yaml


Node selectro has limitation, we cand user only single level and secelator.
If a complex requirment not deon this way, Here we have to user:
nodeSelector and Node Affinity together

affinity:
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
      - matchExpressions:
        - key: color
          operator: In
          values:
          - blue
          - small

Two tyep of affinity now available:

requiredDuringSchedulingIgnoredDuringExecution:
preferredDuringSchedulingIgnoredDuringExecution:

Planned:
requiredDuringSchedulingRequiredDuringExecution:
If any lavel is remove after schedule, in this case pod willbe remove


Taints and Tolerations vs Node Affinity
-------------------------------------------------
Taints and Tolerations usually select the right node for the pod but pod can be   schedule other node.
And With
Node Affinity can schedule to right node but other pod may schedule on the same node
 For Slove this provlem, user taints and Tolerant and Node Affinityh both together.





##Solution8: | Node Affinity
-------------------------------------------------

1)How many Labels exist on node node01?
=>kubectl describe node node01
Check  Labels:             beta.kubernetes.io/arch=amd64

2)Apply/Add a label color=blue to node node01
=>kubectl label node --help
=>kubectl label node node01 color=blue

3)Create a new deployment named blue with the nginx image and 3 replicas.
=>kubectl create deployment --help
=>kubectl create deployment blue --image=nginx --replicas=3

4)Which nodes can the pods for the blue deployment be placed on?
=>kubectl get pod -o wide
=>kubectl describe node node01 | grep Taints


5)Set Node Affinity to the deployment to place the pods on node01 only.
Name: blue
Replicas: 3
Image: nginx
NodeAffinity: requiredDuringSchedulingIgnoredDuringExecution
Key: color
value: blue

=>kubectl label node node01 color=blue

=>kubectl edit deployment blue
Add this: in spec section:

  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: color
            operator: In
            values:
            - blue

Complate deployment yaml will like this:
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: blue
  name: blue
spec:
  replicas: 3
  selector:
    matchLabels:
      app: blue
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: blue
    spec:  
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: color
                operator: In
                values:
                - blue
      containers:
      - image: nginx
        name: nginx
        resources: {}
status: {}


6)Create a new deployment named red with the nginx image and 2 replicas, and ensure it gets placed on the controlplane node only.
Use the label key - node-role.kubernetes.io/control-plane - which is already set on the controlplane node.

apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: red
  name: red
spec:
  replicas: 2
  selector:
    matchLabels:
      app: red
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: red
    spec:  
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: node-role.kubernetes.io/control-plane
                operator: Exists
      containers:
      - image: nginx
        name: nginx
        resources: {}
status: {}







##Section 3: Multi-Container Pods
==================================================

##Multi-Container Pods
--------------------------------------------------
One o the benifet of multicontainer pod its can share same storage same data as localhost, no need volume or other share data.
Its work as a pod a localmachine and containers are process or running service in it.

Like a Business service produce some logs and a logs service send it to a central logs process system.


Design Pattern of MultiContainer Pod:
   - Sidecar Pattern                   | a log container with a webserver to send log to a central logging system.
   - Adapter Pattern                   | Before sending the log file to system convart to log a common pattern for this a container.
   - Ambassador Pattern                | To connect multple db server a Proxy localhost db container.

all pattern implementation is same: Define multicontainer as list in pod.





##Solution1 - Multi-Container Pods
--------------------------------------------------

1)Create a multi-container pod with 2 containers.
Use the spec given below.
If the pod goes into the crashloopbackoff then add the command sleep 1000 in the lemon container.

Name: yellow
Container 1 Name: lemon
Container 1 Image: busybox
Container 2 Name: gold
Container 2 Image: redis

=>kubectl run yellow --image busybox --dry-run=client -o yaml

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: yellow
  name: yellow
spec:
  containers:
  - image: busybox
    name: lemon
    command: ["sleep", "1000"]
  - image: redis
    name: gold

=>kubectl describe pod laymon


2)We have deployed an application logging stack in the elastic-stack namespace. Inspect it.
Before proceeding with the next set of questions, please wait for all the pods in the elastic-stack namespace to be ready. This can take a few minutes.

=>kubectl get pod -n elastic-stack

Once the pod is in a ready state, inspect the Kibana UI using the link above your terminal. There shouldn't be any logs for now.
We will configure a sidecar container for the application to send logs to Elastic Search.
NOTE: It can take a couple of minutes for the Kibana UI to be ready after the Kibana pod is ready.

You can inspect the Kibana logs by running:
kubectl -n elastic-stack logs kibana


3)The application outputs logs to the file /log/app.log. View the logs and try to identify the user having issues with Login.
Inspect the log file inside the pod

=>kubectl logs app -n elastic-stack
=>kubectl -n elastic-stack exec -it app -- cat /log/app.log


4)Edit the pod to add a sidecar container to send logs to Elastic Search. Mount the log volume to the sidecar container.
Only add a new container. Do not modify anything else. Use the spec provided below.

Note: State persistence concepts are discussed in detail later in this course. For now please make use of the below documentation link for updating the concerning pod.
https://kubernetes.io/docs/tasks/access-application-cluster/communicate-containers-same-pod-shared-volume/

Name: app
Container Name: sidecar
Container Image: kodekloud/filebeat-configured
Volume Mount: log-volume
Mount Path: /var/log/event-simulator/
Existing Container Name: app
Existing Container Image: kodekloud/event-simulator


=>kubectl edit pod -n elastic-stack

- image: kodekloud/filebeat-configured
  name: sidecar
  volumeMounts:
    - mountPath: /var/log/event-simulator/
      name: log-volume
https://www.loom.com/share/c2ae70197e8340a0ba77fc1de8179182

OR
apiVersion: v1
kind: Pod
metadata:
  name: app
  namespace: elastic-stack
  labels:
    name: app
spec:
  containers:
  - name: app
    image: kodekloud/event-simulator
    volumeMounts:
    - mountPath: /log
      name: log-volume

  - name: sidecar
    image: kodekloud/filebeat-configured
    volumeMounts:
    - mountPath: /var/log/event-simulator/
      name: log-volume

  volumes:
  - name: log-volume
    hostPath:
      # directory location on host
      path: /var/log/webapp
      # this field is optional
      type: DirectoryOrCreate




##initContainers
--------------------------------------------------
An initContainer is configured in a pod like all other containers, 
except that it is specified inside a initContainers section, like this:


apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
spec:
  containers:
  - name: myapp-container
    image: busybox:1.28
    command: ['sh', '-c', 'echo The app is running! && sleep 3600']
  initContainers:
  - name: init-myservice
    image: busybox
    command: ['sh', '-c', 'git clone <some-repository-that-will-be-used-by-application> ;']


When a POD is first created the initContainer is run, 
and the process in the initContainer must run to a completion before the real container hosting the application starts.

You can configure multiple such initContainers as well, like how we did for multi-pod containers. 
In that case each init container is run one at a time in sequential order.
If any of the initContainers fail to complete, Kubernetes restarts the Pod repeatedly until the Init Container succeeds.

apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
spec:
  containers:
  - name: myapp-container
    image: busybox:1.28
    command: ['sh', '-c', 'echo The app is running! && sleep 3600']
  initContainers:
  - name: init-myservice
    image: busybox:1.28
    command: ['sh', '-c', 'until nslookup myservice; do echo waiting for myservice; sleep 2; done;']
  - name: init-mydb
    image: busybox:1.28
    command: ['sh', '-c', 'until nslookup mydb; do echo waiting for mydb; sleep 2; done;']


Read more about initContainers here. And try out the upcoming practice test.
https://kubernetes.io/docs/concepts/workloads/pods/init-containers/






##Section4: Observability
==================================================
Pod lifecycle:
  - Pending state 
    When a pod is start it is on pending state, scheduler find the ringht -node for it, if not find keep it pending sate.
    If a pod pending state run describe command, its tell you why.
    =>kubectl describe pod my-pod
    
  - ContainerCreating
    When a pod schedule its on Container creating state, pull the image and run all the container of the pod in side node.
    
  - Running state
    When all container is ready it is gor for running state.

This three stat show on STATUS colume.
 


Pod Conditions:
- PodScheduled    = True/false
- Initialized     = True/false
- ContainerReady  = True/false
- Ready           = True/fasle

Run describe command and check conditaiton section.
=>kubectl describe pod my-pod


A pod is show Ready state but internal application may take more time to 
recieving user request, for this create a custome condation for set when a pod actually ready.
Add this rediness probe under containers: section

readinessProbe:
  httpGet:
    path: /ready
    port:8080


Its cna be: httpGet:, tcpSocket:, exec: connand   
With readinessProbe you can add different funcationality:

initialDelaySeconds: 
periodSeconds:
failureThreshold:  etc.


##Readiness and Liveness Probes
--------------------------------------------------
If a container inside a pod crush or exitsted, kubernetes try to restart the container ageing, and continue to do it.

Liveness Probes: 
  Its a machinesume to check/monitor the pod is healthy or running, You cand define it checking with call a http-api, tch or cmd.

livenessProbe:
  httpGet:
    path: /ready
    port:8080


cat curl-test.sh 
for i in {1..20}; do
   kubectl exec --namespace=kube-public curl -- sh -c 'test=`wget -qO- -T 2  http://webapp-service.default.svc.cluster.local:8080/ready 2>&1` && echo "$test OK" || echo "Failed"';
   echo ""
done

=>./curl-test.sh


crash-app.sh 
kubectl exec --namespace=kube-public curl -- wget -qO- http://webapp-service.default.svc.cluster.local:8080/crash


freeze-app.sh 
nohup kubectl exec --namespace=kube-public curl -- wget -qO- http://webapp-service.default.svc.cluster.local:8080/freeze &




##Solution: Readiness adn Liveness Probes
--------------------------------------------------

1)Update the newly created pod 'simple-webapp-2' with a readinessProbe using the given spec
Spec is given on the below. Do not modify any other properties of the pod.

Pod Name: simple-webapp-2
Image Name: kodekloud/webapp-delayed-start
Readiness Probe: httpGet
Http Probe: /ready
Http Port: 8080

=>kubectl get pod simple-webapp-2 -o yaml>newpod.yaml
=>kubectl delete pod simple-webapp-2 
=>vi newpod.yaml

Add this rediness probe under containers: section
readinessProbe:
  httpGet:
    path: /ready
    port:8080

=>kubectl get pod
Now this pod running but not ready, waiting for ready


=>./crash-app.sh
Crash the application, it will be restart again.
OR
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: "2021-08-01T04:55:35Z"
  labels:
    name: simple-webapp
  name: simple-webapp-2
  namespace: default
spec:
  containers:
  - env:
    - name: APP_START_DELAY
      value: "80"
    image: kodekloud/webapp-delayed-start
    imagePullPolicy: Always
    name: simple-webapp
    ports:
    - containerPort: 8080
      protocol: TCP
    readinessProbe:
      httpGet:
        path: /ready
        port: 8080

=>kubectl replace -f webpod2.yaml --force

When the application crashes, the container is restarted. During this period the service directs users to the available POD, since the POD status is not READY.


2)Update both the pods with a livenessProbe using the given spec
Delete and recreate the PODs.

Pod Name: simple-webapp-1
Image Name: kodekloud/webapp-delayed-start
Liveness Probe: httpGet
Http Probe: /live
Http Port: 8080
Period Seconds: 1
Initial Delay: 80
Pod Name: simple-webapp-2
Image Name: kodekloud/webapp-delayed-start
Liveness Probe: httpGet
Http Probe: /live
Http Port: 8080
Initial Delay: 80
Period Seconds: 1


=>kubectl get pod -o yaml>allwebapp.yaml
=>kubectl delete pod --all
=>vi allwebapp.yaml

Add this on containers: section for bot container

livenessProbe:
  httpGet:
    path: /live
    port: 8080
  periodSeconds: 1
  initialDelaySeconds: 80

OR
Use the following YAML for simple-webapp-1.yaml:

apiVersion: v1
kind: Pod
metadata:
  labels:
    name: simple-webapp
  name: simple-webapp-1
  namespace: default
spec:
  containers:
  - env:
    - name: APP_START_DELAY
      value: "80"
    image: kodekloud/webapp-delayed-start
    imagePullPolicy: Always
    name: simple-webapp
    ports:
    - containerPort: 8080
      protocol: TCP
    readinessProbe:
      httpGet:
        path: /ready
        port: 8080
    livenessProbe:
      httpGet:
        path: /live
        port: 8080
      periodSeconds: 1
      initialDelaySeconds: 80

=>kubectl replace -f simple-webapp-1.yaml --force

for simple-webapp-2, use the following YAML file:

apiVersion: v1
kind: Pod
metadata:
  labels:
    name: simple-webapp
  name: simple-webapp-2
  namespace: default
spec:
  containers:
  - env:
    - name: APP_START_DELAY
      value: "80"
    image: kodekloud/webapp-delayed-start
    imagePullPolicy: Always
    name: simple-webapp
    ports:
    - containerPort: 8080
      protocol: TCP
    readinessProbe:
      httpGet:
        path: /ready
        port: 8080
    livenessProbe:
      httpGet:
        path: /live
        port: 8080
      periodSeconds: 1
      initialDelaySeconds: 80


=>kubectl replace -f simple-webapp-2.yaml --force





##Container Logging and Monitor
--------------------------------------------------
When multiple container running in a pod, for showing container logs you have to mentation container name with log command.
=>kubectl logs podName  -c containerName





##Solution: Container Logging and Monitor
--------------------------------------------------
=>kubectl logs mypod 
=>kubectl logs webapp-2 -c simple-webapp

1)Let us deploy metrics-server to monitor the PODs and Nodes. Pull the git repository for the deployment files.
Run: git clone https://github.com/kodekloudhub/kubernetes-metrics-server.git

=>kubectl create -f .
Create matrice server.

=>kubectl top node
Check resource monitoring of nodes


2)Identify the node that consumes the most CPU(cores).
  Identify the node that consumes the most Memory(bytes).
=>kubectl top node

3)Identify the most memory consumes pod
  Identify the POD that consumes the least CPU(cores).
=>kubectl top pod




##Solution2 – Init Containers
--------------------------------------------------

1)Identify the pod that has an initContainer configured.
=>kubectl describe pod
Check:
IPs:
  IP:  10.42.0.10
Init Containers:
  init-myservice:



2)What is the state of the initContainer on pod blue?
=>kubectl describe pod
Cehck out this:
      State:          Terminated
      Reason:       Completed

3)We just created a new app named purple. How many initContainers does it have?
=>kubectl describe pod purple


4)What is the state of the POD?
=>kubectl describe pod purple
Status:           Pending
IP:               10.42.0.12


5)How long after the creation of the POD will the application come up and be available to users?
=>kubectl describe pod purple
add all the init container wait or sleep time then 
Check the commands used in the initContainers. The first one sleeps for 600 seconds (10 minutes) and the second one sleeps for 1200 seconds (20 minutes)
Adding the sleep times for both initContainers, the application will start after 1800 seconds or 30 minutes.



6)Update the pod red to use an initContainer that uses the busybox image and sleeps for 20 seconds
Delete and re-create the pod if necessary. But make sure no other configurations change.

Pod: red
initContainer Configured Correctly

=>kubectl get pod red -o yaml>red.yaml
=>kubectl delete pod red 
=>vi red.yaml

  initContainers:
    - image: busybox
      name: red-initcontainer
      command:
        - "sleep"
        - "20"

Add it like this:

spec:
  initContainers:
    - image: busybox
      name: red-initcontainer
      command:
        - "sleep"
        - "20"
  containers:
  - command:
    - sh
    - -c
    - echo The app is running! && sleep 3600
    image: busybox:1.28
    imagePullPolicy: IfNotPresent
    name: red-container
OR
---
apiVersion: v1
kind: Pod
metadata:
  name: red
  namespace: default
spec:
  containers:
  - command:
    - sh
    - -c
    - echo The app is running! && sleep 3600
    image: busybox:1.28
    name: red-container
  initContainers:
  - image: busybox
    name: red-initcontainer
    command: 
      - "sleep"
      - "20"
7)A new application orange is deployed. There is something wrong with it. Identify and fix the issue.
Once fixed, wait for the application to run before checking solution.

=>kubectl describe pod orange
Proble show in pod config properties like connadn, args image name typho etc.
=>kubectl get pod orange -o yaml > /root/orange.yaml








##Section 6: POD Design
==================================================

##Labels, Selectors and Annotations
--------------------------------------------------
In kubernetes we add level on kubernetes object, for select then diffenere option. add level on pod:
apiVersion: v1
kind: Pod
metadata:
  name: throw-dice-pod
  labels:
    evn: prod
    app: back-end   
spec:
  containers: 
  -  image: kodekloud/throw-dice
     name: throw-dice
  restartPolicy: Never

Now to get this level pod:
=>kubectl get pods --selector env=prod


##Solution1: Labels, Selectors and Annotations
--------------------------------------------------

1)We have deployed a number of PODs. They are labelled with tier, env and bu. How many PODs exist in the dev environment (env)?
Use selectors to filter the output

=>kubectl get pod --selector env=dev
OR
=>kubectl get pod --selector env=dev --no-headers
=>kubectl get pod --selector env=dev --no-headers | wc -l

2)How many objects are in the prod environment including PODs, ReplicaSets and any other objects?
=>kubectl get all
=>kubectl get all --selector env=prod
OR
=>kubectl get all --selector env=prod | wc -l


3)Identify the POD which is part of the prod environment, the finance BU and of frontend tier?
=>kubectl get pod --selector env=prod,bu=finance,tier=frontend


4)A ReplicaSet definition file is given replicaset-definition-1.yaml. Try to create the replicaset. There is an issue with the file. Try to fix it.
=>kubectl apply -f replicaset-definition-1.yaml 
=>vi replicaset-definition-1.yaml 
=>kubectl apply -f replicaset-definition-1.yaml 

=>kubectl get rs





##Deployment Rolling Updte and Rollback
--------------------------------------------------
kubectl create for creating object, kubectl apply for update object.

=>kubectl create -f mydeployment.yaml --record
=>kubectl rollout status deployment/mydeployment
=>kubectl rollout history deployment/mydeployment
Creating a deployment

Creating a deployment, checking the rollout status and history:
In the example below, we will first create a simple deployment and inspect the rollout status and the rollout history:

=>kubectl create deployment nginx --image=nginx:1.16
=>kubectl rollout status deployment nginx
=>kubectl rollout history deployment nginx


Using the --revision flag:
Here the revision 1 is the first version where the deployment was created.
You can check the status of each revision individually by using the --revision flag:

=>kubectl rollout history deployment nginx --revision=1


Using the --record flag:
You would have noticed that the "change-cause" field is empty in the rollout history output. 
We can use the --record flag to save the command used to create/update a deployment against the revision number.

=>kubectl set image deployment nginx nginx=nginx:1.17 --record
=>kubectl rollout history deployment nginx
=>kubectl edit deployments nginx --record
=>kubectl set image deployment nginx nginx=nginx:1.17 --record=true
=>kubectl edit deployments nginx --record=true
=>kubectl rollout history deployment nginx --revision=3


Undo a change:Lets now rollback to the previous revision:
=>kubectl rollout history deployment nginx
=>kubectl rollout history deployment nginx --revision=3
=>kubectl describe deployments nginx | grep -i image:
With this, we have rolled back to the previous version of the deployment with the image = nginx:1.17.

=>kubectl rollout history deployment nginx --revision=1
=>kubectl rollout undo deployment nginx --to-revision=1
=>kubectl describe deployments. nginx | grep -i image:

To rollback to specific revision we will use the --to-revision flag.
With --to-revision=1, it will be rolled back with the first image we used to create a deployment




##Solution2: Deployment Rolling Updte and Rollback
--------------------------------------------------
cat curl-test.sh 
for i in {1..35}; do
   kubectl exec --namespace=kube-public curl -- sh -c 'test=`wget -qO- -T 2  http://webapp-service.default.svc.cluster.local:8080/info 2>&1` && echo "$test OK" || echo "Failed"';
   echo ""
done

1)What container image is used to deploy the applications?

=>k describe deployment frontend 

2)Inspect the deployment and identify the current strategy?
=>k describe deployment frontend 
StrategyType:           RollingUpdate

3)Let us try that. Upgrade the application by setting the image on the deployment to kodekloud/webapp-color:v2

=>kubectl edit deployment frontend
Edit the yaml file
OR
=>kubectl set image --help
=>kubectl set image deploy frontend simple-webapp=kodekloud/webapp-color:v2
=>kubectl describe deployment frontend 


4)Up to how many PODs can be down for upgrade at a time
Consider the current strategy settings and number of PODs - 4

=>kubectl describe deploy frontend 
checkout:
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Its mean 1 pod what is 25%.


5)Change the deployment strategy to Recreate

=>kubectl get deployment frontend -o yaml>newdeployment.yaml
=>kuebctl delete deploy frontend
 strategy:
    type: Recreate
=>kubectl apply -f newdeployment.yaml
=>kubectl describe deploy frondend
Check:
StrategyType:       Recreate




##Job and CronJob
--------------------------------------------------
Job start imidetly whenre we can set schedule on CronJob.

ReplicaSet keep runnign pod allows, where Job bring the pod for a specific task and after it finish its go down.


##Solution3: Job and CronJob
--------------------------------------------------
cat throw-dice.yaml
apiVersion: v1
kind: Pod
metadata:
  name: throw-dice-pod
spec:
  containers:
  -  image: kodekloud/throw-dice
     name: throw-dice
  restartPolicy: Never

=>kubectl logs throw-dice-pod

1)Create a Job using this POD definition file or from the imperative command and look at how many attempts does it take to get a '6'.
Use the specification given on the below.

Job Name: throw-dice-job
Image Name: kodekloud/throw-dice

=>kubectl create job throw-dice-job --image=kodekloud/throw-dice --dry-run=client -o yaml > throw-dice-job.yaml

apiVersion: batch/v1
kind: Job
metadata:
  name: throw-dice-job
spec:
  backoffLimit: 15 # This is so the job does not quit before it succeeds.
  template:
    spec:
      containers:
      - name: throw-dice
        image: kodekloud/throw-dice
      restartPolicy: Never

=>vi myjob.yaml
=>kubectl apply -f myjon.yaml
=>kubectl describe job throw-dice-job 



How many attempts did it take to complete the job?
=>kubectl describe job throw-dice-job 
Check this:
Pods Statuses:    0 Active (0 Ready) / 1 Succeeded / 1 Failed
togeher both: 2

2)Update the job definition to run as many times as required to get 3 successful 6's.
Delete existing job and create a new one with the given spec. Monitor and wait for the job to succeed.

Job Name: throw-dice-job
Image Name: kodekloud/throw-dice
Completions: 3
Job Succeeded: True

=>vi jobn3.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: throw-dice-job
spec:
  template:
    spec:
      containers:
      - name: throw-dice-job
        image: kodekloud/throw-dice
      restartPolicy: Never
  backoffLimit: 30
  completions: 3

=>kubectl delete job throw-dice-job
=>kubectl apply -f job3.yaml

=> k describe job throw-dice-job 
Check:
Pods Statuses:    1 Active (0 Ready) / 2 Succeeded / 5 Failed


3)That took a while. Let us try to speed it up, by running upto 3 jobs in parallel.
Update the job definition to run 3 jobs in parallel.

Job Name: throw-dice-job
Image Name: kodekloud/throw-dice
Completions: 3
Parallelism: 3

=>vi jon4.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: throw-dice-job
spec:
  template:
    spec:
      containers:
      - name: throw-dice-job
        image: kodekloud/throw-dice
      restartPolicy: Never
  backoffLimit: 30
  completions: 3
  parallelism: 3

=>k replace --force -f myjob1.yaml 


4)Let us now schedule that job to run at 21:30 hours every day.
Create a CronJob for this.
CronJob Name: throw-dice-cron-job
Image Name: kodekloud/throw-dice
Schedule: 30 21 * * *

apiVersion: batch/v1
kind: CronJob
metadata:
  name: throw-dice-cron-job
spec:
  schedule: "30 21 * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: throw-dice-cron-job
            image: kodekloud/throw-dice
          restartPolicy: OnFailure






##Services & Networking
==================================================
Kubernetes Services enable communication group of application with another user or objects.
Services make losscoupling with microservice.

Services just a object, its listaning port on Node and make communicate with a port of pod.
A service is a virtual server inside a node, its has a IP.

Service can make communication kunbernetes different group of apps or service.

Three type of Series:
 - NodePort
   Make access the app with external, To map a Node port and a pod port.

 - ClusterIP
   Service create a vertual IP in side the cluster to communication with oter service, it has ip and port.
   Diffeetnt server like a group of frontend server or backend serer.

 - LoadBalancer
   We provision a loadbalancer for our application in support by cloud provider.



##Services - nodePort
--------------------------------------------------

apiVersion: v1
kind: Service
metadata:
  name: webapp-service
  namespace: default
spec:
  type: NodePort
  ports:
  - nodePort: 30080               | Node port
    port: 8080                    | Service port
    targetPort: 80                | application port
  selector:                       | Identify the pods
    name: simple-webapp

In a service only port: field is mandatory, if not TargetPort then it assign  port value as TargetPort,
and if not provide nodePort: if automatically take a port in valide range of node port.

You can mape multiple port in a single service.

Services can spand multiple node with same port.
To access that user node ip and the same port any node.

apiVersion: v1
kind: Service
metadata:
  name: webapp-service
  namespace: default
spec:
  ports:
  - nodePort: 30080
    port: 8080
    targetPort: 8080
  selector:
    name: simple-webapp
  type: NodePort


=>kubectl create service nodeport mysvc4 --tcp=8080:80 --node-port=30040 --dry-run=client -o yaml>mysvc.yaml



##Services - ClusterIP
--------------------------------------------------
This is the defafult service type of kubernetes.
ClusterIP service make layer or group of application communication easy wdfith a self IP call clusterIP.

Suppose a three tier application: frondend - backend - db
In this case ClusterIP Service will create one ClusterIP-Service for each tier with a IP.
Now all tire communicate with each other by this IP.



apiVersion: v1
kind: Service
metadata:
  name: webapp-service
  namespace: default
spec:
  type: ClusterIP
  ports:
    port: 8080                    | Pod/app port
    targetPort: 80                | this service port
  selector:                       | Identify the pods
    tier: backend-app
    name: simple-webapp


##Solution1: Services - ClusterIP
--------------------------------------------------z

1)What is the targetPort configured on the kubernetes service?
=>kubectl describe svc kubernetes 
Check:
TargetPort:        6443/TCP



2)How many labels are configured on the kubernetes service?
=>kubectl describe svc kubernetes 
Check:
Labels:            component=apiserver
                   provider=kubernetes


3)Create a new service to access the web application using the service-definition-1.yaml file.
Name: webapp-service
Type: NodePort
targetPort: 8080
port: 8080
nodePort: 30080
selector:
  name: simple-webapp

=>vi service.yaml
apiVersion: v1
kind: Service
metadata:
  name: webapp-service
  namespace: default
spec:
  ports:
  - nodePort: 30080
    port: 8080
    targetPort: 8080
  selector:
    name: simple-webapp
  type: NodePort





##Network Policies | Network Security
--------------------------------------------------
Kubernetes make all Allow for all pod/service can make communication each other in a cluster.

Two type Network Policy:
 - Ingress           | In comming request from the user to he POD.
 - Egress            | Outgoing request from POD app server to out side user.


Network Policy also a Object in kubernetes, link one or more pod and set rule/policy to enable communication.


apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: db-pod-policy
spec:
  podSelector:             | Which pod policy apply to like a db pod/server with pod label role=db.
    matchLabels:
      role: db
  policyTypes:
    - Ingress
  ingress:
    - from:
        - podSelector:     | From which pod ingress traffic allow on the db pod, here a api-server with this label. 
            matchLabels:
              name: api-server
      ports:
        - protocol: TCP
          port: 8080


Network solution which support NetworkPolicy:
 - Kube-route
 - Calico
 - Romana
 - Weave-net
 
Network solution which  not support NetworkPolicy:
- Flannel


We still able to create NetworkPolicy althrow the network solution not support policy.
It not work but not throw any error.


Developing Network Policies
--------------------------------------------------
Suppose we have a Three tire application: webapp, api-server, db

Step:
  Apply a policy to db pod, and block all in/out traffic.
  Now need to undestanding which type of communication/rule have to add with this policy.
  In this case for db server receiving incommuning traffice from api-server and return query result.
  For this need to add a Ingress policy to this db pod policy from api-server.
  No need to worry about db output query result to api-server, no policy needed.
  When the db pod allow Ingress traffice from api-server auto respone result has been allow. 
But DB server not allow make any query/request to api-server in this policy.
This is a Egress traffic, to allow this need a extra Egress role in db pod.


A single policy can hold Ingress or Egress or Both role.

We add namespace in the policy, with pod selector or without pod selector.
We can also only  specify namespace only, no need this pod, this case all pod able to communicate in this ns.
We cna also spacify a IP or blick of IP to cummunicate with a server out side of cluster.

So three type of selector:
 - podSelector
 - namespaceSelector
 - ipBlock
This three support apply to -from and -to block in policy.
OR
We cna user this rule as tow (podSelector AND namespaceSelector) type of selector:
 - podSelector
   namespaceSelector
 - ipBlock




##Solution2: Network Policies
--------------------------------------------------
1)How many network policies do you see in the environment?
=>kubectl get networkpolicy 
=>kubectl get netpol
https://prnt.sc/FF-fgT5W8thh


2)Which pod is the Network Policy applied on?
=>kubectl get networkpolicy 
Check pod-selector:
NAME             POD-SELECTOR   AGE
payroll-policy   name=payroll   2m34s

Then find this pod
=>kubectl get po --show-labels | grep name=payroll



3)What type of traffic is this Network Policy configured to handle?
=>kubectl describe networkpolicy
=>kubectl describe netpol payroll-policy
Chck this:
Policy Types: Ingress


4)What is the impact of the rule configured on this Network Policy?
Internal pod can access on 8080 port
Spec:
  PodSelector:     name=payroll
  Allowing ingress traffic:
    To Port: 8080/TCP
    From:
      PodSelector: name=internal




5)Create a network policy to allow traffic from the Internal application only to the payroll-service and db-service.
Use the spec given below. You might want to enable ingress traffic to the pod to test your rules in the UI.

Policy Name: internal-policy
Policy Type: Egress
Egress Allow: payroll
Payroll Port: 8080
Egress Allow: mysql
MySQL Port: 3306

mynetpolicy.yaml

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: internal-policy
  namespace: default
spec:
  podSelector:
    matchLabels:
      name: internal
  policyTypes:
    - Egress
  egress:
    - to:
        - podSelector:
            matchLabels:
              name: payroll
      ports:
        - protocol: TCP
          port: 8080
    - to:
        - podSelector:
            matchLabels:
              name: mysql
      ports:
        - protocol: TCP
          port: 3306

OR
Solution manifest file for a network policy internal-policy as follows:

Note: We have also allowed Egress traffic to TCP and UDP port. This has been added to ensure that the internal DNS resolution works from the internal pod.
Remember: The kube-dns service is exposed on port 53:
root@controlplane:~# kubectl get svc -n kube-system 
NAME       TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                  AGE
kube-dns   ClusterIP   10.96.0.10   <none>        53/UDP,53/TCP,9153/TCP   93m



apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: internal-policy
  namespace: default
spec:
  podSelector:
    matchLabels:
      name: internal
  policyTypes:
  - Egress
  - Ingress
  ingress:
    - {}
  egress:
  - to:
    - podSelector:
        matchLabels:
          name: mysql
    ports:
    - protocol: TCP
      port: 3306

  - to:
    - podSelector:
        matchLabels:
          name: payroll
    ports:
    - protocol: TCP
      port: 8080

  - ports:
    - port: 53
      protocol: UDP
    - port: 53
      protocol: TCP

=>kubectl apply -f mynetpolicy.yaml
Note: We have also allowed Egress traffic to TCP and UDP port. This has been added to ensure that the internal DNS resolution works from the internal pod.
Remember: The kube-dns service is exposed on port 53:





#Ingress Networking
--------------------------------------------------
Ingress help you access your application with a extranal url for each of service each path with ssl security.
Its a layer7 loadbalancer.
You have to publish using nodePort or any cloudNative provider with a cloud loadbalancer.

For Ingress need tow component:

- Ingress Controller
  You have no ingressController in kubernatis by default. 
  You have to deploy any one of solution like: Nginx, GCP(http), Countour etc.
  Currently Nginx and GCP support by kubernatis.

- Ingress Resource
  We declar different service path in a ingress declarative file with path.
  IngressResource declare using yaml kind: Ingress file with app service name and service port etc.



IngressController:
  For IngressController we have to deploy it as a declarative way with kind: Deployment
  A Service requird to expose to our side with a configMap data value
  Also need a ServiceAccount for get permisson to do all this thing.

So required component for IngressController:
- Deployment
- Service
- ConfigMap
- ServiceAccount

Simple Ingress Controller:
cat ingress-controller.yaml 

apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.1.2
    helm.sh/chart: ingress-nginx-4.0.18
  name: ingress-nginx-controller
  namespace: ingress-nginx
spec:
  minReadySeconds: 0
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/name: ingress-nginx
  template:
    metadata:
      labels:
        app.kubernetes.io/component: controller
        app.kubernetes.io/instance: ingress-nginx
        app.kubernetes.io/name: ingress-nginx
    spec:
      containers:
      - args:
        - /nginx-ingress-controller
        - --publish-service=$(POD_NAMESPACE)/ingress-nginx-controller
        - --election-id=ingress-controller-leader
        - --watch-ingress-without-class=true
        - --default-backend-service=app-space/default-http-backend
        - --controller-class=k8s.io/ingress-nginx
        - --ingress-class=nginx
        - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller
        - --validating-webhook=:8443
        - --validating-webhook-certificate=/usr/local/certificates/cert
        - --validating-webhook-key=/usr/local/certificates/key
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: LD_PRELOAD
          value: /usr/local/lib/libmimalloc.so
        image: registry.k8s.io/ingress-nginx/controller:v1.1.2@sha256:28b11ce69e57843de44e3db6413e98d09de0f6688e33d4bd384002a44f78405c
        imagePullPolicy: IfNotPresent
        lifecycle:
          preStop:
            exec:
              command:
              - /wait-shutdown
        livenessProbe:
          failureThreshold: 5
          httpGet:
            path: /healthz
            port: 10254
            scheme: HTTP
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        name: controller
        ports:
        - name: http
          containerPort: 80
          protocol: TCP
        - containerPort: 443
          name: https
          protocol: TCP
        - containerPort: 8443
          name: webhook
          protocol: TCP
        readinessProbe:
          failureThreshold: 3
          httpGet:
            path: /healthz
            port: 10254
            scheme: HTTP
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        resources:
          requests:
            cpu: 100m
            memory: 90Mi
        securityContext:
          allowPrivilegeEscalation: true
          capabilities:
            add:
            - NET_BIND_SERVICE
            drop:
            - ALL
          runAsUser: 101
        volumeMounts:
        - mountPath: /usr/local/certificates/
          name: webhook-cert
          readOnly: true
      dnsPolicy: ClusterFirst
      nodeSelector:
        kubernetes.io/os: linux
      serviceAccountName: ingress-nginx
      terminationGracePeriodSeconds: 300
      volumes:
      - name: webhook-cert
        secret:
          secretName: ingress-nginx-admission

---
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: null
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.1.2
    helm.sh/chart: ingress-nginx-4.0.18
  name: ingress-controller
  namespace: ingress-nginx
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 80
    nodePort: 30080
  selector:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
  type: NodePort


          
Ingress Resource:
  For different sercice of our app we define differern path, in ingress servie this.
  In Ingress yml file we have to create rule with path and assign serviceName and port.
  We cna handel this routing two way:
    - /path-routing
    - host: ware.domain-name

For get Ingress Resource:
=>kubectl get ingress --all-namespaces

Ingress resource, Resource path cand edit by only one line and effected:
=>k edit ingress ingress-wear-watch -n app-space

Simple Ingress Resource:
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: test-ingress
  namespace: critical-space
spec:
  rules:
  - http:
      paths:
      - path: /pay
        pathType: Prefix
        backend:
          service:
           name: pay-service
           port:
            number: 8282

OR
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress-wear-watch
  namespace: app-space
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/ssl-redirect: "false"
spec:
  rules:
  - http:
      paths:
      - backend:
          service:
            name: wear-service
            port:
              number: 8080
        path: /wear
        pathType: Prefix
      - backend:
          service:
            name: video-service
            port:
              number: 8080
        path: /stream
        pathType: Prefix
      - backend:
          service:
            name: food-service
            port:
              number: 8080
        path: /eat
        pathType: Prefix


For a new application added:
- Need a service what is haveto route traffic and mentation on Ingress defenation yaml file.
- Need a deployment with this service, pod running with this deployment

(app mean a deployment)
=>kubectl get deploy --all-namespaces


Format:
=>kubectl create ingress <ingress-name> --rule="host/path=service:port"

Example:
=>kubectl create ingress ingress-test --rule="wear.my-online-store.com/wear*=wear-service:80"






##Solution: Ingress Networking-1
--------------------------------------------------
1)We have deployed Ingress Controller, resources and applications. Explore the setup.

=>kubectl get deployment -A
=>kubectl get po -A
=>kubectl get all -A
Show all resource from all namespace


2)Which namespace is the Ingress Controller deployed in?
=>kubectl get all -A

3)What is the name of the Ingress Controller Deployment?
=>kubectl get deploy -n ingress-nginx 


4)Which namespace are the applications deployed in?
=>kubectl get pod -A


5)How many applications are deployed in the app-space namespace?
=>kubectl get pod -n app-space

6)Which namespace is the Ingress Resource deployed in?
=>kubectl get ingress -A
=>kubectl get ingress --all-namespaces


7)What backend is the /wear path on the Ingress configured with?
=>kubectl describe ingress ingress-wear-watch -n app-space

8)At what path is the video streaming application made available on the Ingress?
=>kubectl describe ingress ingress-wear-watch -n app-space
Check:
Rules:
  Host        Path  Backends
  ----        ----  --------
  *           
              /wear    wear-service:8080 (10.244.0.6:8080)
              /watch   video-service:8080 (10.244.0.5:8080)


9)You are requested to change the URLs at which the applications are made available.
Make the video application available at /stream.
Ingress: ingress-wear-watch
Path: /stream
Backend Service: video-service
Backend Service Port: 8080

=>kubectl edit ingress ingress-wear-watch -n app-space
Edit the path:         path: /stream

Check:
spec:
  rules:
  - http:
      paths:
      - backend:
          service:
            name: wear-service
            port:
              number: 8080
        path: /wear
        pathType: Prefix
      - backend:
          service:
            name: video-service
            port:
              number: 8080
        path: /stream
        pathType: Prefix



10)You are requested to add a new path to your ingress to make the food delivery application available to your customers.
Make the new application available at /eat.

Ingress: ingress-wear-watch
Path: /eat
Backend Service: food-service
Backend Service Port: 8080

=>kubectl get deploy -n app-space
=>kubectl edit ingress ingress-wear-watch -n app-space
Add this:
     - backend:
          service:
            name: food-service
            port:
              number: 8080
        path: /eat
        pathType: Prefix


11)Identify the namespace in which the new application is deployed.
=>kubectl get pod -A


12)What is the name of the deployment of the new application?
=>kubectl get deployment -n critical-space



13)You are requested to make the new application available at /pay.
Identify and implement the best approach to making this application available on the ingress controller and test to make sure its working. Look into annotations: rewrite-target as well.

Ingress Created
Path: /pay
Configure correct backend service
Configure correct backend port

Create a new Ingress for the new pay application in the critical-space namespace.
Use the command kubectl get svc -n critical-space to know the service and port details.

Solution manifest file to create a new ingress service to make the application available at /pay as follows:

---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: test-ingress
  namespace: critical-space
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/ssl-redirect: "false"
spec:
  rules:
  - http:
      paths:
      - path: /pay
        pathType: Prefix
        backend:
          service:
           name: pay-service
           port:
            number: 8282

OR
=>kubectl get svc -n critical-space
Check this servie name and port:
pay-service   ClusterIP   10.97.197.16   <none>        8282/TCP   6m8s

=>kubectl create ingress -h
=kubectl create ingress simple --rule="foo.com/bar=svc1:8080,tls=my-cert"

=>kubectl create ingress ingress-pay -n critical-space --rule="/pay=pay-service:8282"
=>kubectl get ingress -n critical-space
=>kubectl describe ingress ingress-pay -n critical-space

=>kubectl get pod -n critical-space
=>kubectl logs webapp-pay-58cdc69889-s4xd5  -n critical-space

=>kubectl edit ingress ingress-pay -n critical-space
Add this in metadata section:

annotations:
  nginx.ingress.kubernetes.io/rewrite-target: /






##Solution: Ingress Networking-2
--------------------------------------------------
Create a NGINX Ingress Controller App:

1)Create a name space ingress-nginx
=>kubectl create namespace ingress-nginx
=>kubectl get namespace

2)The NGINX Ingress Controller requires a ConfigMap object. 
Create a ConfigMap object with name ingress-nginx-controller in the ingress-nginx namespace.

No data needs to be configured in the ConfigMap.
Name: ingress-nginx-controller

=>kubectl create configmap -h
=>kubectl create configmap ingress-nginx-controller -n ingress-nginx



3)The NGINX Ingress Controller requires two ServiceAccounts. 
Create both ServiceAccount with name ingress-nginx and ingress-nginx-admission in the ingress-nginx namespace.
Use the spec provided below.
Name: ingress-nginx
Name: ingress-nginx-admission

=>kubectl create sa -h
=>kubectl create sa ingress-nginx -n ingress-nginx
=>kubectl create sa ingress-nginx-admission -n ingress-nginx

=>kubectl get sa -n ingress-nginx

4)We have created the Roles, RoleBindings, ClusterRoles, and ClusterRoleBindings for the ServiceAccount. Check it out!!

=>kubectl get roles -n ingress-nginx
=>kubectl get rolebindings -n ingress-nginx
=>kubectl describe role ingress-nginx -n ingress-nginx



5)Let us now deploy the Ingress Controller. Create the Kubernetes objects using the given file.
The Deployment and it's service configuration is given at /root/ingress-controller.yaml. There are several issues with it. Try to fix them.
Deployed in the correct namespace.
Replicas: 1
Use the right image
Namespace: ingress-nginx
Service name: ingress-nginx-controller
NodePort: 30080


=>kubectl apply -f ingress-controller.yaml

=>cat ingress-controller.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.1.2
    helm.sh/chart: ingress-nginx-4.0.18
  name: ingress-nginx-controller
  namespace: ingress-nginx
spec:
  minReadySeconds: 0
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/name: ingress-nginx
  template:
    metadata:
      labels:
        app.kubernetes.io/component: controller
        app.kubernetes.io/instance: ingress-nginx
        app.kubernetes.io/name: ingress-nginx
    spec:
      containers:
      - args:
        - /nginx-ingress-controller
        - --publish-service=$(POD_NAMESPACE)/ingress-nginx-controller
        - --election-id=ingress-controller-leader
        - --watch-ingress-without-class=true
        - --default-backend-service=app-space/default-http-backend
        - --controller-class=k8s.io/ingress-nginx
        - --ingress-class=nginx
        - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller
        - --validating-webhook=:8443
        - --validating-webhook-certificate=/usr/local/certificates/cert
        - --validating-webhook-key=/usr/local/certificates/key
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: LD_PRELOAD
          value: /usr/local/lib/libmimalloc.so
        image: registry.k8s.io/ingress-nginx/controller:v1.1.2@sha256:28b11ce69e57843de44e3db6413e98d09de0f6688e33d4bd384002a44f78405c
        imagePullPolicy: IfNotPresent
        lifecycle:
          preStop:
            exec:
              command:
              - /wait-shutdown
        livenessProbe:
          failureThreshold: 5
          httpGet:
            path: /healthz
            port: 10254
            scheme: HTTP
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        name: controller
        ports:
        - name: http
          containerPort: 80
          protocol: TCP
        - containerPort: 443
          name: https
          protocol: TCP
        - containerPort: 8443
          name: webhook
          protocol: TCP
        readinessProbe:
          failureThreshold: 3
          httpGet:
            path: /healthz
            port: 10254
            scheme: HTTP
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        resources:
          requests:
            cpu: 100m
            memory: 90Mi
        securityContext:
          allowPrivilegeEscalation: true
          capabilities:
            add:
            - NET_BIND_SERVICE
            drop:
            - ALL
          runAsUser: 101
        volumeMounts:
        - mountPath: /usr/local/certificates/
          name: webhook-cert
          readOnly: true
      dnsPolicy: ClusterFirst
      nodeSelector:
        kubernetes.io/os: linux
      serviceAccountName: ingress-nginx
      terminationGracePeriodSeconds: 300
      volumes:
      - name: webhook-cert
        secret:
          secretName: ingress-nginx-admission

---
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: null
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.1.2
    helm.sh/chart: ingress-nginx-4.0.18
  name: ingress-controller
  namespace: ingress-nginx
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 80
    nodePort: 30080
  selector:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
  type: NodePort


6)Create the ingress resource to make the applications available at /wear and /watch on the Ingress service.
Also, make use of rewrite-target annotation field: -

nginx.ingress.kubernetes.io/rewrite-target: /
Ingress resource comes under the namespace scoped, so don't forget to create the ingress in the app-space namespace.

Ingress Created
Path: /wear
Path: /watch
Configure correct backend service for /wear
Configure correct backend service for /watch
Configure correct backend port for /wear service
Configure correct backend port for /watch service

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress-wear-watch
  namespace: app-space
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/ssl-redirect: "false"
spec:
  rules:
  - http:
      paths:
      - path: /wear
        pathType: Prefix
        backend:
          service:
           name: wear-service
           port: 
            number: 8080
      - path: /watch
        pathType: Prefix
        backend:
          service:
           name: video-service
           port:
            number: 8080

OR

=>kubectl get deployment -n ingress-nginx

=>kubectl expose deploy ingress-nginx-controller -n ingress-nginx --name ingress --port=80 --target-port=80 --type NodePort
=>kubectl get svc -n ingress-nginx
=>kubectl edit svc ingress -n ingress-nginx


=>kubectl get ingress -n ingress-nginx
=>ubectl create ingress -h
=>kubectl create ingress simple --rule="foo.com/bar=svc1:8080,tls=my-cert"

=>kubectl create ingress ingress-wear-watch -n app-space --rule="/wear=wear-service:8080" --rule="/watch=video-service:8080"
=>kubectl describe ingress -n app-space

=>kubectl edit ingress ingress-wear-watch -n app-space

Add this under metadata:
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/ssl-redirect: "false"








##Section 8: State Persistence
==================================================


Volume in Docker
--------------------------------------------------
Generally Docker do not save data any host path or volume.
Docker Staorage:
- Storage Drivers
- Volume Drivers

Docker stoarage in local machine file system in: /var/lib/docker
Docker manage Image and Container in Layer model, When we Create a container from a Image
It crete a container (R/W) layer top on Image. Image is readonly and Container layer copy all file in 
Contaner layer what is Read/Write layer.


Docker has Two type mounting:
- Volume Mounting | data seve on var/lib/docker/volumes directory
- Bind Mounting   | In this case container mount its data in host absulate path

Docker has different staorage driver:
 - AUFS
 - ZFS
 - BTRFS
 - DEVECE MAPPER
 - OVERLAY

  

Volume in Kubernetes
--------------------------------------------------
Generally Kubernetes do not save data any host path or volume.
In Kubernetes we have to create volumem in host or other or Storage provide like AWS location to keep data
and Mounting it on Pod

For this in Multi Node cluster with multiple pod it make huge task, to reduce this operation consists
Prestent volume come in to picture.




Using PVCs in Pods:

apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
    - name: myfrontend
      image: nginx
      volumeMounts:
      - mountPath: "/var/www/html"
        name: mypd
  volumes:
    - name: mypd
      persistentVolumeClaim:
        claimName: myclaim

The same is true for ReplicaSets or Deployments. Add this to the pod template section of a Deployment on ReplicaSet.



What type of volume is used to mount a Microsoft Azure Data Disk into a Pod?
AzurDisk

What volume type is to be used to mount a directory from the node on which the pod is running?
hostPath

What volume type is used to store data in a Pod only as long as that Pod is running on that node? 
When the Pod is deleted the files are to be deleted as well.
emptyDir

What is the fc volume type used for?
To mount and existing Fiber channel volume into a Pod



apiVersion: v1
kind: Pod
metadata:
  name: webapp
spec:
  containers:
  - name: event-simulator
    image: kodekloud/event-simulator
    env:
    - name: LOG_HANDLERS
      value: file
    volumeMounts:
    - mountPath: /log           | Inside Container path
      name: log-volume

  volumes:
  - name: log-volume
    hostPath:
      path: /var/log/webapp      | Host machine path
      type: Directory

This hostPath good for single host machine, for multi node cluster this is not a good approce.
Have to use external replicated storage solution, there are many like: awsAlasticStoraverVolume 


##Persistent Volumes | Persistent Volumes Claims
--------------------------------------------------
In Kubernetes for centrally mange pod data and volume user combinagion of:
- Presistent Volume and 
- PersistentVolumeClaim

apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-log
spec:
  capacity:
    storage: 100Mi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteMany
  persistentVolumeReclaimPolicy: Retain
  hostPath:
    path: /pv/log

Its has Three Access Mode:
 - ReadWriteMany
 - ReadWriteOnce
 - ReadOnlyMany


 Persistent Volumes and Persistent Volumes Claims are two dirrerent Object in kunbernetes.
 Usually admin create PV and User user to PVC
 Every PVC bind with a single PV(One -to -One)




##Solution1: Persistent Volumes | Persistent Volumes Claims
--------------------------------------------------

1)The application stores logs at location /log/app.log. View the logs.
You can exec in to the container and open the file:

=>kubectl exec webapp -- cat /log/app.log


2)Configure a volume to store these logs at /var/log/webapp on the host.
Use the spec provided below.
Name: webapp
Image Name: kodekloud/event-simulator
Volume HostPath: /var/log/webapp
Volume Mount: /log

=>kubectl get po webapp -o yaml > webapp.yaml

apiVersion: v1
kind: Pod
metadata:
  name: webapp
spec:
  containers:
  - name: event-simulator
    image: kodekloud/event-simulator
    env:
    - name: LOG_HANDLERS
      value: file
    volumeMounts:
    - mountPath: /log
      name: log-volume

  volumes:
  - name: log-volume
    hostPath:
      # directory location on host
      path: /var/log/webapp
      # this field is optional
      type: Directory
OR
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: webapp
  name: webapp
spec:
  containers:
  - image: kodekloud/event-simulator
    name: webapp
    volumeMounts:
      - mountPath: /log
        name: my-vol
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
  volumes:
    - name: my-vol
      hostPath:
        path: /var/log/webapp

=>kubectl replace --force -f pod4.yaml
=>ls  /var/log/webapp/app.log 



3)Create a Persistent Volume with the given specification.
Volume Name: pv-log
Storage: 100Mi
Access Modes: ReadWriteMany
Host Path: /pv/log
Reclaim Policy: Retain

apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-log
spec:
  capacity:
    storage: 100Mi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteMany
  persistentVolumeReclaimPolicy: Retain
  hostPath:
    path: /pv/log

=>kubectl apply -f pv1.yaml 
=>kubectl get pv


4)Let us claim some of that storage for our application. Create a Persistent Volume Claim with the given specification.
Volume Name: claim-log-1
Storage Request: 50Mi
Access Modes: ReadWriteOnce


apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: claim-log-1
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 50Mi

=>kubectl get pvc


5)Why is the claim not bound to the available Persistent Volume?
=>kubectl get pv,pvc
Because those both has access Modes Mismatch.

6)Update the Access Mode on the claim to bind it to the PV.
Delete and recreate the claim-log-1.


apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: claim-log-1
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 50Mi

=>kubectl replace --force -f pvc.yaml 
=>kubectl get pvc


7)You requested for 50Mi, how much capacity is now available to the PVC?
=>kubectl get pvc



8)Update the webapp pod to use the persistent volume claim as its storage.
Replace hostPath configured earlier with the newly created PersistentVolumeClaim.
Name: webapp
Image Name: kodekloud/event-simulator
Volume: PersistentVolumeClaim=claim-log-1
Volume Mount: /log

=>ls /pv/log

apiVersion: v1
kind: Pod
metadata:
  name: webapp
spec:
  containers:
  - name: event-simulator
    image: kodekloud/event-simulator
    env:
    - name: LOG_HANDLERS
      value: file
    volumeMounts:
    - mountPath: /log
      name: log-volume
  volumes:
  - name: log-volume
    persistentVolumeClaim:
      claimName: claim-log-1
OR
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: webapp
  name: webapp
spec:
  containers:
  - image: kodekloud/event-simulator
    name: webapp
    volumeMounts:
      - mountPath: /log
        name: my-vol
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
  volumes:
    - name: my-vol
      persistentVolumeClaim:
        claimName: claim-log-1


=>kubectl replace --force -f pod5.yaml
=>ls /pv/log


9)What is the Reclaim Policy set on the Persistent Volume pv-log?
=>kubectl get pv
Check:
RECLAIM POLICY=Retain

Its mean: After deleteging PVC PV will Retain but not available for use. 


10)Try deleting the PVC and notice what happens.
If the command hangs, you can use CTRL + C to get back to the bash prompt OR check the status of the pvc from another terminal

=>kubectl delete pvc claim-log-1
Now STATUS will be Terminating 



11)Why is the PVC stuck in Terminating state?
The PVC being use by the pod of webapp.
The PVC was still being used by the webapp pod when we issued the delete command. Until the pod is deleted, the PVC will remain in a terminating state.



CKAD exam this is all about Presistent volume:
We have covered the required topics for the exam for the Storage section.




##Storage Class
--------------------------------------------------
Every pcv bind with pv, so have to create it before binging, this is call static Provisioning.
Storage class come to Dynamic Provisioning.

In this case we not have to create any pv(It will create auto by Storage class), 
we have to create just StorageClass and come it with pvc to pod binging automatically.







StatefulSets:
--------------------------------------------------
StatefulSets are like DeploymentSet, dirrerent is its run pod sequential with pod index number.

Deployment not possible to run pod sequentially what is required, then come to StatefulSets.
StatefulSets same as deploument, its create pod flow the deployment template and scale as required also rolling update.



Headless Service:
--------------------------------------------------


PersistentVolumeClaimTemplate for create auto vpc.





##Solution2: Storage Class
--------------------------------------------------
=>kubectl get sc
=>kubectl get storageclass


1)What is the name of the Storage Class that does not support dynamic volume provisioning?
local-storage

The "local-storage" Storage Class in Kubernetes does not support dynamic volume provisioning. 
It is used to represent local storage devices attached to the nodes in the cluster. 
With this Storage Class, you need to manually manage the provisioning and allocation of local storage volumes.


What is the Volume Binding Mode used for this storage class (the one identified in the previous question)?
=>kubectl describe sc local-storage
=>kubectl get sc

2)Let's fix that. Create a new PersistentVolumeClaim by the name of local-pvc that should bind to the volume local-pv.
Inspect the pv local-pv for the specs.
PVC: local-pvc
Correct Access Mode?
Correct StorageClass Used?
PVC requests volume size = 500Mi?

Inspect the persistent volume and look for the Access Mode, Storage and StorageClassName used. Use this information to create the PVC.
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: local-pvc
spec:
  accessModes:
  - ReadWriteOnce
  storageClassName: local-storage
  resources:
    requests:
      storage: 500Mi
OR
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: local-pvc
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: local-storage
  resources:
    requests:
      storage: 500Mi
  selector:
    matchLabels:
      volume: local-pv

The Storage Class called local-storage makes use of VolumeBindingMode set to WaitForFirstConsumer. 
This will delay the binding and provisioning of a PersistentVolume until a Pod using the PersistentVolumeClaim is created.
Run the command: kubectl describe pvc local-pvc and look under the Events section.


3)Create a new pod called nginx with the image nginx:alpine. The Pod should make use of the PVC local-pvc and mount the volume at the path /var/www/html.
apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    name: nginx
spec:
  containers:
  - name: nginx
    image: nginx:alpine
    volumeMounts:
      - name: local-persistent-storage
        mountPath: /var/www/html
  volumes:
    - name: local-persistent-storage
      persistentVolumeClaim:
        claimName: local-pvc
OR
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
    - name: nginx
      image: nginx:alpine
      ports:
        - containerPort: 80
      volumeMounts:
        - name: data-volume
          mountPath: /var/www/html
  volumes:
    - name: data-volume
      persistentVolumeClaim:
        claimName: local-pvc

apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    name: nginx
spec:
  containers:
  - name: nginx
    image: nginx:alpine
    volumeMounts:
      - name: local-persistent-storage
        mountPath: /var/www/html
  volumes:
    - name: local-persistent-storage
      persistentVolumeClaim:
        claimName: local-pvc


4)Create a new Storage Class called delayed-volume-sc that makes use of the below specs:
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer

apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: delayed-volume-sc
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer










##Update for Sep 2021
==================================================

##Define, Build and Modify Container Image
--------------------------------------------------
In a Dockerfile every thing in RIGHT CAPS all are INSTRACTION and left are ARGUMENT.
Docker image are Layers architecture.



##Solution1: Docker
--------------------------------------------------

1)When a container is created using the image built with this Dockerfile, what is the command used to RUN the application inside it.

Open the Dockerfile and look for ENTRYPOINT command.


2)Run an instance of the image webapp-color and publish port 8080 on the container to 8282 on the host.
=>docker run -p 8282:8080 -d webapp-color 


3)What is the base Operating System used by the python:3.6 image?
=>docker run python:3.6 cat /etc/*release*

4)Build a new smaller docker image by modifying the same Dockerfile and name it webapp-color and tag it lite.
Hint: Find a smaller base image for python:3.6. Make sure the final image is less than 150MB.
Ans:
In the webapp-color directory, run the ls -l command to list the Dockerfile and other files.
And modify Dockerfile to use python:3.6-alpine image and then build using docker build -t webapp-color:lite .

5)Run an instance of the new image webapp-color:lite and publish port 8080 on the container to 8383 on the host.
=>docker run -d -p 8383:8080 webapp-color:lite





##Authentication, Authorization and Admission Control
--------------------------------------------------
You must have to secure kubernetes cluster host secure.

kube-apiserver one of  is the entry door of kunbernetes cluster, almost all operation can be done by this api server.
So, this is the second point to make secure. Also make controll of accessing of kube-apiserver its self.

Authentication of kunbe-apiserver by:
 - Files-Username and Password
 - Files-Username and Token
 - certificates
 - Externaem provider - LDAP
 - Servcie Account


Authorization of kube-apiserver by:
- RBAC Authorization
- ABAC Authorization
- Node Authorization
- Webhook Mode

All Communication between kube-apiserver and other component like kubelet, etcd, controller etc are TLS Encryped.

Authentication
---------------------------------------------------
Two type of kubernetes user:
- User/human
- Service Account/ Machine
Kubernetes its not manage account natively, user exterrna machinesume. like LDAP.
But kubernetes manage ServiceAccount.


All request go to kube-apiserver for authencation:
kubectl ------------------------------>
                                          -> kube-apiserver
crul https://kube-server-ip ---------->

Auth Mechanisms:
 - Static Password File
 - Static Token File
 - certificates
 - Identity Server


CERTIFICATE generation not part on CKAD exam.





##KubeConfig
--------------------------------------------------
By default kubectl tools find a file under user home directory, $HOME/.kube/config for authencation.
This why you dont add any file kubectl command any file.


KubeConfig will three section in a single file called Config in .kube folder:
Section are:
 - Cluster       | Array of cluster
 - Contexts      | Array of contents, Cluster and user thogether build context
 - Users         | Array of Users

=>kubectl config -h
=>kubectl config view
=>kubectl config view --kubeconfig=my-custome-config

=>kubectl config use-context prod-user@production

Set Context:
=>kubectl config --kubeconfig=/root/my-kube-config use-context research
=>kubectl config --kubeconfig=/root/my-kube-config current-context


my-custom-config:
apiVersion: v1
kind: Config
clusters:
- name: production
  cluster:
    certificate-authority: /etc/kubernetes/pki/ca.crt
    server: https://controlplane:6443

- name: development
  cluster:
    certificate-authority: /etc/kubernetes/pki/ca.crt
    server: https://controlplane:6443

- name: kubernetes-on-aws
  cluster:
    certificate-authority: /etc/kubernetes/pki/ca.crt
    server: https://controlplane:6443

- name: test-cluster-1
  cluster:
    certificate-authority: /etc/kubernetes/pki/ca.crt
    server: https://controlplane:6443

contexts:
- name: test-user@development
  context:
    cluster: development
    user: test-user

- name: aws-user@kubernetes-on-aws
  context:
    cluster: kubernetes-on-aws
    user: aws-user

- name: test-user@production
  context:
    cluster: production
    user: test-user

- name: research
  context:
    cluster: test-cluster-1
    user: dev-user

users:
- name: test-user
  user:
    client-certificate: /etc/kubernetes/pki/users/test-user/test-user.crt
    client-key: /etc/kubernetes/pki/users/test-user/test-user.key
- name: dev-user
  user:
    client-certificate: /etc/kubernetes/pki/users/dev-user/developer-user.crt
    client-key: /etc/kubernetes/pki/users/dev-user/dev-user.key
- name: aws-user
  user:
    client-certificate: /etc/kubernetes/pki/users/aws-user/aws-user.crt
    client-key: /etc/kubernetes/pki/users/aws-user/aws-user.key

current-context: test-user@development
preferences: {}


current-context: test-user@development
This is for current contest set, To see default config file this:
=>kubectl config view
For custome file view command:
=>kubectl config view --kubeconfig=my-custome-config

To user Dfirrent config bile bring this in default flder .kube, then to set deferent context user:
=>kubectl config use-context prod-user@production





##Solution2: KubeConfig
--------------------------------------------------

1)Where is the default kubeconfig file located in the current environment?
Find the current home directory by looking at the HOME environment variable.

=>echo $HOME
=>pwd 
=>ls .kube
=>cat .kube/config

2)How many cluster and Users are defined in the default kubeconfig file?
=>kubectl config view
OR
=>cat .kube/config

3)What is the name of the cluster/user/context configured in the default kubeconfig file?
=>kubectl config view
=>cat .kube/config
Check:
users:
- name: kubernetes-admin

contexts:
- context:
    cluster: kubernetes


4)I would like to use the dev-user to access test-cluster-1. Set the current context to the right one so I can do that.
Once the right context is identified, use the kubectl config use-context command.
Current context set

To use that context, run the command: 
=>kubectl config --kubeconfig=/root/my-kube-config use-context research

To know the current context, run the command: 
=>kubectl config current-context
=>kubectl config --kubeconfig=/root/my-kube-config current-context
OR
By default kubectl config command user /root/.kube/config file, to user custome file have to mentation like this

=>kubectl config use-context research --kubeconfig /root/my-kube-config

=>cat /root/my-kube-config
Check:
current-context: research
kind: Config
preferences: {}


5)We don't want to have to specify the kubeconfig file option on each command. 
Make the my-kube-config file the default kubeconfig.
Default kubeconfig file configured

=>mv /root/my-kube-config /root/.kube/config
=>cat /root/.kube/config
=>kubectl config view



6)With the current-context set to research, we are trying to access the cluster. However something seems to be wrong. Identify and fix the issue.
Try running the kubectl get pods command and look for the error. All users certificates are stored at /etc/kubernetes/pki/users.

=>ls /etc/kubernetes/pki/users/dev-user/
Its look file name issue in config file, lets fixed it
=>vi  /root/.kube/config

=>kubectl get pod
Now request no error.





##API Groups
--------------------------------------------------
curl http://localhost:6443 -k

=>kubectl proxy
=>curl http://localhost:8081- -k

=>curl https://kube-master:6443/version
=>curl https://kube-master:6334/api/v1/pods

Kubernetes has different type of api Groups: /metrics, /healthz, /version,/api,/apis,/logs

This two Groups 
- /api   | Core group
- /apis  | Name Group

Under this Core grpup: /api
/v1 - under this
/namespace, /pods,/rc,events,/endpoint,/nods,/binding,/pv,/pvc,/configmap,/service,/secrets etc.


Under this named grpup: /apis
/apps,/extensions,/networking.k8s.io,/storage.k8s.io,/storage.k8s.io,/authentication.k8s.io,/certificate.k8s.io etc.
and Unser this named grpup: all apis has its own resource and resources has its action.

/apps:
 - /v1
   - /deployment
       - /list
       - /get
       - /create 
       - /delete
       - /update
       - /watch 
   - /replicaset
   - /StatefulSets
     
Like:
/networking.k8s.io
- /v1 
  - /networkpolicy

Like:
/certificate.k8s
- /v1
    -/certificatesigningrequests

Kubernetes API Groups Documentation show all details.
Also in kunbernetes cluster:
For show all aveilable api groups:
curl http://localhost:6443 -k

Then, show for names api
curl http://localhost:6443/apis -k | grep "name"

To access kubectl cluster make a proxy:
=>kubectl proxy
=>curl http://localhost:8081- -k

kube proxy not equeal as kubectl proxy.







##Authorization
--------------------------------------------------

Authorization Mechanisms:
 - Node
   Node authorization.

 - ABAC   
   attributes base authorization, create attributes in json formate a file and add this.
   every time upde this file need to restart.

 - RBAC
   Create role and add user on this role.
 - Webhook
   Outsource for authencation.
-AlwaysAllow
-AlwaysDeny

You cand user multiple authorization mode at a time. all mode working on flow the order.


##141. Role Based Access Controls
--------------------------------------------------
Create a role using file:

apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: default
  name: developer
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["list", "create","delete"]

Now we have to create a role bindign, rolebinding bind the role to the user.

kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: dev-user-binding
subjects:
- kind: User
  name: dev-user
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: developer
  apiGroup: rbac.authorization.k8s.io


To check Role and rolebinding:
=>kubectl get roles
=>kubectl get rolebindings
=>kubectl describe role developer

To check access or other persmission:
=>kubectl auth can-i create deployment
=>kubectl auth can-i delete nodes

=>kubectl auth can-i create deployment --as dev-user
=>kubectl auth can-i create deployment --as dev-user --namespace test


Role: Create Role Object by specifice name, rule with a yaml file
RoleBinding: Bind User with a role








##Role Based Access Controls
--------------------------------------------------

1) Inspect the environment and identify the authorization modes configured on the cluster.
Check the kube-apiserver settings.
Use the command 
=>kubectl describe pod kube-apiserver-controlplane -n kube-system 
and look for --authorization-mode.
OR
=>cat /etc/kubernetes/manifests/kube-apiserver.yaml
Check: authorization-mode=
spec:
  containers:
  - command:
    - kube-apiserver
    - --advertise-address=192.7.86.6
    - --allow-privileged=true
    - --authorization-mode=Node,RBAC
OR
=>ps -aux | grep authorization


2)How many roles exist in the default namespace?
=>kubectl get roles

3)How many roles exist in all namespaces together?
=>kubectl get roles -A --no-headers | wc -l


4)What are the resources the kube-proxy role in the kube-system namespace is given access to?
=>kubectl describe role kube-proxy -n kube-system


5)Which account is the kube-proxy role assigned to?
=>kubectl get rolebinding kube-proxy -n kube-system
=>kubectl describe rolebinding kube-proxy -n kube-system



6)A user dev-user is created. User's details have been added to the kubeconfig file. Inspect the permissions granted to the user. Check if the user can list pods in the default namespace.
Use the --as dev-user option with kubectl to run commands as the dev-user.

=>kubectl config view
=>kubectl auth can-i list pods --as dev-user
=>kubectl get pods --as dev-user



7)Create the necessary roles and role bindings required for the dev-user to create, list and delete pods in the default namespace.
Use the given spec:
Role: developer
Role Resources: pods
Role Actions: list
Role Actions: create
Role Actions: delete
RoleBinding: dev-user-binding
RoleBinding: Bound to dev-user

Use the command kubectl create to create a role developer and rolebinding dev-user-binding in the default namespace.

To create a Role:- 
=>kubectl create role developer --namespace=default --verb=list,create,delete --resource=pods

To create a RoleBinding:- 
=>kubectl create rolebinding dev-user-binding --namespace=default --role=developer --user=dev-user
OR
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: default
  name: developer
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["list", "create","delete"]

---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: dev-user-binding
subjects:
- kind: User
  name: dev-user
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: developer
  apiGroup: rbac.authorization.k8s.io

OR

Create Role:
=>kubectl create role -h
=>kubectl create role developer --verb=list,create,delete --resource=pods
=>kubectl get role
=>kubectl describe roles developer 

CreateRoleBinding:
=>kubectl create rolebinding -h
=>kubectl create rolebinding dev-user-binding --role=developer --user=dev-user
=>kubectl describe rolebinding dev-user-binding 


8)A set of new roles and role-bindings are created in the blue namespace for the dev-user. However, the dev-user is unable to get details of the dark-blue-app pod in the blue namespace. Investigate and fix the issue.
We have created the required roles and rolebindings, but something seems to be wrong.

New roles and role bindings are created in the blue namespace.
Check out the resourceNames configured on the role.

Run the command: 
=>kubectl edit role developer -n blue 
and correct the resourceNames field. You don't have to delete the role.
OR

=>kubectl get pod dark-blue-app -n blue --as dev-user
=>kubectl get rolebinding -n blue

=>kubectl describe role developer -n blue
Check the Resource Name: dark-blue-app  not in there
PolicyRule:
  Resources  Non-Resource URLs  Resource Names  Verbs
  ---------  -----------------  --------------  -----
  pods       []                 [blue-app]      [get watch create delete]


Need to add this app on resource, for that need to Edit role:

=>kubectl edit role developer -n blue

Add app name on resourceName
rules:
- apiGroups:
  - ""
  resourceNames:
  - dark-blue-app

Check, this ok now
=>kubectl get pod dark-blue-app -n blue --as dev-user



9)Add a new rule in the existing role developer to grant the dev-user permissions to create deployments in the blue namespace.
Remember to add api group "apps".

Use the command kubectl edit to add a new rule for user dev-user to grant permissions to create deployments in the blue namespace.

=>kubectl create deployment mydeploy --image=nginx -n blue --as dev-user
=>kubectl edit role developer -n blue
add this:
- apiGroups:
  - apps
  resources:
  - deployments
  verbs:
  - get
  - watch
  - create
  - delete

So it looks like this:

apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: developer
  namespace: blue
rules:
- apiGroups:
  - apps
  resourceNames:
  - dark-blue-app
  resources:
  - pods
  verbs:
  - get
  - watch
  - create
  - delete
- apiGroups:
  - apps
  resources:
  - deployments
  verbs:
  - create







##144. Cluster Roles
--------------------------------------------------
In kubernetes all Resource are catagoies are namespace or cluster scope.
Namespace Resource:
- pods
- replicaset
- roles
- service 
- deployment etc.
If you not defined any namespace it will create in default namespace.

Cluter Scoped:
- nodes
- pv
- clusterroles
- cluseterrolebinding
- namespaces etc.
Cluster role and Cluster binding for create cluster wise resource and namespace role/binding for namespace wise 
resource modify.
Same as namespace role and rolebinding, first create cluster role and rolebinding for user.

Cluster role can access all namespace resources.



To get the all kist on namespace/non-namesapce resource list:
=>kubectl api-resources --namespaced=true
=>kubectl api-resources --namespaced=false





##Solution4: Cluster Roles
--------------------------------------------------
1)How many ClusterRoles and ClusterRoleBindings do you see defined in the cluster?
=>kubectl get clusterroles --no-headers | wc -l
=>kubectl get clusterroles --no-headers -o json | jq '.items | length'

=>kubectl get clusterrolebinding --no-headers | wc -l
=>kubectl get clusterrolebindings --no-headers -o json | jq '.items | length'


2)What namespace is the cluster-admin clusterrole part of?
Cluster roles and cluster are not part of any namespace


3)What user/groups are the cluster-admin role bound to?
The ClusterRoleBinding for the role is with the same name.
=>kubectl describe clusterrolebinding cluster-admin


4)What level of permission does the cluster-admin role grant?
Inspect the cluster-admin role's privileges.
=>kubectl describe clusterrole cluster-admin


5)A new user michelle joined the team. She will be focusing on the nodes in the cluster. Create the required ClusterRoles and ClusterRoleBindings so she gets access to the nodes.

Use the command kubectl create to create a clusterrole and clusterrolebinding for user michelle to grant access to the nodes.
After that test the access using the command kubectl auth can-i list nodes --as michelle.

=>kubectl auth can-i list nodes --as michelle

=>kubectl create clusterrole cls-role --verb=get,list,create,delete --resource=nodes
OR
=>kubectl create clusterrole node-admin --verb=get,watch,list,create,delete --resource=nodes --dry-run=client -o yaml > node-admin.yaml
cluste.yaml 
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: node-admin
rules:
- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["get", "watch", "list", "create", "delete"]


=>kubectl create clusterrolebinding michelle-binding --clusterrole=node-admin --user=michelle --dry-run=client -o yaml > michelle-binding.yaml
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: michelle-binding
subjects:
- kind: User
  name: michelle
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: node-admin
  apiGroup: rbac.authorization.k8s.io

=>kubectl apply -f cluste.yaml 
OR

=>kubectl get nodes --as michelle

CreateClusterRole:
=>kubectl create clusterrole -h
=>kubectl create clusterrole pod-reader --verb=get,list,watch --resource=pods
=>kubectl create clusterrole michelle-role --verb=get,list,watch --resource=nodes

CreateClusterBinding:
=>kubectl create clusterrolebinding michelle-role-binding --clusterrole=michelle-role --user=michelle

=>kubectl describe clusterrole michelle-role
=>kubectl describe clusterrolebinding michelle-role-binding

Check:
=>k auth can-i list node --as michelle
=>k config can-i list node --as michelle




6)michelle's responsibilities are growing and now she will be responsible for storage as well. Create the required ClusterRoles and ClusterRoleBindings to allow her access to Storage.
Get the API groups and resource names from command kubectl api-resources. Use the given spec:

ClusterRole: storage-admin
Resource: persistentvolumes
Resource: storageclasses
ClusterRoleBinding: michelle-storage-admin
ClusterRoleBinding Subject: michelle
ClusterRoleBinding Role: storage-admin

Use the command kubectl create to create a new ClusterRole and ClusterRoleBinding.
Assign it correct resources and verbs.
After that test the access using the command kubectl auth can-i list storageclasses --as michelle.

---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: storage-admin
rules:
- apiGroups: [""]
  resources: ["persistentvolumes"]
  verbs: ["get", "watch", "list", "create", "delete"]
- apiGroups: ["storage.k8s.io"]
  resources: ["storageclasses"]
  verbs: ["get", "watch", "list", "create", "delete"]

---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: michelle-storage-admin
subjects:
- kind: User
  name: michelle
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: storage-admin
  apiGroup: rbac.authorization.k8s.io

=>kubectl apply -f clusterRoleBinding.yaml 
OR
=>kubectl api-resources

=>kubectl create clusterrole storage-admin --resource=persistentvolumes,storageclasses --verb=list,create,get,watch
=>kubectl describe clusterrole storage-admin 

=>kubectl get clusterrole storage-admin -o yaml
Check out two Role


Create ClusterroleBinding:
=>kubectl create clusterrolebinding michelle-storage-admin --user=michelle --clusterrole=storage-admin
=>kubectl describe clusterrolebinding michelle-storage-admin

=>kubectl get storageclass --as michelle






##147. Admission Controllers
--------------------------------------------------
kubectl -> Aluthentication -> Authorization  -> Create Pod
Usually Aluthentication done by certificate. may with role base Authorization.

But, Role base Authorization are not able make restrection in more details like:
- Only permite image allow, and certain registry
- Do not permit runAs Root user
- Only permit certain capabilities
- Pod always has labels etc.

For make this kind of restraction AdmissionController come in to picture.



kubectl -> Aluthentication -> Authorization -> AdmissionController -> Create Pod
AdmissionController List:
 - AlwaysPullImages
 - DefaultStorageClass
 - EventRateLimit
 - NamespaceLifeCycle
and More..........

=>kub-apiserver -h | grep enable-admission-plugins
=>kubectl exec kube-apiserver-controlplane -n kube-system -- kube-apiserver -h | grep enable-admission-plugins

AdmissionController not only valaid and reject user request, It can also done back end operation.
and May change thei request its self.

NamespaceAutoProvision and NamespaceExists controller deprecated,
Now it NamespaceLifeCycle admission controller.






##Admission Controllers
--------------------------------------------------
1)What is not a function of admission controller?
Authencate a user.

2)Which admission controller is not enabled by default?
Check enable-admission-plugins in kube-apiserver help options
NamespaceAutoProvision

=>kubectl get pods -n kube-system
=>kubectl exec -it kube-apiserver-controlplane -n kube-system -- kube-apiserver -h | grep 'enable-admission-plugins'
OR
=>kubectl exec -it kube-apiserver-controlplane -n kube-system -- kube-apiserver -h
Check this:
--disable-admission-plugins strings                                                         


3)Which admission controller is enabled in this cluster which is normally disabled?
Check enable-admission-plugins in /etc/kubernetes/manifests/kube-apiserver.yaml

=>cat /etc/kubernetes/manifests/kube-apiserver.yaml
=>grep enable-admission-plugin /etc/kubernetes/manifests/kube-apiserver.yaml



4)The previous step failed because kubernetes have NamespaceExists admission controller enabled which rejects requests to namespaces that do not exist. So, to create a namespace that does not exist automatically, we could enable the NamespaceAutoProvision admission controller
Enable the NamespaceAutoProvision admission controller
Note: Once you update kube-apiserver yaml file, please wait for a few minutes for the kube-apiserver to restart completely.

=>kubectl run nginx --image nginx -n blue

Edit /etc/kubernetes/manifests/kube-apiserver.yaml 
and add NamespaceAutoProvision admission controller to --enable-admission-plugins list

=>vi /etc/kubernetes/manifests/kube-apiserver.yaml
Add this:
    - --enable-admission-plugins=NodeRestriction,NamespaceAutoProvision

Add NamespaceAutoProvision admission controller to --enable-admission-plugins list to /etc/kubernetes/manifests/kube-apiserver.yaml
It should look like below

    - --enable-admission-plugins=NodeRestriction,NamespaceAutoProvision
API server will automatically restart and pickup this configuration.

=>kubectl get ns
=>kubectl run mynginx --image=nginx -n blue

Note that the NamespaceExists and NamespaceAutoProvision admission controllers are deprecated and now replaced by 
NamespaceLifecycle admission controller.

The NamespaceLifecycle admission controller will make sure that requests
to a non-existent namespace is rejected and that the default namespaces such as
default, kube-system and kube-public cannot be deleted.



5)Disable DefaultStorageClass admission controller
This admission controller observes creation of PersistentVolumeClaim objects that do not request any specific storage class and automatically adds a default storage class to them. This way, users that do not request any special storage class do not need to care about them at all and they will get the default one.
Note: Once you update kube-apiserver yaml file then please wait few mins for the kube-apiserver to restart completely.

Add DefaultStorageClass to disable-admission-plugins in /etc/kubernetes/manifests/kube-apiserver.yaml


=>vi /etc/kubernetes/manifests/kube-apiserver.yaml
Add this under -command: section
- --disable-admission-plugins=DefaultStorageClass


Since the kube-apiserver is running as pod you can check the process to see enabled and disabled plugins.
=>ps -ef | grep kube-apiserver | grep admission-plugins






##Validating and Mutating Admission Controllers
--------------------------------------------------
Two type AdmissionController
- Mutation AdmissionController            | Change the Object, the request its self.
- Validation AdmissionController          | Validate the request, Allow or Dineis

MutationController working before ValidationController, Like: Change the request berore validate.
This is all builtin Controller.


For Custome Admission Controller:

First Create Webhook Server, then add this on. For this we can build our own webhook server any langualge.
just support validate amd mutate api as json data of request and response.

CKAD exam dont have to develop webhook server code, just need to deploy and what we cand do using this.

For working with custom webhook sever:
- First deploy the webhook server
- then Need a service to access this
- then need a configurarion to access the service in cluster, so create a config Object




##Solution6: 150. Validating and Mutating Admission Controllers
--------------------------------------------------
1)Which of the below combination is correct for Mutating and validating admission controllers ?
NamespaceAutoProvision- Mutating , NamespaceExists - Validating


2)What is the flow of invocation of admission controllers?
First Mutating then Validating


3)Create TLS secret webhook-server-tls for secure webhook communication in webhook-demo namespace.
We have already created below cert and key for webhook server which should be used to create secret.
Certificate : /root/keys/webhook-server-tls.crt
Key : /root/keys/webhook-server-tls.key

Create tls secret type in kubernetes with --cert and --key options

=>kubectl create secret tls webhook-server-tls --cert "/root/keys/webhook-server-tls.crt" --key "/root/keys/webhook-server-tls.key" -n webhook-demo
=>kubectl -n webhook-demo create secret tls webhook-server-tls \
--cert "/root/keys/webhook-server-tls.crt" \
--key "/root/keys/webhook-server-tls.key"


4) Now create a deployment.
=>k apply -f webhook-deployment.yaml 
cat webhook-deployment.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webhook-server
  namespace: webhook-demo
  labels:
    app: webhook-server
spec:
  replicas: 1
  selector:
    matchLabels:
      app: webhook-server
  template:
    metadata:
      labels:
        app: webhook-server
    spec:
      securityContext:
        runAsNonRoot: true
        runAsUser: 1234
      containers:
      - name: server
        image: stackrox/admission-controller-webhook-demo:latest
        imagePullPolicy: Always
        ports:
        - containerPort: 8443
          name: webhook-api
        volumeMounts:
        - name: webhook-tls-certs
          mountPath: /run/secrets/tls
          readOnly: true
      volumes:
      - name: webhook-tls-certs
        secret:
          secretName: webhook-server-tls


Create webhook service now so that admission controller can communicate with webhook.
We have already added sample service definition under /root/webhook-service.yaml so just create service with that definition.
=>k apply -f webhook-service.yaml 

cat /root/webhook-service.yaml 
apiVersion: v1
kind: Service
metadata:
  name: webhook-server
  namespace: webhook-demo
spec:
  selector:
    app: webhook-server
  ports:
    - port: 443
      targetPort: webhook-api



We have added MutatingWebhookConfiguration under /root/webhook-configuration.yaml.
If we apply this configuration which resource and actions it will affect?

Check operations and resources under rules in /root/webhook-configuration.yaml
Pod with create operation.

Create webhookConfiguration:
=>kubectl apply -f webhook-configuration.yaml 

In previous steps we have deployed demo webhook which does below
- Denies all request for pod to run as root in container if no securityContext is provided.
- If no value is set for runAsNonRoot, a default of true is applied, and the user ID defaults to 1234
- Allow to run containers as root if runAsNonRoot set explicitly to false in the securityContext


Deploy a pod with no securityContext specified.
We have added pod definition file under /root/pod-with-defaults.yaml

=>kubectl apply -f /root/pod-with-defaults.yaml
cat /root/pod-with-defaults.yaml
# A pod with no securityContext specified.
# Without the webhook, it would run as user root (0). The webhook mutates it
# to run as the non-root user with uid 1234.
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-defaults
  labels:
    app: pod-with-defaults
spec:
  restartPolicy: OnFailure
  containers:
    - name: busybox
      image: busybox
      command: ["sh", "-c", "echo I am running as user $(id -u)"]



5)What are runAsNonRoot and runAsUser values for previously created pods securityContext?
We did not specify any securityContext values in pod definition so check out the changes done by mutation webhook in pod

=>k get pod
=>kubectl get pod pod-with-defaults -o yaml
Check Security Context section:
 securityContext:
    runAsNonRoot: true
    runAsUser: 1234
  serviceAccount: default






6)Deploy pod with a securityContext explicitly allowing it to run as root
We have added pod definition file under /root/pod-with-override.yaml
Validate securityContext after you deploy this pod

Run below command

=>kubectl apply -f /root/pod-with-override.yaml
then Validate securityContext with
=>kubectl get po pod-with-override -o yaml | grep -A2 " securityContext:"

=>cat pod-with-override.yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-override
  labels:
    app: pod-with-override
spec:
  restartPolicy: OnFailure
  securityContext:
    runAsNonRoot: false
  containers:
    - name: busybox
      image: busybox
      command: ["sh", "-c", "echo I am running as user $(id -u)"]

=>k apply -f pod-with-override.yaml 


7)Deploy a pod with a conflicting securityContext i.e. pod running with a user id of 0 (root)
We have added pod definition file under /root/pod-with-conflict.yaml
Mutating webhook should reject the request as its asking to run as root user without setting runAsNonRoot: false

Run below command

=>kubectl apply -f /root/pod-with-conflict.yaml
You should get error like below
Error from server: error when creating "/root/pod-with-conflict.yaml": admission webhook "webhook-server.webhook-demo.svc" denied the request: runAsNonRoot specified, but runAsUser set to 0 (the root user)

=>cat pod-with-conflict.yaml 
# A pod with a conflicting securityContext setting: it has to run as a non-root
# user, but we explicitly request a user id of 0 (root).
# Without the webhook, the pod could be created, but would be unable to launch
# due to an unenforceable security context leading to it being stuck in a
# 'CreateContainerConfigError' status. With the webhook, the creation of
# the pod is outright rejected.
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-conflict
  labels:
    app: pod-with-conflict
spec:
  restartPolicy: OnFailure
  securityContext:
    runAsNonRoot: true
    runAsUser: 0
  containers:
    - name: busybox
      image: busybox
      command: ["sh", "-c", "echo I am running as user $(id -u)"]








##153. API Versions/API Deprecations
--------------------------------------------------
VersionName:
Alpha -> Beta ->GA(Stable)

Alpha: Not enable by default, have to ebale vi flag.
Alpha: May not come to next releae version or its can be remove with any notice.
After all of the bug fixed and testing it come to beta version or drop.

Beta:
Beta stage also have minor bug, it enable bu defautl.

GA: Its ( v1 ), Ebable by default.


PraferedVersion:
For a single thisng mya has multiple api version, to see the prefered version use:
=>kubectl explain

StorageVersion:
Object save on ETCD storage may or may not to different version from prefered version.



If any apiGroup has multiple version, PreferredVersion will work by default. To check tis:
=>localhost:8002/apis/batch
Other version call: Storage Version

To enable other version:
add the line in :
ExecStart = /
  --runtime-config=batch/v2alpha1\\
and Restart thsi api server.





API Deprection
--------------------------------------------------
Which versin we have to user for this we can check API Deprection.and ists rule.

Rules:
RuleOne: Remove a version on nesxt encrement version.
Rule2: For next version have to be same filed of previous version.
Role3: Beta and GA version must have to mantin in 9 to 12 month, alpha 0.
       alpha version not replace GA, and not to be depracate before GA is comming. v2 can be depricated v1 only.
       


=>kubectl convert -f oldFile --output-version newAPIVersion
=>kubectl convert -f mydeployment,yaml --output-version apps/v1
For convert older version to new version.



  





##Solution7: 153. API Versions/API Deprecations
--------------------------------------------------

1)Identify the short names of the deployments, replicasets, cronjobs and customresourcedefinitions.
=>kubectl api-resources
=>kubectl api-resources | grep -e customresourcedefinitions -e cronjobs
=>k api-resources | grep -e deployments -e replicasets -e cronjobs -e customresourcedefinitions

2)What is the patch version in the given Kubernetes API version?
Kubernetes API version - 1.22.2
2, the last one

=>kubectl version --short



3)Identify which API group a resource called job is part of?

Run the command kubectl explain job and see the API Version in the top of the line.
At first will be the API group and second will be the version : <group>/<version>

=>kubectl explain job
Check
KIND:     Job
VERSION:  batch/v1


4)What is the preferred version for authorization.k8s.io api group?

It supports v1 and v1beta1 but the preferred version is v1.

=>kubectl proxy 8001&
=>curl localhost:8081/apis/authorization.k8s.io

Where & runs the command in the background and kubectl proxy command starts the proxy to the kubernetes API server.




5)Enable the v1alpha1 version for rbac.authorization.k8s.io API group on the controlplane node.
Note: If you made a mistake in the config file could result in the API server being unavailable and can break the cluster.

Add the --runtime-config=rbac.authorization.k8s.io/v1alpha1 option to the kube-apiserver.yaml file.

As a good practice, take a backup of that apiserver manifest file before going to make any changes.

In case, if anything happens due to misconfiguration you can replace it with the backup file.

root@controlplane:~# cp -v /etc/kubernetes/manifests/kube-apiserver.yaml /root/kube-apiserver.yaml.backup
Now, open up the kube-apiserver manifest file in the editor of your choice. It could be vim or nano.

root@controlplane:~# vi /etc/kubernetes/manifests/kube-apiserver.yaml
Add the --runtime-config flag in the command field as follows :-

 - command:
    - kube-apiserver
    - --advertise-address=10.18.17.8
    - --allow-privileged=true
    - --authorization-mode=Node,RBAC
    - --client-ca-file=/etc/kubernetes/pki/ca.crt
    - --enable-admission-plugins=NodeRestriction
    - --enable-bootstrap-token-auth=true
    - --runtime-config=rbac.authorization.k8s.io/v1alpha1 --> This one 
After that kubelet will detect the new changes and will recreate the apiserver pod.

It may take some time.

root@controlplane:~# kubectl get po -n kube-system
Check the status of the apiserver pod. It should be in running condition.

OR
=>cp /etc/kubernetes/manifests/kube-apiserver.yaml /root/kube-apiserver.yaml.backup
Add this line:
- --runtime-config=rbac.authorization.k8.io/v1alpha1

6)Install the kubectl convert plugin on the controlplane node.
If unsure how to install then refer to the official k8s documentation page which is available at the top right panel.

Download the latest release version from the curl command :-
=>curl -LO https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl-convert

Change the permission of the file and move to the /usr/local/bin/ directory.
=>chmod +x kubectl-convert 
=>mv kubectl-convert /usr/local/bin/kubectl-convert
Use the --help option to see more option.
=>kubectl-convert --help
If it'll show more options that means it's configured correctly if it'll give an error that means we haven't set up properly.


OR
=>curl -LO "https://dl.k8s.io/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl-convert.sha256"

=>chmod +x kubectl-convert
=>mv kubectl-convert /usr/local/bin/
=>kubectl-convert -h


7)Ingress manifest file is already given under the /root/ directory called ingress-old.yaml.
With help of the kubectl convert command, change the deprecated API version to the networking.k8s.io/v1 and create the resource.

Run the command: kubectl-convert -f ingress-old.yaml --output-version networking.k8s.io/v1 | kubectl apply -f -


Run the command kubectl-convert to change the deprecated API version as follows :-
=>kubectl-convert -f ingress-old.yaml --output-version networking.k8s.io/v1

# store new changes into a file 
=>kubectl-convert -f ingress-old.yaml --output-version networking.k8s.io/v1 > ingress-new.yaml
After changing the API version and storing into a file, use the kubectl create -f command to deploy the resource :-

=>kubectl create -f ingress-new.yaml
Inspect the apiVersion as follows :-

=>kubectl get ing ingress-space -oyaml | grep apiVersion
Note: Maybe you will not see the service and other resources mentioned in the ingress YAML on the controlplane node because we have to only deploy the ingress resource with the latest API version.

OR
=>cat ingress-old.yaml
=>kubectl-convert -f ingress-old.yaml --output-version networking.k8s.io/v1
=>kubectl-convert -f ingress-old.yaml --output-version networking.k8s.io/v1>newintrss.yaml
=>kubectl apply -f newintrss.yaml







##Custom Resource Definition
--------------------------------------------------
What is a custom resource?
It is an extension of the Kubernetes API that is not necessarily available in a default Kubernetes installation.
CRD Object can be either namespaced or cluster scoped.

What is kubernetes Resource:
A resource is object in kubernetes, like deployment,job,ReplicsSet etc.
How resource work:
Deployment(Resource run)-> ETCD(Database) Keep the Data-> Controller(Monitoring tools) Monitor the run as expectedly.

We not need to create controller for deployment, in builtin with kubernatis source code wirtten by Go langualge.


As it you can create your won Custome Resources in Kubernetes. For that you have to config Kubernetes API.
So, Where you defive apis, this is called CRD(Custome resource defination) file a yaml file with details of api 
for custome resource.

Now Create the CR by using the file and you able to create your resource.
But until now this resource do notheng its only create object and save info in ECTD no taken action.
For that you need a Controller ! the second part is this to watch and take action create Controller.



Custome Resouece Controller
--------------------------------------------------
Custome resource create doen and data in ETCD, Now we have to Monitor on data in ETCD and take action.
thats Why ed need a custome controller.
A Custome Controller is a any process what is contionuslly monitor the etcd to permorm action.
You can develop Custome controller by any langualge like: Go, Python etc.

It is better build a container and run int on kubernatis cluster and user it.



CKAD exam not have to build customer controller, May need to build a customer resource using defination file.
and working  with existing Custome resource Controller what is already may deploy.

Not so much possible to come in exam to create a custome resource, but it is possible. and better to know






// Operator Framework
--------------------------------------------------
Until now we see that create and CRD and Custome Controller together for working.
Its cand be package this two as Framework called: Operator Framework.Its more only CRD and CC.

Operator can do as human Operatior like: manage app install, uninstall etc.
All Operator in avalialbelin operation Hub with documentation. etcd,graffena,myusql etc.


CKAD exam not commming Operaton only may come CRD.



##Solution | Custom Resource
--------------------------------------------------
1)CRD Object can be either namespaced or cluster scoped.
Is this statement true or false?

ans: True
A CRD can be either namespaced or cluster-scoped. When you create a CRD, you can specify its scope in the spec.scope field of the CRD manifest.

What is a custom resource?
It is an extension of the Kubernetes API that is not necessarily available in a default Kubernetes installation.


2)We have provided an incomplete CRDs manifest file called crd.yaml under the /root directory. Let’s complete it and create a custom resource definition from it.
Let’s create a custom resource definition called internals.datasets.kodekloud.com. Assign the group to datasets.kodekloud.com and the resource is accessible only from a specific namespace.

Make sure the version should be v1 and needed to enable the version so it’s being served via REST API.
So finally create a custom resource from a given manifest file called custom.yaml.


Note :- Make sure resources should be created successfully from the custom.yaml file.

In-complate CRD:
cat crd.yaml 
---
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name:  
spec:
  group: 
  versions:
    - name: v2
      served: false
      storage: true
      schema:
        openAPIV3Schema:
          type: object
          properties:
            spec:
              type: object
              properties:
                internalLoad:
                  type: string
                range:
                  type: integer
                percentage:
                  type: string
  scope: 
  names:
    plural: internal
    singular: internal
    kind: Internal
    shortNames:
    - int

The solution file for crd.yaml is pasted below:

---
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: internals.datasets.kodekloud.com 
spec:
  group: datasets.kodekloud.com
  versions:
    - name: v1
      served: true
      storage: true
      schema:
        openAPIV3Schema:
          type: object
          properties:
            spec:
              type: object
              properties:
                internalLoad:
                  type: string
                range:
                  type: integer
                percentage:
                  type: string
  scope: Namespaced 
  names:
    plural: internals
    singular: internal
    kind: Internal
    shortNames:
    - int



=>kubectl create -f custom.yaml 
after correcting and creating CRD.


3)What are the properties given to the CRD’s called collectors.monitoring.controller?
=>kubectl describe crd collectors.monitoring.controller

Run the commad: kubectl describe crd collectors.monitoring.controller and inspect the given CRD.



4)Create a custom resource called datacenter and the apiVersion should be traffic.controller/v1.
Set the dataField length to 2 and access permission should be true.
To create a custom resource called datacenter :-


apiVersion: traffic.controller/v1
kind: Global
metadata:
  name: datacenter
spec:
  dataField: 2
  access: true



5)What is the short name given to the CRD globals.traffic.controller ?
Inspect the CRD globals.traffic.controller and identify the short names.
gb
=>kubectl describe crd globals.traffic.controller






##Deployment strategies
--------------------------------------------------
Currently Only RollingUpdate and Recreate strategies can be define on deployment file.
But thereis other deployment strategies aveilable: 
 - BlueGreen
   In this case all new service deploy and at a time switch traffic to news services. 

So, How to deploy blue green:
Suppose current running service run with tag blue(in pod template label), now Update Service new version app with green tag.
Now update the tage of deploy ment blue to green and all traffic go to greent pods.

 - Canary
   In this case Deploy all new version app and a few traffic flow to new app. 
   If every thisng ok then go for full traffic.

So, how to deploy Canary:
As Current deployment and service running with a label, and new version deploy with to a dirrent label.
Now add another common label on both deployment as result both version reecive the traffic.
To provide a few traffice in new version reduce the number of pod on new app version deployment like pod 1.

in kunbernetes Traffice to deployment % are the number of pod.
By using ServcieMess we can route traffice as required to target deployment.




##164. Solution8: Deployment strategies
--------------------------------------------------

1)A deployment has been created in the default namespace. What is the deployment strategy used for this deployment?
Run: kubectl describe deployments.apps frontend and inspect the StrategyType
=>kubectl describe deployment frontend

2)The deployment called frontend app is exposed on the NodePort via a service.
Identify the name of this service.
Inspect the services created in the default namespace.

=>kubectl describe deployment frontend
=>k get svc
=>k describe svc frontend-service 


3)A new deployment called frontend-v2 has been created in the default namespace using the image kodekloud/webapp-color:v2. This deployment will be used to test a newer version of the same app.
Configure the deployment in such a way that the service called frontend-service routes less than 20% of traffic to the new deployment.
Do not increase the replicas of the frontend deployment.


The frontend-v2 deployment currently has 2 replicas. The frontend service now routes traffic to 7 pods in total ( 5 replicas on the frontend deployment and 2 replicas from frontend-v2 deployment).
Since the service distributes traffic to all pods equally, in this case, approximately 29% of the traffic will go to frontend-v2 deployment.
To reduce this below 20%, scale down the pods on the v2 version to the minimum possible replicas = 1.
Run: kubectl scale deployment --replicas=1 frontend-v2
Once this is done, only ~17% of traffic should go to the v2 version.


=>kubectl get deployment
=>kubectl describe service frontend-service
=>kubectl describe deployment frontend-v2

=>kubectl scale deployment --replicas=1 frontend-v2

We have now established that the new version v2 of the application is working as expected.
We can now safely redirect all users to the v2 version.


4)Scale down the v1 version of the apps to 0 replicas and scale up the new(v2) version to 5 replicas.
=>kubectl scale deployment --replicas=5 frontend-v2
=>kubectl scale deployment --replicas=0 frontend

=>kubectl delete deployment frontend
You can now reload the Webapp tab to validate that all user traffic now uses the v2 version of the app.






##169.  Install Helm
--------------------------------------------------
https://helm.sh/docs/intro/install/#from-apt-debianubuntu

For Helm Install you need a fuctional kubernetes Cluster and with kubectl tools with propepr config/access.



Helm call package manger, In kunbernetes we bild our app with different type deployment file.
In Helm all make together build the app. Name of the package in know what and how many package need to deploy.

To Install the helm, Need a complate kubernetes cluster and run:

=>sudo snap install helm --classic
Or 
Useing package pkg Manager
Or may download package file using Curl




##Solution - | Install Helm
--------------------------------------------------
1)Identify the name of the Operating system installed.
=>cat /etc/*release

2)Install the helm package.

curl https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg > /dev/null
sudo apt-get install apt-transport-https --yes
echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list
sudo apt-get update
sudo apt-get install helm

=>helm version --short

3)Use the help page of helm command to identify the command used to retrieve helm client environment information.
=>helm -h
=>helm env


4)What is a command line flag used to enable verbose output?
 helm -h | grep verbose
=>helm --debug




##Helm Concept
--------------------------------------------------

ObjectYamlFile=>| ObjectTemplateYamlFile+valueYamlFile=>| HelmChart
It has Chart.yaml file for this chart

=>helm search hub wordpress

=>helm list
=>helm uninstall my-release

CKAD: This is all need on Exam.



##Solution  | Helm Concept
--------------------------------------------------
1)Which command is used to search for a wordpress helm chart package from the Artifact Hub?
=>helm search hub wordpress

2)Add a bitnami helm chart repository in the controlplane node.
name - bitnami
chart repo name - https://charts.bitnami.com/bitnami

=>helm repo add bitnami https://charts.bitnami.com/bitnami


3)Which command is used to search for the joomla package from the added repository?
=>helm search repo joomla


4)What is the app version of joomla in the bitnami helm repository?
=>helm search repo joomla
look at the APP VERSION



5)How many helm repositories are added in the controlplane node?
=>helm repo list


6)Install drupal helm chart from the bitnami repository.
Release name should be bravo.
Chart name should be bitnami/drupal.
Note: Ignore the state of the application now.

=>helm install bravo bitnami/drupal
=>helm list

7)Uninstall the drupal helm package which we installed earlier.
=>helm uninstall bravo


8)Download the bitnami apache package under the /root directory.
Note: Do not install the package. Just download it.
=>helm pull --untar bitnami/apache



9)Inspect the file values.yaml and make changes so that 2 replicas of the webserver are running and the http is exposed on nodeport 30080.
Note: You can read the Bitnami documentation for more.
https://github.com/bitnami/charts/tree/master/bitnami/apache/#installing-the-chart



10)Install the apache from the downloaded helm package.
Release name: mywebapp
Note: Do make changes accordingly so that 2 replicas of the webserver are running and the http is exposed on nodeport 30080.
Make sure that the pods are in the running state.

Edit the values.yaml file and change the parameters to replicaCount: 2 and http: 30080 under the nodePorts section. Then run helm install command to install the package.

  nodePorts:
    replicaCount: 2
    http: "30080"
    https: ""

Once you have modified the values.yaml file , run the below command to install the apache package on the controlplane node:
=>helm install mywebapp ./apache

After installation, run the below command to list the mywebapp release:
=>helm list 











##Kubernetes Challenges
==================================================
https://kodekloud.com/lessons/challenge-1-3/























@@@
ZonVovg
=================================================
#D1:CoreConcept               
================================================= 
pod1:
apiVersion: v1
kind: Pod
metadata:
  name: my-webserver-pod
spec:
  containers:
    - name: mynginx
      image: nginx

pod2:
apiVersion: v1
kind: Pod
metadata:
  name: my-pod1
spec:
  containers:
    - name: mycont1
      image: busybox
      command: ["sleep","3360"]
   or args: ["3600"]
   or args: ["sleep","3360"]

pod3:
apiVersion: v1
kind: Pod
metadata:
  name: busybox
spec:
  containers:
  - name: busybox
    image: busybox
    command: ["/bin/sh"]

pod4:
apiVersion: v1 
kind: Pod 
metadata: 
  name: bu1 
spec: 
  containers: 
  - name: busybox 
    image: busybox 
    command: ["sh","-ec","ping google.com"] 

pod5:
apiVersion: v1
kind: Pod
metadata:
  name: ubuntu
spec:
  containers:
  - name: ubuntu
    image: ubuntu:latest
    # Just spin & wait forever
    command: [ "/bin/bash", "-c", "--" ]
    args: [ "while true; do sleep 30; done;"]


pod6:
kind: Pod
metadata:
  name: nginx-ports
spec:
  containers:
  - image: nginx
    name: nginx-ports
    ports:
    - containerPort: 80


=>kubectl apply -f myfile.yml
=>kubectl get pods
=>kubectl exec -it my-pod1 sh


#PracticeDomain1:
-----------------------------------------------------
Q1:
apiVersion: v1
kind: Pod
metadata:
  name: kplabs-nginx
spec:
  containers:
    - name: mycontainer
      image: nginx

Q2:
apiVersion: v1
kind: Pod
metadata:
  name: kplabs-cmdargs
spec:
  containers:
    - name: cmdcontainer
      image: busybox
      command: ["sleep"]
      args: ["3500"]

Q3:
apiVersion: v1
kind: Pod
metadata:
  name: kplabs-pods
spec:
  containers:
    - name: nginx
      image: nginx
      ports: 
      - containerPort: 80


Q4:
apiVersion: v1
kind: Pod
metadata:
  name: log-ng
spec:
  containers:
    - name: log-ngcont
      image: nginx
      args:
       - /bin/sh
       - -c
       - >
         i=0;
         while true;
         do
          echo "$i: $(date)" >> /var/log/1.log;
          echo "$(date) INFO $i" >> /var/log/2.log;
          i=$((i+1));
          sleep 1;
         done


=================================================
#D2:PodDesign               
================================================= 

#Label and Selectors:
-------------------------------------------------
=>kubectl label --help

=>kubectl run pod1 --image=nginx
=>kubectl get pods --show-labels
Show Labels

=>kubectl label pod pod1 env=prod
=>kubectl label  pods --all status=runing
Add Label

=>kubectl get pods -l env=prod
Filter by label

=>kubectl label pod pod2 env-
Delete label



#ReplicaSet:
-------------------------------------------------
=>kubectl create deployment myreplica --image=nginx --replicas 3 --dry-run=client -o yaml
Generate a deployment file and Update for ReplicaSet

=>kubectl get rs
=>kubectl get replicaset
=>kubectl apply -f rpset.yaml

=>kubectl describe replicaset myrpset

=>kubectl get pods --show-labels
=>kubectl delete rs myrpset
If a pod got sutdown then replicatSet re create it


repliSet1:
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: myrpset
spec:
  replicas: 3
  selector:
    matchLabels:
      tier: myrpset
  template:
    metadata:
      labels:
        tier: myrpset
    spec:
      containers:
      - name: myngcont
        image: nginx



#Deployment:
-------------------------------------------------
=>kubectl create deployment --help

deployment1:
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mydeployment
spec:
  replicas: 3
  selector:
    matchLabels:
      tier: myproj
  template:
    metadata:
      labels:
        tier: myproj
    spec:
      containers:
      - name: myng
        image: nginx


=>kubectl apply -f mydeploy.yaml
=>kubectl create deployment mydeployment --image=nginx --replicas 3
=>kubectl get deployment
=>kubectl describe deployment mydeployment

=>kubectl create deployment mydeployment --image=nginx --replicas 3 --dry-run=client -o yaml
Generate deployment yaml file

=>kubectl delete deployment mydeployment
Delete deployment


Edit yaml file for Update the deployment, like change the image version
then check the rollout history
=>kubectl rollout --help

=>kubectl rollout history deployment.v1.apps/mydeployment
=>kubectl rollout history deployment.v1.apps/mydeployment --revision 2
Get Rollout history and check revision details

=>kubectl get deployment my-dep -o yaml
=>kubectl describe deployment mydeployment
Check which revision version currently runing 

=>kubedtl rollout undo --help

=>kubectl rollout history deployment/dep1 --revision=3
Check rollout history data

=>kubectl rollout undo deployment/dep1 --to-revision=2

=>kubectl rollout history deployment.v1.apps/mydeployment
=>kubectl rollout undo deployment.v1.apps/mydeployment
=>kubectl rollout undo deployment.v1.apps/mydeployment --to-revision=2
=>kubectl describe deploy mydeployment
Deployment rolback



Form axSurge and maxUnavailable:
=>kubectl get deployment my-dep -o yaml
Check masSurge and maxUnavailable
=>kubectl create deployment mydeploy --images=nginx --replica 3

maxSurge: The number of pods that can be created above the desired amount of pods.
maxUnavailable: The number of pods that can be unavailable during the update process.


=>kubectl set --help
=>kubectl set image --help
=>kubectl set image deployment mydeploy nginx=httpd
Update  deployemnt


=>kubectl get deploy dep1 -o yaml>dep2.yaml
Then update the file of dep2.yaml 
OR
=>kubectl edit deployment mydeploy
=>kubectl get pods
Edit deoloyement



=>kubectl set image deployment mydeploy nginx=httpd
=>kubectl set image deployment mydeploy nginx=httpd --record
=>kubectl rollout history deployment mydeploy
=>kubectl rollout undo deployment/mydeploy
Roolout/Undo the deployemnt last deployment

=>kubectl scale deployment --help
=>kubectl scale deployment mydeploy --replicas 1
Scele up/down deployemnt


#InP
------------------------------------------------
1)How to set a new Image to deployment as part of rolling Update
2)Need to know --record Instruction
3)You should know how to rollback a deployment
4)you should be able to scale the deployment






#Batch Job:
-------------------------------------------------
Job are two type:
1)jobs (Run to completion)
2)CronJob

=>kubectl get jobs
=>kubectl create job --help
=>kubectl create job my-job --image=busybox --dry-run=client -o yaml


simpldJob:
apiVersion: batch/v1
kind: Job
metadata: 
  name: myjob
spec:
  template:
    spec:
      containers:
        - name: myjoncont
          image: busybox
          command: ["/bin/sh"]
          args: ["-c","echo Hello Imran"]
      restartPolicy: Never


=>kubectl apply -f myjob.yml
=>kubectl logs myjob-pod 
After complate the job task pod are not exit, stay with complate Status.

=>kubectl get jobs
=>kubectl delete job myjob
When delete the job all related pos will be delete



=>kubectl create cronjob --help
=>kubectl create cronjob my-job --image=busybox --schedule="*/1 * * * *"  --dry-run=client -o yaml

cronJob:
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: mycronjob
spec:
  schedule: "*/1 * * * *"
  jonTemplate:
    spec:
      template:
        spec:
          containers:
            - name: mycronjob-pod
              image: busybox
              args: 
              - /bin/sh
              - -c
              - date; echo Hello Imran, This is from CronJob.
          restartPolicy: OnFailure


=>kubectl get cronjob
=>kubectl get job
=>kubectl get job -w

=>kubectl describe cronjob cron1
=>kubectl delete cronjob mycronjob



#PracticeDomain02:
-----------------------------------------------------
Question 1: Labels
Create a pod named kplabs-label. The pod should be launched from nginx image. The name of container should be nginx-container. Attach following label to the pod.
env=production
app=webserver

Solution:
=>kubectl run --help
=>kubectl run kplabs-label --image=nginx --labels="app=webservr,env=production" --dry-run -o yaml>pod1.yaml

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    env: production
    app: webserver
  name: kplabs-label
spec:
  containers:
  - image: nginx
    name: kplabs-container



Question 2: Deployments
Create a deployment named kplabs-deployment. The deployment should be launched from nginx image. The deployment should have three replicas. The selector should be based on the label of app=nginx

=>kubectl create deployment --help
=>kubectl create deployment kplabs --image=nginx --replicas=3 --dry-run=client -o yaml>dep2.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: kplabs-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - image: nginx
        name: nginx


Question 3: Deployments - Rolling Updates and Rollbacks
Create a deployment named kplabs-updates. The deployment should be launched from nginx image. There should be two  replicas. Verify the status of the deployment. As part of rolling update, update the image to nginx2:alpine. Verify the status of deployment. Perform a rollback to the previous version. Verify the status of deployment.

apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-deploy
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - image: nginx
        name: nginx-cont




=>kubectl apply -f mydeployment.yaml
=>kubectl set image deployment my-deploy nginx-cont=nginx2:alpine --record=true
=>kubectl get pods
  If show ImagePullBackOff then its deployment update failed

=>kubectl rollout undo deployment/my-deploy
Undo deploument and rollback


Question 4: Labels and Selectors
Create a deployment named kplabs-selector. The pods should be launched from nginx image.The pods should only be launched in a node which has a label of disk=ssd. Observe the status of deployment. Add the appropriate label to the worker node and then observe the status of the deployment.

=>kubectl create deployment --help
=>kubectl create deployment my-dep --image=nginx --replicas=3  --dry-run=client -o yaml>dep4.yaml

Edit yaml file and add this under spec as containers
      nodeSelector:
        disktype: ssd

apiVersion: apps/v1
kind: Deployment
metadata:
  name: node-select-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx-node-select
  template:
    metadata:
      labels:
        app: nginx-node-select
    spec:
      containers:
      - image: nginx
        name: node-select-cont
      nodeSelector:
        disktype: ssd

=>kubectl get pods
Pod will show panding status becouse disktype: ssd not found on node
Now need to add this on node 

=>kubectl get node
=>kubectl get node --show-labels
=>kubectl label nodes node01 disktype=ssd
=>kubectl get deployment


Question 5:  CronJob
Create a job named kplabs-job. The job should run every minute and should print out the current date.

=>kubectl create cronjob --help
=>kubectl create cronjob kplabs-job --image=busybox --schedule="*/1 * * * *" --dry-run=client -o yaml>cron5.yaml

apiVersion: batch/v1
kind: CronJob
metadata:
  name: mycronjob
spec:
  schedule: "*/1 * * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: mycroncont
            image: busybox
            args:
            - /bin/sh
            - -c
            - date
          restartPolicy: OnFailure

=>kubectl get pods
=>kubectl logs mycronjob

=>kubectl get cronjob
=>kubectl get job
=>kubectl get job -w
=>kubectl delete cronjob mycronjob


Question 6:  CronJob
Create a job named kplabs-cron. The job should run every minute and should run following command "curl kplabs.in/ping". Terminate the container within 10 seconds if it does not run.

apiVersion: batch/v1
kind: CronJob
metadata:
  name: mycron2
spec:
  schedule: "*/1 * * * *"
  jobTemplate:
    spec:
      activeDeadlineSeconds: 15
      template:
        spec:
          containers:
          - name: hello
            image: busybox
            command: ["curl",  "hello.in/ping"]
          restartPolicy: OnFailure


Question 7:  Deployment Configuration
Create a deployment named kplabs-configuration. The deployment should have 3 replicas of nginx image. Once the deployment is created, verify the maxSurge and maxUnavailable parameters. Edit the the maxUnavailable to 0 and maxSurge to 30% on the live deployment object. Once those two parameters are modified, change the image of the deployment to nginx:alpine. Make sure to use the record instruction on rolling updates.

apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    run: mydeploy-label
  name: mydeploy
spec:
  replicas: 3
  selector:
    matchLabels:
      run: mydeploy-slt
  template:
    metadata:
      labels:
        run: mydeploy-slt
    spec:
      containers:
      - image: nginx
        name: mydeploy-cont

=>kubectl rollout history deployment mydeploy
=>kubectl rollout undo deployment/mydeploy
Roolout/Undo the deployemnt last deployment

=>kubectl set image --help
=>kubectl set image deployment mydeploy mydeploy-cont=httpd
=>kubectl set image deployment mydeploy mydeploy-cont=nginx --record









=================================================
#D3:Service and Networking               
================================================= 

Services act as a Gateway of variable amount pods in different node.

ServiceType:
  NodePort
  ClusterIP
  LoadBalancer
  ExternalName


serviceExample1:
Step 1: Creating Backend and Frontend PODS
=>kubectl run bkpod1 --image=nginx
=>kubectl run bkpod1 --image=nginx
=>kubectl run fndpod --image=ubuntu --command -- sleep 1h
=>echo -e "Welcome to nginx! \nHost Name: $(hostname -f)\nHost IP: $(hostname -i)">/usr/share/nginx/html/index.html
=>kubectl exec mypod1 -- ls /usr/share/nginx/html

Step 2: Test the Connection between Frontend and Backend PODs
=>kubectl get pods -o wide
=>kubectl exec -it fndpod -- bash
=>apt-get update && apt-get -y install curl
Curl to backend IP



Step 3: Create a new Service (clusterIp by default)
vi myservice.yaml

=>kubectl create service --help
=>kubectl create service clusterip --help
=>kubectl create service clusterip svc1 --tcp=80:80 --dry-run=client -o yaml

apiVersion: v1
kind: Service
metadata:
   name: app-service
spec:
   ports:
   - port: 8181
     targetPort: 80

=>kubectl apply -f myservice.yaml
=>kubectl get service
=>kubectl describe service app-service

Step 4: Associate Endpoints with Service
vi myendpoint.yaml
apiVersion: v1
kind: Endpoints
metadata:
  name: app-service
subsets:
  - addresses:
      - ip: 10.244.1.3
      - ip: 10.244.1.2
    ports:
      - port: 80

=>kubectl apply -f myendpoint.yaml



Step 5: Test the Connection
=>kubectl exec -it fndpod -- bash
curl to service IP with port


Create Service with Selector:
-------------------------------------------------

Step 1: Creating Service
apiVersion: v1
kind: Service
metadata:
   name: sltsvc1
spec:
   selector:
     env: webapp
   ports:
   - port: 80
     targetPort: 80

Step 2: Creating Deployments

=>kubectl get node --show-labels
=>kubectl label nodes node01 disktype=ssd

apiVersion: apps/v1
kind: Deployment
metadata:
  name: mydep
  labels:
    env: mydep-lb
spec:
  replicas: 5
  selector:
    matchLabels:
      env: webapp
  template:
    metadata:
      labels:
        env: webapp
    spec:
      containers:
      - name: mydep-cont
        image: imranmadbar/nginx
        ports:
        - containerPort: 80
      nodeSelector:
        disktype: ssd

=>kubectl apply -f mydeployment.yaml
=>kubectl get pods --show-labels


=>kubectl apply -f myselector-service.yaml
=>kubectl describe service myselector-service

=>kubectl scale deployment/mydeployment --replicas=5
=>kubectl describe service myselector-service

=>kubectl run manual-added-pod --image=nginx
=>kubectl label pods manual-added-pod  env=backend-service

=>kubectl describe service myselector-service
=>kubectl describe endpoints kplabs-service-selector

=>kubectl get endpoints
=>kubectl describe endpoints myselector-service




#NodePort Service
-------------------------------------------------
=>kubectl create service nodeport --help
Step 1: Create Sample POD with Label
=>kubectl run nppod --labels="type=publicpod" --image=nginx
=>kubectl get pods --show-labels

Step 2: Create NodePort service
apiVersion: v1
kind: Service
metadata:
   name: npsvc
spec:
   selector:
     type: publicpod
   type: NodePort
   ports:
   - port: 80
     targetPort: 80

=>kubectl apply -f mynodeport.yaml
=>kubectl get service

Step 3: Fetch the Worker Node Public IP

=>kubectl get nodes -o wide
=>curl 192.2.145.12:32613
Copy the Public IP of Worker Node and Paste it in browser along with NodePort


=>kubectl delete pod nppod
=>kubectl delete -f mynodeport.yaml



#LoadBalancer Service
-------------------------------------------------
Step 1: Create Sample POD with Label
=>kubectl run lb-pod --labels="type=loadbalanced" --image=nginx
=>kubectl get pods --show-labels

Step 2: Create LoadBalancer service
apiVersion: v1
kind: Service
metadata:
  name: elb-service
spec:
  type: LoadBalancer
  ports:
  - port: 80
    protocol: TCP
  selector:
    type: loadbalanced
    
=>kubectl apply -f elb-service.yaml

Step 3: Verify Service Logs
=>kubectl describe service elb-service

=>kubectl delete pod lb-pod
=>kubectl delete -f elb-service.yaml



#Service generated by CLI
-------------------------------------------------
=>kubectl run mynginx --image=nginx
=>kubectl expose pod mynginx --name nginx-service --port=80 --target-port=80 --dry-run=client -o yaml
=>kubectl expose pod mynginx --name nginx-service --port=80 --target-port=80 --dry-run=client -o yaml > service2.yaml

=>kubectl expose pod mynginx --name nginx-nodeport-service --port=80 --target-port=80 --type=NodePort --dry-run=client -o yaml
=>kubectl get service
=>kubectl expose deployment mydeployment --name nginx-deployment-service --port=80 --target-port=8000
=>kubectl describe service nginx-deployment-service




#Name Space
-------------------------------------------------
=>kubectl get namespace
=>kubectl get pod --namespace kube-system
=>kubectl create namespace prod-namespace

=>kubectl run prod-ng --image=nginx --namespace prod-namespace



#Service Account
-------------------------------------------------
=>cat /run/secrets/kubernetes.io/serviceaccount/token
Pod token

=>kubectl get sa
=>kubectl get serviceaccount
=>kubectl get serviceaccount -n my-mynmspc
=>kubectl get secret -n my-mynmspc

=>kubectl get pod -o yaml
Chace current pod service account

=>kubectl get secrets -n prod-mynmspc 
=>kubectl get sa default -o yaml

=>kubectl get pod myng -o yaml

=>kubectl create sa my-saacc
=>kubectl get secret
=>kubectl run myng-sa --image=nginx --serviceaccount="mysaacc"
=>kubectl get pod
=>kubectl get pod myng-sa -o yaml

=>kubectl get pod deployment-sa-8585d89b57-46rtb -o yaml


#Network Security Policies
-------------------------------------------------

=>kubectl run mybx1 --image=busybox --sh
=>kubectl run mybx2 --image=busybox --sh
Run two Pod and ping each other


policy1:
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: test-network-policy
  namespace: default
spec:
  podSelector:
    matchLabels:
      run: mybx2
  policyTypes:
    - Ingress

=>kubectl apply -f my-net-poilicy





#PracticeDomain3:
-----------------------------------------------------

Q1: Create a deployment named kplabs-service. 
The deployment should have three replicas and the image should be based on nginx. 
Create a service based on NodePort. 
The service port should be 8080. The website should be accessible from port 32001 from all hosts.

=>kubectl create deployment kplabs-service --image=nginx --replicas=1 --dry-run=client -o yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: kplabs-service
spec:
  replicas: 3
  selector:
    matchLabels:
      app: kplabs-service
  template:
    metadata:
      labels:
        app: kplabs-service
    spec:
      containers:
      - image: nginx
        name: nginx


=>kubectl create service nodeport --help
=>kubectl create service nodeport my-ns --tcp=32001:8080 --node-port=32001 --dry-run=client -o yaml

apiVersion: v1
kind: Service
metadata:
  name: myservice
  labels:
    run: myservice
spec:
  type: NodePort
  ports:
  - port: 8080
    targetPort: 80
    nodePort: 32001
    protocol: TCP
  selector:
    run: kplabs-service

SSH to node and curl with nodeport

=>curl serviceIP:8080
=>curl nodeIP:32001



Q2: Rung this services and access nginx from service IP, this is with port mapping issue.
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    run: kplabs-service
  name: kplabs-fix
spec:
  replicas: 2
  selector:
    matchLabels:
      run: kplabs-fix
  template:
    metadata:
      labels:
        run: kplabs-fix
    spec:
      containers:
      - image: nginx
        name: kplabs-service
---
apiVersion: v1
kind: Service
metadata:
  name: fix-service
  labels:
    run: fix-service
spec:
  ports:
  - port: 8080
    targetPort: 80
    protocol: TCP
  selector:
    run: kplabs-fix

Question 3: Namespace
Create a pod named redis-pod . The pod should be part of the namespace my-namespace.
The pod should make use of redis image. Expose port 6379.

=>kubectl create namespace --help
=>kubectl create namespace my-namespace

=>kubectl run pod --help
=>kubectl run kplabs-namespace --image=redise --help
=>kubectl run redis-pod --image=redis --port=6379 -n my-namespace



Question 4: Service Account
Create a new service account named kplabs. Launch a new pod named kplabs-sa from nginx image. 
The pod should be launched from the kplabs service account. Verify whether the token has been mounted inside the pod.

=>kubectl create serviceaccount myseracc
=>kubectl get sa
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: kplabns-sa
  name: kplabns-sa
spec:
  containers:
  - image: nginx
    name: kplabns-sa
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
  serviceAccountName: kplabs
status: {}


Question 5: Deployments and Service Account
Create a deployment named deployment-sa. The deployment should have 2 replicas of nginx image. 
After the deployment has been created, check the service account associated with the pods. 
Modify the deployment so that all pods shall use the service account of kplabs.

=>kubectl create deployment --help
=>kubectl create deployment deployment-sa  --image=nginx --replicas=3
=>kubectl edit deployment deployment-sa

Add this two property below  securityContext: {}
      serviceAccount: kplabs
      serviceAccountName: kplabs

=>kubectl get pod deployment-sa-75dd877cbf-ddjjd -o yaml
Check updated service acc



Q6:
=>kubectl apply -f troubleshoot-deployment.yaml

apiVersion: v1
kind: Namespace
metadata:
  name: newkplabs
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    run: troubleshoot-deployment
  name: troubleshoot-deployment
  namespace: newkplabs
spec:
  replicas: 2
  selector:
    matchLabels:
      run: troubleshoot-deployment
  template:
    metadata:
      labels:
        run: troubleshoot-deployment
    spec:
      containers:
      - image: ninx
        name: troubleshoot-deployment

=>kubectl get deployment --all-namespaces
=>kubectl get pod -n newkplabs
=>kubectl edit deployment troubleshoot-deployment  -n newkplabs
Fixed the image name ninx to nginx



================================================= 
Section 5: Domain 4 - Configuration
================================================= 
Three Secret type:
  1)Generic
    File, directory, literal value
  2)Docker Registry
  3)TLS


=>kubectl get secret

GenericType:
=>kubectl create secret generic --help
=>kubectl create secret generic mysec --from-literal=dbpass=dbpass12345
=>kubectl create secret generic mysec1 --from-literal=key1=supersecret --from-literal=key2=topsecret
=>kubectl describe secret mysec

=>kubectl get secret mysec -o yaml
=>echo ZGJwYXNzMTIzNDU= | base64 -d

apiVersion: v1
kind: Secret
metadata:
  name: mysec2
type: Opaque
data:
  #key1: root
  #key2: 54321
  key1: cm9vdAo=
  key2: NTQzMjEK


FileType:
=>echo key1=supersec>secFile.txt
=>kubectl create secret generic mysec3 --from-file=./secFile.txt
=>kubectl get secret
=>kubectl describe secret mysec3


apiVersion: v1
kind: Secret
metadata:
  name: mysec4
type: Opaque
stringData:
  config.yaml: | 
    username: root
    password: 54321

=>kubectl get secret -o yaml


MountingSecret:
apiVersion: v1
kind: Pod
metadata:
  name: myngpod
spec:
  containers:
  - name: myngpod
    image: nginx
    volumeMounts:
      - name: mymount
        mountPath: "/etc/secmount"
        readOnly: true
  volumes: 
  - name: mymount
    secret:
      secretName: mysec1

=>kubectl apply -f pod1.yaml
=>kubectl get pod
=>kubectl exec -it myngpod -- bash
=>cd /etc/mymount/


MountingEnvVariableSecret:
apiVersion: v1
kind: Pod
metadata:
  name: pod2
spec:
  containers:
  - name: myngpod-env
    image: nginx
    env:
      - name: SECRET_USERNAME
        valueFrom:
          secretKeyRef:
            name: mysec2
            key: key1

apiVersion: v1
kind: Pod
metadata:
  name: pod4
spec:
  containers:
  - name: myngpod-env
    image: nginx
    env:
      - name: SECRET_USERNAME
        valueFrom:
          secretKeyRef:
            name: mysec4
            key: key1

=>kubectl apply -f pod1.yaml
=>kubectl get pod
=>kubectl exec -it myngpod -- bash
=>cd /etc/mymount/


ResourceLimites:
apiVersion: v1
kind: Pod
metadata:
  name: pod2
spec:
  containers:
  - name: pod2-cont
    image: nginx
    resources:
      requests:
        memory: "64Mi"
        cpu: "250m"
      limits:
        memory: "128Mi"
        cpu: "500m"

=>kubectl get pod2 -o wide
=>kubectl describe node node01
=>kubectl delete -f pod2.yaml

apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
  - image: nginx
    name: my-cont
    resources:
      requests:
        memory: "64Mi"
      limits:
        memory: "128Mi"
        cpu: "1"




#Practice Test - Domain 4
-------------------------------------------------
Question 1: Resource Quotas
Create a pod named kplabs-quota. The pod should have following configuration:
a. Should run with nginx image.
b. It should use maximum of 512 MiB of memory.
c. It should use maximum of 2 core CPU.
d. The POD should require a minimum of 128 MiB of memory before it is 
scheduled.

apiVersion: v1
kind: Pod
metadata:
  name: kplabs-quota
spec:
  containers:
  - name: kplabs-quota-cont
    image: nginx
    resources:
      requests:
        memory: "128Mi"
      limits: 
        memory: "512Mi"
        cpu: "2"



Question 2: Secrets
Create a secret named kplabs-secret. The secret should have content where user=admin and pass=12345. Create a pod from the nginx image. Mount the secret as environment variables in the pod. The username should be available as DB_USER and password should be available as DB_PASSWORD inside the pod

=>kubectl create secret generic --help
=>kubectl create secret generic mysec1 --from-literal=user=admin --from-literal=pass=12345

apiVersion: v1
kind: Pod
metadata:
  name: pod3
spec:
  containers:
  - name: myngpod-env
    image: nginx
    env:
      - name: DB_USER
        valueFrom:
          secretKeyRef:
            name: mysec1
            key: user
      - name: DB_PASSWORD
        valueFrom:
          secretKeyRef:
            name: mysec1
            key: pass




================================================= 
Section 6: Domain 5 - Observability
================================================= 

Liveness:
-------------------------------------------------
livenessProbe three type:
  http, command, tcp
=>kubectl run -it ubuntu --image=ubuntu
=>kubectl exec -it ubuntu -- service nginx status
=>echo $?
=>kubectl exec -it ubunty -- bash
  =apt-get update && apt-get install nginx -y
  =service nginx start
  =service nginx status
  =echo $?

apiVersion: v1
kind: Pod
metadata:
  name: lnpod
spec:
  containers:
  - image: ubuntu
    name: lnpod
    tty: true
    livenessProbe:
      exec:
        command: 
        - service
        - nginx
        - status
      initialDelaySeconds: 10
      periodSeconds: 5

=>kubectl apply -f pod.yaml
=>kubectl exec -it lnpod -- service nginx status
=>kubectl get pods



Readiness:
-------------------------------------------------
apiVersion: v1
kind: Pod
metadata:
  name: rdpod
spec:
  containers:
  - image: ubuntu
    name: rdpod
    tty: true
    readinessProbe:
      exec:
        command: 
        - cat
        - /tmp/healthy
      initialDelaySeconds: 10
      periodSeconds: 5


=>kubectl apply -f pod2.yaml 
=>kubectl get pods
=>kubectl exec -it rdpod -- touch /tmp/healthy
=>kubectl exec -it rdpod -- rm /tmp/healthy




Appliation logs:
-------------------------------------------------
Fache multi container podlog:
apiVersion: v1
kind: Pod
metadata:
  name: mlpod
spec:
  containers:
  - image: busybox
    name: cont1
    command: ["ping"]
    args: ["google.com"]
  - image: busybox
    name: cont2
    command: ["ping"]
    args: ["8.8.8.8"]


=>kubectl logs mlpod cont2



Monitoring Components 
-------------------------------------------------
Install Metric server:
=>kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

=>kubectl top pods
=>kubectl top nodes

=>kubectl top pods --all-namespaces




#Event
-------------------------------------------------
=>kubectl get events -n kube-system
=>kubectl get events


#Field Selector
-------------------------------------------------
=>kubectl get pods --all-namespaces --field-selector metadata.namespace!=default
=>kubectl get events --field-selector involvedObject.name=mynginx
=>kubectl get pods --field-selector ""

=>kubectl create namespace myns
=>kubectl  get namespace
=>kubectl run mung --image=nginx -n myns
=>kubectl get pods --all-namespaces --field-selector metadata.namespace=myns



#Practice Test - Domain 5
---------------------------------------------------
Question 1: Probes
Create a POD from the nginx image. Pod should be named kplabs-probe. The pod should be created in such a way that if the application inside is not responding to HTTP requests made on port 8080, then Kubernetes should restart the POD.

apiVersion: v1
kind: Pod
metadata:
  name: kplabs-probe
spec:
  containers:
  - name: liveness
    image: nginx
    livenessProbe:
      httpGet:
        path: /
        port: 8080
      initialDelaySeconds: 3
      periodSeconds: 3


Question 2: Probes
Create a POD named newprobe. Pod should run from nginx image. The Pod should run with arguments defined below. Create a probe that checks if a file on that path /tmp/myfile exists. If it does not exist, the POD should be restarted.

    - /bin/sh
    - -c
    - touch /tmp/myfile; 3600

apiVersion: v1
kind: Pod
metadata:
  labels:
    test: liveness
  name: pod3
spec:
  containers:
  - name: liveness
    image: registry.k8s.io/busybox
    args:
    - /bin/sh
    - -c
    - touch /tmp/healthy; sleep 10; rm -f /tmp/healthy; sleep 600
    livenessProbe:
      exec:
        command:
        - cat
        - /tmp/healthy
      initialDelaySeconds: 2
      periodSeconds: 2



Question 4: Events
Store all the events associated with the pod liveness-http and store it to /tmp/podlog.txt. Make sure to use the kubectl command and do not modify anything in the output.

=>kubectl get events --field-selector involvedObject.name=liveness-http
=>echo | kubectl get events --field-selector involvedObject.name=newprobe > myoutput.txt




================================================= 
#Section 7: Domain 6 - State Persistence
================================================= 
There are many type volume: hostPath:
apiVersion: v1
kind: Pod
metadata:
  name: mypod-vol
spec:
  containers:
  - name: mypod-cont
    image: nginx
    volumeMounts:
    - mountPath: /data
      name: podvol
  volumes:
  - name: podvol
    hostPath:
      path: /mydata
      type: DirectoryOrCreate



apiVersion: v1
kind: Pod
metadata:
  name: mypod-vol
spec:
  containers:
  - name: mypod-cont
    image: nginx
    volumeMounts:
    - mountPath: /data
      name: podvol
  volumes:
  - name: podvol
    hostPath:
      path: /root/mydata
      type: DirectoryOrCreate



#PersistentVolume and PersistentVolumeClaim | pv
-------------------------------------------------
Persistent Volume:pv.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: my-pv
spec:
  storageClassName: manual
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: /tmp/data


=>kubectl apply -f pv.yaml 
=>kubectl get pv
This pv create in host node

PersistentVolumeClaim: pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-pvc
spec:
  storageClassName: manual
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi

=>kubectl apply -f pvc.yaml
=>kubectl get pvc

PVC pod: pod-pvc.yaml
apiVersion: v1
kind: Pod
metadata:
  name: pvcpod
spec:
  containers:
    - name: mypvccont
      image: nginx
      volumeMounts:
      - mountPath: "/data"
        name: my-volume
  volumes:
    - name: my-volume
      persistentVolumeClaim:
        claimName: my-pvc

=>kubectl apply -f pvc-pod.yaml
=>df -h
=>kubectl exec -it mycont -- bash



#ConfigMap
-----------------------------------------------
=>kubectl create configmap --help
=>kubectl get configmap
=>kubectl create configmap my-dev-config --from-literal=app.mem=1024m

=>kubectl get configmap my-config -o yaml


File Base configMap:
app.user=imran
app.pass=root
app.dburl=http://some-url.com
=>kubectl create configmap my-file-base-config --from-file=dev.properties
=>kubectl get configmap -o yaml

ConfigMap Mounted to Pod:
apiVersion: v1
kind: Pod
metadata:
  name: pod2
spec:
  containers:
    - name: conmap-cont
      image: nginx
      volumeMounts:
      - name: configmap-vol
        mountPath: "/data"
  volumes:
    - name: configmap-vol
      configMap: 
        name: my-configmap
  restartPolicy: Never

=>kubectl exec -it myconfigmap-pod2 -- bash
=>cd data
=>cat dev.properties




#Security Contex
-----------------------------------------------
Three Important Permission Aspects:
  runAsUser, runAsGroup, fsGroup

  =>kubectl run my-bubx --image=busybox -it sh

apiVersion: v1
kind: Pod
metadata:
  name: my-bubx2
spec:
  containers:
  - image: busybox
    name: my-bubx2
    command: ["sh", "-c", "sleep 1h"]
  securityContext: 
    runAsUser: 1000
    runAsGroup: 3000

=>kubectl apply -f pod3.yaml
=>kubectl exec my-bubx2 -it -- sh
=>cd /tmp && touch myfile.txt
=>ls -l

With fsGroup:
apiVersion: v1
kind: Pod
metadata:
  name: my-bubx
spec:
  containers:
  - image: busybox
    name: my-bubx-cont
    command: ["sh", "-c", "sleep 1h"]
    volumeMounts:
    - name: my-sec-ctx-vol
      mountPath: /mydata
  volumes:
    - name: my-sec-ctx-vol
      emptyDir: {}
  securityContext: 
    runAsUser: 1000
    runAsGroup: 3000
    fsGroup: 4000




#Practice Test - Domain 6
-----------------------------------------------
Question 1 - ConfigMap
  Create a configmap named kplabs-config which contains all the contents of that file.
  course: kubernetes 2020
  instructor: zeal
  type: certification

Mount the configmap to a pod named configmap-pod based on nginx image in such a way that all contents are available at /etc/config/kplabs.config

Ans:
Create a properties file with this:
=>vi app.properties
=>kubectl create configmap my-file-base-config --from-file=app.config
=>kubectl get configmap

apiVersion: v1
kind: Pod
metadata:
  name: configmappod
spec:
  containers:
    - name: conmap-cont
      image: nginx
      volumeMounts:
      - name: myfconfigmap-vol
        mountPath: /etc/config
  volumes:
    - name: myfconfigmap-vol
      configMap: 
        name: app.config 
  restartPolicy: Never



Question 2 - PV and PVC
Create a persistent volume with the name kplabs-pv. The size should be 2Gi and hostpath should be /tmp/mydata. It should have access mode of ReadWriteOnce
Create a persistent volume claim that will make use of the PV created earlier.
Create a Pod named kplabs-pv-pod. The POD should have the volume mounted at /mydata directory.

Create pv: 
apiVersion: v1
kind: PersistentVolume
metadata:
  name: my-pv
spec:
  capacity:
    storage: 2Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/tmp/mydata"

Create a Persistent Volume Claim
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 2Gi

apiVersion: v1
kind: Pod
metadata:
  name: mypvcpod
spec:
    containers:
    - name: pod
      image: nginx
      volumeMounts:
      - mountPath: /mydata
        name: myvolume
    volumes:
    - name: myvolume
      persistentVolumeClaim:
        claimName: my-pvc

=>ssh node01
=>ls /tmp/mydata
=>cat /tmp/mydata/myFile.txt


Question 3 - Security Context
Create a POD named busybox-security. The pod should run a command sleep 3600.  The primary process in POD should run with UID of 1000 and GID of 2000 all newly created contents of volume should have the group ID of 3000.

apiVersion: v1
kind: Pod
metadata:
  name: sctpod
spec:
  securityContext:
    runAsUser: 1000
    runAsGroup: 2000
    fsGroup: 3000
  containers:
  - name: busybox-container
    image: busybox
    command: ["sleep","3600"]

=>kubectl exec -it sctpod -- sh
=>id



Question 4 - Secrets and Environment Variables
a. Create a secret name db-creds which has following data:
user: dbreadonly
pass: myDBPassword#%
b. Create a pod from nginx image. The pod should be named secret-pod
c. Mount the secret to the POD in such a way that the contents of the database user are available in the form of DB_USER environment variable and database password is available in the form of DB_PASSWORD environment variable inside the container.

Create a secret
=>kubectl create secret generic mydbsec --from-literal=user=dbreadonly --from-literal=pass=myDBPassword

apiVersion: v1
kind: Pod
metadata:
  name: mysecpod
spec:
  containers:
  - image: nginx
    name: mysec-cont
    env:
      - name: DB_USER
        valueFrom:
          secretKeyRef:
            name: mydbsec
            key: user
      - name: DB_PASSWORD
        valueFrom:
          secretKeyRef:
            name: mydbsec
            key: pass

=>kubectl exec -it mysecpod -- bash
=>env

Question 5 - Secrets and Volumes
a. Create a secret name app-creds that has the following data:
appuser: dbreadonly
apppass: myDBPassword
b. Create a pod based on nginx image with the name of app-pod
c. Mount the secret to the pod so that it is available in the path of /etc/secret


Create a secret
=>kubectl create secret generic mydbsec --from-literal=user=dbreadonly --from-literal=pass=myDBPassword

apiVersion: v1
kind: Pod
metadata:
  name: podsec2
spec:
  containers:
  - image: nginx
    name: app-pod
    volumeMounts:
    - name: myvolume
      mountPath: "/etc/secret"
      readOnly: true
  volumes:
  - name: myvolume
    secret:
      secretName: mydbsec







================================================= 
#Section 8: Domain 7 - Multi Continer
================================================= 

apiVersion: v1
kind: Pod
metadata:
  name: app-pod
spec:
  containers:
  - image: nginx
    name: cont1
  - image: busybox
    name: cont2
    command: 
     - sleep
     - "1h"

=>kubectl get pods
=>kubectl describe pods
=>kubectl exec -it app-pod -c cont2 -- sh

=>wget 10.244.1.2




Multi-Container POD Patterns
-----------------------------------------------

SideCare Pattern:


Ambassador Pattern( a type of sidecare pattern):



Adapter Pattern
---------------------------------------------------






#Practice Test - D7
-------------------------------------------------
apiVersion: v1
kind: Pod
metadata:
  name: pod1
spec:
  containers:
  - name: first-container
    image: nginx
  - name: second-container
    image: mykplabs/kubernetes:nginx
  - name: third-container
    image: busybox
    command: ["sleep", "3600"]



2) Ambassador Pattern:
Create a pod named kplabs-ambassador-pod from the legacy application image. 
The image is mykplabs/kubernetes:nginx
Create configmap called as kplabs-ambassador-config which has the following data:

    global
        daemon
        maxconn 256
 
    defaults
        mode http
        timeout connect 5000ms
        timeout client 50000ms
        timeout server 50000ms
 
    listen http-in
        bind *:80
        server server1 127.0.0.1:9080 maxconn 32


Create an ambassador container named haproxy-container from the image of haproxy:1.7
Expose the port 80 from Haproxy container.

Mount the configmap to the haproxy in such a way that HAProxy config is 
available at the following file:
/usr/local/etc/haproxy/haproxy.cfg



Create a Busybox pod from following pod definition:
apiVersion: v1
kind: Pod
metadata:
  name: kplabs-busybox-curl
spec:
  containers:
  - name: curl-container
    image: yauritux/busybox-curl
    command: ['sh', '-c', 'while true; do sleep 3600; done']

Verify if you can perform CURL from busybox pod towards the ambassador pod on port 80.

Solution:
----------------------------------------------

Create a file with config
=>kubectl create configmap kplabs-ambassador-config --from-file ./haproxy.cfg    


apiVersion: v1
kind: Pod
metadata:
  name: ambspod
spec:
  containers:
  - name: first-container
    image: mykplabs/kubernetes:nginx
  - name: haproxy-container
    image: haproxy:1.7
    ports:
       - containerPort: 80
    volumeMounts:
     - name: config-volume
       mountPath: /usr/local/etc/haproxy/haproxy.cfg
  volumes:
    - name: config-volume
      configMap:
        name: kplabs-ambassador-config

=>kubectl get pods -o wide       
=>kubectl run mybusybox -it --image=busybox -- sh
=>wget ip of ambspod


Question  : Adapter Pattern
Create a pod named kplabs-adapter-logging.
The Pod should have a container running from the busybox image with the following arguments:

    - /bin/sh
    - -c
    - >
      i=0;
      while true;
      do
        echo "$i: $(date)" >> /var/log/1.log;
        echo "$(date) INFO $i" >> /var/log/2.log;
        i=$((i+1));
        sleep 1;
      done


Create and mount a volume under the mount path of /var/log of the first container. The volume should be removed as soon as the pod is deleted.

Create a second container in the pod. It should be launched from the following image.

k8s.gcr.io/fluentd-gcp:1.30



The container should have an environment variable named FLUENTD_ARGS with following values:

-c /etc/fluentd-config/fluentd.conf



The second container should also have the same volume as the first container mounted on the path of /var/log



The second container should also have a fluentd configuration (mentioned in below configmap) available in the following path:
/etc/fluentd-config/fluentd.conf


Create a ConfigMap object with the name of fluentd-config. The ConfigMap should have the following configuration:
    <source>
      type tail
      format none
      path /var/log/1.log
      pos_file /var/log/1.log.pos
      tag PHP
    </source>
 
    <source>
      type tail
      format none
      path /var/log/2.log
      pos_file /var/log/2.log.pos
      tag JAVA
    </source>
 
    <match **>
       @type file
       path /var/log/fluent/access
    </match>


Verify if you can see log files with the tag of PHP and JAVA under the following directory
/var/log/fluent/access


Adapter Pattern Solution:
------------------------------------------------

apiVersion: v1
kind: ConfigMap
metadata:
  name: fluentd-config
data:
  fluentd.conf: |
    <source>
      type tail
      format none
      path /var/log/1.log
      pos_file /var/log/1.log.pos
      tag PHP
    </source>
    <source>
      type tail
      format none
      path /var/log/2.log
      pos_file /var/log/2.log.pos
      tag JAVA
    </source>
    <match **>
       @type file
       path /var/log/fluent/access
    </match>
---
apiVersion: v1
kind: Pod
metadata:
  name: adppod
spec:
  containers:
  - name: adppod-count
    image: busybox
    args:
    - /bin/sh
    - -c
    - >
      i=0;
      while true;
      do
        echo "$i: $(date)" >> /var/log/1.log;
        echo "$(date) INFO $i" >> /var/log/2.log;
        i=$((i+1));
        sleep 1;
      done
    volumeMounts:
    - name: varlog
      mountPath: /var/log
  - name: count-agent
    image: k8s.gcr.io/fluentd-gcp:1.30
    env:
    - name: FLUENTD_ARGS
      value: -c /etc/fluentd-config/fluentd.conf
    volumeMounts:
    - name: varlog
      mountPath: /var/log
    - name: config-volume
      mountPath: /etc/fluentd-config
  volumes:
  - name: varlog
    emptyDir: {}
  - name: config-volume
    configMap:
      name: fluentd-config


=>kubectl apply -f mypod.yaml
=>kubectl exec -it adppod -- sh

=>tail -f /var/log/1.log
=>tail -f /var/log/2.log
=>tail -f /var/log/fluent/access.20230408.b5f8ccf3468ae28ef





================================================= 
#New Update | Exam Preparation
================================================= 

#Deployment  Strategy:
-------------------------------------------------
=>kubectl explain deployment.spec.strategy
=>kubectl create deployment my-deployment --image=nginx --dry-run=client -o yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: my-deployment
  name: my-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: my-deployment
  strategy: 
    type: Recreate
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: my-deployment
    spec:
      containers:
      - image: nginx
        name: nginx
        resources: {}
status: {}


=>kubectl apply -f dpl1.yaml
=>kubectl get deployment
=>kubectl get pods
=>kubectl get rs

Now change deployment dpl1.yaml with a invalid image name 
Deploy again thne:

=>kubectl apply -f dpl1.yaml
Or
=>kubectl edit deployment my-deployment

=>kubectl get deployment
=>kubectl get pods
=>kubectl get rs

Now show all pod now sutdown, now container are running
this is of Recreate stategy.
Fixed the proper deployment config (image) then pod will be OutsideOfWork
Or
=>kubectl rollout undo deployment/my-deployment

Samelair process for RollingUpdate:

apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: my-deployment
  name: my-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: my-deployment
  strategy: 
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: my-deployment
    spec:
      containers:
      - image: nginx
        name: nginx
        resources: {}
status: {}



#Blue Green deployment:
------------------------------------------------------

=>kubectl run blue-pod --image=nginx
=>kubectl run green-pod --image=nginx

=>kubectl label pod blue-pod app=app-v1
=>kubectl label pod green-pod app=app-v2

=>kubectl get pods --show-labels

Create a service
=>kubectl create service nodeport my-service --tcp=80:80 --dry-run=client -o yaml
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec: 
  type: NodePort
  ports:
  - name: http
    port: 80
    targetPort: 80
  selector:
    app: app-v1

=>kubectl get svc
=>kubectl describe service my-service

Update service "selector" value app-v2
and deploy again, then check the end point

Or 
Can Edit directly 
=>kubectl edit svc



#Canary deployment:
------------------------------------------------------
1. Create Deployment Manifests:

kubectl create deployment deployment1 --image=nginx --replicas 3 --dry-run=client -o yaml
kubectl create deployment deployment2 --image=httpd --replicas 1 --dry-run=client -o yaml
2. Store these manifests in v1-canary.yaml and v2-canary.yaml

3. Add a common label of (deptype:canary ) to both of these manifest files for the pod template section
v1-canary.yaml:
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deployment1
spec:
  replicas: 3
  selector:
    matchLabels:
      app: app-nginx
  template:
    metadata:
      labels:
        app: app-nginx
        deptype: mycanary 
    spec:
      containers:
      - image: nginx
        name: nginx-cont
v1-canary.yaml:
Same as two only change replica and image
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deployment2
spec:
  replicas: 2
  selector:
    matchLabels:
      app: app-httpd
  template:
    metadata:
      labels:
        app: app-httpd
        deptype: mycanary 
    spec:
      containers:
      - image: httpd
        name: nginx-cont

kubectl apply -f deployment1.yaml
kubectl apply -f deployment2.yaml
5. Verify if PODS are created with appropriate labels:

kubectl get pods --show-labels

6. Create a Canary Service

canary-svc.yaml

apiVersion: v1
kind: Service
metadata:
 name: canary-svc 
spec:
 type: NodePort
 ports:
 - name: http
   port: 80
   targetPort: 80
 selector:
   deptype: mycanary
kubectl apply -f canary-svc.yaml

7. Verify if Service have 4 Endpoint IPs:

kubectl describe svc canary-deployment
8. Make CURL request to see if requests are distributed among v1 and v2 apps
curl PUBLIC-IP:NODEPORT


#Custom Resource
------------------------------------------
Kubernetes Documentation Referred for Base CRD manifests:

https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/

=>kubectl proxy --port=8080
=>curl 127.0.0.1:8080

Manifests File (Step 1 and Step 2)

crd.yaml

apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: crontabs.kplabs.internal
spec:
  group: kplabs.internal
  versions:
    - name: v1
      served: true
      storage: true
      schema:
        openAPIV3Schema:
          type: object
          properties:
            spec:
              type: object
              properties:
                cronSpec:
                  type: string
                image:
                  type: string
                replicas:
                  type: number
  scope: Namespaced
  names:
    plural: crontabs
    singular: crontab
    kind: CronTab
    shortNames:
    - ct

=>k apply -f crd.yaml 


crd-object.yaml

apiVersion: "kplabs.internal/v1"
kind: CronTab
metadata:
  name: my-new-cron-object
spec:
  cronSpec: "* * * * */5"
  image: my-awesome-cron-image
  replicas: 3

=>kubectl apply -f crdo.yaml 
=>kubectl get crontab



AWS Service Operator Manifest File:
https://raw.githubusercontent.com/awslabs/aws-service-operator/master/configs/aws-service-operator.yaml










#Authentication
----------------------------------------------
=>cat .kube/config
=>curl -k https://controlplane:6443

=>cd /etc/kubernetes/manifests/
=>ls
=>cat kube-apiserver.yaml 


=>curl -k https://controlplane:6443 --header "Authorization: 
Bearer ewrwfsdlkfjdlf="
Call with token

=>kubectl create serviceaccount myservicacc

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: ngpod
  name: ngpod
spec:
  serviceAccountName: myservicacc
  containers:
  - image: nginx
    name: ngpod
    resources: {}
  dnsPolicy: ClusterFirst


=>kubectl get pod -o yaml
=>k exec -it ngpod -- bash
=>cat /run/secrets/kubernetes.io/serviceaccount/token


=>curl -k https://controlplane:6443/api/v1 --header "Authorization: 
Bearer sfsfsf"






#Authorization
----------------------------------------------

Create a Role:
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: default
  name: pod-reader
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["list"]

=>kubectl apply -f myrole.yaml 

=>kubectl get role
=>kubectl describe role pod-reader


Role banding:

apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: read-pods
  namespace: default
subjects:
- kind: User
  name: system:serviceaccount:default:myadmin
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role 
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io


=>kubectl apply -f role-banding.yaml
=>kubectl get rolebinding
=>kubectl describe rolebinding read-pods



ClusterRole:
---------------------------------------------

Create a Cluster role:
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: pod-reader
rules:
- apiGroups: [""]
  resources: ["secrets"]
  verbs: ["get", "watch", "list"]


=>kubectl get clusterrole
=>kubectl describe clusterrole pod-role

Cluster Role banding:

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: pod-list-global
subjects:
- kind: User
  name: system:serviceaccount:default:myadmin
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io

  =>kubectl apply -f cls-role-banign.yaml
  =>kubectl get clusterrolbinding 
  =>kubectl describne clusterrolbinding pod-list-global



Cluster role to Role Banding:
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: read-pods
  namespace: default
subjects:
- kind: User
  name: system:serviceaccount:default:myadmin
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole 
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io





#Exam Preparation
----------------------------------------------------
=>kubectl api-resources
=>alias k=kubectl
Create alias for code
=>k get pods

=>k run mynginx --image=nginx --port=80 --dry-run=client -o yaml
Generate a POD  yaml file

=>k create deployment -h
How to crearte a deployment get Help
=>kubectl create deployment my-dep --image=nginx --replicas=3 --dry-run=client -o yaml



Domain1-Core concept:
1) Vary well to create pod
=>kubectl run mynginx --image=nginx --port=80 --restart=Never --dry-run -o yaml>newpod.yaml
On this command, It will creatre a pod if not mentation (--restart=Never) it will create a deployemnt
2) Have to very femilary with tis cli command
3) Can not modify all config/expect in live pod


=>kubectl eidt pod mybusybox
  add or update label on the pod is ok.
But all Properties are not able to update on live pod. It will a Forbidden error

For update any properties flow this step:
=>kubectl get pod mybusybox -o yaml>myupdatedbusybox.yaml
Save current pod config and delete it then run update pod.
=>kubectl apply -f myupdatedbusybox.yaml



Domain2-Pod Design:
1) Very familier with labels and selectors.
2) How can apply to and object both vi manifest file and live modification.
3) Network policy with label

4) Very familary deployment.
5) Deploymetn is the key of CKAD exam.
6) Have to undestandi ths deployment from screate.
7) Create and modify live deployment.
8) Understand the Important of labels and selectors in deployment.
9) Have a good idea about masSurge and maxUnavailable in deployment and how to modify it
10) Be good at with rollign update and rollbacks, its very Important.

11) Jobs and CronJob familary.
12) Dont worry about cron date/time, find the exmple fron docuemtation and use it.
13) Understanding of activeDeadlineSeconds with in CronJob



Domain3-Service and Neworkign:
1) NodePort service 
2) Network policy
3) Diffeetnt between servie Port and targetPort
4) How to change live service port
5) Be sure services and Pod must be in same namespaces.

6) Network policy with label and troubleshooting question.



Domain4-configuration:
1) ConfigMap
2) How to mount configMap to a specific path vi volume
3) SecurityContext (runAsUser, runAsGroup, fsGroup)
4) Good idea about Resource Quotas
5) Request and limits for Resource Quotas and set at Pod level
6) What happen if nodes not match the minum request value
7) May no direct question for Resource Quotas.
8) Undestranding of Kubernetes Secret
9) Create Mount it as env variable and volume fo Pod
10) Familiar with environment variables

11) How can associate service account with existing deployment and existing POD
12) Name space has its own default service account
13) Add secrect on live deployment and Pod



Domain5- Liveness and Readiness:
1) Exma may not asky for liveness and readyness 
2) Liveness/Readiness not be able to update on live pod, save pod a new yaml and apply it.
3) Diffeetnt between bot and some Troubleshooting question for this relevent.
4) Like: deploy a pod by dont ready yet, what is problm fixed.

5) Have to konw only kubectl logs command about it.
6) Question like: save pod log a file.
7) Event is inport Topics

8) kubectl top pods/ kubectl top nodes 
9) Metric server will pre-install in exam.
10) Have to familier to see CPU/Memory usege of a specific pod or nods.
11) Queston like: on 3 pod wihich pod use most cpu and save this on a file

12) Many Debugging related question will be with:
  Network Policies
  Services
  Liveness/Readiness Probes
  Deployment etc



Domain6-PV and PVC:
1) Be familier with PV and PVC
2) Why use hostPath and emptyDir, be clear for this defferent.
3) Question Like: Create a pod attach a voliume ton path of mount is a such way
volume should be deleted once pod is removed.



Domain7-MultiContainer:
1) Be prepared for the question based on Sidecar Pattern
2) Practice queston related to HAProxy and FluentD 10 time before exam
3) HAProxy and FluentD  have to vary family with tis two.
4) HAProxy and FluentD all configuration provide on exam dont worry about congig.




  




















================================================= 
#Debug
================================================= 
=>kubectl api-resources
Details about api and with short name

Pod Details:
-----------------------------------------------
=>kubectl describe pod -n default
=>systemctl status kubelet


Create a deployment
=>kubectl create deployment mydeployment --image=nginx --dry-run=client -o yaml
=>kubectl explain deployment.spec.strategy


Edit Pod Information
=>kubectl edit pods mybusybox
=>kubectl get pods --show-laels


























Install Kubernetes
-------------------------------------------------
1)Mamage Kubernetes Services
2)MiniKube
3)Install Kubernetes Manually

Three 03 Thing need in a typical Kubernetes server:
     1)kubectl
     2)Kubernetes Master
     3)Worker Node Agents


Extract pod definition to a file using the below command:
=>kubectl get pod <pod-name> -o yaml > pod-definition.yaml


=>kubectl create namespace test-123 --dry-run -o json/yaml
Formatting Output




=================================================
#  Pre-Requisites                                          
=================================================

Topics Covered on the Exam
-------------------------------------------------

Cluster Architecture, Installation, and Configuration (25%)
A big part of the exam will focus on the Kubernetes setup and configuration. The tutorial, “Kubernetes The Hard Way” is a very helpful tool as you prepare for this section. I’ll talk more about this tutorial later.

Workloads and Scheduling (15%)
You’ll be expected to create robust deployments..

Storage (10%)
A small section will test your knowledge about volumes and volume claims.

Troubleshooting (30%)
The biggest section of the exam will test you on troubleshooting a Kubernetes cluster. This is a task you can only improve at through practice.



Certified Kubernetes Administrator (CKA)
-------------------------------------------------
The CKA tests your ability to deploy and configure a Kubernetes cluster as well as your understanding of core concepts. Candidates have three hours to take the exam and must score 74% or higher to earn the certification.

The CKA exam tests the following areas:

8% – Application lifecycle management
12% – Installation, configuration & validation
19% – Core concepts
11% – Networking
5% – Scheduling
12% – Security
11% – Cluster maintenance
5% – Logging/monitoring
7% – Storage
10% – Troubleshooting


Certified Kubernetes Application Developer (CKAD)
-------------------------------------------------
The CKAD tests your ability to deploy and configure applications running on the Kubernetes cluster and your understanding of some core concepts. You’ll have two hours to complete the CKAD exam. Scoring a 66% or higher means you’ve passed.

For the CKAD exam, you will be tested in the following areas:

13% – Core concepts
18% – Configuration
10% – Multi-container pods
18% – Observability
20% – Pod design
13% – Services & networking
8% – State persistence









Linux
-------------------------------------------------------
https://www.youtube.com/watch?v=Q8Nh8r6_tkQ&list=PLd3UqWTnYXOnar-GXf1taqzw5Z8nAAxod&index=1
https://www.youtube.com/watch?v=mzMD5duBA-A&list=PLd3UqWTnYXOkCdIbrnfB7A51jOlsP4i3w&index=1
https://www.youtube.com/watch?v=UoJ94MirYmw&list=PLd3UqWTnYXOny6ntfCKt9S4mwDM4GDaAG&index=1



CKA
===============================================================================


Domains & Competencies
-------------------------------------------------
Storage10%
Understand storage classes, persistent volumes
Understand volume mode, access modes and reclaim policies for volumes
Understand persistent volume claims primitive
Know how to configure applications with persistent storage

Troubleshooting30%
Evaluate cluster and node logging
Understand how to monitor applications
Manage container stdout & stderr logs
Troubleshoot application failure
Troubleshoot cluster component failure
Troubleshoot networking

Workloads & Scheduling15%
Understand deployments and how to perform rolling update and rollbacks
Use ConfigMaps and Secrets to configure applications
Know how to scale applications
Understand the primitives used to create robust, self-healing, application deployments
Understand how resource limits can affect Pod scheduling
Awareness of manifest management and common templating tools

Cluster Architecture, Installation & Configuration25%
Manage role based access control (RBAC)
Use Kubeadm to install a basic cluster
Manage a highly-available Kubernetes cluster
Provision underlying infrastructure to deploy a Kubernetes cluster
Perform a version upgrade on a Kubernetes cluster using Kubeadm
Implement etcd backup and restore

Services & Networking20%
Understand host networking configuration on the cluster nodes
Understand connectivity between Pods
Understand ClusterIP, NodePort, LoadBalancer service types and endpoints
Know how to use Ingress controllers and Ingress resources
Know how to configure and use CoreDNS
Choose an appropriate container network interface plugin





ExamTips
------------------------------------------------------------------------------
After 2 years of procrastination, finally booked the CKA certification exam.

Happy to share that I passed Certified Kubernetes Administrator (CKA) Exam today with 91% score.

I can never thank enough Mr. Mumshad Mannambeth for the excellent course, keeping concepts simple yet informative. His solution walkthroughs on KodeKloud labs were so helpful. Also I thank Udemy for bringing the best lecturer to the platform.

Tips for CKA exam prep:
💡 1. Check your understanding on below competencies (source: https://t.ly/gtXI)
     -  Storage : Persistent Volumes, mountpoints, storage classes etc.
     - Troubleshooting: debugging services on nodes, debugging error logs, understanding node logs, using metrics server
     -  Workloads & Scheduling: Understanding deployments, manifest files, RBAC
     -  Cluster Architecture, Installation & Configuration: Cluster setup, Kubeadm, k8 version upgrades
     - Services & Networking: Ingress, Network policies, Network interface plugins, CoreDNS, connectivity between pods and services, various ports.   
💡2. k8 documentation:
     - Practice all tasks in the documentation (https://lnkd.in/gCsHrdjp)
     - Make a habit of checking k8 documentation for manifest formats, it comes handy during exam.
     - Only official documentation is allowed on the exam remote desktop.
💡3. Mock exams:
     - Experience the exam environment which is remote desktop, in killerKoda (https://t.ly/1wYz)
     - KodeKloud mock exams that comes with Udemy course (https://t.ly/4Vf8) to get an idea how questions would be.
     - I highly reccommed to take exam simulator by Killer.sh only before 1 week to your exam. (you will get 2 free sessions on purchase of exam), the questions you face in this are lot tougher than the real exam.
💡4. Commands:
     - Practice imperative commands for pods, deployments and services. Instead of writing manifest files from scratch.
     - Try to make use of short commands using alias.
             
💡5. Main exam:
     - CKA exam time limit of 2 hours with 17 questions, each with different weights (4%, 5%, 7%, 13%), you need to get 66% to pass the exam.
     - All 7% looks simple in plain, but you have to pay attention to each detail. It may have two or three steps in it.
     - After implementing a task, take some time to test it.
     - Don’t stick to the hard question more than 10mins. Just flag it, you can visit back after completing easy ones.
     - Always you have one free retake, if you fail first time dont worry. Practice well to score well.

💡6. Patience, Preparation and Practice :
     - It was not an easy thing to crack this test, needs a lot of patience to give time to understand and prepare notes on each concept. I took nearly a month for practicing all scenarios


Lab:
CKA-LAB:
https://kodekloud.com/lessons/core-concepts-4/
Docker-lab:
https://kodekloud.com/lessons/hands-on-labs-2/
DockerSwarmLab
https://kodekloud.com/lessons/labs-5/
PythonLab
https://kodekloud.com/lessons/quizzes/
DevPOpsLan
https://kodekloud.com/lessons/labs-6/


