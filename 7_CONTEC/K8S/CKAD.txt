#################################################
#          CKAD EXAM-PREPARATION                #
#################################################

Exam Taken: 06/10/2023





=================================================
#General                                
=================================================

=>kubectl api-resources

=>kubectl version
=>kubectl version --short 
=>kubectl get nodes

=>kubectl get pods
=>kubectl get pods -o wide
=>kubectl get -o json pod prodName

=>kubectl explain pod
=>kubectl explain deployment

=kubectl logs myPod

=>kubectl run --help
=>kubectl run nginx --image=nginx --dry-run=client
=>kubectl run nginx --image=nginx --dry-run=client -o yaml
=>kubectl run myng --image=nginx --dry-run=client -o yaml>labelpod.yaml
=>kubectl create deployment --image=nginx nginx --dry-run=client -o yaml
=>kubectl run ng3 --image=nginx -- /bin/sh -c "while true; do echo  $(date); sleep 1; done "

=>kubectl run pod1 --image=imranmadbar/nginx && sleep 2 && kubectl exec -it pod1 -- bash
=>echo -e "Welcome to nginx! \nHost Name: $(hostname -f)\nHost IP: $(hostname -i)">/usr/share/nginx/html/index.html

=>kubectl run mynginx --image=nginx
=>kubectl run  bu1 --image=busybox -- sh  -c "hostname -i"
=>kubectl run logpod --image=busybox -- sh -c "ping google.com"
=>kubectl run bx2 --image=busybox -- sh -c "while true; do echo $(date)>>/var/1.log; sleep 1; done"

=>kubectl run -it ubuntu1 --image=ubuntu --restart=Never -- bash -ec "apt update; apt install mysql-server; bash"
          =while true; do echo "infinity"; sleep 1; done

=>kubectl exec -it ub1 -- bash
=>kubectl exec -it mynginx -- bash
=>kubectl exec -it mynginx -- ls -l


=>kubectl apply -f sample.yaml
=>kubectl delete -f sample.yaml
=>kubectl delete pod my-pod1 

=>kubectl describe pod mynginx

=>kubectl get nodes -o jsonpath='{.items[*].status.addresses[?(@.type=="ExternalIP")].address}'
Using JSONPath to get the IPs of all your nodes


=>kubectl get pv --sort-by=.spec.capacity.storage
A set of Persistent Volumes are available. Sort them based on their capacity


Send every request for specific pod
--------------------------------------------------
apiVersion: v1
kind: Service
spec:
sessionAffinity: ClientIP



=>k get po ng1 -o jsonpath='{.spec.nodeName}'

=>helm upgrade lvm-crystal-apd lvm-crystal-apd/nginx  --set replicaCount=2 -n crystal-apd-ns

=>$ kubectl delete all --all
=>k run tmp --rm --restart=Never -i --image=nginx:alpine -- curl -m3 ng-svc:8080

=>kubectl label po kubia-manual creation_method=manual
=>kubectl label po kubia-manual-v2 env=debug --overwrite


List the pods again to see the updated labels:
$ kubectl get po -L creation_method,env


And those that don’t have the env label:
$ kubectl get po -l '!env'

creation_method!=manual to select pods with the creation_method label with
any value other than manual



Exec to service:
$ kubectl exec kubia-3inly env

=>k get po --output=custom-columns="NAME":.metadata.name,"QOS":.status.qosClass>qos_status_aecs
=>k get po -o=custom-columns='POD_NAME:metadata.name,IP_ADDR:status.podIP' --sort-by=.status.podIP > /root/pod_ips_ckad02_svcn


DaemonSet
=>kubectl get daemonset
Demonset every node can include NodeAffinity


=>echo "imran"| base64
=>echo 'aW1yYW4K' | base64 -d

=>base64 my-file.txt
=>base64 -d encod-file.txt


=>sudo apt-get update && apt-get install iputils-ping && sudo apt install net-tools

=>kubectl run ckad-probe-aom --image=nginx --restart=Always -l=run=ckad-probe-aom
=>k run ng -l env=prod,tier=backend --image=nginx

=>kubectl create quota my-quota --hard=cpu=1,memory=1G,pods=2,services=3,replicationcontrollers=2,resourcequotas=1,secrets=5,persistentvolumeclaims=10


=>kubectl create cronjob my-alarm -n ckad-job               --image=busybox:1.28 --schedule="0 0 * * 0" -- date
=>kubectl create cronjob simple-python-job -n ckad-job      --image=python       --schedule="*/30 * * * *" -- 'ps –eaf'
=>kubectl create cronjob learning-every-minute -n ckad-job --restart=OnFailure --image=busybox:1.28 --schedule="*/1 * * * *" -- echo 'I am practicing for CKAD certification'


=>k create clusterrole healthz-access --verb=get,post --non-resource-url=/healthz,/healthz/*



=>scp ../media/* root@node01:/web
Copey file to another

=>ps -ef | grep kube-apiserver | grep admission-plugins
=>kubectl exec -it kube-apiserver-controlplane -n kube-system -- kube-apiserver -h | grep 'enable-admission-plugins'


=>kubectl create ingress world -n world --class=nginx --rule="world.universe.mine/europe*=europe:80" --rule="world.universe.mine/asia*=asia:80" 
=>kubectl create ingress ingress-ckad09-svcn -n critical-space  --rule="/pay=pay-service:8282
=>kubectl create ingress ingress-resource-svcn \
  --namespace app-space \
  --rule='/wear'='wear-service:8080' \
  --rule='/watch'='video-service:8080' \
  --annotation='nginx.ingress.kubernetes.io/rewrite-target=/' \
  --annotation='nginx.ingress.kubernetes.io/ssl-redirect=false' \
  --class=nginx


 annotations:
    "api-approved.kubernetes.io": "unapproved, experimental-only"



=>kubectl --namespace=ckad17-nqoss-aecs get pod --output=custom-columns="NAME:.metadata.name,QOS:.status.qosClass"
NAME                QOS
ckad17-qos-aecs-1   BestEffort
ckad17-qos-aecs-2   Guaranteed
=>kubectl --namespace=ckad17-nqoss-aecs get pod --output=custom-columns="NAME:.metadata.name,QOS:.status.qosClass" > /root/qos_status_aecs

Perform the connectivity test using 
=>kubectl  exec testpod -- curl backend-ckad-svcn

=>helm upgrade lvm-crystal-apd -n crystal-apd-ns lvm-crystal-apd/nginx --version=13.2.30 --set replicaCount=2

SAVE OUTPI  :
=>k get po | sudo tree them/logfile.log













@@@KodeKloud
##kc10
Kubernetes Challenges

RND-PROB
=================================================
###
We have an external webserver running on student-node which is exposed at port 9999.
We have also created a service called external-webserver-ckad01-svcn 
that can connect to our local webserver from within the cluster3 but, at the moment, it is not working as expected.

Fix the issue so that other pods within cluster3 can use external-webserver-ckad01-svcn service to access the webserver.

Solution
Let's check if the webserver is working or not:
student-node ~ ➜  curl student-node:9999
  <h1>Welcome to nginx!</h1>


Now we will check if service is correctly defined:
 =>kubectl describe svc external-webserver-ckad01-svcn 
  Name:              external-webserver-ckad01-svcn
  Namespace:         default
  .
  Endpoints:         <none> # there are no endpoints for the service
  ...
  
As we can see there is no endpoints specified for the service, hence we won't be able to get any output. 
Since we can not destroy any k8s object, let's create the endpoint manually for this service as shown below:

student-node ~ ➜  export IP_ADDR=$(ifconfig eth0 | grep inet | awk '{print $2}')
student-node ~ ➜ kubectl --context cluster3 apply -f - <<EOF
apiVersion: v1
kind: Endpoints
metadata:
  # the name here should match the name of the Service
  name: external-webserver-ckad01-svcn
subsets:
  - addresses:
      - ip: $IP_ADDR
    ports:
      - port: 9999
EOF

Finally check if the curl test works now:
student-node ~ ➜  kubectl --context cluster3 run --rm  -i test-curl-pod --image=curlimages/curl --restart=Never -- curl -m 2 external-webserver-ckad01-svcn
...
<title>Welcome to nginx!</title>
...



###
IngressIssue:
=>k get pods -n ingress-nginx 

NAME                                        READY   STATUS      RESTARTS      AGE
ingress-nginx-admission-create-l6fgw        0/1     Completed   0             11m
ingress-nginx-admission-patch-sfgc4         0/1     Completed   0             11m
ingress-nginx-controller-5f8964959d-278rc   0/1     Error       2 (26s ago

=>k logs -n ingress-nginx ingress-nginx-controller-5f8964959d-278rc 
No service with name default-backend-service found in namespace default:



###
=>kubectl create clusterrole healthz-access --non-resource-url=/healthz,/healthz/* --verb=get,post
Create a clusterrole for non-resource with path and subpath.





=================================================
Challenge 1:
=================================================
JaklyPod:

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: jekyll
  name: jekyll
  namespace: development
spec:
  volumes:
    - name: site
      persistentVolumeClaim:
        claimName: jekyll-site
  initContainers:
  - image: kodekloud/jekyll
    name: copy-jekyll-site
    command: [ "jekyll", "new", "/site" ]
    volumeMounts:
      - name: site
        mountPath: /site
  containers:
  - image: kodekloud/jekyll-serve
    name: jekyll
    volumeMounts:
      - name: site
        mountPath: /site
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}


=>k expose pod jekyll --name=jekyll --type=NodePort --port=8080 --target-port=4000 -n development
=>kubectl create role developer-role --verb=* --resource=pods,services,persistentvolumeclaims -n development



Challenge 2:
This 2-Node Kubernetes cluster is broken!


=>vi /etc/kubernetes/manifests/kube-apiserver.yaml
Change this;
 - --client-ca-file=/etc/kubernetes/pki/ca-authority.crt

Change:
registry.k8s.io/kubedns:1.3.1


Chalange 4:
Build a highly available Redis Cluster based on the given architecture diagram.


cat stful.yaml 
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: redis-cluster
spec:
  selector:
    matchLabels:
      app: redis-cluster  # has to match .spec.template.metadata.labels
  serviceName: "redis-cluster-service"
  replicas: 6 # by default is 1
  minReadySeconds: 10 # by default is 0
  template:
    metadata:
      labels:
        app: redis-cluster # has to match .spec.selector.matchLabels
    spec:
      terminationGracePeriodSeconds: 10
      volumes:
        - name: 'conf'
          configMap:
            name: 'redis-cluster-configmap'
            defaultMode: 0755
      containers:
      - name: redis
        env:
          - name: 'POD_IP'
            valueFrom: 
              fieldRef: 
                fieldPath: 'status.podIP' 
        image: redis:5.0.1-alpine
        command:  ["/conf/update-node.sh", "redis-server", "/conf/redis.conf"]
        ports:
        - containerPort: 6379
          name: client
        - containerPort: 16379
          name: gossip
        volumeMounts:
        - name: conf
          mountPath: /conf
          readOnly: false
        - name: data
          mountPath: '/data'
          readOnly: false
  volumeClaimTemplates:
  - metadata:
      name: 'data'
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 1Gi


Q15
In the ckad13-ns-sa-aecs namespace, configure the ckad13-nginx-pod-aecs Pod to include a projected volume named vault-token.
Mount the service account token to the container at /var/run/secrets/tokens, 
with an expiration time of 9000 seconds.
Additionally, set the intended audience for the token to vault and path to vault-token.


Ans:
apiVersion: v1
kind: Pod
metadata:
  name: ckad13-nginx-pod-aecs
  namespace: ckad13-ns-sa-aecs
spec:
  containers:
    - name: nginx-container
      image: nginx
      volumeMounts:
        - name: token-volume
          mountPath: /var/run/secrets/tokens
          readOnly: true
  volumes:
    - name: token-volume
      projected:
        sources:
          - serviceAccountToken:
              expirationSeconds: 9000
              path: vault-token
              audience: vault


13:
Create a custom resource my-anime of kind Anime with the below specifications:
Name of Anime: Death Note
Episode Count: 37
TIP: You may find the respective CRD with anime substring in it.

=>kubectl get crd animes.animes.k8s.io \
                 -o json \
                 | jq .spec.versions[].schema.openAPIV3Schema.properties.spec.properties

apiVersion: "animes.k8s.io/v1alpha1"
kind: Anime
metadata:
  name: my-anime
spec:
  animeName: Death Note
  episodeCount: 37


Q. 14
Question
We have a Kubernetes namespace called ckad12-ctm-sa-aecs, which contains a service account and a pod. 
Your task is to modify the pod so that it uses the service account defined in the same namespace.

Additionally, you need to ensure that the pod has access to the API credentials associated with the 
service account by enabling the automounting feature for the credentials.


Solution
Here we will do two things:
apiVersion: v1
kind: Pod
metadata:
  name: ckad12-ctm-nginx-aecs
  namespace: ckad12-ctm-sa-aecs
spec:
  # automountServiceAccountToken: false  *removed to enable automount of creds
  containers:
  - image: nginx
    imagePullPolicy: Always
    name: nginx
  serviceAccountName: ckad12-my-custom-sa-aecs # using custom sa




Q. 16
Question
Create a pod named ckad17-qos-aecs-3 in namespace ckad17-nqoss-aecs 
with image nginx and container name ckad17-qos-ctr-3-aecs.

Define other fields such that the Pod is configured to use the Quality of Service (QoS) class of Burstable.

Also retrieve the name and QoS class of each Pod in the namespace ckad17-nqoss-aecs in the below 
format and save the output to a file named qos_status_aecs in the /root directory.


Format:
NAME    QOS
pod-1   qos_class
pod-2   qos_class


Solution

apiVersion: v1
kind: Pod
metadata:
  name: ckad17-qos-aecs-3
  namespace: ckad17-nqoss-aecs
spec:
  containers:
  - name: ckad17-qos-ctr-3-aecs
    image: nginx
    resources:
      limits:
        memory: "200Mi"
      requests:
        memory: "100Mi"

=>kubectl --namespace=ckad17-nqoss-aecs get pod --output=custom-columns="NAME:.metadata.name,QOS:.status.qosClass"
NAME                QOS
ckad17-qos-aecs-1   BestEffort
ckad17-qos-aecs-2   Guaranteed
ckad17-qos-aecs-3   Burstable
=>kubectl --namespace=ckad17-nqoss-aecs get pod --output=custom-columns="NAME:.metadata.name,QOS:.status.qosClass" > /root/qos_status_aecs




7Question
In this task, we have to create two identical environments that are running different versions of the application. 
The team decided to use the Blue/green deployment method to deploy a total 
of 10 application pods which 30% green pod and rest 70% to blue.

The name of the deployment is blue-apd.
Use the label type-one: blue.
Use the image kodekloud/webapp-color:v1.
Add labels to the pod type-one: blue and version: v1.

The name of the deployment is green-apd.
Use the label type-two: green.
Use the image kodekloud/webapp-color:v2.
Add labels to the pod type-two: green and version: v1.

The name of the service is route-apd-svc.
Use the correct service type to access the application from outside the cluster and application should listen on port 8080.
Use the selector label version: v1.

curl http://cluster3-controlplane:NODE-PORT


Solution
Use the kubectl create command to create a deployment manifest file as follows: -

=>kubectl create deployment blue-apd --image=kodekloud/webapp-color:v1 --dry-run=client -o yaml > <FILE-NAME-1>.yaml
=kubectl create deployment green-apd --image=kodekloud/webapp-color:v2 --dry-run=client -o yaml > <FILE-NAME-2>.yaml
=>kubectl create service nodeport route-apd-svc --tcp=8080:8080 --dry-run=client -oyaml > <FILE-NAME-3>.yaml

---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    type-one: blue
  name: blue-apd
spec:
  replicas: 7
  selector:
    matchLabels:
      type-one: blue
      version: v1
  template:
    metadata:
      labels:
        version: v1
        type-one: blue
    spec:
      containers:
        - image: kodekloud/webapp-color:v1
          name: blue-apd
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    type-two: green
  name: green-apd
spec:
  replicas: 3
  selector:
    matchLabels:
      type-two: green
      version: v1
  template:
    metadata:
      labels:
        type-two: green
        version: v1
    spec:
      containers:
        - image: kodekloud/webapp-color:v2
          name: green-apd
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: route-apd-svc
  name: route-apd-svc
spec:
  type: NodePort
  ports:
    - port: 8080
      protocol: TCP
      targetPort: 8080
  selector:
    version: v1



11Question
A manifest file located at root/ckad-flash89.yaml Which can be used to create a multi-containers pod. 
There are issues with the manifest file

Solution
=>kubectl create -f ckad-flash89.yaml
error: resource mapping not found for name: "ckad-flash-89" namespace: "" from "ckad-flash89.yaml": 
no matches for kind "Pod" in version "V1"
ensure CRDs are installed first
about error shows us there is something wrong with apiVersion. So change it v1 and try again. and check status.

kubectl get pods
NAME            READY   STATUS             RESTARTS      AGE
ckad-flash89-aom   1/2     CrashLoopBackOff   3 (39s ago)   93s

Now check for reason using
=>kubectl describe pod ckad-flash89-aom
we will see that there is problem with nginx container

open yaml file and check in spec -> nginx container you can see error with 
mountPath --> mountPath: "/var/log" change it to mountPath: /var/log/nginx and apply changes.




14Question
Create a Kubernetes Pod named ckad16-memory-aecs, with a container named ckad16-memory-ctr-aecs 
running the polinux/stress image, and configure it to use the following specifications:

Command: stress
Arguments: ["--vm", "1", "--vm-bytes", "15M", "--vm-hang", "1"]
Requested memory: 10Mi
Memory limit: 20Mi


Solution
student-node ~ ➜  kubectl config use-context cluster1
Switched to context "cluster1".

student-node ~ ➜  cat << EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: ckad16-memory-aecs
spec:
  containers:
  - name: ckad16-memory-ctr-aecs
    image: polinux/stress
    resources:
      requests:
        memory: "10Mi"
      limits:
        memory: "20Mi"
    command: ["stress"]
    args: ["--vm", "1", "--vm-bytes", "15M", "--vm-hang", "1"]
EOF



In the ckad13-ns-sa-aecs namespace, configure the ckad13-nginx-pod-aecs Pod to include a projected volume named vault-token. Mount the service account token to the container at /var/run/secrets/tokens, with an expiration time of 9000 seconds.
Additionally, set the intended audience for the token to vault and path to vault-token.

Ans:
student-node ~ ➜  kubectl config use-context cluster2
Switched to context "cluster2".

student-node ~ ➜  k get po -n ckad13-ns-sa-aecs ckad13-nginx-pod-aecs -o yaml > ckad-pro-vol.yaml

student-node ~ ➜  cat ckad-pro-vol.yaml
apiVersion: v1
kind: Pod
metadata:
  name: ckad13-nginx-pod-aecs
  namespace: ckad13-ns-sa-aecs
spec:
  containers:
  - image: nginx
    imagePullPolicy: Always
    name: nginx
.
.
.
   volumeMounts:                              # Added
    - mountPath: /var/run/secrets/tokens       # Added
      name: vault-token                        # Added
.
.
.
  serviceAccount: ckad13-sa-aecs
  serviceAccountName: ckad13-sa-aecs
  volumes:
  - name: vault-token                   # Added
    projected:                          # Added
      sources:                          # Added
      - serviceAccountToken:            # Added
          path: vault-token             # Added
          expirationSeconds: 9000       # Added
          audience: vault               # Added

student-node ~ ➜  k replace -f ckad-pro-vol.yaml --force 
pod "ckad13-nginx-pod-aecs" deleted
pod/ckad13-nginx-pod-aecs replaced


16Question
Define a Kubernetes custom resource definition (CRD) for a new resource kind called Foo (plural form - foos) 
in the samplecontroller.k8s.io group.
This CRD should have a version of v1alpha1 with a schema that includes two properties as given below:

deploymentName (a string type) and 
replicas (an integer type with minimum value of 1 and maximum value of 5).

It should also include a status subresource which enables retrieving and updating the status of Foo object,
including the availableReplicas property, which is an integer type.
The Foo resource should be namespace scoped.

Solution
student-node ~ ➜  cat foo-crd-aecs.yaml 
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: foos.samplecontroller.k8s.io
  annotations:
    "api-approved.kubernetes.io": "unapproved, experimental-only"
spec:
  group: samplecontroller.k8s.io
  scope: Namespaced
  names:
    kind: Foo
    plural: foos
  versions:
    - name: v1alpha1
      served: true
      storage: true
      schema:
        # schema used for validation
        openAPIV3Schema:
          type: object
          properties:
            spec:
              # Spec for schema goes here !
              type: object
              properties:
                deploymentName:
                  type: string
                replicas:
                  type: integer
                  minimum: 1
                  maximum: 5
            status:
              type: object
              properties:
                availableReplicas:
                  type: integer
      # subresources for the custom resource
      subresources:
        # enables the status subresource
        status: {}



18Question
We have deployed several applications in the ns-ckad17-svcn namespace that are exposed inside the cluster via ClusterIP.
Your task is to create a LoadBalancer type service that will serve traffic to the applications based on its labels. 

Service lb1-ckad17-svcn for serving traffic at port 31890 to pods with labels "exam=ckad, criteria=location".
Service lb2-ckad17-svcn for serving traffic at port 31891 to pods with labels "exam=ckad, criteria=cpu-high".

Solution
To create the loadbalancer for the pods with the specified lables, 
first we need to find the pods with the mentioned lables.

=>kubectl -n ns-ckad17-svcn get pod -l exam=ckad,criteria=location
-----
NAME               READY   STATUS    RESTARTS   AGE
geo-location-app   1/1     Running   0          10m

=>kubectl -n ns-ckad17-svcn get pod -l exam=ckad,criteria=cpu-high
-----
NAME           READY   STATUS    RESTARTS   AGE
cpu-load-app   1/1     Running   0          11m

Now we know which pods use the labels, we can create the LoadBalancer type service using the imperative command.

=>kubectl -n ns-ckad17-svcn expose pod geo-location-app --type=LoadBalancer --name=lb1-ckad17-svcn
=>kubectl -n ns-ckad17-svcn expose pod cpu-load-app --type=LoadBalancer --name=lb2-ckad17-svcn

Once the services are created, you can edit the services to use the correct nodePorts 
as per the question using kubectl -n ns-ckad17-svcn edit svc lb2-ckad17-svcn.



Q5
On the cluster2-controlplane node, a Helm chart repository is given under the /opt/ path. 
It contains the files that describe a set of Kubernetes resources that can be deployed as a single unit. 
The files have some issues. Fix those.

1. The release name should be webapp-color-apd.
2. All the resources should be deployed on the frontend-apd namespace.
3. The service type should be node port.
4. Scale the deployment to 3.
5. Application version should be 1.20.0.

NOTE: - Remember to make necessary changes in the values.yaml and Chart.yaml files 
according to the specifications, and, to fix the issues, inspect the template files.


Solution
Update the values according to the given specifications as follows: -
a.) Update the value of the appVersion to 1.20.0 in the Chart.yaml file.
b.) Update the value of the replicaCount to 3 in the values.yaml file.
c.) Update the value of the type to NodePort in the values.yaml file.
These are the values we have to update.

Now, we will use the helm lint command to check the Helm chart 

=>cd /opt/
=>helm lint ./webapp-color-apd/

If there is no misconfiguration, we will see the similar output: -

helm lint ./webapp-color-apd/
==> Linting ./webapp-color-apd/
[INFO] Chart.yaml: icon is recommended

1 chart(s) linted, 0 chart(s) failed

But in our case, there are some issues with the given templates.
Deployment apiVersion needs to be correctly written. It should be apiVersion: apps/v1.

In the service YAML, there is a typo in the template variable {{ .Values.service.name }} because of that, 
it's not able to reference the value of the name field defined in the values.yaml 
file for the Kubernetes service that is being created or updated.

=>helm install webapp-color-apd -n frontend-apd ./webapp-color-apd
=>helm ls -n frontend-apd



14
Create a service account named ckad23-sa-aecs in the namespace ckad23-nssa-aecs.
Grant the service account get and list permissions to access all resources within the namespace using a 
Role named wide-access-aecs.
Also bind the Role to the service account using a RoleBinding named wide-access-rb-aecs, 
restricting the access to the ckad23-nssa-aecs namespace only.


Solution
=>kubectl create ns ckad23-nssa-aecs
=>kubectl create serviceaccount ckad23-sa-aecs -n ckad23-nssa-aecs

=>kubectl create role wide-access-aecs --namespace=ckad23-nssa-aecs --verb=get,list --resource=* 
=>kubectl create role pod-creater -n=ckad20-auth-aecs --verb=list,create,get --resource=pods

=>kubectl create rolebinding wide-access-rb-aecs \
   --role=wide-access-aecs \
   --serviceaccount=ckad23-nssa-aecs:ckad23-sa-aecs \
   --namespace=ckad23-nssa-aecs
=>kubectl create rolebinding mock-user-binding -n=ckad20-auth-aecs --role=pod-creater --user=mock-user


Q. 12Question
Please fix any possible mistakes in the manifest file located at /root/ckad11-obj-aecs.yaml, 
which contains a sample object created from a custom resource definition.
Note: Create the object using the above manifest when done.

Solution
student-node ~ ➜  kubectl apply -f /root/ckad11-obj-aecs.yaml
error: line 7: mapping values are not allowed in this context

student-node ~ ➜  kubectl apply -f /root/ckad11-obj-aecs.yaml
error: unable to recognize "/root/ckad11-obj-aecs.yaml": no matches for kind "MyCustomResource" in version "apps/v1"

student-node ~ ✖ kubectl api-resources | grep -i MyCustomResource
mycustomresources                 mcr          example.com/v1                         true         MyCustomResource

student-node ~ ➜  kubectl apply -f /root/ckad11-obj-aecs.yaml
The MyCustomResource "ckad11-my-custom-resource-aecs" is invalid: 
* spec.age: Invalid value: -10: spec.age in body should be greater than or equal to 5
* status.phase: Unsupported value: "Invalid": supported values: "Pending", "Running", "Completed"

student-node ~ ✖ vim /root/ckad11-obj-aecs.yaml

student-node ~ ➜  cat /root/ckad11-obj-aecs.yaml
apiVersion: example.com/v1  #2 fixed the apiVersion used
kind: MyCustomResource
metadata:
  name: ckad11-my-custom-resource-aecs
spec:
  name: John  #1 Fixed the indentation issue
  age: 5 #3 use the valid entries
status:
  phase: Running #4 use the valid entries



Q. 9
Identify the kube api-resources that use api_version=storage.k8s.io/v1 using kubectl command line interface 
and store them in /root/api-version.txt on student-node.

Solution
Use the following command to get details:
=>kubectl api-resources --sort-by=kind | grep -i storage.k8s.io/v1  > /root/api-version.txt



Q. 8Question
The team Garuda has deployed a web and security application using the Helm tool. 
Inspect those deployed resources and write their release name to the /root/apps-release-names.txt

Also, the same team deployed one application in the testing-apd namespace. 
The testing was done, and the team wants you to delete that release. 
Find out the release name and delete it.


Solution
To check all the resources in all namespaces in the cluster2, we would have to run the following command:
=>kubectl get all -A

It will list all the available resources of all namespaces.
We can see that the resources have used prefix called garuda in the name.

To list all of the releases on the garuda-apps-apd namespace. Run the following command as follows: -
=>helm ls -n garuda-apps-apd

We can see the release names of the web and security applications, 
and they are deployed on the garuda-apps-apd namespace.

Write their release names by using the echo command as follows: -
=>echo "garuda-secret-apd,garuda-web-apd" > /root/apps-release-names.txt

On the same cluster, one more testing application is running on the testing-apd namespace, 
which we must delete because it is consuming resources.
=>helm ls -n testing-apd
=>helm uninstall -n testing-apd image-scanner 


Q. 7
Question
One co-worker deployed an nginx helm chart on the cluster3 server called lvm-crystal-apd. 
A new update is pushed to the helm chart, and the team wants you to update the helm repository to fetch the new changes.
After updating the helm chart, upgrade the helm chart version to above 13.2.9 and increase the replica count to 2.


Solution
=>helm ls -A
Here lists all the releases of all the namespaces.
Identify the namespace where the resources get deployed.

=>helm repo ls 
Use the helm repo ls command to list the helm repositories.

=>helm repo update lvm-crystal-apd -n crystal-apd-ns
The above command updates the local cache of available charts from the configured chart repositories.

The helm search command searches for all the available charts in a specific Helm chart repository. 
In our case, it's the nginx helm chart.

=>helm search repo lvm-crystal-apd/nginx -n crystal-apd-ns -l | head -n30
The -l or --versions option is used to display information about all available chart versions.

Upgrade the helm chart to above 13.2.9 and also, increase the replica count of the deployment to 2 from the command line. 
Use the helm upgrade command as follows: -

=>helm upgrade lvm-crystal-apd lvm-crystal-apd/nginx -n crystal-apd-ns --version=13.2.12 --set replicaCount=2
  helm upgrade lvm-crystal-apd lvm-crystal-apd/nginx  -n crystal-apd-ns --version=14.0.0 --set replicaCount 2


After upgrading the chart version, you can verify it with the following command: -
=>helm ls -n crystal-apd-ns
Look under the CHART column for the chart version.

Use the kubectl get command to check the replicas of the deployment: -
=>kubectl get deploy -n crystal-apd-ns
The available count 2 is under the AVAILABLE column.



Q. 6Question
One application, webpage-server-01, is deployed on the Kubernetes cluster by the Helm tool. 
Now, the team wants to deploy a new version of the application by replacing the existing one. 
A new version of the helm chart is given in the /root/new-version directory on the student-node. 
Validate the chart before installing it on the Kubernetes cluster. 

Use the helm command to validate and install the chart. 
After successfully installing the newer version, uninstall the older version. 


Solution
Use the helm ls command to list the release deployed on the default namespace using helm.
=>helm ls -n default

First, validate the helm chart by using the helm lint command: -
cd /root/
=>helm lint ./new-version

Now, install the new version of the application by using the helm install command as follows: -
=>helm install --generate-name ./new-version

We haven't got any release name in the task, so we can generate the random name from the --generate-name option.
Finally, uninstall the old version of the application by using the helm uninstall command: -
=>helm uninstall webpage-server-01 -n default



2Question
In the ckad-job namespace, create a cron job called my-alarm that 
prints current datetime once a week at midnight on Sunday morning.

In case the container in pod failed for any reason, it should be restarted automatically.
Use busybox:1.28 image to create job.
Sample output:
Sun Mar  12 00:00:00 UTC 2023


Solution
Use below YAML to create cronjob:

apiVersion: batch/v1
kind: CronJob
metadata:
  name: my-alarm
  namespace: ckad-job
spec:
  schedule: "0 0 * * 0"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: hello
            image: busybox:1.28
            imagePullPolicy: IfNotPresent
            command:
            - /bin/sh
            - -c
            - date;
          restartPolicy: OnFailure

OR
=>kubectl create cronjob my-alarm -n ckad-job --image=busybox:1.28 --schedule="0 0 * * 0" -- date

Q. 5
Question
On the cluster1, the team has installed multiple helm charts on a different namespace. 
By mistake, those deployed resources include one of the vulnerable images called kodekloud/click-counter:latest.
Find out the release name and uninstall it.


Solution:
In this task, we will use the helm commands and jq tool. Here are the steps: -

Run the helm ls command with -A option to list the releases deployed on all the namespaces using helm.
=>helm ls -A


We will use the jq tool to extract the image name from the deployments.
=>kubectl get deploy -n <NAMESPACE> <DEPLOYMENT-NAME> -o json | jq -r '.spec.template.spec.containers[].image'

After finding the kodekloud/click-counter:latest image, 
use the helm uninstall to remove the deployed chart that are using this vulnerable image.

=>helm uninstall <RELEASE-NAME> -n <NAMESPACE> 



Q. 18
Question
We have already deployed an ingress resource in the app-space namespace.
But for better SEO practices, you are requested to change the URLs at which the applications are made available.
Change the path of the video application to make it available at /stream.
Note: Check the backend services configured for the paths in the ingress resource.

Solution
To change the path of the service, you need to edit the ingress

Edit the ingress using 
=>kubectl edit -n app-space ingress ingress-resource-svcn.
Edit the /watch to make it as /stream.
spec:
  ingressClassName: nginx
  rules:
  - http:
      paths:
      - backend:
          service:
            name: wear-service
            port:
              number: 8080
        path: /wear
        pathType: Prefix
      - backend:
          service:
            name: video-service
            port:
              number: 8080
        path: /watch  #change path to /stream
        pathType: Prefix
status:
  loadBalancer:
    ingress:
    - ip: 10.108.22.212
OR:
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  namespace: app-space
  name: ingress-resource-svcn
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  ingressClassName: nginx
  rules:
  - http:
      paths:
      - path: /stream
        pathType: Prefix
        backend:
          service:
            name: video-service
            port:
              number: 8080
      - path: /wear
        pathType: Prefix
        backend:
          service:
            name: wear-service
            port:
              number: 8080



Q. 4
Question
In the ckad-multi-containers namespaces, create a ckad-sidecar-pod pod that matches the following requirements.
Pod has an emptyDir volume named my-vol.
The first container named main-container, runs nginx:1.16 image. 
This container mounts the my-vol volume at /usr/share/nginx/html path.

The second container named sidecar-container, runs busybox:1.28 image. 
This container mounts the my-vol volume at /var/log path.
Every 5 seconds, this container should write the current date along with greeting message 
Hi I am from Sidecar container to index.html in the my-vol volume.


Solution
Use below YAML to create the desired pod:

apiVersion: v1
kind: Pod
metadata:
  namespace: ckad-multi-containers
  name: ckad-sidecar-pod
spec:
  containers:
    - image: nginx:1.16
      name: main-container
      resources: {}
      ports:
        - containerPort: 80
      volumeMounts:
        - name: my-vol
          mountPath: /usr/share/nginx/html
    - image: busybox:1.28
      command:
        - /bin/sh
        - -c
        - while true; do echo $(date -u) Hi I am from Sidecar container >> /var/log/index.html; sleep 5;done
      name: sidecar-container
      resources: {}
      volumeMounts:
        - name: my-vol
          mountPath: /var/log
  dnsPolicy: Default
  volumes:
    - name: my-vol
      emptyDir: {}


You can verify the logs with below command:
kubectl exec -n ckad-multi-containers ckad-sidecar-pod --container main-container -- cat /usr/share/nginx/html/index.html

Similar logs should be displayed as below:
Mon Mar 20 06:02:07 UTC 2023 Hi I am from Sidecar container




Q. 5Question
Our new client wants to deploy the resources through the popular Helm tool. 
In the initial phase, our team lead wants to deploy nginx, a very powerful and versatile web server 
software that is widely used to serve static content, reverse proxying, load balancing, 
from the bitnami helm chart on the cluster3-controlplane node.
The chart URL and other specifications are as follows: -

1. The chart URL link - https://charts.bitnami.com/bitnami
2. The chart repository name should be polar.
3. The release name should be nginx-server.
4. All the resources should be deployed on the cd-tool-apd namespace.

Solution
Add the repostiory to Helm with the following command: -
=>helm repo add polar https://charts.bitnami.com/bitnami
=>helm repo ls 

Search for the nginx chart in a polar chart repository as follows: -
=>helm search repo polar | grep nginx
=>kubectl create ns cd-tool-apd
=>helm install nginx-server polar/nginx -n cd-tool-apd



Challenge:2
Change .kube/config file kunbe-apiserver port
kube-apiserver.yaml client-cert file
change dns deployment image in kube-system namespace

and Node01
=>kubectl uncordon node01


Q1:Copy all images from the directory '/media' on the controlplane node to '/web' directory on node01
=>scp * node01:/web







##lightLab
=================================================
LIGHTNING LABS-01
=================================================

Q1:Create a Persistent Volume called log-volume. It should make use of a storage class name manual. 
It should use RWX as the access mode and have a size of 1Gi. 
The volume should use the hostPath /opt/volume/nginx
Next, create a PVC called log-claim requesting a minimum of 200Mi of storage. This PVC should bind to log-volume.
Mount this in a pod called logger at the location /var/www/nginx. This pod should use the image nginx:alpine.

log-volume created with correct parameters?

pv.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: log-volume
spec:
  storageClassName: "manual"
  capacity:
    storage: 1Gi
  accessModes:
    - ReadWriteMany
  hostPath:
    path: /opt/volume/nginx


pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: log-claim
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 200Mi
  storageClassName: manual


=>kubectl run logger --image=nginx:alpine --dry-run=client -oyaml>logger.yaml
Then need add volume info to pod:

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: logger
  name: logger
spec:
  volumes:
    - name: log
      persistentVolumeClaim:
        claimName: log-claim
  containers:
  - image: nginx:alpine
    name: logger
    volumeMounts:
      - name: log
        mountPath: /var/www/nginx
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}




Q2:We have deployed a new pod called secure-pod and a service called secure-service. Incoming or Outgoing connections to this pod are not working.
Troubleshoot why this is happening.

Make sure that incoming connection from the pod webapp-color are successful.
Important: Don't delete any current objects deployed.
Important: Don't Alter Existing Objects!
Connectivity working?


Ans:
=>kubectl exec -it webapp-color -- sh
=>/opt # nc -v -z -w 2 secure-service 80

=>kubectl get netpol
=>kubectl describe netpol

=>kubectl get netpol default-deny -oyaml>netpol.yaml
Modify adn add posd selector:

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  creationTimestamp: "2023-07-04T16:15:39Z"
  generation: 1
  name: my-netpol
  namespace: default
  resourceVersion: "7919"
  uid: c63c2815-bb39-48ef-b114-17e941c8356b
spec:
  podSelector:
    matchLabels:
      run: secure-pod
  policyTypes:
  - Ingress
  ingress:
    - from:
        - podSelector:
            matchLabels:
              name: webapp-color
      ports:
        - protocol: TCP
          port: 80
status: {}




Q3:Create a pod called time-check in the dvl1987 namespace. This pod should run a container called time-check that uses the busybox image.
Create a config map called time-config with the data TIME_FREQ=10 in the same namespace.
The time-check container should run the command: while true; do date; sleep $TIME_FREQ;done and write the result to the location /opt/time/time-check.log.
The path /opt/time on the pod should mount a volume that lasts the lifetime of this pod.

Pod time-check configured correctly?

Ans:

=>kubectl create ns dvl1987
=>kubectl create cm time-config -n dvl1987 --from-literal=TIME_FREQ=10

=>kubectl run time-check --image=busybox --dry-run=client -oyaml>podtime.yaml
Now add Volume commdnd and EnvInfo:

cat podtime.yaml 
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: time-check
  name: time-check
  namespace: dvl1987
spec:
  volumes:
    - name: log-volume
      emptyDir: {}
  containers:
  - image: busybox
    name: time-check
    command: ["/bin/sh", "-c", "while true; do date; sleep $TIME_FREQ;done > /opt/time/time-check.log"]
    volumeMounts:
      - name: log-volume
        mountPath: /opt/time
    env:
      - name: TIME_FREQ
        valueFrom:
          configMapKeyRef:
            name: time-config
            key: TIME_FREQ
    resources: {}





Q4:Create a new deployment called nginx-deploy, with one single container called nginx, image nginx:1.16 and 4 replicas.
The deployment should use RollingUpdate strategy with maxSurge=1, and maxUnavailable=2.
Next upgrade the deployment to version 1.17.
Finally, once all pods are updated, undo the update and go back to the previous version.


Deployment created correctly?
Was the deployment created with nginx:1.16?
Was it upgraded to 1.17?
Deployment rolled back to 1.16?

Ans:

=>kubectl create deploy nginx-deploy --image=nginx:1.16 --replicas=4 --dry-run=client -oyaml>dep.yaml
Now add Stategy:
cat dep.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: nginx-deploy
  name: nginx-deploy
spec:
  replicas: 4
  selector:
    matchLabels:
      app: nginx-deploy
  strategy:
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 2
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: nginx-deploy
    spec:
      containers:
      - image: nginx:1.16
        name: nginx
        resources: {}
status: {}

=>kubectl set image deployment nginx-deploy nginx=nginx:1.17
=>kubectl rollout undo deployment nginx-deploy



Q5:Create a redis deployment with the following parameters:
Name of the deployment should be redis using the redis:alpine image. It should have exactly 1 replica.
The container should request for .2 CPU. It should use the label app=redis.
It should mount exactly 2 volumes.

a. An Empty directory volume called data at path /redis-master-data.
b. A configmap volume called redis-config at path /redis-master.
c. The container should expose the port 6379.


The configmap has already been created.
Deployment created correctly?


Ans:
=>kubectl create deploy redis --image=redis:alpine --replicas=1 --dry-run=client -oyaml>deprd.yaml
Now need to add volume and cpu resource:

cat deprd.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: redis
  name: redis
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: redis
    spec:
      volumes:
        - name: data
          emptyDir: {}
        - name: redis-config
          configMap:
            name: redis-config
      containers:
      - image: redis:alpine
        name: redis
        resources:
          requests:
            cpu: "0.2"
        volumeMounts:
          - name: data
            mountPath: /redis-master-data
          - name: redis-config
            mountPath: /redis-master
        ports:
          - containerPort: 6379
status: {}




=================================================
LIGHTNING LABS-02
=================================================

Q1:We have deployed a few pods in this cluster in various namespaces. Inspect them and identify the pod which is not in a Ready state. Troubleshoot and fix the issue.
Next, add a check to restart the container on the same pod if the command ls /var/www/html/file_check fails. This check should start after a delay of 10 seconds and run every 60 seconds.


You may delete and recreate the object. Ignore the warnings from the probe.
Task completed correctly?


Ans:
=>kubectl get po -A 
Check reday status for problem pod
=>kubectl describe po  nginx1401 -n dev1401

=>kubectl get po  nginx1401 -n dev1401 -oyaml>pod.yaml
Check the problem add editional fumction:
Update this section:
spec:
  containers:
  - image: kodekloud/nginx
    imagePullPolicy: IfNotPresent
    name: nginx
    ports:
    - containerPort: 9080
      protocol: TCP
    readinessProbe:
      failureThreshold: 3
      httpGet:
        path: /
        port: 9080
        scheme: HTTP
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 1
    livenessProbe:
      exec: 
        command:
          - ls
          - /var/www/html/file_check
      initialDelaySeconds: 10
      periodSeconds: 60
    resources: {}


=>k replace -f pod.yaml --force

Q2:Create a cronjob called dice that runs every one minute. Use the Pod template located at /root/throw-a-dice. The image throw-dice randomly returns a value between 1 and 6. The result of 6 is considered success and all others are failure.

The job should be non-parallel and complete the task once. Use a backoffLimit of 25.

If the task is not completed within 20 seconds the job should fail and pods should be terminated.


You don't have to wait for the job completion. As long as the cronjob has been created as per the requirements.

Cronjob created correctly?


Ans:
Copy the base line cronjob code from documentation adn this:
cat cron.yaml 
apiVersion: batch/v1
kind: CronJob
metadata:
  name: dice
spec:
  schedule: "*/1 * * * *"
  jobTemplate:
    spec:
      completions: 1
      backoffLimit: 25
      activeDeadlineSeconds: 20
      template:
        spec:
          containers:
          - name: dice
            image: kodekloud/throw-dice
            imagePullPolicy: IfNotPresent
          restartPolicy: Never




Q3:Create a pod called my-busybox in the dev2406 namespace using the busybox image. The container should be called secret and should sleep for 3600 seconds.
The container should mount a read-only secret volume called secret-volume at the path /etc/secret-volume. The secret being mounted has already been created for you and is called dotfile-secret.
Make sure that the pod is scheduled on controlplane and no other node in the cluster.

Pod created correctly?

Ans:
=>kubectl run my-busybox --image=busybox --dry-run=client -oyaml>podbusy.yaml
Add volume:
cat podbusy.yaml 

apiVersion: v1
kind: Pod
metadata:
  name: my-busybox
  namespace: dev2406
spec:
  nodeName: controlplane
  volumes:
    - name: secret-volume
      secret:
        secretName: dotfile-secret
  containers:
    - name: secret
      image: busybox
      command:
         - sleep
         - "3600"
      volumeMounts:
        - name: secret-volume
          readOnly: true
          mountPath: /etc/secret-volume

OR | as Ans:
Use the following YAML file to create the pod:

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: my-busybox
  name: my-busybox
  namespace: dev2406
spec:
  volumes:
  - name: secret-volume
    secret:
      secretName: dotfile-secret
  nodeSelector:
    kubernetes.io/hostname: controlplane
  containers:
  - command:
    - sleep
    args:
    - "3600"
    image: busybox
    name: secret
    volumeMounts:
    - name: secret-volume
      readOnly: true
      mountPath: "/etc/secret-volume"

      

Q4:Create a single ingress resource called ingress-vh-routing. The resource should route HTTP traffic to multiple hostnames as specified below:

The service video-service should be accessible on http://watch.ecom-store.com:30093/video

The service apparels-service should be accessible on http://apparels.ecom-store.com:30093/wear


Here 30093 is the port used by the Ingress Controller

Ingress resource configured correctly?

Ans:
Get a ingress from documentation and update this:
cat ingress.yaml 
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress-vh-routing
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  ingressClassName: nginx-example
  rules:
    - host: watch.ecom-store.com
      http:
        paths:
          - pathType: Prefix
            path: "/video"
            backend: 
              service:
                name: video-service
                port: 
                  number: 8080
    - host: apparels.ecom-store.com
      http:
        paths:
          - pathType: Prefix
            path: "/wear"
            backend:
              service:
                name: apparels-service
                port:
                  number: 8080

OR | as Ans:
Use the following YAML to create the ingress resource:

---
kind: Ingress
apiVersion: networking.k8s.io/v1
metadata:
  name: ingress-vh-routing
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - host: watch.ecom-store.com
    http:
      paths:
      - pathType: Prefix
        path: "/video"
        backend:
          service:
            name: video-service
            port:
              number: 8080
  - host: apparels.ecom-store.com
    http:
      paths:
      - pathType: Prefix
        path: "/wear"
        backend:
          service:
            name: apparels-service
            port:
              number: 8080



Q5:A pod called dev-pod-dind-878516 has been deployed in the default namespace. Inspect the logs for the container called log-x and redirect the warnings to /opt/dind-878516_logs.txt on the controlplane node
Redirect warnings to file


Ans:
=>kubectl logs dev-pod-dind-878516 -c log-x | grep WARNING > /opt/dind-878516_logs.txt
=>cat /opt/dind-878516_logs.txt





















##mok
=================================================
##MockExam-1         
================================================= 

Q1:Deploy a pod named nginx-448839 using the nginx:alpine image.

Ans: 
=>kubectl run nginx-448839 --image=nginx:alpine


Q2:Create a namespace named apx-z993845
=>kunectl create ns apx-z993845


Q3:Create a new Deployment named httpd-frontend with 3 replicas using image httpd:2.4-alpine

Ans:
=>kubectl create deployment httpd-frontend --image=httpd:2.4-alpine --replicas=3



Q4:Deploy a messaging pod using the redis:alpine image with the labels set to tier=msg.
Ans:
=>kubectl run messaging --image=redis:alpine -l tier=msg


Q5:A replicaset rs-d33393 is created. However the pods are not coming up. Identify and fix the issue.
Once fixed, ensure the ReplicaSet has 4 Ready replicas.

Ans:
=>kubectl describe rs rs-d33393 
=>kubectl edit rs rs-d33393 
With a- single commd Edit Operation done
=>kubectl delete pod -l name=busybox-pod
For delete all running pod by label



Q6:Create a service messaging-service to expose the redis deployment in the marketing namespace within the cluster on port 6379.

Ans:
=>kubectl expose deployment redis --port=6379 --name=messaging-service -n marketing



Q7:Update the environment variable on the pod webapp-color to use a green background.

Ans:
=>kubectl get po webapp-color -oyaml>pod1.yaml
=>kubectl replace -f pod1.yaml --force



Q8:Create a new ConfigMap named cm-3392845. Use the spec given on the below.
Ans:
=>kubectl create configmap cm-3392845 --from-literal=DB_NAME=SQL3322 --from-literal=DB_HOST=sql322.mycompany.com --from-literal=DB_PORT=3306
=>kubectl describe cm cm-3392845



Q9:Create a new Secret named db-secret-xxdf with the data given (on the below).

Ans:
=>kubectl create secret generic db-secret-xxdf --from-literal=DB_Host=sql01 --from-literal=DB_User=root --from-literal=DB_Password=password123


Q10:Update pod app-sec-kff3345 to run as Root user and with the SYS_TIME capability.

Ans:

sepc:
  securityContext:
    runAsUser: 0

Actually Container:
    securityContext:
      capabilities:
        add: ["SYS_TIME"]


Q11:Export the logs of the e-com-1123 pod to the file /opt/outputs/e-com-1123.logs

Ans:
=>k logs e-com-1123 -n e-commerce>/opt/outputs/e-com-1123.logs



Q12:Create a Persistent Volume with the given specification.

Ans:
Collect From Documentation:

apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-analytics
spec:
  capacity:
    storage: 100Mi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteMany
  hostPath:
    path: "/pv/data-analytics"


Q14:Create a redis deployment using the image redis:alpine with 1 replica and label app=redis. Expose it via a ClusterIP service called redis on port 6379. Create a new Ingress Type NetworkPolicy called redis-access which allows only the pods with label access=redis to access the deployment.

Ans:
=>kubectl create deploy redis --image=redis:alpine --replicas=1
=>kubectl expose deployment redis --name=redis --port=6379 --target-port=6379

then create a netpol using documentation.
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: redis-access/
  namespace: default
spec:
  podSelector:
    matchLabels:
      app: redis
  policyTypes:
    - Ingress
  ingress:
    - from:
        - podSelector:
            matchLabels:
              access: redis
      ports:
        - protocol: TCP
          port: 6379



Q15:Create a Pod called sega with two containers:
Container 1: Name tails with image busybox and command: sleep 3600.
Container 2: Name sonic with image nginx and Environment variable: NGINX_PORT with the value 8080.

Ans:
apiVersion: v1
kind: Pod
metadata:
  name: sega
spec:
  containers:
  - name: tails
    image: busybox
    command: 
      - sleep
      - "3600"
  - name: sonic
    image: nginx
    env:
      - name: NGINX_PORT
        value: "8080"




=================================================
##MockExam-2
================================================= 


Q1:Create a deployment called my-webapp with image: nginx, label tier:frontend and 2 replicas. 
Expose the deployment as a NodePort service with name front-end-service , port: 80 and NodePort: 30083

Ans:
=>kubectl create deployment my-webapp --image=nginx --replicas=2 --dry-run=client -oyaml>mydep.yaml
=>kubectl expose deployment my-webapp --name front-end-service --type NodePort --port=80 --dry-run=client -oyaml>svc1.yaml
Then add this nodePort: 30083


Q2:Add a taint to the node node01 of the cluster. Use the specification below:
key: app_type, value: alpha and effect: NoSchedule
Create a pod called alpha, image: redis with toleration to node01.

Ans:
=>kubectl taint node node01 app_type=alpha:NoSchedule
=>kubectl run alpha --image=redis --dry-run=client -oyaml>pod1.yaml
Now add tolerations on spec: section.

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: alpha
  name: alpha
spec:
  tolerations:
    - effect: NoSchedule
      key: app_type
      value: alpha
  containers:
  - image: redis
    name: alpha
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}


Q3:Apply a label app_type=beta to node controlplane. Create a new deployment called beta-apps with image: nginx and replicas: 3. 
Set Node Affinity to the deployment to place the PODs on controlplane only.
NodeAffinity: requiredDuringSchedulingIgnoredDuringExecution

Ans:
=>kubectl label node controlplane app_type=beta
=>kubectl create deploy beta-apps --image=nginx --replicas=3 --dry-run=client -oyaml>mydep.yaml

Add this:
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: app_type
                operator: In
                values: ["beta"]

=>k get po -o wide



Q4:Create a new Ingress Resource for the service my-video-service to be made available at the 
URL: http://ckad-mock-exam-solution.com:30093/video.

To create an ingress resource, the following details are: -
annotation: nginx.ingress.kubernetes.io/rewrite-target: /

host: ckad-mock-exam-solution.com
path: /video

Once set up, the curl test of the URL from the nodes should be successful: HTTP 200

Ans:
=>kubectl create ingress ingress --rule="ckad-mock-exam-solution.com/video*=my-video-service:8080" --dry-run=client -oyaml>ing.yaml




Q5:We have deployed a new pod called pod-with-rprobe. This Pod has an initial delay before it is Ready. Update the newly created pod pod-with-rprobe with a readinessProbe using the given spec
httpGet path: /ready
httpGet port: 8080

Ans:
=>kubectl get pod pod-with-rprobe -oyaml>pod.yaml
Edit this and add this on containers: section:
   readinessProbe:
      httpGet:
        path: /ready
        port: 8080



Q6:Create a new pod called nginx1401 in the default namespace with the image nginx. 
Add a livenessProbe to the container to restart it if the command ls /var/www/html/probe fails. 
This check should start after a delay of 10 seconds and run every 60 seconds.
You may delete and recreate the object. Ignore the warnings from the probe.

Ans:

apiVersion: v1
kind: Pod
metadata: 
  name: nginx1401
spec:
  containers:
    - image: nginx
      name: nginx1401
      livenessProbe:
        exec:
          command: ["ls /var/www/html/probe"]
        initialDelaySeconds: 10
        periodSecond: 60





Q7:Create a job called whalesay with image docker/whalesay and command "cowsay I am going to ace CKAD!".
completions: 10
backoffLimit: 6
restartPolicy: Never
This simple job runs the popular cowsay game that was modifed by docker

Ans:
=> kubectl create job --image=docker/whalesay whalesay --dry-run=client -oyaml>myjob.yaml
Then:
apiVersion: batch/v1
kind: Job
metadata:
  creationTimestamp: null
  name: whalesay
spec:
  completions: 10
  backoffLimit: 6
  template:
    metadata:
      creationTimestamp: null
    spec:
      containers:
      - image: docker/whalesay
        name: whalesay
        resources: {}
        command:
          - sh
          - -c
          - "cowsay I am going to ace CKAD!"
      restartPolicy: Never
status: {}



Q8:Create a pod called multi-pod with two containers.

Container 1:
name: jupiter, image: nginx

Container 2:
name: europa, image: busybox
command: sleep 4800

Environment Variables:
Container 1:
type: planet
Container 2:
type: moon

Ans:

apiVersion: v1
kind: Pod
metadata:
  name: multi-pod
spec:
  containers:
  - name: jupiter
    image: nginx
    env:
      - name: type
        value: planet
  - name: europa
    image: busybox
    command: ["/bin/sh","-c","sleep 4800"]
    env:
      - name: type
        value: moon


Q9:Create a PersistentVolume called custom-volume with size: 50MiB reclaim policy:retain, Access Modes: ReadWriteMany and hostPath: /opt/data

Ans:
Get from documetation, and eidt:

apiVersion: v1
kind: PersistentVolume
metadata:
  name: custom-volume
spec:
  capacity:
    storage: 50Mi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteMany
  persistentVolumeReclaimPolicy: Retain
  hostPath:
    path: /opt/data








































##tut
=================================================
##CoreConcept         
================================================= 

Two type Node: 
-------------------------------------------------
- Master
       It has install kube-apiserver and this is  make it master node.
       Master also containe: etcd, controller, scheduler etc.
- Worker
       Install container runtime or rkt or cri-o etc.
       Its contain kubelet component what is response master node kube-apiserver.


kubectl CLI: 
      Its a commandline utility, user to deploy manage application on kubernetes cluster.
      Get the cluster information, status the other node etc.
      Using command like:
        =>kubectl run hellow-world-app
        =>kubectl cluster-info
        =>kubectl get nodes


When instal kubernetes, install this component:
-------------------------------------------------
- API Server
  API Server act as a frontend of kubernetes, user, devece, terminal interface etc.

- etcd
  Key-value prayers storage, Store all data for managing cluster.

- Scheduler
  Scheduler Responsibel for distribute work to across multiple node in cluster.
  Like createing container and assign this to approvaid node.

- Controller
  Controller is the brain behind the workstation, its notice when node,container,endpoint gose down, 
  and make decision bring new one. and find right node for right pod and other component.

- ContainerRuntime
  ContainerRuntime is the underlying software for running container(Docker, containerD).

- kubelet
  Kubelet is an agent present on each worker node, make sure the container running on node as aspected.



Docker vs ContainerD 
-------------------------------------------------
Kubernetes->Docker was titly couple at first time, then kubernetes get popularity
and Other vendor want to join like rkt etc. 
This time kubernetes make an interface for CRI(Container runtime Interface) and now 
every vendor can join if it mentain OCI(Open Container Initiative=imagespace,runtimespace).

Docker was before OCI this why it not flow the OCI, to help this use a tool called dockershim with kubernetes.

Docker component (CLI,api, volumes,auth,security,build) working with containerD
and ContainerD support OCI so, from kubernetes v1.24 remove dockershim, only working with containerD.

ContainerD - CLI = ctr 
ctr come with containerD not friendly only support limited feature for debuging purpose.
Like:
=>ctr image pull docker.io/library/redis
=>ctr run docker.io/library/redis redis

A alternetis of ctr is nerdctl, its provide docker like cli for containerD.
Its support newest feature in containerD, lazy pulling, dokcer composer, namespace in kubernetes etc.

NERDCTL command like docker:
=>nerdctl run --name redis redis
=>nerdctl run --name mywebserver -p8181:80 -d nginx

CLI - CRICTL
crictl provides a CLI for CRI compatable container runtime, install separately.
User to inspect, debug container runtimes work accross dirrerent runtime and mantaine develop by kubernetes.

kubernetes->CRI->crictl->| rkt, containerD etc.
=>crictl pull nginx
=>crictl images
=>crictl ps -a
=>crictl logs 567890987654456789
=>crictl pod





##POD
-------------------------------------------------
Kubernetes use yaml defination file as inpur to kubernetes.

All defination file flow same strecture, all file 4 top/root lavel field, this field are required.

apiVersion: v1               | Object version
kind: Pod                    | Object type       
metadata:                    | Data about the object
spac:                        | Object spacefication secrion, provide aditional iinfo about object.


apiVersion: v1               | Object version
kind: Pod                    | Object type       
metadata:                    | Data about the object
  name: my-pod
  labels:
    app: myapp
    type: back-end
spac:                        | Object spacefication secrion, provide aditional iinfo about object.

Wen a pod is start it accessable within from of its host machine, for out side user need to add 
Another component of kubernatis called Service.




##Solution:01 | Pod
-------------------------------------------------
=>kubectl run nginx --image=nginx

What is the image used to create the new pods?
=>kubectl describe pod newpods-j44dw | grep -i image

Show pod IP, NODe
=>k get pod -o wide

How many container part of the webapp pod and What images are used in the new webapp pod?
=>kubectl describe pod webapp
Check the container section,and look at the images used.


Why do you think the container agentx in pod webapp is in error?
=>kubectl describe pod webapp 
and look under the events section, An image by that name does not exist in DockerHub.



1)Which nodes are these pods placed on?
=>kubectl get pod -o wide

2)What does the READY column in the output of the kubectl get pods command indicate?
runing container/total container.

3)Create a new pod with the name redis and with the image redis123.
=>kubectl run redis --image=redis123 --dry-run=client -o yaml
=>kubectl run redis --image=redis123 --dry-run=client -o yaml > redis-definition.yaml

4)Now change the image on this pod to redis.
=>kubectl edit pod redis 
For Edit/Change pod image is easy done by =>kubectl edit command with in a single command.
No need to take other step.


Edit current running pod, like change image
5) kubectl edit pod myng
OR
6) Edit then pod defanation yaml file and run again
=>Kubectl apply -f mypod.yaml


Generate another pod defenation yaml file from current pod
=>>kubectl get pod mypod -o yaml
=>kubectl get pod mypod -o yaml> pod.yaml


Remember, you CANNOT edit specifications of an existing POD other than the below.
spec.containers[*].image
spec.initContainers[*].image
spec.activeDeadlineSeconds
spec.tolerations

For edit other file generate another yaml file from current pod status and update it, delete perivious pod and run new one.
=>kubectl get pod mypod -o yaml> pod.yaml
=>kubectl apply -f pod.yaml





##ReplicaSet
-------------------------------------------------
RsVersion:  apiVersion: apps/v1

Replica controller and ReplicaSet are same purpose, controller is older and ReplicaSet are newar.
ReplicaSet make certain number of pod allows running.

Event you need only a single pod you cand user replicaSet, it keep allow one podn runing in your system.

One of mazor Diffeetnt of ReplicaSet and RsController is selector property, what is optional in RsController.
Using this selector ReplicaSet select the pod on its own area.



Simple ReplicaSet:
apiVersion:apps/v1
kind: ReplicaSet
metadata:
  name: replicaset-1
spec:
  replicas: 2
  selector:
    matchLabels:
      tier: frontend
  template:
    metadata:
      labels:
        tier: frontend
    spec:
      containers:
      - name: nginx
        image: nginx


For finding any issu in defination yaml file, first run the yamldate
=>kubectl create -f myreplicaset.yaml
And now check the error message


BasicCommand:
=>kubectl create -f rsfilw.yaml
=>kubectl get rs
=>kubectl delete rs myrsx
=>kubectl replace -f myrs.yaml
=>kubectl scale -replicas=5 -f myrsfile.yaml




##Update/Scale ReplicaSet
-------------------------------------------------
1) Update number of replica(replicas:5) in yaml file and run
=>kubectl replace -f myreplica.yaml
OR
2) Run
=>kubectl scale --replicas=5 -f myreplica.yaml
OR
3) Run with replical type(replicaset) and name (myreplicaset) 
=>kubectl scale --replicas=5 replicaset myreplicaset
2 and 3 Not made change the original defination file

There are other approce to Update replicaset count.this is advance.

Check ReplicaSet info like: uses image, etc
=>kubectl describe rs new-replica-set

Check the pod status of ReplicaSet
=>kubectl get rs new-replica-set


If need to Update ReplicaSet like Image change, then its does not automatically re-create the  pod.
First you can eidt current rs
=>kubectl edit rs new-replica-set 
Then delete existing pod
=>kubectl delete pod --all
and RS will create new pod with new Image
OR
Delete old rs and create again


=>kubectl get rs myreplicaset1 -o yaml
=>kubectl get rs myreplicaset1 -o yaml>myrs2.yaml



##Solution:02 | ReplicaSet
-------------------------------------------------
1)How many ReplicaSets exist on the system?
=>kubectl get replicaset

2)What is the image used to create the pods in the new-replica-set?
=>kubectl describe rs new-replica-set


3)Why do you think the PODs are not ready in replicaset?
=>kubectl get rs
=>kubectl get pods

=>kubectl get events --field-selector involvedObject.name=podName
=>kubectl get events | grep objName
OR
=>kubectl logs podName
=>kubectl describe po new-replica-set-pcn6t

4)Delete the two newly created ReplicaSets - replicaset-1 and replicaset-2
=>kubectl delete rs replicaset-1 replicaset-2


5)Fix the original replica set new-replica-set to use the correct busybox image.
Either delete and recreate the ReplicaSet or Update the existing ReplicaSet and then delete all PODs, so new ones with the correct image will be created.

=>kubectl get rs
=>kubectl edit rs new-replica-set
=>k delete pod --all

Edit/Change Image name from Rs is done only one command kubectl edit rs rsName, but need to delete previously created pod.
Because old pod running on with old image, for effact update image, need to delte all pod created by this rs,
If delte all pod using =>kubectl delte pod --all command, thend this rs will create new pod wfith update image.




6)Scale the ReplicaSet to 5 PODs.
Use kubectl scale command or edit the replicaset using kubectl edit replicaset.
=>kubectl scale replicaset new-replica-set --replicas=5
OR
=>kubectl edit rs new-replica-set
Update new value in:
spec:
  replicas: 2
  selector:



##Deployment
-------------------------------------------------
Deployment defination file as ReplicaSet just kind willbe Deployment.
Deployment mainly provide different type approce for pod Update like: Recreate, RollingUpdate.


=>kubectl create -f mydeploy.yaml
Its create three Object:

=>k get deploy
=>k get rs
=>k get pods
OR
=>k get all

apiVersion: apps/v1
kind: Deployment
metadata:
  name: deployment-1
spec:
  replicas: 2
  selector:
    matchLabels:
      name: busybox-pod
  template:
    metadata:
      labels:
        name: busybox-pod
    spec:
      containers:
      - name: busybox-container
        image: busybox
        command:
        - sh
        - "-c"
        - echo Hello Kubernetes! && sleep 3600

With Deployments you can easily edit any field/property of the POD template. 

=>kubectl create deployment --help
=>kubectl create deployment httpd-frontend --image=httpd:2.4-alpine --replicas=3 --dry-run=client -o yaml>mydeployment1.yaml


Generate a yaml file from current deployment
=>kubectl get deployment mydeployment -o yaml
=>kubectl get deployment httpd-frontend -o yaml>mydeployment2.yaml



##Solution:03 Deployment
-------------------------------------------------

1)What is the image used to create the pods in the new deployment?
=>kubectl describe deploy frontend-deployment 

2)Why do you think the deployment is not ready?

=>kubectl get deployment
=>kubectl describe deployment
=>kubectl get pod
=>kubectl get events --field-selector involvedObject.name=podName

3)Create a new Deployment with the below attributes using your own deployment definition file.
Name: httpd-frontend;
Replicas: 3;
Image: httpd:2.4-alpine
Name: httpd-frontend
Replicas: 3
Image: httpd:2.4-alpine

=>kubectl create deployment -h
=>kubectl create deployment httpd-frontend --image=httpd:2.4-alpine --replicas=3 --dry-run=client -o yaml>mydep.yaml





##NameSpace
-------------------------------------------------
Call one service to other service in different namespace:
app-service call db-service in DEV namespace:
  app-service.connect("db-service.dev.svc.cluster.local")
      
         db-service     .dev        .svc       .cluster.local
Here:    serviceName    Namespace   Service    Default Domain name in k8      v            

Switch Namespace:
=>kubectl config set-context $(kubectl config current-contest) --namespace=dev
=>kubectl get pods --all-namespace

=>kubectl get all

=>kubectl get namespace
=>kubectl get pod --namespace=research
=>kubectl get pod -n=research
=>kubectl run redis --image=redis -n=research


=>kubectl get pod -A
=>kubectl get pod --all-namespaces
Show all pod with namespace

=>kubectl get pods -n=marketing
=>kubectl get svc -n=marketing
Show service of a specific namespace

We can set Resource Quota for specifice namespace.
like:
apiVersion: v1
kind: ResourceQuota
metadata: 
  name: compute-quota
  namespace: dev
spac:
  hard:
    prods: "10"
    request.cpu: "4"
    request.memory: "5Gi"
    limits.cpu: "10"
    limits.memory: "10Gi"




##Solution:04 NameSpace
-------------------------------------------------

=>k get namespace --no-headers | wc -l

=>k get pod -A
Get all pod in all namespacw


1)How many Namespaces exist on the system?
=>kubectl get ns

2)How many pods exist in the research namespace?
=>kubectl get pod -n research

3)Create a POD in the finance namespace.
Name: redis
Image Name: redis
=>kubectl run redis --image=redis --namespace finance

4)Which namespace has the blue pod in it?
=>kubectl get pods --all-namespaces | grep blue


5)What DNS name should the Blue application use to access the database db-service in the dev namespace?
You can try it in the web application UI. Use port 6379.

The FQDN consists of the <service-name>.<namespace>.svc.cluster.local format.
like:my-service.my-namespace.svc.cluster.local
=>db-service.dev.svc.cluster.local




Creating Kubernetes objects imperatively
-------------------------------------------------
There are two main ways to manage Kubernetes objects: 
  imperative (with kubectl commands) and 
  declarative (by writing manifests and then using kubectl apply).


=>kubectl run --help
=>kubectl run redis --image=redis:alpine --labels="tier=db"
=>get pod --show-labels


Create a service redis-service to expose the redis application within the cluster on port 6379.
Use imperative commands.
=>kubectl expose pod redis --port=6379 --name=redis-service
=>kubectl describe svc redis-service
OR
=>kubectl create service --help
=>kubectl create service clusterip --help
=>kubectl create service clusterip redis-service --tcp=6379:6379


expose:
=>kubectl expose --help
"expose" command user for expose existing pod for expose.
=>kubectl expose pod redis --port=6379 --name=redis-service
OR 
Create pod and service at a same time
=>kubectl run httpd --image=httpd:alpine --port=80 --expose=true
This command create pod as well as service with same name with labels and bind service with pod at a time




##Solution:06 | IMPERATIVE COMMANDS
-------------------------------------------------



1)Deploy a pod named nginx-pod using the nginx:alpine image.
Name: nginx-pod
Image: nginx:alpine

=>kubectl run nginx-pod --image=nginx:alpine

2)Deploy a redis pod using the redis:alpine image with the labels set to tier=db.
=>kubectl run --help
=>kubectl run redis --image=redis:alpine --labels="tier=db"
=>get pod --show-labels


3)Create a service redis-service to expose the redis application within the cluster on port 6379.
Use imperative commands.
Service: redis-service
Port: 6379
Type: ClusterIP

=>k expose pod -h
=>kubectl expose pod redis --port=6379 --name=redis-service
=>kubectl describe svc redis-service
OR
=>kubectl create service --help
=>kubectl create service clusterip --help
=>kubectl create service clusterip redis-service --tcp=6379:6379


4)Create a deployment named webapp using the image kodekloud/webapp-color with 3 replicas.
Try to use imperative commands only. Do not create definition files.
Name: webapp
Image: kodekloud/webapp-color
Replicas: 3

=>kubectl create deployment --help
=>kubectl create deployment webapp --image=kodekloud/webapp-color --replicas=3


5)Create name space and Create a new deployment called redis-deploy in the dev-ns namespace with the redis image. It should have 2
=>kubectl run custom-nginx --image=nginx --port=8080
=>kubectl create namespace dev-ns
=>kubectl create deployment redis-deploy --image=redis --replicas=2 -n=dev-ns


6)Create a pod called httpd using the image httpd:alpine in the default namespace. 
Next, create a service of type ClusterIP by the same name (httpd). The target port for the service should be 80.

=>kubectl run httpd --image=httpd:alpine --port=80 --expose
OR
=>kubectl run httpd --image=httpd:alpine --port=80
=>kubectl expose pod httpd --port=80 --name=httpd
OR
=>kubectl create service clusterip --help
=>kubectl create service clusterip httpd --tcp=5678:80

=>kubectl get pod --show-labels
=>kubectl edit pod httpd
Update the lavel as service





=================================================
##Configuration         
================================================= 


##Command and Args in Docker 
-------------------------------------------------
Container unlike vM its a specific task, when finish the task its tarminated automatically.
Container remain unitl a process or task inside it running on it. 
If a web service in container stop or cursh container are exists.


NGINX Image:
-------------------------------------------------
# Pull the minimal Ubuntu image
FROM ubuntu

# Install Nginx
RUN apt-get -y update && apt-get -y install nginx

# Copy the Nginx config
COPY default /etc/nginx/sites-available/default

# Expose the port for access
EXPOSE 80/tcp

# Run the Nginx server
CMD ["/usr/sbin/nginx", "-g", "daemon off;"]



UBUNTU Image:
-------------------------------------------------
# Use the official Ubuntu base image
FROM ubuntu:latest

# Update package lists and upgrade existing packages
RUN apt-get update && apt-get upgrade -y

# Your additional instructions here (install packages, configure settings, etc.)

# Clean up the package cache to reduce image size
RUN apt-get clean && rm -rf /var/lib/apt/lists/*

# Set the default command when running the container
CMD ["bash"]



This Nginx image and Ubuntu image has tow dirrerent CMD command:

  Nginx start a process in side container using this command 
  CMD ["/usr/sbin/nginx", "-g", "daemon off;"]

  Whenre Ubuntu container run this bash Command
  CMD ["bash"]



Bash is not a actual process, its a shell for listing inpur from tarminal.
Expect to receiving actuall process command, if not find then it exist.

By default docker not add a termional when a container run, and bash not found any terminal and it exited.

So, how prevent it: 

FirstWay:
Override the default command like this:
=>docker run ubuntu sleep 5

SecondWay:
Create your won image with a sleep command or other process start with the command.

From Ubuntu
CMD sleep 2


We can provide this command different way:

AsBash:
CMD:
command parameters

CMD ["command", "parameter1"]

OR
AsJSONArraty Formate:
CMD sleep 2
CMD ["sleep","5"]

Remember: when cmd will be JSON formate, First element of array have to be executable.
And Command and Arg not mixed as, 
CMD ["sleep 5"]
command and param should be different.

If this way i build by image like, and run it, it will be 5 sec. No need parse arge on run time.
From Ubuntu
CMD sleep 2

=>docker build -t my-ubuntu .
=>docker run my-ubuntu
Now if we wanto change the number of sleep thne we cand do it:(again overrides the command)
=>docker run my-ununtu sleep 10

But what about we cand only pass the number:
=>docker run my-ubunty 100

For this ENTRYPOINT Come into picture:

FROM Ubuntu
ENTRYPOINT["sleep"]

Now if run this:
=>docker run my-ubunty 100

100 will append on this sleep command like this: sleep 100

Thas the different between CMD and ENTRYPOINT. CMD replace, ENTRYPOINT append.

Now for default value of ENTRYPOINT:
FROM Ubuntu
ENTRYPOINT["sleep"]
CMD["5"]

Now if we pass param then it will override cmd default or it run with defafutl value
=>docker run my-ubunty | will be 5 second sleep
=>docker run my-ubunty 10 | will be 10 second sleep

Remembar: for combine both ENTRYPOINT and CMD must use JSON formate.
Then:
If you need to pass the ENTRYPOINT then:
=>docker run --entrypoint ping my-ubuntu google.com -T
This will override the docker file entry point value





##Command and Args in Kubernetes | Pod
-------------------------------------------------
A simeple Pod Defination file willbe using privious Docker image:
apiVersion: v1
kind: Pod
metadata:
  name: my-ubuntu-pod
spec:
  containers:
    - name: my-ununtu-cont
      image: my-ununtu
      args: ["10"]


If we want to Override the command form Pod file, it will be like this:
apiVersion: v1
kind: Pod
metadata:
  name: my-ubuntu-pod
spec:
  containers:
    - name: my-ununtu-cont
      image: my-ununtu
      command: ["ping"]
      args: ["google.com"]


Kubernetes "command" override Docker "ENTRYPOINT"
kubernetes "args"    override Docker "CMD"




=>kubectl run ub1 --image=ubuntu sleep 4000
If we pass value on inparageve way, then its reveive in pod as Args.

This is with args:
apiVersion: v1 
kind: Pod 
metadata:
  name: ub3 
spec:
  containers:
  - name: ubuntu
    image: ubuntu
    args:
       - "sleep"
       - "5000"
OR
apiVersion: v1
kind: Pod 
metadata:
  name: ub4
spec:
  containers:
  - name: ubuntu
    image: ubuntu
    args: ["sleep","5000"]


This is with command:
apiVersion: v1 
kind: Pod 
metadata:
  name: ub3 
spec:
  containers:
  - name: ubuntu
    image: ubuntu
    command:
       - "sleep"
       - "5000"
OR
apiVersion: v1
kind: Pod 
metadata:
  name: ub5
spec:
  containers:
  - name: ubuntu
    image: ubuntu
    command: ["sleep","5000"]



## Solution1: Pod command and Args
-------------------------------------------------
=>kubectl run ub1 --image=ubuntu sleep 4000

1)What is the command used to run the pod ubuntu-sleeper?
=>kubectl get pod ubntu-sleeper -o yaml
=>kubectl describe pod ubuntu-sleeper

2)Create a pod with the ubuntu image to run a container to sleep for 5000 seconds.
apiVersion: v1 
kind: Pod 
metadata:
  name: ubuntu-sleeper-2 
spec:
  containers:
  - name: ubuntu
    image: ubuntu
    command:  ["sleep"]
    args: ["5000"]

apiVersion: v1 
kind: Pod 
metadata:
  name: ubuntu-sleeper-2 
spec:
  containers:
  - name: ubuntu
    image: ubuntu
    args: ["sleep","5000"]

apiVersion: v1 
kind: Pod 
metadata:
  name: pod3 
spec:
  containers:
  - name: ubuntu
    image: ubuntu
    args:
      - "sleep"
      - "1200"

apiVersion: v1 
kind: Pod 
metadata:
  name: ubuntu-sleeper-3 
spec:
  containers:
  - name: ubuntu
    image: ubuntu
    command:
      - "sleep"
      - "1200"

apiVersion: v1 
kind: Pod 
metadata:
  name: webapp-green
  labels:
      name: webapp-green 
spec:
  containers:
  - name: simple-webapp
    image: kodekloud/webapp-color
    command: ["python", "app.py"]
    args: ["--color", "pink"]


SimpleDockerFile:
  FROM python:3.6-alpine
  RUN pip install flask
  COPY . /opt/
  EXPOSE 8080
  WORKDIR /opt
  ENTRYPOINT ["python", "app.py"]
  CMD ["--color", "red"]

FinalRunningCommandWillBe:
=>python app.py --color red



cat Dockerfile 
FROM python:3.6-alpine
RUN pip install flask
COPY . /opt/
EXPOSE 8080
WORKDIR /opt
ENTRYPOINT ["python", "app.py"]
CMD ["--color", "red"]

cat webapp-color-pod.yaml 
  apiVersion: v1 
  kind: Pod 
  metadata:
    name: webapp-green
    labels:
        name: webapp-green 
  spec:
    containers:
    - name: simple-webapp
      image: kodekloud/webapp-color
      command: ["--color","green"]

FinalRunningCommandWillBe:
=>--color green


cat Dockerfile 
  FROM python:3.6-alpine
  RUN pip install flask
  COPY . /opt/
  EXPOSE 8080
  WORKDIR /opt
  ENTRYPOINT ["python", "app.py"]
  CMD ["--color", "red"]

cat webapp-color-pod-2.yaml 
apiVersion: v1 
kind: Pod 
metadata:
  name: webapp-green
  labels:
      name: webapp-green 
spec:
  containers:
  - name: simple-webapp
    image: kodekloud/webapp-color
    command: ["python", "app.py"]
    args: ["--color", "pink"]

FinalRunningCommandWillBe:
=>python app.py --color pink




2)Update pod ubuntu-sleeper-3 to sleep for 2000 seconds.
=>kubectl get pod ubuntu-sleeper-3 -o yaml>newpod.yaml
Modify the newpod.yaml file and run again
OR
=>kubectl replace --force -f newpod.yaml
It will delete existing pod and create new one
OR
=>kubectl apply --force -f /tmp/kubectl-edit-219235037.yaml



Dockerfile with ENTRYPOINT and cmd
FROM python:3.6-alpine
RUN pip install flask
COPY . /opt/
EXPOSE 8080
WORKDIR /opt
ENTRYPOINT ["python", "app.py"]
CMD ["--color", "red"]

apiVersion: v1 
kind: Pod 
metadata:
  name: webapp-green
  labels:
      name: webapp-green 
spec:
  containers:
  - name: simple-webapp
    image: kodekloud/webapp-color
    command: ["python", "app.py"]
    args: ["--color", "pink"]


=>kubectl run webapp-green2 --image=kodekloud/webapp-color --command -- python app.py  -- --color=green

3)Create a pod with the given specifications. By default it displays a blue background. 
Set the given command line arguments to change it to green.
=>kubectl run webapp-green --image=kodekloud/webapp-color -- --color=green
OR
apiVersion: v1 
kind: Pod 
metadata:
  name: webapp-green
  labels:
      name: webapp-green 
spec:
  containers:
  - name: simple-webapp
    image: kodekloud/webapp-color
    args: ["--color", "green"]





##Environment Varibale | evn
-------------------------------------------------
Set Env varibale in Docker:
=>docker run -e APP_COLOR=green my-colorwebapp

Set Env varibale in Kubernetes pod:
apiVersion: v1 
kind: Pod 
metadata:
  name: webapp-green
  labels:
      name: webapp-green 
spec:
  containers:
  - name: simple-webapp
    image: kodekloud/webapp-color
    args: ["--color", "green"]
    
    env:
      - name: APP_COLOR
        value: green


In pod we can set env from different source:
env: 
  - name: APP_COLOR
    value: green
OR
env: 
  - name: APP_COLOR
    valueFrom:
      configMapKeyRef:
OR
env: 
  - name: APP_COLOR
    valueFrom:
      secretKeyRef:
  



 
##ConfigMap
-------------------------------------------------
When we have loat of evn data it is deficult to manage  those variable data.This is why ConfigMap.
For this create a ConfigMap using keyvalue payers and interact in pod:



Create a configmap both two way: Imparative and Diclarative:

=>kubectl create configmap --help
=>kubectl create configmap my-configmap --from-literal=key1=val1 --from-literal=key2=val2
Or from a keyvalue file  

Declarative:
apiVersion: v1
kind: ConfigMap
metadata: 
  name: my-configmap
data:
  APP_COLOR: blue
  APP_TYPE: prod

=>kubectl get configmap
=>kubectl describe configmap my-configmap



apiVersion: v1 
kind: Pod 
metadata:
  name: webapp-green
  labels:
      name: webapp-green 
spec:
  containers:
  - name: simple-webapp
    image: kodekloud/webapp-color
    args: ["--color", "green"]

    envFrom:
      - configMapRef:
          name: my-configmap



We can ingect the configmap: as evn, as a single value for a confifmap, or whole data at-once as a volume:
ENV: 
envFrom:
  - configMapRef:
    name: myapp-configmap

SINGLE ENV:
 containers:
  - env:
    - name: APP_COLOR
      valueFrom:
       configMapKeyRef:
         name: webapp-config-map
         key: APP_COLOR
    image: kodekloud/webapp-color
    name: webapp-color

OR as VOLUME:
volume:
  - name: my-app-volume
    configMap:
      name: my-configmap
    


SetSimpleEnvinPod:
apiVersion: v1
kind: Pod
metadata:
  name: webapp-color
spec:
  containers:
  - env:
    - name: APP_COLOR
      value: green
    image: kodekloud/webapp-color
    name: webapp-colo


SetSimpleEnvinPodFromConfigMP:
apiVersion: v1
kind: Pod
metadata:
  name: webapp-color
spec:
  containers:
  - env:
    - name: APP_COLOR
        valueFrom:
          configMapKeyRef:
            name: game-demo 
            key: player_initial_lives
    image: kodekloud/webapp-color
    name: webapp-colo

Whole configmapAtaTime:

apiVersion: v1
kind: Pod
metadata:
  name: dapi-test-pod
spec:
  containers:
    - name: test-container
      image: registry.k8s.io/busybox
      envFrom:
      - configMapRef:
          name: special-config


WholeSecretMapatATime:

FroamSecret:
apiVersion: v1
kind: Pod
metadata:
  name: envfrom-secret
spec:
  containers:
  - name: envars-test-container
    image: nginx
    envFrom:
    - secretRef:
        name: test-secret


FromSecretKeyRef:
apiVersion: v1
kind: Pod
metadata:
  name: envvars-multiple-secrets
spec:
  containers:
  - name: envars-test-container
    image: nginx
    env:
    - name: BACKEND_USERNAME
      valueFrom:
        secretKeyRef:
          name: backend-user
          key: backend-username
    - name: DB_USERNAME
      valueFrom:
        secretKeyRef:
          name: db-user
          key: db-username


##Solution | ConfigMap
-------------------------------------------------
What is the environment variable name set on the container in the pod?
=>kubectl describe pod webapp-color
=>kubectl get pod webapp-color -o yaml
=>kubectl exec webapp-color -- env


1)Update the environment variable on the POD to display a green background.
Note: Delete and recreate the POD. 
=>kubectl get pod webapp-color -o yaml>newpod.yaml
=>kubectl replace --force -f newpod.yaml


Identify the database host from the config map db-config?
=>kubectl describe configmap db-config


2)Create a new ConfigMap for the webapp-color POD. Use the spec given below?
  ConfigMap Name: webapp-config-map
  Data: APP_COLOR=darkblue
  Data: APP_OTHER=disregard

=>kubectl create configmap --help
=>kubectl create configmap my-config --from-literal=key1=config1 --from-literal=key2=config2

=>kubectl create configmap webapp-config-map --from-literal=APP_COLOR=darkblue --from-literal=APP_OTHER=disregard
OR
apiVersion: v1
data:
  APP_COLOR: darkblue
  APP_OTHER: disregard
kind: ConfigMap
metadata:
  creationTimestamp: null
  name: webapp-config-map

=>kubectl get cm

We can Edit configmap any falue with a single command using kubectl edit command.
=>kubectl edit configmap webapp-config-map 



3)Update the environment variable on the POD to use only the APP_COLOR key from the newly created ConfigMap.
Note: Delete and recreate the POD.
  Pod Name: webapp-color
  ConfigMap Name: webapp-config-map

=>k get po -o yaml>newpod.yaml
=>vi ewpod.yaml
add this:
  spec:
    containers:
    - env:
      - name: APP_COLOR
        valueFrom:
            configMapKeyRef:
              name: webapp-config-map
              key: APP_COLOR

=>kubectl replace --force -f newpod.yaml





##Secret
-------------------------------------------------
There are two step to use a secret: Create a secret, Ingect it on pod.

For pass data to a pod we can use configmap, but configmap data are plane text.
For database credentials and this type sensitive data should not keep as planetex formate this why come secret in kubernetes.
Secret data are encoded

We can create secret both way Imparative and Declarative.

=>kubectl create secret --help
=>kubectl create secret generic --help
=>kubectl create secret generic my-secret --from-literal=key1=supersecret --from-literal=key2=topsecret
OR
apiVersion: v1
kind: Secret
metadata:
  name: my-secret
data:
  DB_HOSTNAME: erewr4234234sdf=
  DB_USER: sfsfsfsfsfs==
  DB_PP: sfsdff2342

You must proved data in base64 encoded formate, for encode in linux:
=>echo -n "mysql" | base64
Decode:
=>echo -n "3df32==" | base64 --decode


When you create secret by imparative command no need to providce secret as encoded, plan tex is ok.
Onther hand Declarative way not allow plantext data, need base64 endoded.



=>kubectl get secret
=>kubectl get secret db-secret -o yaml
=>echo "sfsdfrweewer" | base64 --decode

=>kubectl describe secret dashboard-token 
Show How many secret data on a secret, What is the type of the dashboard-token secret.



For createing secret, secret data can be collect from fa file usig from file method.
the file carry data as keyvalue payers and myfile.properties extension.


We can Edit Secret using only a single command eidt:
=>k edit secret db-secret 






##Solution3: | Secret
-------------------------------------------------
How many secrets are defined in the dashboard-token secret?
=>kubectl describe secrets dashboard-token 
and look at the data field.
There are three secrets - ca.crt, namespace and token.


Which of the following is not a secret data defined in dashboard-token secret?
=>kubectl describe secrets dashboard-token 
and look at the data field.
There are three secrets - ca.crt, namespace and token. type is not a secret data.



We are going to deploy an application with the below architecture
We have already deployed the required pods and services. Check out the pods and services created. 
Check out the web application using the Webapp MySQL link above your terminal, next to the Quiz Portal Link.
https://prnt.sc/hAc-9XtgJ4eX

The reason the application is failed is because we have not created the secrets yet. 
Create a new secret named db-secret with the data given below.
You may follow any one of the methods discussed in lecture to create the secret.

Secret Name: db-secret
Secret 1: DB_Host=sql01
Secret 2: DB_User=root
Secret 3: DB_Password=password123



1)Create a new secret named db-secret with the data given below.
  Secret Name: db-secret
  Secret 1: DB_Host=sql01
  Secret 2: DB_User=root
  Secret 3: DB_Password=password123

=>kubectl create secret --help
=>kubectl create secret generic --help
=>kubectl create secret generic my-secret --from-literal=key1=supersecret --from-literal=key2=topsecret
=>kubectl create secret generic db-secret --from-literal=DB_Host=sql01 --from-literal=DB_User=root --from-literal=DB_Password=password123

=>kubectl get secret db-secret -o yaml
=>kubectl describe secret db-secret


=>kubectl describe pod webapp-pod
Show Is the pod has any secret

2)Configure webapp-pod to load environment variables from the newly created secret.
Delete and recreate the pod if required.

Pod name: webapp-pod
Image name: kodekloud/simple-webapp-mysql
Env From: Secret=db-secret


envFrom:
 - secretRef:
    name: db-secret

Add this under container like this:
spec:
  containers:
  - image: kodekloud/simple-webapp-mysql
    imagePullPolicy: Always
    name: webapp
    resources: {}
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    envFrom:
      - secretRef:
          name: db-secret


=>kubectl edit pod webapp-pod
=>kubectl replace --force -f /tmp/kubectl-edit-3803810208.yaml





##Encrypting the Secret data in Rest
-------------------------------------------------








##Security in Docker
-------------------------------------------------
Unlike VM Container and host shard the same OS kernel. Host has namespace and container has it s namespace,.

Docker container run its own namespace, and it not see other namespace process or host presecc.
This why when we run container ps command like this:
=>ps aux
Its only show one process runing on it. 
This is call Process Isolation.

Docker User:
Container has root user its like this root user of host, By default container run under root prvelise.
If you dont want to run container with default user you can pass user id on run command like this:
=>docker run --user=1000 ubuntu sleep 1000
OR
We cand set this user on docker file:
FROM ubuntu 
USER 1000
CMD ["sleep 1000"]

Docker container root user are not powerful as host root user, It can not has parmission as host root user.
Docer user Linux Capability to implement this.
Using  Linux Capability(dap-add/cap-drop) you cand add and remove capability power of docker container root user.
=>docker run --user=1000 ubuntu sleep 5
=>docker run --cap-add MAC_ADMIN ubuntu sleep 5
=>docker run --cap-drop KILL ubuntu sleep 5

You and run docker container with all privileged enable using priviledge flag
=>docker run --privileged ubuntu




##Kubernetes Security Contexts
-------------------------------------------------

As docker container we can add security on container lavel or pod lavel
When add it on pod level, it is for all container in a pod. 
If add security on container lavels then its is override thsi pod lavel security.

Add Security on Pod Level:
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  securityContext:
    runAsUser: 1000
  
  containers:
    - name: ubuntu
      image: ubuntu
      command: ["sleep", "1000"]

Add Security on container Level:
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
    - name: ubuntu
      image: ubuntu
      command: ["sleep", "1000"]
        securityContext:
          runAsUser: 1000

          capabilities:
            add: ["MAC_ADMIN"]
  


##Solution4: | Security Contexts
-------------------------------------------------
1)What is the user used to execute the sleep process within the ubuntu-sleeper pod?
=>kubectl exec -it ubuntu-sleeper -- whoami

2)Edit the pod ubuntu-sleeper to run the sleep process with user ID 1010.

Pod Name: ubuntu-sleeper
Image Name: ubuntu
SecurityContext: User 1010

add this under containers: section
  securityContext:
    runAsUser:1010

Like this:
spec:
  containers:
  - command:
    - sleep
    - "4800"
    image: ubuntu
    securityContext:
      runAsUser: 1010

=>kubectl replace --force -f /tmp/kubectl-edit-1907445851.yaml


Pod and container lavel SecurityContext:

A Pod definition file named multi-pod.yaml is given.
The pod is created with multiple containers and security contexts defined at the Pod and Container level.

apiVersion: v1
kind: Pod
metadata:
  name: multi-pod
spec:
  securityContext:
    runAsUser: 1001
  containers:
  -  image: ubuntu
     name: web
     command: ["sleep", "5000"]
     securityContext:
      runAsUser: 1002
  -  image: ubuntu
     name: sidecar
     command: ["sleep", "5000"]

With what user are the processes in the web container started?
The User ID defined in the securityContext of the container overrides the User ID in the POD. 1002

With what user are the processes in the sidecar container started?
The pod is created with multiple containers and security contexts defined at the Pod and Container level.1001





3)Update pod ubuntu-sleeper to run as Root user and with the SYS_TIME capability.
Pod Name: ubuntu-sleeper
Image Name: ubuntu
SecurityContext: Capability SYS_TIME

=>kubectl edit pod ubuntu-sleeper
add this under  SecurityContext:
capabilities:
  add: ["SYS_TIME"]

=>kubectl replace --force -f /tmp/kubectl-edit-2505242802.yaml


4)Now update the pod to also make use of the NET_ADMIN capability.

Pod Name: ubuntu-sleeper
Image Name: ubuntu
SecurityContext: Capability SYS_TIME
SecurityContext: Capability NET_ADMIN
Add this under containers section:
    securityContext:
      capabilities:
        add: ["SYS_TIME", "NET_ADMIN"]

Security Contest Update/Edit not done by one edit command, need to take another step from temp file run.




##Resource Requirements | Resource Limits
-------------------------------------------------
1 cpu = 1 Core,  1 AWS vCPU, 1GCP Core, 1 Hyperthread

cup 0.1 = 100Mi

Memory:
1G = Gigabyte = 1000 made
1k = kilobyte = 1000 bytes

1Gi = Gibibyte - 1024 mi
1Ki = Kibibyte - 1024 byte

A Docker container has no by default set limit to consume host resource, it can consume as much as needed.
Kubernetes by default set limit to container if you not set to.
Default value is:
CPU = 1vCPU, Momory= 512Mi

When you set resource limit on pod and If a pod try to cross the limit:
  In CPU kubernets throttle the uses of cpu, but not limit make throttle on memory.
  If a pod continouslly try to use more memory then pod will terminated.




##Solution6: | Resource Requirements
-------------------------------------------------
When a pod is created the containers are assigned a default CPU request of .5 and 
memory of 256Mi". 
For the POD to pick up those defaults you must have first set those as default values for request and limit 
by creating a LimitRange in that namespace.

apiVersion: v1
kind: LimitRange
metadata:
  name: mem-limit-range
spec:
  limits:
  - default:
      memory: 512Mi
    defaultRequest:
      memory: 256Mi
    type: Container

apiVersion: v1
kind: LimitRange
metadata:
  name: cpu-limit-range
spec:
  limits:
  - default:
      cpu: 1
    defaultRequest:
      cpu: 0.5
    type: Container 

1)A pod called rabbit is deployed. Identify the CPU requirements set on the Pod?
=>kubectl describe pod rabbit
Check:    
Requests:
      cpu:        1

2)Another pod called elephant has been deployed in the default namespace. It fails to get to a running state. Inspect this pod and identify the Reason why it is not running.

=>kubectl describe pod elephant

Check this section:
    Last State:     Terminated
      Reason:       OOMKilled
      Exit Code:    1
The status OOMKilled indicates that it is failing because the pod ran out of memory. Identify the memory limit set on the POD.


The elephant pod runs a process that consumes 15Mi of memory. Increase the limit of the elephant pod to 20Mi.
apiVersion: v1
kind: Pod
metadata:
  name: elephant
  namespace: default
spec:
  containers:
  - args:
    - --vm
    - "1"
    - --vm-bytes
    - 15M
    - --vm-hang
    - "1"
    command:
    - stress
    image: polinux/stress
    name: mem-stress
    resources:
      limits:
        memory: 20Mi
      requests:
        memory: 5Mi


3)The status OOMKilled indicates that it is failing because the pod ran out of memory. Identify the memory limit set on the POD.

=>kubectl describe pod elephant
    Limits:
      memory:  10Mi

4)The elephant pod runs a process that consumes 15Mi of memory. Increase the limit of the elephant pod to 20Mi.

=>kubectl get pod -o yaml elephant >newpod.yaml
=>kubectl delete pod elephant 
=>vi newpod.yaml 
Update the memory limit from 10 20
=>kubectl apply -f newpod.yaml 




##Service Accounts
-------------------------------------------------
CKAD EXAM: Only know how to woring with service Account.


Service Accounts for Other security purpose like: Authorization, Authentication, Role etc.
In CKAD only need to how work with service account.

Totally Two type fo Account in Kubernetes: 
  UserAccout: User human. Like a admin user do adminastatiov task and a developer deploy pod.
  ServiceAccoutn: Use by Machine. Like: a Jenken app need to deploy a pod in kubernetes cluster.



=>kubectl create serviceaccount my-serviceAccount
=>kubectl get serviceaccount
When a service account create a Token automatically created, what is for extrrnal authencation for kubernetes cluster.

When a service acc created its create:
  aServiceAccObject
  aToken
  aSecretWithToken
Then secretObjet link with service account.

To view the token run:
=>kunbectl describe serviceaccount my-serviceaccount
Chekout and token:
 Tokens:
=>kubectrl describe secret my-sec-token-kbbdm
Check:
 token:
This token can be use Bearer token on rest call api.
Like any curl request this token can be user as Authorization header Bearar token.

then secret object link with service account.
You can use this token to access outside of cluster.
If the app in same cluster then kubernetes make it simple to mounting the secret inside ths pod.

When create a pod by default default secret mounted inside all pod. to see this:
=>kubect describe pod my-pod



If the thard-party app placed in the same kubernetes cluster then no need to user this token as bearer.
This token can be mounted inside the pod.


Every name space has its own default service account.
It has a Secrect Object and a Token accoucated with it.
When a pod created auto assocated this servieaccount to the pod and mounted token as volume mount 
in this well know place of the pod.

For the show where the file mounted in pod:
=>kubectl describe pod mypod
Check Mounts:
=>kubectl exec -it my-pod -- ls /var/run/secrets/kubernatis.is/serviceaccount
It show three file inside it, its one is token.

If requird custome service accoutn add field of serviceAccountName

apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
    -  image: ubuntu
      name: web
      command: ["sleep", "5000"]
  serviceAccountName: my-serviceaccount

Remembar: You cna not edit the service account for a existing pod, You must delete and recreated the pod.
For Assigniing auto service account user:

apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
    -  image: ubuntu
      name: web
      command: ["sleep", "5000"]
  automountServiceAccountToken: false



 
1.22 to 1.24 Update of ServieAcc:
---------------------------------------------------

From kubernetes v1.22 not automatically mounted. token has no time limit.
From kubernetes v1.24 not auto create a toke and secret.
  Need to create secre token expectedly.
  First create s Service account then associated to Secre and token.





##Solution5: | Service Accounts
-------------------------------------------------
=>k get sa
=>kubectl get serviceaccount


1)What is the secret token used by the default service account?
=>kubectl describe sa default

2)Which account does the Dashboard application use to query the Kubernetes API?
Check the error:
pods is forbidden: User "system:serviceaccount:default:default" cannot list resource "pods" in API group "" in the namespace "default"
Its default account identify by "system:serviceaccount:default:default".
OR
=>kubectl logs podName
Chck pod event section:


3)Inspect the Dashboard Application POD and identify the Service Account mounted on it.
Identify the service account name of the pod:

=>kubectl get pod mypod -o yaml
and Find this:
serviceAccount: default

4)what location is the ServiceAccount credentials available within the pod?
=>kubectl describe pod web-dashboard-65b9cf6cbb-vpkkz 
Now check the  Mounts: properties

5)Create a new ServiceAccount named dashboard-sa
=>kubectl create sa dashboard-sa


6)We just added additional permissions for the newly created dashboard-sa account using RBAC.

pod-reader-role.yaml
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: default
  name: pod-reader
rules:
- apiGroups:
  - ''
  resources:
  - pods
  verbs:
  - get
  - watch
  - list

dashboard-sa-role-binding.yaml 
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: read-pods
  namespace: default
subjects:
- kind: ServiceAccount
  name: dashboard-sa # Name is case sensitive
  namespace: default
roleRef:
  kind: Role #this must be Role or ClusterRole
  name: pod-reader # this must match the name of the Role or ClusterRole you wish to bind to
  apiGroup: rbac.authorization.k8s.io

=>kubectl get role
=>kubectl get rolebinding


7)Enter the access token in the UI of the dashboard application. Click Load Dashboard button to load Dashboard

Create an authorization token for the newly created service account, copy the generated token and paste it into the token field of the UI.
To do this, run kubectl create token dashboard-sa for the dashboard-sa service account, copy the token and paste it in the UI.

=>kubectl create token dashboard-sa
OR
=>kubectl get sa
=>kubectl describe sa dashboard-sa
=>kubectl describe secret dashboard-sa-token-rdqw9


8)You shouldn't have to copy and paste the token each time. The Dashboard application is programmed to read token from the secret mount location. However currently, the default service account is mounted. Update the deployment to use the newly created ServiceAccount
Edit the deployment to change ServiceAccount from default to dashboard-sa

=>k get deployment
=>kubectl get deployment -o yaml>mydeployment.yaml
=>kubectl delete deploy web-dashboard 

=>vi mydeployment.yaml
Add serviceAccountName: dashboard-sa under templated spec
OR
=>kubectl edit deploy web-dashboard
Add this serviceAccount in pod space

    template:
      metadata:
        creationTimestamp: null
        labels:
          name: web-dashboard
      spec:
        serviceAccountName: dashboard-sa
        containers:





##Taints and Tolerations
-------------------------------------------------
=>kubectl taint nodes nodeName key=value:taint-effect
Three type of taint-effect:
 - NoSchedule               | No pod will- be deploy
 - PreferNoSchedule         | Try to nod pod deploy
 - NoExecute                | No New pod will be deploy, existing pod will evect if not can tolarent


Tain = Node
Toleration = Pod

=>kubectl describe node controlplane | grep Taints


=>kuhectl taint nodes node1 app=blue:NoSchedule
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
    - name: my-ngcontainer
      image: nginx

    tolerations:
      - key: "app"
        operator: "Equal"
        value: "blue"
        effect: "NoSchedule"




#Solution7: | Taints and Tolerations
-------------------------------------------------


1)How many nodes exist on the system, Do any taints exist on node01 node?
=>kubectl get node
=>kubectl describe node node01 | grep -i taints


2)Create a taint on node01 with key of spray, value of mortein and effect of NoSchedule
Key = spray
Value = mortein
Effect = NoSchedule

=>kubectl taint --help
=>kubectl taint nodes node01 spray=mortein:NoSchedule
=>kubectl describe node node01 | grep Taints


3) Create a pod name mosquito, Why pod on pending state ?
=>kubectl describe pod mosquito

4)Create another pod named bee with the nginx image, which has a toleration set to the taint mortein.

Image name: nginx
Key: spray
Value: mortein
Effect: NoSchedule
Status: Running

=>kubectl run bee --image=nginx --dry-run=client -o yaml>bee.yaml
=>vi bee.yaml

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: bee
  name: bee
spec:
  containers:
  - image: nginx
    name: bee
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
  tolerations:
    - key: "spray"
      value: "mortein"
      effect: "NoSchedule"
      operator: "Equal"
status: {}

=>k apply -f bee.yaml 


5)Notice the bee pod was scheduled on node node01 despite the taint?
=>kuebctl get pod -o wide

6)Remove the taint on controlplane, which currently has the taint effect of NoSchedule.
=>kubectl describe node controlplane
Check:
Taints:             node-role.kubernetes.io/control-plane:NoSchedule

Remove it:
=>kubectl taint node controlplane node-role.kubernetes.io/control-plane:NoSchedule-






##Node Selectors| Affinity
-------------------------------------------------
To deploy specific pod to specific Node we cand user Node Selector:
Lavel the node and add this to pod as Node selector

NodeAffinity user for complex node select operation:

=>kubectl label nodes node01 size=learge
Labeling a node

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: bee
  name: bee
spec:
  containers:
  - image: nginx
    name: bee
  nodeSelector:
    size: Large

=>kuebctl apply -f newpod.yaml


Node selectro has limitation, we cand user only single level and secelator.
If a complex requirment not deon this way, Here we have to user:
nodeSelector and Node Affinity together

affinity:
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
      - matchExpressions:
        - key: color
          operator: In
          values:
          - blue
          - small

Two tyep of affinity now available:

requiredDuringSchedulingIgnoredDuringExecution:
preferredDuringSchedulingIgnoredDuringExecution:

Planned:
requiredDuringSchedulingRequiredDuringExecution:
If any lavel is remove after schedule, in this case pod willbe remove


Taints and Tolerations vs Node Affinity
-------------------------------------------------
Taints and Tolerations usually select the right node for the pod but pod can be   schedule other node.
And With
Node Affinity can schedule to right node but other pod may schedule on the same node
 For Slove this provlem, user taints and Tolerant and Node Affinityh both together.





##Solution8: | Node Affinity
-------------------------------------------------

1)How many Labels exist on node node01?
=>kubectl describe node node01
Check  Labels:             beta.kubernetes.io/arch=amd64

2)Apply/Add a label color=blue to node node01
=>kubectl label node --help
=>kubectl label node node01 color=blue

3)Create a new deployment named blue with the nginx image and 3 replicas.
=>kubectl create deployment --help
=>kubectl create deployment blue --image=nginx --replicas=3

4)Which nodes can the pods for the blue deployment be placed on?
=>kubectl get pod -o wide
=>kubectl describe node node01 | grep Taints


5)Set Node Affinity to the deployment to place the pods on node01 only.
Name: blue
Replicas: 3
Image: nginx
NodeAffinity: requiredDuringSchedulingIgnoredDuringExecution
Key: color
value: blue

=>kubectl label node node01 color=blue

=>kubectl edit deployment blue
Add this: in spec section:

  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: color
            operator: In
            values:
            - blue

Complate deployment yaml will like this:
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: blue
  name: blue
spec:
  replicas: 3
  selector:
    matchLabels:
      app: blue
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: blue
    spec:  
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: color
                operator: In
                values:
                - blue
      containers:
      - image: nginx
        name: nginx
        resources: {}
status: {}


6)Create a new deployment named red with the nginx image and 2 replicas, and ensure it gets placed on the controlplane node only.
Use the label key - node-role.kubernetes.io/control-plane - which is already set on the controlplane node.

apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: red
  name: red
spec:
  replicas: 2
  selector:
    matchLabels:
      app: red
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: red
    spec:  
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: node-role.kubernetes.io/control-plane
                operator: Exists
      containers:
      - image: nginx
        name: nginx
        resources: {}
status: {}







##Section 3: Multi-Container Pods
==================================================

##Multi-Container Pods
--------------------------------------------------
One o the benifet of multicontainer pod its can share same storage same data as localhost, no need volume or other share data.
Its work as a pod a localmachine and containers are process or running service in it.

Like a Business service produce some logs and a logs service send it to a central logs process system.


Design Pattern of MultiContainer Pod:
   - Sidecar Pattern                   | a log container with a webserver to send log to a central logging system.
   - Adapter Pattern                   | Before sending the log file to system convart to log a common pattern for this a container.
   - Ambassador Pattern                | To connect multple db server a Proxy localhost db container.

all pattern implementation is same: Define multicontainer as list in pod.





##Solution1 - Multi-Container Pods
--------------------------------------------------

1)Create a multi-container pod with 2 containers.
Use the spec given below.
If the pod goes into the crashloopbackoff then add the command sleep 1000 in the lemon container.

Name: yellow
Container 1 Name: lemon
Container 1 Image: busybox
Container 2 Name: gold
Container 2 Image: redis

=>kubectl run yellow --image busybox --dry-run=client -o yaml

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: yellow
  name: yellow
spec:
  containers:
  - image: busybox
    name: lemon
    command: ["sleep", "1000"]
  - image: redis
    name: gold

=>kubectl describe pod laymon


2)We have deployed an application logging stack in the elastic-stack namespace. Inspect it.
Before proceeding with the next set of questions, please wait for all the pods in the elastic-stack namespace to be ready. This can take a few minutes.

=>kubectl get pod -n elastic-stack

Once the pod is in a ready state, inspect the Kibana UI using the link above your terminal. There shouldn't be any logs for now.
We will configure a sidecar container for the application to send logs to Elastic Search.
NOTE: It can take a couple of minutes for the Kibana UI to be ready after the Kibana pod is ready.

You can inspect the Kibana logs by running:
kubectl -n elastic-stack logs kibana


3)The application outputs logs to the file /log/app.log. View the logs and try to identify the user having issues with Login.
Inspect the log file inside the pod

=>kubectl logs app -n elastic-stack
=>kubectl -n elastic-stack exec -it app -- cat /log/app.log


4)Edit the pod to add a sidecar container to send logs to Elastic Search. Mount the log volume to the sidecar container.
Only add a new container. Do not modify anything else. Use the spec provided below.

Note: State persistence concepts are discussed in detail later in this course. For now please make use of the below documentation link for updating the concerning pod.
https://kubernetes.io/docs/tasks/access-application-cluster/communicate-containers-same-pod-shared-volume/

Name: app
Container Name: sidecar
Container Image: kodekloud/filebeat-configured
Volume Mount: log-volume
Mount Path: /var/log/event-simulator/
Existing Container Name: app
Existing Container Image: kodekloud/event-simulator


=>kubectl edit pod -n elastic-stack

- image: kodekloud/filebeat-configured
  name: sidecar
  volumeMounts:
    - mountPath: /var/log/event-simulator/
      name: log-volume
https://www.loom.com/share/c2ae70197e8340a0ba77fc1de8179182

OR
apiVersion: v1
kind: Pod
metadata:
  name: app
  namespace: elastic-stack
  labels:
    name: app
spec:
  containers:
  - name: app
    image: kodekloud/event-simulator
    volumeMounts:
    - mountPath: /log
      name: log-volume

  - name: sidecar
    image: kodekloud/filebeat-configured
    volumeMounts:
    - mountPath: /var/log/event-simulator/
      name: log-volume

  volumes:
  - name: log-volume
    hostPath:
      # directory location on host
      path: /var/log/webapp
      # this field is optional
      type: DirectoryOrCreate




##initContainers
--------------------------------------------------
An initContainer is configured in a pod like all other containers, 
except that it is specified inside a initContainers section, like this:


apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
spec:
  containers:
  - name: myapp-container
    image: busybox:1.28
    command: ['sh', '-c', 'echo The app is running! && sleep 3600']
  initContainers:
  - name: init-myservice
    image: busybox
    command: ['sh', '-c', 'git clone <some-repository-that-will-be-used-by-application> ;']


When a POD is first created the initContainer is run, 
and the process in the initContainer must run to a completion before the real container hosting the application starts.

You can configure multiple such initContainers as well, like how we did for multi-pod containers. 
In that case each init container is run one at a time in sequential order.
If any of the initContainers fail to complete, Kubernetes restarts the Pod repeatedly until the Init Container succeeds.

apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
spec:
  containers:
  - name: myapp-container
    image: busybox:1.28
    command: ['sh', '-c', 'echo The app is running! && sleep 3600']
  initContainers:
  - name: init-myservice
    image: busybox:1.28
    command: ['sh', '-c', 'until nslookup myservice; do echo waiting for myservice; sleep 2; done;']
  - name: init-mydb
    image: busybox:1.28
    command: ['sh', '-c', 'until nslookup mydb; do echo waiting for mydb; sleep 2; done;']


Read more about initContainers here. And try out the upcoming practice test.
https://kubernetes.io/docs/concepts/workloads/pods/init-containers/






##Section4: Observability
==================================================
Pod lifecycle:
  - Pending state 
    When a pod is start it is on pending state, scheduler find the ringht -node for it, if not find keep it pending sate.
    If a pod pending state run describe command, its tell you why.
    =>kubectl describe pod my-pod
    
  - ContainerCreating
    When a pod schedule its on Container creating state, pull the image and run all the container of the pod in side node.
    
  - Running state
    When all container is ready it is gor for running state.

This three stat show on STATUS colume.
 


Pod Conditions:
- PodScheduled    = True/false
- Initialized     = True/false
- ContainerReady  = True/false
- Ready           = True/fasle

Run describe command and check conditaiton section.
=>kubectl describe pod my-pod


A pod is show Ready state but internal application may take more time to 
recieving user request, for this create a custome condation for set when a pod actually ready.
Add this rediness probe under containers: section

readinessProbe:
  httpGet:
    path: /ready
    port:8080


Its cna be: httpGet:, tcpSocket:, exec: connand   
With readinessProbe you can add different funcationality:

initialDelaySeconds: 
periodSeconds:
failureThreshold:  etc.


##Readiness and Liveness Probes
--------------------------------------------------
If a container inside a pod crush or exitsted, kubernetes try to restart the container ageing, and continue to do it.

Liveness Probes: 
  Its a machinesume to check/monitor the pod is healthy or running, 
  You cand define it checking with call a http-api, tch or cmd.

livenessProbe:
  httpGet:
    path: /ready
    port:8080


cat curl-test.sh 
for i in {1..20}; do
   kubectl exec --namespace=kube-public curl -- sh -c 'test=`wget -qO- -T 2  http://webapp-service.default.svc.cluster.local:8080/ready 2>&1` && echo "$test OK" || echo "Failed"';
   echo ""
done

=>./curl-test.sh


crash-app.sh 
kubectl exec --namespace=kube-public curl -- wget -qO- http://webapp-service.default.svc.cluster.local:8080/crash


freeze-app.sh 
nohup kubectl exec --namespace=kube-public curl -- wget -qO- http://webapp-service.default.svc.cluster.local:8080/freeze &




##Solution: Readiness adn Liveness Probes
--------------------------------------------------

1)Update the newly created pod 'simple-webapp-2' with a readinessProbe using the given spec
Spec is given on the below. Do not modify any other properties of the pod.

Pod Name: simple-webapp-2
Image Name: kodekloud/webapp-delayed-start
Readiness Probe: httpGet
Http Probe: /ready
Http Port: 8080

=>kubectl get pod simple-webapp-2 -o yaml>newpod.yaml
=>kubectl delete pod simple-webapp-2 
=>vi newpod.yaml

Add this rediness probe under containers: section
readinessProbe:
  httpGet:
    path: /ready
    port:8080

=>kubectl get pod
Now this pod running but not ready, waiting for ready


=>./crash-app.sh
Crash the application, it will be restart again.
OR
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: "2021-08-01T04:55:35Z"
  labels:
    name: simple-webapp
  name: simple-webapp-2
  namespace: default
spec:
  containers:
  - env:
    - name: APP_START_DELAY
      value: "80"
    image: kodekloud/webapp-delayed-start
    imagePullPolicy: Always
    name: simple-webapp
    ports:
    - containerPort: 8080
      protocol: TCP
    readinessProbe:
      httpGet:
        path: /ready
        port: 8080

=>kubectl replace -f webpod2.yaml --force

When the application crashes, the container is restarted. During this period the service directs users to the available POD, since the POD status is not READY.


2)Update both the pods with a livenessProbe using the given spec
Delete and recreate the PODs.

Pod Name: simple-webapp-1
Image Name: kodekloud/webapp-delayed-start
Liveness Probe: httpGet
Http Probe: /live
Http Port: 8080
Period Seconds: 1
Initial Delay: 80
Pod Name: simple-webapp-2
Image Name: kodekloud/webapp-delayed-start
Liveness Probe: httpGet
Http Probe: /live
Http Port: 8080
Initial Delay: 80
Period Seconds: 1


=>kubectl get pod -o yaml>allwebapp.yaml
=>kubectl delete pod --all
=>vi allwebapp.yaml

Add this on containers: section for bot container

livenessProbe:
  httpGet:
    path: /live
    port: 8080
  periodSeconds: 1
  initialDelaySeconds: 80

OR
Use the following YAML for simple-webapp-1.yaml:

apiVersion: v1
kind: Pod
metadata:
  labels:
    name: simple-webapp
  name: simple-webapp-1
  namespace: default
spec:
  containers:
  - env:
    - name: APP_START_DELAY
      value: "80"
    image: kodekloud/webapp-delayed-start
    imagePullPolicy: Always
    name: simple-webapp
    ports:
    - containerPort: 8080
      protocol: TCP
    readinessProbe:
      httpGet:
        path: /ready
        port: 8080
    livenessProbe:
      httpGet:
        path: /live
        port: 8080
      periodSeconds: 1
      initialDelaySeconds: 80

=>kubectl replace -f simple-webapp-1.yaml --force

for simple-webapp-2, use the following YAML file:

apiVersion: v1
kind: Pod
metadata:
  labels:
    name: simple-webapp
  name: simple-webapp-2
  namespace: default
spec:
  containers:
  - env:
    - name: APP_START_DELAY
      value: "80"
    image: kodekloud/webapp-delayed-start
    imagePullPolicy: Always
    name: simple-webapp
    ports:
    - containerPort: 8080
      protocol: TCP
    readinessProbe:
      httpGet:
        path: /ready
        port: 8080
    livenessProbe:
      httpGet:
        path: /live
        port: 8080
      periodSeconds: 1
      initialDelaySeconds: 80


=>kubectl replace -f simple-webapp-2.yaml --force





##Container Logging and Monitor
--------------------------------------------------
When multiple container running in a pod, for showing container logs you have to mentation container name with log command.
=>kubectl logs podName  -c containerName





##Solution: Container Logging and Monitor
--------------------------------------------------
=>kubectl logs mypod 
=>kubectl logs webapp-2 -c simple-webapp

1)Let us deploy metrics-server to monitor the PODs and Nodes. Pull the git repository for the deployment files.
Run: git clone https://github.com/kodekloudhub/kubernetes-metrics-server.git

=>kubectl create -f .
Create matrice server.

=>kubectl top node
Check resource monitoring of nodes


2)Identify the node that consumes the most CPU(cores).
  Identify the node that consumes the most Memory(bytes).
=>kubectl top node

3)Identify the most memory consumes pod
  Identify the POD that consumes the least CPU(cores).
=>kubectl top pod




##Solution2 – Init Containers
--------------------------------------------------

1)Identify the pod that has an initContainer configured.
=>kubectl describe pod
Check:
IPs:
  IP:  10.42.0.10
Init Containers:
  init-myservice:



2)What is the state of the initContainer on pod blue?
=>kubectl describe pod
Cehck out this:
      State:          Terminated
      Reason:       Completed

3)We just created a new app named purple. How many initContainers does it have?
=>kubectl describe pod purple


4)What is the state of the POD?
=>kubectl describe pod purple
Status:           Pending
IP:               10.42.0.12


5)How long after the creation of the POD will the application come up and be available to users?
=>kubectl describe pod purple
add all the init container wait or sleep time then 
Check the commands used in the initContainers. The first one sleeps for 600 seconds (10 minutes) and the second one sleeps for 1200 seconds (20 minutes)
Adding the sleep times for both initContainers, the application will start after 1800 seconds or 30 minutes.



6)Update the pod red to use an initContainer that uses the busybox image and sleeps for 20 seconds
Delete and re-create the pod if necessary. But make sure no other configurations change.

Pod: red
initContainer Configured Correctly

=>kubectl get pod red -o yaml>red.yaml
=>kubectl delete pod red 
=>vi red.yaml

  initContainers:
    - image: busybox
      name: red-initcontainer
      command:
        - "sleep"
        - "20"

Add it like this:

spec:
  initContainers:
    - image: busybox
      name: red-initcontainer
      command:
        - "sleep"
        - "20"
  containers:
  - command:
    - sh
    - -c
    - echo The app is running! && sleep 3600
    image: busybox:1.28
    imagePullPolicy: IfNotPresent
    name: red-container
OR
---
apiVersion: v1
kind: Pod
metadata:
  name: red
  namespace: default
spec:
  containers:
  - command:
    - sh
    - -c
    - echo The app is running! && sleep 3600
    image: busybox:1.28
    name: red-container
  initContainers:
  - image: busybox
    name: red-initcontainer
    command: 
      - "sleep"
      - "20"
7)A new application orange is deployed. There is something wrong with it. Identify and fix the issue.
Once fixed, wait for the application to run before checking solution.

=>kubectl describe pod orange
Proble show in pod config properties like connadn, args image name typho etc.
=>kubectl get pod orange -o yaml > /root/orange.yaml








##Section 6: POD Design
==================================================

##Labels, Selectors and Annotations
--------------------------------------------------
In kubernetes we add level on kubernetes object, for select then diffenere option. add level on pod:
apiVersion: v1
kind: Pod
metadata:
  name: throw-dice-pod
  labels:
    evn: prod
    app: back-end   
spec:
  containers: 
  -  image: kodekloud/throw-dice
     name: throw-dice
  restartPolicy: Never

Now to get this level pod:
=>kubectl get pods --selector env=prod


##Solution1: Labels, Selectors and Annotations
--------------------------------------------------

1)We have deployed a number of PODs. They are labelled with tier, env and bu. How many PODs exist in the dev environment (env)?
Use selectors to filter the output

=>kubectl get pod --selector env=dev
OR
=>kubectl get pod --selector env=dev --no-headers
=>kubectl get pod --selector env=dev --no-headers | wc -l

2)How many objects are in the prod environment including PODs, ReplicaSets and any other objects?
=>kubectl get all
=>kubectl get all --selector env=prod
OR
=>kubectl get all --selector env=prod | wc -l


3)Identify the POD which is part of the prod environment, the finance BU and of frontend tier?
=>kubectl get pod --selector env=prod,bu=finance,tier=frontend


4)A ReplicaSet definition file is given replicaset-definition-1.yaml. Try to create the replicaset. There is an issue with the file. Try to fix it.
=>kubectl apply -f replicaset-definition-1.yaml 
=>vi replicaset-definition-1.yaml 
=>kubectl apply -f replicaset-definition-1.yaml 

=>kubectl get rs





##Deployment Rolling Updte and Rollback
--------------------------------------------------
kubectl create for creating object, kubectl apply for update object.

=>kubectl create -f mydeployment.yaml --record
=>kubectl rollout status deployment/mydeployment
=>kubectl rollout history deployment/mydeployment
Creating a deployment

Creating a deployment, checking the rollout status and history:
In the example below, we will first create a simple deployment and inspect the rollout status and the rollout history:

=>kubectl create deployment nginx --image=nginx:1.16
=>kubectl rollout status deployment nginx
=>kubectl rollout history deployment nginx


Using the --revision flag:
Here the revision 1 is the first version where the deployment was created.
You can check the status of each revision individually by using the --revision flag:

=>kubectl rollout history deployment nginx --revision=1


Using the --record flag:
You would have noticed that the "change-cause" field is empty in the rollout history output. 
We can use the --record flag to save the command used to create/update a deployment against the revision number.

=>kubectl set image deployment nginx nginx=nginx:1.17 --record
=>kubectl rollout history deployment nginx
=>kubectl edit deployments nginx --record
=>kubectl set image deployment nginx nginx=nginx:1.17 --record=true
=>kubectl edit deployments nginx --record=true
=>kubectl rollout history deployment nginx --revision=3


Undo a change:Lets now rollback to the previous revision:
=>kubectl rollout history deployment nginx
=>kubectl rollout history deployment nginx --revision=3
=>kubectl describe deployments nginx | grep -i image:
With this, we have rolled back to the previous version of the deployment with the image = nginx:1.17.

=>kubectl rollout history deployment nginx --revision=1
=>kubectl rollout undo deployment nginx --to-revision=1
=>kubectl describe deployments. nginx | grep -i image:

To rollback to specific revision we will use the --to-revision flag.
With --to-revision=1, it will be rolled back with the first image we used to create a deployment





=>kubectl rollout history deploy frontend
=>kubectl rollout history deployment frontend --revision=3

=>kubectl rollout undo deployment frontend --to-revision=1



##Solution2: Deployment Rolling Updte and Rollback
--------------------------------------------------
cat curl-test.sh 
for i in {1..35}; do
   kubectl exec --namespace=kube-public curl -- sh -c 'test=`wget -qO- -T 2  http://webapp-service.default.svc.cluster.local:8080/info 2>&1` && echo "$test OK" || echo "Failed"';
   echo ""
done

1)What container image is used to deploy the applications?

=>k describe deployment frontend 

2)Inspect the deployment and identify the current strategy?
=>k describe deployment frontend 
StrategyType:           RollingUpdate

3)Let us try that. Upgrade the application by setting the image on the deployment to kodekloud/webapp-color:v2

=>kubectl edit deployment frontend
Edit the yaml file
OR
=>kubectl set image --help
=>kubectl set image deploy frontend simple-webapp=kodekloud/webapp-color:v2
=>kubectl describe deployment frontend 


4)Up to how many PODs can be down for upgrade at a time
Consider the current strategy settings and number of PODs - 4

=>kubectl describe deploy frontend 
checkout:
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Its mean 1 pod what is 25%.


5)Change the deployment strategy to Recreate

=>kubectl get deployment frontend -o yaml>newdeployment.yaml
=>kuebctl delete deploy frontend
 strategy:
    type: Recreate
=>kubectl apply -f newdeployment.yaml
=>kubectl describe deploy frondend
Check:
StrategyType:       Recreate




##Job and CronJob
--------------------------------------------------
 Job ensures that the specified number of pods are executed successfully to complete the task. 
 Other workload resources like Deployment try to maintain the desired state of an application that is 
 required to run for a long time.

When a pod of job rech complated state with out any error/iss its mean the job are complated.
If the pod thorew are nto able to rech complate state or throw erroer its mean job not complated yet.



Job start imidetly. whenre we can set schedule on CronJob.

ReplicaSet keep runnign pod allows, where Job bring the pod for a specific task and after it finish its go down.


##Solution3: Job and CronJob
--------------------------------------------------
cat throw-dice.yaml
apiVersion: v1
kind: Pod
metadata:
  name: throw-dice-pod
spec:
  containers:
  -  image: kodekloud/throw-dice
     name: throw-dice
  restartPolicy: Never

=>kubectl logs throw-dice-pod

1)Create a Job using this POD definition file or from the imperative command and look at how many attempts does it take to get a '6'.
Use the specification given on the below.

Job Name: throw-dice-job
Image Name: kodekloud/throw-dice

=>kubectl create job throw-dice-job --image=kodekloud/throw-dice --dry-run=client -o yaml > throw-dice-job.yaml

apiVersion: batch/v1
kind: Job
metadata:
  name: throw-dice-job
spec:
  backoffLimit: 15 # This is so the job does not quit before it succeeds.
  template:
    spec:
      containers:
      - name: throw-dice
        image: kodekloud/throw-dice
      restartPolicy: Never

=>vi myjob.yaml
=>kubectl apply -f myjon.yaml
=>kubectl describe job throw-dice-job 



How many attempts did it take to complete the job?
=>kubectl describe job throw-dice-job 
Check this:
Pods Statuses:    0 Active (0 Ready) / 1 Succeeded / 1 Failed
togeher both: 2

2)Update the job definition to run as many times as required to get 3 successful 6's.
Delete existing job and create a new one with the given spec. Monitor and wait for the job to succeed.

Job Name: throw-dice-job
Image Name: kodekloud/throw-dice
Completions: 3
Job Succeeded: True

=>vi jobn3.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: throw-dice-job
spec:
  template:
    spec:
      containers:
      - name: throw-dice-job
        image: kodekloud/throw-dice
      restartPolicy: Never
  backoffLimit: 30
  completions: 3

=>kubectl delete job throw-dice-job
=>kubectl apply -f job3.yaml

=> k describe job throw-dice-job 
Check:
Pods Statuses:    1 Active (0 Ready) / 2 Succeeded / 5 Failed


3)That took a while. Let us try to speed it up, by running upto 3 jobs in parallel.
Update the job definition to run 3 jobs in parallel.

Job Name: throw-dice-job
Image Name: kodekloud/throw-dice
Completions: 3
Parallelism: 3

=>vi jon4.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: throw-dice-job
spec:
  template:
    spec:
      containers:
      - name: throw-dice-job
        image: kodekloud/throw-dice
      restartPolicy: Never
  backoffLimit: 30
  completions: 3
  parallelism: 3

=>k replace --force -f myjob1.yaml 


4)Let us now schedule that job to run at 21:30 hours every day.
Create a CronJob for this.
CronJob Name: throw-dice-cron-job
Image Name: kodekloud/throw-dice
Schedule: 30 21 * * *

apiVersion: batch/v1
kind: CronJob
metadata:
  name: throw-dice-cron-job
spec:
  schedule: "30 21 * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: throw-dice-cron-job
            image: kodekloud/throw-dice
          restartPolicy: OnFailure






##Services and Networking
==================================================
Kubernetes Services enable communication group of application with another user or objects.
Services make losscoupling with microservice.

Services just a object, its listaning port on Node and make communicate with a port of pod.
A service is a virtual server inside a node, its has a IP.

Service can make communication kunbernetes different group of apps or service.

Three type of Series:
 - NodePort
   Make access the app with external, To map a Node port and a pod port.

 - ClusterIP
   Service create a vertual IP in side the cluster to communication with oter service, it has ip and port.
   Diffeetnt server like a group of frontend server or backend serer.

 - LoadBalancer
   We provision a loadbalancer for our application in support by cloud provider.



##Services - nodePort
--------------------------------------------------

apiVersion: v1
kind: Service
metadata:
  name: webapp-service
  namespace: default
spec:
  type: NodePort
  ports:
  - nodePort: 30080               | Node port
    port: 8080                    | Service port
    targetPort: 80                | application port
  selector:                       | Identify the pods
    name: simple-webapp

In a service only port: field is mandatory, if not TargetPort then it assign  port value as TargetPort,
and if not provide nodePort: if automatically take a port in valide range of node port.

You can mape multiple port in a single service.

Services can spand multiple node with same port.
To access that user node ip and the same port any node.

apiVersion: v1
kind: Service
metadata:
  name: webapp-service
  namespace: default
spec:
  ports:
  - nodePort: 30080
    port: 8080
    targetPort: 8080
  selector:
    name: simple-webapp
  type: NodePort


=>kubectl create service nodeport mysvc4 --tcp=8080:80 --node-port=30040 --dry-run=client -o yaml>mysvc.yaml



##Services - ClusterIP
--------------------------------------------------
This is the defafult service type of kubernetes.
ClusterIP service make layer or group of application communication easy wdfith a self IP call clusterIP.

Suppose a three tier application: frondend - backend - db
In this case ClusterIP Service will create one ClusterIP-Service for each tier with a IP.
Now all tire communicate with each other by this IP.



apiVersion: v1
kind: Service
metadata:
  name: webapp-service
  namespace: default
spec:
  type: ClusterIP
  ports:
    port: 8080                    | Pod/app port
    targetPort: 80                | this service port
  selector:                       | Identify the pods
    tier: backend-app
    name: simple-webapp




##Solution1: Services - ClusterIP
--------------------------------------------------z

1)What is the targetPort configured on the kubernetes service?
=>kubectl describe svc kubernetes 
Check:
TargetPort:        6443/TCP



2)How many labels are configured on the kubernetes service?
=>kubectl describe svc kubernetes 
Check:
Labels:            component=apiserver
                   provider=kubernetes


3)Create a new service to access the web application using the service-definition-1.yaml file.
Name: webapp-service
Type: NodePort
targetPort: 8080
port: 8080
nodePort: 30080
selector:
  name: simple-webapp

=>vi service.yaml
apiVersion: v1
kind: Service
metadata:
  name: webapp-service
  namespace: default
spec:
  ports:
  - nodePort: 30080
    port: 8080
    targetPort: 8080
  selector:
    name: simple-webapp
  type: NodePort





##Network Policies | Network Security
--------------------------------------------------
Kubernetes make all Allow for all pod/service can make communication each other in a cluster.

Two type Network Policy:
 - Ingress           | In comming request from the user to he POD.
 - Egress            | Outgoing request from POD app server to out side user.


Network Policy also a Object in kubernetes, link one or more pod and set rule/policy to enable communication.


apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: db-pod-policy
spec:
  podSelector:             | Which pod policy apply to like a db pod/server with pod label role=db.
    matchLabels:
      role: db
  policyTypes:
    - Ingress
  ingress:
    - from:
        - podSelector:     | From which pod ingress traffic allow on the db pod, here a api-server with this label. 
            matchLabels:
              name: api-server
      ports:
        - protocol: TCP
          port: 8080


Network solution which support NetworkPolicy:
 - Kube-route
 - Calico
 - Romana
 - Weave-net
 
Network solution which  not support NetworkPolicy:
- Flannel


We still able to create NetworkPolicy althrow the network solution not support policy.
It not work but not throw any error.


Developing Network Policies
--------------------------------------------------
Suppose we have a Three tire application: webapp, api-server, db

Step:
  Apply a policy to db pod, and block all in/out traffic.
  Now need to undestanding which type of communication/rule have to add with this policy.
  In this case for db server receiving incommuning traffice from api-server and return query result.
  For this need to add a Ingress policy to this db pod policy from api-server.
  No need to worry about db output query result to api-server, no policy needed.
  When the db pod allow Ingress traffice from api-server auto respone result has been allow. 
But DB server not allow make any query/request to api-server in this policy.
This is a Egress traffic, to allow this need a extra Egress role in db pod.


A single policy can hold Ingress or Egress or Both role.

We add namespace in the policy, with pod selector or without pod selector.
We can also only  specify namespace only, no need this pod, this case all pod able to communicate in this ns.
We cna also spacify a IP or blick of IP to cummunicate with a server out side of cluster.

So three type of selector:
 - podSelector
 - namespaceSelector
 - ipBlock
This three support apply to -from and -to block in policy.
OR
We cna user this rule as tow (podSelector AND namespaceSelector) type of selector:
 - podSelector
   namespaceSelector
 - ipBlock




##Solution2: Network Policies
--------------------------------------------------
1)How many network policies do you see in the environment?
=>kubectl get networkpolicy 
=>kubectl get netpol
https://prnt.sc/FF-fgT5W8thh


2)Which pod is the Network Policy applied on?
=>kubectl get networkpolicy 
Check pod-selector:
NAME             POD-SELECTOR   AGE
payroll-policy   name=payroll   2m34s

Then find this pod
=>kubectl get po --show-labels | grep name=payroll



3)What type of traffic is this Network Policy configured to handle?
=>kubectl describe networkpolicy
=>kubectl describe netpol payroll-policy
Chck this:
Policy Types: Ingress


4)What is the impact of the rule configured on this Network Policy?
Internal pod can access on 8080 port
Spec:
  PodSelector:     name=payroll
  Allowing ingress traffic:
    To Port: 8080/TCP
    From:
      PodSelector: name=internal




5)Create a network policy to allow traffic from the Internal application only to the payroll-service and db-service.
Use the spec given below. You might want to enable ingress traffic to the pod to test your rules in the UI.

Policy Name: internal-policy
Policy Type: Egress
Egress Allow: payroll
Payroll Port: 8080
Egress Allow: mysql
MySQL Port: 3306

mynetpolicy.yaml

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: internal-policy
  namespace: default
spec:
  podSelector:
    matchLabels:
      name: internal
  policyTypes:
    - Egress
  egress:
    - to:
        - podSelector:
            matchLabels:
              name: payroll
      ports:
        - protocol: TCP
          port: 8080
    - to:
        - podSelector:
            matchLabels:
              name: mysql
      ports:
        - protocol: TCP
          port: 3306

OR
Solution manifest file for a network policy internal-policy as follows:

Note: We have also allowed Egress traffic to TCP and UDP port. This has been added to ensure that the internal DNS resolution works from the internal pod.
Remember: The kube-dns service is exposed on port 53:
root@controlplane:~# kubectl get svc -n kube-system 
NAME       TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                  AGE
kube-dns   ClusterIP   10.96.0.10   <none>        53/UDP,53/TCP,9153/TCP   93m



apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: internal-policy
  namespace: default
spec:
  podSelector:
    matchLabels:
      name: internal
  policyTypes:
  - Egress
  - Ingress
  ingress:
    - {}
  egress:
  - to:
    - podSelector:
        matchLabels:
          name: mysql
    ports:
    - protocol: TCP
      port: 3306

  - to:
    - podSelector:
        matchLabels:
          name: payroll
    ports:
    - protocol: TCP
      port: 8080

  - ports:
    - port: 53
      protocol: UDP
    - port: 53
      protocol: TCP

=>kubectl apply -f mynetpolicy.yaml
Note: We have also allowed Egress traffic to TCP and UDP port. This has been added to ensure that the internal DNS resolution works from the internal pod.
Remember: The kube-dns service is exposed on port 53:





#Ingress Networking
--------------------------------------------------
Ingress help you access your application with a extranal url for each of service each path with ssl security.
Its a layer7 loadbalancer.
You have to publish using nodePort or any cloudNative provider with a cloud loadbalancer.

For Ingress need tow component:

- Ingress Controller
  You have no ingressController in kubernatis by default. 
  You have to deploy any one of solution like: Nginx, GCP(http), Countour etc.
  Currently Nginx and GCP support by kubernatis.

- Ingress Resource
  We declar different service path in a ingress declarative file with path.
  IngressResource declare using yaml kind: Ingress file with app service name and service port etc.



IngressController:
  For IngressController we have to deploy it as a declarative way with kind: Deployment
  A Service requird to expose to our side with a configMap data value
  Also need a ServiceAccount for get permisson to do all this thing.

So required component for IngressController:
- Deployment
- Service
- ConfigMap
- ServiceAccount

Simple Ingress Controller:
cat ingress-controller.yaml 

apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.1.2
    helm.sh/chart: ingress-nginx-4.0.18
  name: ingress-nginx-controller
  namespace: ingress-nginx
spec:
  minReadySeconds: 0
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/name: ingress-nginx
  template:
    metadata:
      labels:
        app.kubernetes.io/component: controller
        app.kubernetes.io/instance: ingress-nginx
        app.kubernetes.io/name: ingress-nginx
    spec:
      containers:
      - args:
        - /nginx-ingress-controller
        - --publish-service=$(POD_NAMESPACE)/ingress-nginx-controller
        - --election-id=ingress-controller-leader
        - --watch-ingress-without-class=true
        - --default-backend-service=app-space/default-http-backend
        - --controller-class=k8s.io/ingress-nginx
        - --ingress-class=nginx
        - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller
        - --validating-webhook=:8443
        - --validating-webhook-certificate=/usr/local/certificates/cert
        - --validating-webhook-key=/usr/local/certificates/key
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: LD_PRELOAD
          value: /usr/local/lib/libmimalloc.so
        image: registry.k8s.io/ingress-nginx/controller:v1.1.2@sha256:28b11ce69e57843de44e3db6413e98d09de0f6688e33d4bd384002a44f78405c
        imagePullPolicy: IfNotPresent
        lifecycle:
          preStop:
            exec:
              command:
              - /wait-shutdown
        livenessProbe:
          failureThreshold: 5
          httpGet:
            path: /healthz
            port: 10254
            scheme: HTTP
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        name: controller
        ports:
        - name: http
          containerPort: 80
          protocol: TCP
        - containerPort: 443
          name: https
          protocol: TCP
        - containerPort: 8443
          name: webhook
          protocol: TCP
        readinessProbe:
          failureThreshold: 3
          httpGet:
            path: /healthz
            port: 10254
            scheme: HTTP
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        resources:
          requests:
            cpu: 100m
            memory: 90Mi
        securityContext:
          allowPrivilegeEscalation: true
          capabilities:
            add:
            - NET_BIND_SERVICE
            drop:
            - ALL
          runAsUser: 101
        volumeMounts:
        - mountPath: /usr/local/certificates/
          name: webhook-cert
          readOnly: true
      dnsPolicy: ClusterFirst
      nodeSelector:
        kubernetes.io/os: linux
      serviceAccountName: ingress-nginx
      terminationGracePeriodSeconds: 300
      volumes:
      - name: webhook-cert
        secret:
          secretName: ingress-nginx-admission

---
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: null
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.1.2
    helm.sh/chart: ingress-nginx-4.0.18
  name: ingress-controller
  namespace: ingress-nginx
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 80
    nodePort: 30080
  selector:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
  type: NodePort


          
Ingress Resource:
  For different sercice of our app we define differern path, in ingress servie this.
  In Ingress yml file we have to create rule with path and assign serviceName and port.
  We cna handel this routing two way:
    - /path-routing
    - host: ware.domain-name

For get Ingress Resource:
=>kubectl get ingress --all-namespaces

Ingress resource, Resource path cand edit by only one line and effected:
=>k edit ingress ingress-wear-watch -n app-space

Simple Ingress Resource:
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: test-ingress
  namespace: critical-space
spec:
  rules:
  - http:
      paths:
      - path: /pay
        pathType: Prefix
        backend:
          service:
           name: pay-service
           port:
            number: 8282

OR
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress-wear-watch
  namespace: app-space
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/ssl-redirect: "false"
spec:
  rules:
  - http:
      paths:
      - backend:
          service:
            name: wear-service
            port:
              number: 8080
        path: /wear
        pathType: Prefix
      - backend:
          service:
            name: video-service
            port:
              number: 8080
        path: /stream
        pathType: Prefix
      - backend:
          service:
            name: food-service
            port:
              number: 8080
        path: /eat
        pathType: Prefix


For a new application added:
- Need a service what is haveto route traffic and mentation on Ingress defenation yaml file.
- Need a deployment with this service, pod running with this deployment

(app mean a deployment)
=>kubectl get deploy --all-namespaces


Format:
=>kubectl create ingress <ingress-name> --rule="host/path=service:port"

Example:
=>kubectl create ingress ingress-test --rule="wear.my-online-store.com/wear*=wear-service:80"






##Solution: Ingress Networking-1
--------------------------------------------------
1)We have deployed Ingress Controller, resources and applications. Explore the setup.

=>kubectl get deployment -A
=>kubectl get po -A
=>kubectl get all -A
Show all resource from all namespace


2)Which namespace is the Ingress Controller deployed in?
=>kubectl get all -A

3)What is the name of the Ingress Controller Deployment?
=>kubectl get deploy -n ingress-nginx 


4)Which namespace are the applications deployed in?
=>kubectl get pod -A


5)How many applications are deployed in the app-space namespace?
=>kubectl get pod -n app-space

6)Which namespace is the Ingress Resource deployed in?
=>kubectl get ingress -A
=>kubectl get ingress --all-namespaces


7)What backend is the /wear path on the Ingress configured with?
=>kubectl describe ingress ingress-wear-watch -n app-space

8)At what path is the video streaming application made available on the Ingress?
=>kubectl describe ingress ingress-wear-watch -n app-space
Check:
Rules:
  Host        Path  Backends
  ----        ----  --------
  *           
              /wear    wear-service:8080 (10.244.0.6:8080)
              /watch   video-service:8080 (10.244.0.5:8080)


9)You are requested to change the URLs at which the applications are made available.
Make the video application available at /stream.
Ingress: ingress-wear-watch
Path: /stream
Backend Service: video-service
Backend Service Port: 8080

=>kubectl edit ingress ingress-wear-watch -n app-space
Edit the path:         path: /stream

Check:
spec:
  rules:
  - http:
      paths:
      - backend:
          service:
            name: wear-service
            port:
              number: 8080
        path: /wear
        pathType: Prefix
      - backend:
          service:
            name: video-service
            port:
              number: 8080
        path: /stream
        pathType: Prefix



10)You are requested to add a new path to your ingress to make the food delivery application available to your customers.
Make the new application available at /eat.

Ingress: ingress-wear-watch
Path: /eat
Backend Service: food-service
Backend Service Port: 8080

=>kubectl get deploy -n app-space
=>kubectl edit ingress ingress-wear-watch -n app-space
Add this:
     - backend:
          service:
            name: food-service
            port:
              number: 8080
        path: /eat
        pathType: Prefix


11)Identify the namespace in which the new application is deployed.
=>kubectl get pod -A


12)What is the name of the deployment of the new application?
=>kubectl get deployment -n critical-space



13)You are requested to make the new application available at /pay.
Identify and implement the best approach to making this application available on the ingress controller and test to make sure its working. Look into annotations: rewrite-target as well.

Ingress Created
Path: /pay
Configure correct backend service
Configure correct backend port

Create a new Ingress for the new pay application in the critical-space namespace.
Use the command kubectl get svc -n critical-space to know the service and port details.

Solution manifest file to create a new ingress service to make the application available at /pay as follows:

---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: test-ingress
  namespace: critical-space
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/ssl-redirect: "false"
spec:
  rules:
  - http:
      paths:
      - path: /pay
        pathType: Prefix
        backend:
          service:
           name: pay-service
           port:
            number: 8282

OR
=>kubectl get svc -n critical-space
Check this servie name and port:
pay-service   ClusterIP   10.97.197.16   <none>        8282/TCP   6m8s

=>kubectl create ingress -h
=kubectl create ingress simple --rule="foo.com/bar=svc1:8080,tls=my-cert"

=>kubectl create ingress ingress-pay -n critical-space --rule="/pay=pay-service:8282"
=>kubectl get ingress -n critical-space
=>kubectl describe ingress ingress-pay -n critical-space

=>kubectl get pod -n critical-space
=>kubectl logs webapp-pay-58cdc69889-s4xd5  -n critical-space

=>kubectl edit ingress ingress-pay -n critical-space
Add this in metadata section:

annotations:
  nginx.ingress.kubernetes.io/rewrite-target: /






##Solution: Ingress Networking-2
--------------------------------------------------
Create a NGINX Ingress Controller App:

1)Create a name space ingress-nginx
=>kubectl create namespace ingress-nginx
=>kubectl get namespace

2)The NGINX Ingress Controller requires a ConfigMap object. 
Create a ConfigMap object with name ingress-nginx-controller in the ingress-nginx namespace.

No data needs to be configured in the ConfigMap.
Name: ingress-nginx-controller

=>kubectl create configmap -h
=>kubectl create configmap ingress-nginx-controller -n ingress-nginx



3)The NGINX Ingress Controller requires two ServiceAccounts. 
Create both ServiceAccount with name ingress-nginx and ingress-nginx-admission in the ingress-nginx namespace.
Use the spec provided below.
Name: ingress-nginx
Name: ingress-nginx-admission

=>kubectl create sa -h
=>kubectl create sa ingress-nginx -n ingress-nginx
=>kubectl create sa ingress-nginx-admission -n ingress-nginx

=>kubectl get sa -n ingress-nginx

4)We have created the Roles, RoleBindings, ClusterRoles, and ClusterRoleBindings for the ServiceAccount. Check it out!!

=>kubectl get roles -n ingress-nginx
=>kubectl get rolebindings -n ingress-nginx
=>kubectl describe role ingress-nginx -n ingress-nginx



5)Let us now deploy the Ingress Controller. Create the Kubernetes objects using the given file.
The Deployment and it's service configuration is given at /root/ingress-controller.yaml. 
There are several issues with it. Try to fix them.
Deployed in the correct namespace.

Replicas: 1
Use the right image
Namespace: ingress-nginx
Service name: ingress-nginx-controller
NodePort: 30080


=>kubectl apply -f ingress-controller.yaml

=>cat ingress-controller.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.1.2
    helm.sh/chart: ingress-nginx-4.0.18
  name: ingress-nginx-controller
  namespace: ingress-nginx
spec:
  minReadySeconds: 0
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/component: controller
      app.kubernetes.io/instance: ingress-nginx
      app.kubernetes.io/name: ingress-nginx
  template:
    metadata:
      labels:
        app.kubernetes.io/component: controller
        app.kubernetes.io/instance: ingress-nginx
        app.kubernetes.io/name: ingress-nginx
    spec:
      containers:
      - args:
        - /nginx-ingress-controller
        - --publish-service=$(POD_NAMESPACE)/ingress-nginx-controller
        - --election-id=ingress-controller-leader
        - --watch-ingress-without-class=true
        - --default-backend-service=app-space/default-http-backend
        - --controller-class=k8s.io/ingress-nginx
        - --ingress-class=nginx
        - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller
        - --validating-webhook=:8443
        - --validating-webhook-certificate=/usr/local/certificates/cert
        - --validating-webhook-key=/usr/local/certificates/key
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: LD_PRELOAD
          value: /usr/local/lib/libmimalloc.so
        image: registry.k8s.io/ingress-nginx/controller:v1.1.2@sha256:28b11ce69e57843de44e3db6413e98d09de0f6688e33d4bd384002a44f78405c
        imagePullPolicy: IfNotPresent
        lifecycle:
          preStop:
            exec:
              command:
              - /wait-shutdown
        livenessProbe:
          failureThreshold: 5
          httpGet:
            path: /healthz
            port: 10254
            scheme: HTTP
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        name: controller
        ports:
        - name: http
          containerPort: 80
          protocol: TCP
        - containerPort: 443
          name: https
          protocol: TCP
        - containerPort: 8443
          name: webhook
          protocol: TCP
        readinessProbe:
          failureThreshold: 3
          httpGet:
            path: /healthz
            port: 10254
            scheme: HTTP
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        resources:
          requests:
            cpu: 100m
            memory: 90Mi
        securityContext:
          allowPrivilegeEscalation: true
          capabilities:
            add:
            - NET_BIND_SERVICE
            drop:
            - ALL
          runAsUser: 101
        volumeMounts:
        - mountPath: /usr/local/certificates/
          name: webhook-cert
          readOnly: true
      dnsPolicy: ClusterFirst
      nodeSelector:
        kubernetes.io/os: linux
      serviceAccountName: ingress-nginx
      terminationGracePeriodSeconds: 300
      volumes:
      - name: webhook-cert
        secret:
          secretName: ingress-nginx-admission

---
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: null
  labels:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/part-of: ingress-nginx
    app.kubernetes.io/version: 1.1.2
    helm.sh/chart: ingress-nginx-4.0.18
  name: ingress-controller
  namespace: ingress-nginx
spec:
  ports:
  - port: 80
    protocol: TCP
    targetPort: 80
    nodePort: 30080
  selector:
    app.kubernetes.io/component: controller
    app.kubernetes.io/instance: ingress-nginx
    app.kubernetes.io/name: ingress-nginx
  type: NodePort


6)Create the ingress resource to make the applications available at /wear and /watch on the Ingress service.
Also, make use of rewrite-target annotation field: -

nginx.ingress.kubernetes.io/rewrite-target: /
Ingress resource comes under the namespace scoped, so don't forget to create the ingress in the app-space namespace.

Ingress Created
Path: /wear
Path: /watch
Configure correct backend service for /wear
Configure correct backend service for /watch
Configure correct backend port for /wear service
Configure correct backend port for /watch service

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress-wear-watch
  namespace: app-space
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/ssl-redirect: "false"
spec:
  rules:
  - http:
      paths:
      - path: /wear
        pathType: Prefix
        backend:
          service:
           name: wear-service
           port: 
            number: 8080
      - path: /watch
        pathType: Prefix
        backend:
          service:
           name: video-service
           port:
            number: 8080

OR

=>kubectl get deployment -n ingress-nginx

=>kubectl expose deploy ingress-nginx-controller -n ingress-nginx --name ingress --port=80 --target-port=80 --type NodePort
=>kubectl get svc -n ingress-nginx
=>kubectl edit svc ingress -n ingress-nginx


=>kubectl get ingress -n ingress-nginx
=>ubectl create ingress -h
=>kubectl create ingress simple --rule="foo.com/bar=svc1:8080,tls=my-cert"

=>kubectl create ingress ingress-wear-watch -n app-space --rule="/wear=wear-service:8080" --rule="/watch=video-service:8080"
=>kubectl describe ingress -n app-space

=>kubectl edit ingress ingress-wear-watch -n app-space

Add this under metadata:
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/ssl-redirect: "false"



More Info:

https://platform9.com/learn/v1.0/tutorials/nginix-controller-via-yaml
https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v0.44.0/deploy/static/provider/cloud/deploy.yaml






##State Persistence
==================================================


Volume in Docker
--------------------------------------------------
Generally Docker do not save data any host path or volume.
Docker Staorage:
- Storage Drivers
- Volume Drivers

Docker stoarage in local machine file system in: /var/lib/docker
Docker manage Image and Container in Layer model, When we Create a container from a Image
It crete a container (R/W) layer top on Image. Image is readonly and Container layer copy all file in 
Contaner layer what is Read/Write layer.


Docker has Two type mounting:
- Volume Mounting | data seve on var/lib/docker/volumes directory
- Bind Mounting   | In this case container mount its data in host absulate path

Docker has different staorage driver:
 - AUFS
 - ZFS
 - BTRFS
 - DEVECE MAPPER
 - OVERLAY

  

Volume in Kubernetes
--------------------------------------------------
Generally Kubernetes do not save data any host path or volume.
In Kubernetes we have to create volumem in host or other or Storage provide like AWS location to keep data
and Mounting it on Pod

For this in Multi Node cluster with multiple pod it make huge task, to reduce this operation consists
Prestent volume come in to picture.




Using PVCs in Pods:

apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
    - name: myfrontend
      image: nginx
      volumeMounts:
      - mountPath: "/var/www/html"
        name: mypd
  volumes:
    - name: mypd
      persistentVolumeClaim:
        claimName: myclaim

The same is true for ReplicaSets or Deployments. Add this to the pod template section of a Deployment on ReplicaSet.



What type of volume is used to mount a Microsoft Azure Data Disk into a Pod?
AzurDisk

What volume type is to be used to mount a directory from the node on which the pod is running?
hostPath

What volume type is used to store data in a Pod only as long as that Pod is running on that node? 
When the Pod is deleted the files are to be deleted as well.
emptyDir

What is the fc volume type used for?
To mount and existing Fiber channel volume into a Pod


HostPath as Volume:

apiVersion: v1
kind: Pod
metadata:
  name: webapp
spec:
  containers:
  - name: event-simulator
    image: kodekloud/event-simulator
    env:
    - name: LOG_HANDLERS
      value: file
    volumeMounts:
    - mountPath: /log           | Inside Container path
      name: log-volume

  volumes:
  - name: log-volume
    hostPath:
      path: /var/log/webapp      | Host machine path
      type: Directory

This hostPath good for single host machine, for multi node cluster this is not a good approce.
Have to use external replicated storage solution, there are many like: awsAlasticStoraverVolume 



##Persistent Volumes | Persistent Volumes Claims
--------------------------------------------------
In Kubernetes for centrally mange pod data and volume user combinagion of:
- Presistent Volume and 
- PersistentVolumeClaim

apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-log
spec:
  capacity:
    storage: 100Mi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteMany
  persistentVolumeReclaimPolicy: Retain
  hostPath:
    path: /pv/log

Its has Three Access Mode:
 - ReadWriteMany
 - ReadWriteOnce
 - ReadOnlyMany


 Persistent Volumes and Persistent Volumes Claims are two dirrerent Object in kunbernetes.
 Usually admin create PV and User user to PVC
 Every PVC bind with a single PV(One -to -One)


Same path tow mouintpoint:

apiVersion: v1
kind: Pod
metadata:
  name: webapp2
spec:
  containers:
  - name: webapp
    image: kodekloud/event-simulator
    volumeMounts:
      - name: app-log
        mountPath: /log
      - name: app-log2
        mountPath: /log/
  volumes:
    - name: app-log
      hostPath:
        path: /var/log/webapp
    - name: app-log2
      hostPath:
          path: /root


##Solution1: Persistent Volumes | Persistent Volumes Claims
--------------------------------------------------

1)The application stores logs at location /log/app.log. View the logs.
You can exec in to the container and open the file:

=>kubectl exec webapp -- cat /log/app.log


2)Configure a volume to store these logs at /var/log/webapp on the host.
Use the spec provided below.
Name: webapp
Image Name: kodekloud/event-simulator
Volume HostPath: /var/log/webapp
Volume Mount: /log

=>kubectl get po webapp -o yaml > webapp.yaml

apiVersion: v1
kind: Pod
metadata:
  name: webapp
spec:
  containers:
  - name: event-simulator
    image: kodekloud/event-simulator
    env:
    - name: LOG_HANDLERS
      value: file
    volumeMounts:
    - mountPath: /log
      name: log-volume

  volumes:
  - name: log-volume
    hostPath:
      # directory location on host
      path: /var/log/webapp
      # this field is optional
      type: Directory
OR
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: webapp
  name: webapp
spec:
  containers:
  - image: kodekloud/event-simulator
    name: webapp
    volumeMounts:
      - mountPath: /log
        name: my-vol
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
  volumes:
    - name: my-vol
      hostPath:
        path: /var/log/webapp

=>kubectl replace --force -f pod4.yaml
=>ls  /var/log/webapp/app.log 



3)Create a Persistent Volume with the given specification.
Volume Name: pv-log
Storage: 100Mi
Access Modes: ReadWriteMany
Host Path: /pv/log
Reclaim Policy: Retain

apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-log
spec:
  capacity:
    storage: 100Mi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteMany
  persistentVolumeReclaimPolicy: Retain
  hostPath:
    path: /pv/log

=>kubectl apply -f pv1.yaml 
=>kubectl get pv


4)Let us claim some of that storage for our application. Create a Persistent Volume Claim with the given specification.
Volume Name: claim-log-1
Storage Request: 50Mi
Access Modes: ReadWriteOnce


apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: claim-log-1
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 50Mi

=>kubectl get pvc


5)Why is the claim not bound to the available Persistent Volume?
=>kubectl get pv,pvc
Because those both has access Modes Mismatch.

6)Update the Access Mode on the claim to bind it to the PV.
Delete and recreate the claim-log-1.


apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: claim-log-1
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 50Mi

=>kubectl replace --force -f pvc.yaml 
=>kubectl get pvc


7)You requested for 50Mi, how much capacity is now available to the PVC?
=>kubectl get pvc



8)Update the webapp pod to use the persistent volume claim as its storage.
Replace hostPath configured earlier with the newly created PersistentVolumeClaim.
Name: webapp
Image Name: kodekloud/event-simulator
Volume: PersistentVolumeClaim=claim-log-1
Volume Mount: /log

=>ls /pv/log

apiVersion: v1
kind: Pod
metadata:
  name: webapp
spec:
  containers:
  - name: event-simulator
    image: kodekloud/event-simulator
    env:
    - name: LOG_HANDLERS
      value: file
    volumeMounts:
    - mountPath: /log
      name: log-volume
  volumes:
  - name: log-volume
    persistentVolumeClaim:
      claimName: claim-log-1
OR
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: webapp
  name: webapp
spec:
  containers:
  - image: kodekloud/event-simulator
    name: webapp
    volumeMounts:
      - mountPath: /log
        name: my-vol
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
  volumes:
    - name: my-vol
      persistentVolumeClaim:
        claimName: claim-log-1


=>kubectl replace --force -f pod5.yaml
=>ls /pv/log


HostPath Volume:

apiVersion: v1
kind: Pod
metadata:
  name: test-pd
spec:
  containers:
  - image: registry.k8s.io/test-webserver
    name: test-container
    volumeMounts:
    - mountPath: /test-pd
      name: test-volume
  volumes:
  - name: test-volume
    hostPath:
      # directory location on host
      path: /my/host/path
      # this field is optional
      type: Directory


ConfigMap as Volume:

apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
  - name: mypod
    image: redis
    volumeMounts:
    - name: foo
      mountPath: "/etc/foo"
      readOnly: true
  volumes:
  - name: foo
    configMap:
      name: myconfigmap


SecretAsVolume:

apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
    - name: mypod
      image: redis
      volumeMounts:
        - name: foo
          mountPath: "/etc/foo"
          readOnly: true
  volumes:
    - name: foo
      secret:
        secretName: mysecret
        optional: true




9)What is the Reclaim Policy set on the Persistent Volume pv-log?
=>kubectl get pv
Check:
RECLAIM POLICY=Retain

Its mean: After deleteging PVC PV will Retain but not available for use. 


10)Try deleting the PVC and notice what happens.
If the command hangs, you can use CTRL + C to get back to the bash prompt OR check the status of the pvc from another terminal

=>kubectl delete pvc claim-log-1
Now STATUS will be Terminating 



11)Why is the PVC stuck in Terminating state?
The PVC being use by the pod of webapp.
The PVC was still being used by the webapp pod when we issued the delete command. Until the pod is deleted, the PVC will remain in a terminating state.



CKAD exam this is all about Presistent volume:
We have covered the required topics for the exam for the Storage section.




##Storage Class
--------------------------------------------------
Every pcv bind with pv, so have to create it before binging, this is call static Provisioning.
Storage class come to Dynamic Provisioning.

In this case we not have to create any pv(It will create auto by Storage class), 
we have to create just StorageClass and come it with pvc to pod binging automatically.







StatefulSets:
--------------------------------------------------
StatefulSets are like DeploymentSet, dirrerent is its run pod sequential with pod index number.

Deployment not possible to run pod sequentially what is required, then come to StatefulSets.
StatefulSets same as deploument, its create pod flow the deployment template and scale as required also rolling update.



Headless Service:
--------------------------------------------------


PersistentVolumeClaimTemplate for create auto vpc.





##Solution2: Storage Class
--------------------------------------------------
=>kubectl get sc
=>kubectl get storageclass


1)What is the name of the Storage Class that does not support dynamic volume provisioning?
local-storage

The "local-storage" Storage Class in Kubernetes does not support dynamic volume provisioning. 
It is used to represent local storage devices attached to the nodes in the cluster. 
With this Storage Class, you need to manually manage the provisioning and allocation of local storage volumes.


What is the Volume Binding Mode used for this storage class (the one identified in the previous question)?
=>kubectl describe sc local-storage
=>kubectl get sc

2)Let's fix that. Create a new PersistentVolumeClaim by the name of local-pvc that should bind to the volume local-pv.
Inspect the pv local-pv for the specs.
PVC: local-pvc
Correct Access Mode?
Correct StorageClass Used?
PVC requests volume size = 500Mi?

Inspect the persistent volume and look for the Access Mode, Storage and StorageClassName used. Use this information to create the PVC.
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: local-pvc
spec:
  accessModes:
  - ReadWriteOnce
  storageClassName: local-storage
  resources:
    requests:
      storage: 500Mi
OR
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: local-pvc
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: local-storage
  resources:
    requests:
      storage: 500Mi
  selector:
    matchLabels:
      volume: local-pv

The Storage Class called local-storage makes use of VolumeBindingMode set to WaitForFirstConsumer. 
This will delay the binding and provisioning of a PersistentVolume until a Pod using the PersistentVolumeClaim is created.
Run the command: kubectl describe pvc local-pvc and look under the Events section.


3)Create a new pod called nginx with the image nginx:alpine. The Pod should make use of the PVC local-pvc and mount the volume at the path /var/www/html.
apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    name: nginx
spec:
  containers:
  - name: nginx
    image: nginx:alpine
    volumeMounts:
      - name: local-persistent-storage
        mountPath: /var/www/html
  volumes:
    - name: local-persistent-storage
      persistentVolumeClaim:
        claimName: local-pvc
OR
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
    - name: nginx
      image: nginx:alpine
      ports:
        - containerPort: 80
      volumeMounts:
        - name: data-volume
          mountPath: /var/www/html
  volumes:
    - name: data-volume
      persistentVolumeClaim:
        claimName: local-pvc

apiVersion: v1
kind: Pod
metadata:
  name: nginx
  labels:
    name: nginx
spec:
  containers:
  - name: nginx
    image: nginx:alpine
    volumeMounts:
      - name: local-persistent-storage
        mountPath: /var/www/html
  volumes:
    - name: local-persistent-storage
      persistentVolumeClaim:
        claimName: local-pvc


4)Create a new Storage Class called delayed-volume-sc that makes use of the below specs:
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer

apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: delayed-volume-sc
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer










##Update for Sep 2021
==================================================

##Define, Build and Modify Container Image  
--------------------------------------------------
In a Dockerfile every thing in RIGHT CAPS all are INSTRACTION and left are ARGUMENT.
Docker image are Layers architecture.



##Solution1: Docker
--------------------------------------------------

1)When a container is created using the image built with this Dockerfile, what is the command used to RUN the application inside it.

Open the Dockerfile and look for ENTRYPOINT command.


2)Run an instance of the image webapp-color and publish port 8080 on the container to 8282 on the host.
=>docker run -p 8282:8080 -d webapp-color 


3)What is the base Operating System used by the python:3.6 image?
=>docker run python:3.6 cat /etc/*release*

4)Build a new smaller docker image by modifying the same Dockerfile and name it webapp-color and tag it lite.
Hint: Find a smaller base image for python:3.6. Make sure the final image is less than 150MB.

Ans:
In the webapp-color directory, run the ls -l command to list the Dockerfile and other files.
And modify Dockerfile to use python:3.6-alpine image and then build using docker build -t webapp-color:lite .

5)Run an instance of the new image webapp-color:lite and publish port 8080 on the container to 8383 on the host.
=>docker run -d -p 8383:8080 webapp-color:lite





##Authentication, Authorization and Admission Control
--------------------------------------------------
You must have to secure kubernetes cluster host secure.

kube-apiserver one of  is the entry door of kunbernetes cluster, almost all operation can be done by this api server.
So, this is the second point to make secure. Also make controll of accessing of kube-apiserver its self.

Authentication of kunbe-apiserver by:
 - Files-Username and Password
 - Files-Username and Token
 - certificates
 - Externaem provider - LDAP
 - Servcie Account


Authorization of kube-apiserver by:
- RBAC Authorization
- ABAC Authorization
- Node Authorization
- Webhook Mode

All Communication between kube-apiserver and other component like kubelet, etcd, controller etc are TLS Encryped.

Authentication
---------------------------------------------------
Two type of kubernetes user:
- User/human
- Service Account/ Machine
Kubernetes its not manage account natively, user exterrna machinesume. like LDAP.
But kubernetes manage ServiceAccount.


All request go to kube-apiserver for authencation:
kubectl ------------------------------>
                                          -> kube-apiserver
crul https://kube-server-ip ---------->

Auth Mechanisms:
 - Static Password File
 - Static Token File
 - certificates
 - Identity Server


CERTIFICATE generation not part on CKAD exam.






##KubeConfig
--------------------------------------------------
kube-apiserver restrect/authencation/authorization done when a api/activity done in cluster.

For a get request involvement:
=>kubectl get pod
 --server my-kube-playground:6443
 --client-ke admin.key
 --client-certificate admin.crt
 --certificate-suthority ca.crt

OR
=>kubectl get pod 
   --kubeconfig=my-custome-config
This way all relevent data in a single config file

OR
=>kubectl get pod 
If you put notheng by default if find a config file in the path:
$HOME/.kube/config

By default kubectl tools find a file under user home directory, $HOME/.kube/config for authencation.
This why you dont add any file kubectl command any file.


KubeConfig will three section in a single file called Config in .kube folder:
Section are:
 - Cluster       | Array of cluster
 - Contexts      | Array of contents, Cluster and user thogether build context
 - Users         | Array of Users

=>kubectl config -h
=>kubectl config view
=>kubectl config view --kubeconfig=my-custome-config

=>kubectl config use-context prod-user@production

Set Context:
=>kubectl config --kubeconfig=/root/my-kube-config use-context research
=>kubectl config --kubeconfig=/root/my-kube-config current-context

=>k get pod --kubeconfig=my-kube-config

cat my-kube-config
apiVersion: v1
clusters:
- cluster:
    certificate-authority: /etc/kubernetes/pki/ca.crt
    server: https://controlplane:6443
  name: development
- cluster:
    certificate-authority: /etc/kubernetes/pki/ca.crt
    server: https://controlplane:6443
  name: imran-cluster
- cluster:
    certificate-authority: /etc/kubernetes/pki/ca.crt
    server: https://controlplane:6443
  name: kubernetes-on-aws
- cluster:
    certificate-authority: /etc/kubernetes/pki/ca.crt
    server: https://controlplane:6443
  name: production
- cluster:
    certificate-authority: /etc/kubernetes/pki/ca.crt
    server: https://controlplane:6443
  name: test-cluster-1
contexts:
- context:
    cluster: kubernetes-on-aws
    user: aws-user
  name: aws-user@kubernetes-on-aws
- context:
    cluster: imran-cluster
    namespace: kube-system
    user: imran-user
  name: imran
- context:
    cluster: test-cluster-1
    user: dev-user
  name: research
- context:
    cluster: development
    user: test-user
  name: test-user@development
- context:
    cluster: production
    user: test-user
  name: test-user@production
current-context: imran
kind: Config
preferences: {}
users:
- name: aws-user
  user:
    client-certificate: /etc/kubernetes/pki/users/aws-user/aws-user.crt
    client-key: /etc/kubernetes/pki/users/aws-user/aws-user.key
- name: dev-user
  user:
    client-certificate: /etc/kubernetes/pki/users/dev-user/dev-user.crt
    client-key: /etc/kubernetes/pki/users/dev-user/dev-user.key
- name: imran-user
  user:
    client-certificate: /etc/kubernetes/pki/users/test-user/test-user.crt
    client-key: /etc/kubernetes/pki/users/test-user/test-user.key
- name: test-user
  user:
    client-certificate: /etc/kubernetes/pki/users/test-user/test-user.crt
    client-key: /etc/kubernetes/pki/users/test-user/test-user.key


current-context: test-user@development
This is for current contest set, To see default config file this:
=>kubectl config view
For custome file view command:
=>kubectl config view --kubeconfig=my-custome-config

To user Dfirrent config bile bring this in default flder .kube, then to set deferent context user:
=>kubectl config use-context prod-user@production





##Solution2: KubeConfig
--------------------------------------------------

1)Where is the default kubeconfig file located in the current environment?
Find the current home directory by looking at the HOME environment variable.

=>echo $HOME
=>pwd 
=>ls .kube
=>cat .kube/config

2)How many cluster and Users are defined in the default kubeconfig file?
=>kubectl config view
OR
=>cat .kube/config

3)What is the name of the cluster/user/context configured in the default kubeconfig file?
=>kubectl config view
=>cat .kube/config
Check:
users:
- name: kubernetes-admin

contexts:
- context:
    cluster: kubernetes


4)I would like to use the dev-user to access test-cluster-1. Set the current context to the right one so I can do that.
Once the right context is identified, use the kubectl config use-context command.
Current context set

To use that context, run the command: 
=>kubectl config --kubeconfig=/root/my-kube-config use-context research

To know the current context, run the command: 
=>kubectl config current-context
=>kubectl config --kubeconfig=/root/my-kube-config current-context
OR
By default kubectl config command user /root/.kube/config file, to user custome file have to mentation like this

=>kubectl config use-context research --kubeconfig /root/my-kube-config

=>cat /root/my-kube-config
Check:
current-context: research
kind: Config
preferences: {}


5)We don't want to have to specify the kubeconfig file option on each command. 
Make the my-kube-config file the default kubeconfig.
Default kubeconfig file configured

=>mv /root/my-kube-config /root/.kube/config
=>cat /root/.kube/config
=>kubectl config view



6)With the current-context set to research, we are trying to access the cluster. However something seems to be wrong. Identify and fix the issue.
Try running the kubectl get pods command and look for the error. All users certificates are stored at /etc/kubernetes/pki/users.

=>ls /etc/kubernetes/pki/users/dev-user/
Its look file name issue in config file, lets fixed it
=>vi  /root/.kube/config

=>kubectl get pod
Now request no error.





##API Groups
--------------------------------------------------
curl http://localhost:6443 -k

=>kubectl proxy
=>curl http://localhost:8081- -k

=>curl https://kube-master:6443/version
=>curl https://kube-master:6334/api/v1/pods

Kubernetes has different type of api Groups: /metrics, /healthz, /version,/api,/apis,/logs

This two Groups 
- /api   | Core group
- /apis  | Name Group

Under this Core grpup: /api
/v1 - under this
/namespace, /pods,/rc,events,/endpoint,/nods,/binding,/pv,/pvc,/configmap,/service,/secrets etc.


Under this named grpup: /apis
/apps,/extensions,/networking.k8s.io,/storage.k8s.io,/storage.k8s.io,/authentication.k8s.io,/certificate.k8s.io etc.
and Unser this named grpup: all apis has its own resource and resources has its action.

/apps:
 - /v1
   - /deployment
       - /list
       - /get
       - /create 
       - /delete
       - /update
       - /watch 
   - /replicaset
   - /StatefulSets
     
Like:
/networking.k8s.io
- /v1 
  - /networkpolicy

Like:
/certificate.k8s
- /v1
    -/certificatesigningrequests

Kubernetes API Groups Documentation show all details.
Also in kunbernetes cluster:
For show all aveilable api groups:
curl http://localhost:6443 -k

Then, show for names api
curl http://localhost:6443/apis -k | grep "name"

To access kubectl cluster make a proxy:
=>kubectl proxy
=>curl http://localhost:8081- -k



kube proxy not equeal as kubectl proxy.

kube proxy    | for create a proxy server for make a proxy in between tow service or other component
kubectl proxy | for make curl rquest with default certificat info to kube-apiserver





ClusterConfig cluster,user,context relevent to: $HOME/.kube/config  config.
Authorization&Authorization relavent to: kube-apiserver pod | /etc/kubernetes/kube-apiserver.yaml


##Authorization
--------------------------------------------------

Authorization Mechanisms:
 - Node
   Node authorization.

 - ABAC   
   attributes base authorization, create attributes in json formate a file and add this.
   every time upde this file need to restart.

 - RBAC
   Create role and add user on this role.
 - Webhook
   Outsource for authencation.
-AlwaysAllow
-AlwaysDeny

You cand user multiple authorization mode at a time. all mode working on flow the order.


##Role Based Access Controls
--------------------------------------------------
Create a role using file:

apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: default
  name: developer
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["list", "create","delete"]

Now we have to create a role bindign, rolebinding bind the role to the user.

kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: dev-user-binding
subjects:
- kind: User
  name: dev-user
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: developer
  apiGroup: rbac.authorization.k8s.io


To check Role and rolebinding:
=>kubectl get roles
=>kubectl get rolebindings
=>kubectl describe role developer

To check access or other persmission:
=>kubectl auth can-i create deployment
=>kubectl auth can-i delete nodes

=>kubectl auth can-i create deployment --as dev-user
=>kubectl auth can-i create deployment --as dev-user --namespace test


Role: Create Role Object by specifice name, rule with a yaml file
RoleBinding: Bind User with a role








##Solution: Role Based Access Controls
--------------------------------------------------

1) Inspect the environment and identify the authorization modes configured on the cluster.
Check the kube-apiserver settings.
Use the command 
=>kubectl describe pod kube-apiserver-controlplane -n kube-system 
and look for --authorization-mode.
OR
=>cat /etc/kubernetes/manifests/kube-apiserver.yaml
Check: authorization-mode=
spec:
  containers:
  - command:
    - kube-apiserver
    - --advertise-address=192.7.86.6
    - --allow-privileged=true
    - --authorization-mode=Node,RBAC
OR
=>ps -aux | grep authorization


2)How many roles exist in the default namespace?
=>kubectl get roles

3)How many roles exist in all namespaces together?
=>kubectl get roles -A --no-headers | wc -l


4)What are the resources the kube-proxy role in the kube-system namespace is given access to?
=>kubectl describe role kube-proxy -n kube-system


5)Which account is the kube-proxy role assigned to?
=>kubectl get rolebinding kube-proxy -n kube-system
=>kubectl describe rolebinding kube-proxy -n kube-system



6)A user dev-user is created. User's details have been added to the kubeconfig file. Inspect the permissions granted to the user. Check if the user can list pods in the default namespace.
Use the --as dev-user option with kubectl to run commands as the dev-user.

=>kubectl config view
=>kubectl auth can-i list pods --as dev-user
=>kubectl get pods --as dev-user



7)Create the necessary roles and role bindings required for the dev-user to create, list and delete pods in the default namespace.
Use the given spec:
Role: developer
Role Resources: pods
Role Actions: list
Role Actions: create
Role Actions: delete
RoleBinding: dev-user-binding
RoleBinding: Bound to dev-user

Use the command kubectl create to create a role developer and rolebinding dev-user-binding in the default namespace.

To create a Role:- 
=>kubectl create role developer --namespace=default --verb=list,create,delete --resource=pods

To create a RoleBinding:- 
=>kubectl create rolebinding dev-user-binding --namespace=default --role=developer --user=dev-user
OR
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: default
  name: developer
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["list", "create","delete"]

---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: dev-user-binding
subjects:
- kind: User
  name: dev-user
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: developer
  apiGroup: rbac.authorization.k8s.io

OR

Create Role:
=>kubectl create role -h
=>kubectl create role developer --verb=list,create,delete --resource=pods
=>kubectl get role
=>kubectl describe roles developer 

CreateRoleBinding:
=>kubectl create rolebinding -h
=>kubectl create rolebinding dev-user-binding --role=developer --user=dev-user
=>kubectl describe rolebinding dev-user-binding 


8)A set of new roles and role-bindings are created in the blue namespace for the dev-user. However, the dev-user is unable to get details of the dark-blue-app pod in the blue namespace. Investigate and fix the issue.
We have created the required roles and rolebindings, but something seems to be wrong.

New roles and role bindings are created in the blue namespace.
Check out the resourceNames configured on the role.

Run the command: 
=>kubectl edit role developer -n blue 
and correct the resourceNames field. You don't have to delete the role.
OR

=>kubectl get pod dark-blue-app -n blue --as dev-user
=>kubectl get rolebinding -n blue

=>kubectl describe role developer -n blue
Check the Resource Name: dark-blue-app  not in there
PolicyRule:
  Resources  Non-Resource URLs  Resource Names  Verbs
  ---------  -----------------  --------------  -----
  pods       []                 [blue-app]      [get watch create delete]


Need to add this app on resource, for that need to Edit role:

=>kubectl edit role developer -n blue

Add app name on resourceName
rules:
- apiGroups:
  - ""
  resourceNames:
  - dark-blue-app

Check, this ok now
=>kubectl get pod dark-blue-app -n blue --as dev-user



9)Add a new rule in the existing role developer to grant the dev-user permissions to create deployments in the blue namespace.
Remember to add api group "apps".

Use the command kubectl edit to add a new rule for user dev-user to grant permissions to create deployments in the blue namespace.

=>kubectl create deployment mydeploy --image=nginx -n blue --as dev-user
=>kubectl edit role developer -n blue
add this:
- apiGroups:
  - apps
  resources:
  - deployments
  verbs:
  - get
  - watch
  - create
  - delete

So it looks like this:

apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: developer
  namespace: blue
rules:
- apiGroups:
  - apps
  resourceNames:
  - dark-blue-app
  resources:
  - pods
  verbs:
  - get
  - watch
  - create
  - delete
- apiGroups:
  - apps
  resources:
  - deployments
  verbs:
  - create







##144. Cluster Roles
--------------------------------------------------
In kubernetes all Resource are catagoies are namespace or cluster scope.
Namespace Resource:
- pods
- replicaset
- roles
- service 
- deployment etc.
If you not defined any namespace it will create in default namespace.

Cluter Scoped:
- nodes
- pv
- clusterroles
- cluseterrolebinding
- namespaces etc.
Cluster role and Cluster binding for create cluster wise resource and namespace role/binding for namespace wise 
resource modify.
Same as namespace role and rolebinding, first create cluster role and rolebinding for user.

Cluster role can access all namespace resources.



To get the all kist on namespace/non-namesapce resource list:
=>kubectl api-resources --namespaced=true
=>kubectl api-resources --namespaced=false





##Solution4: Cluster Roles
--------------------------------------------------
1)How many ClusterRoles and ClusterRoleBindings do you see defined in the cluster?
=>kubectl get clusterroles --no-headers | wc -l
=>kubectl get clusterroles --no-headers -o json | jq '.items | length'

=>kubectl get clusterrolebinding --no-headers | wc -l
=>kubectl get clusterrolebindings --no-headers -o json | jq '.items | length'


2)What namespace is the cluster-admin clusterrole part of?
Cluster roles and cluster are not part of any namespace


3)What user/groups are the cluster-admin role bound to?
The ClusterRoleBinding for the role is with the same name.
=>kubectl describe clusterrolebinding cluster-admin


4)What level of permission does the cluster-admin role grant?
Inspect the cluster-admin role's privileges.
=>kubectl describe clusterrole cluster-admin


5)A new user michelle joined the team. She will be focusing on the nodes in the cluster. Create the required ClusterRoles and ClusterRoleBindings so she gets access to the nodes.

Use the command kubectl create to create a clusterrole and clusterrolebinding for user michelle to grant access to the nodes.
After that test the access using the command kubectl auth can-i list nodes --as michelle.

=>kubectl auth can-i list nodes --as michelle

=>kubectl create clusterrole cls-role --verb=get,list,create,delete --resource=nodes
OR
=>kubectl create clusterrole node-admin --verb=get,watch,list,create,delete --resource=nodes --dry-run=client -o yaml > node-admin.yaml
cluste.yaml 
---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: node-admin
rules:
- apiGroups: [""]
  resources: ["nodes"]
  verbs: ["get", "watch", "list", "create", "delete"]


=>kubectl create clusterrolebinding michelle-binding --clusterrole=node-admin --user=michelle --dry-run=client -o yaml > michelle-binding.yaml
---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: michelle-binding
subjects:
- kind: User
  name: michelle
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: node-admin
  apiGroup: rbac.authorization.k8s.io

=>kubectl apply -f cluste.yaml 
OR

=>kubectl get nodes --as michelle

CreateClusterRole:
=>kubectl create clusterrole -h
=>kubectl create clusterrole pod-reader --verb=get,list,watch --resource=pods
=>kubectl create clusterrole michelle-role --verb=get,list,watch --resource=nodes

CreateClusterBinding:
=>kubectl create clusterrolebinding michelle-role-binding --clusterrole=michelle-role --user=michelle

=>kubectl describe clusterrole michelle-role
=>kubectl describe clusterrolebinding michelle-role-binding

Check:
=>k auth can-i list node --as michelle
=>k config can-i list node --as michelle




6)michelle's responsibilities are growing and now she will be responsible for storage as well. Create the required ClusterRoles and ClusterRoleBindings to allow her access to Storage.
Get the API groups and resource names from command kubectl api-resources. Use the given spec:

ClusterRole: storage-admin
Resource: persistentvolumes
Resource: storageclasses
ClusterRoleBinding: michelle-storage-admin
ClusterRoleBinding Subject: michelle
ClusterRoleBinding Role: storage-admin

Use the command kubectl create to create a new ClusterRole and ClusterRoleBinding.
Assign it correct resources and verbs.
After that test the access using the command kubectl auth can-i list storageclasses --as michelle.

---
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: storage-admin
rules:
- apiGroups: [""]
  resources: ["persistentvolumes"]
  verbs: ["get", "watch", "list", "create", "delete"]
- apiGroups: ["storage.k8s.io"]
  resources: ["storageclasses"]
  verbs: ["get", "watch", "list", "create", "delete"]

---
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: michelle-storage-admin
subjects:
- kind: User
  name: michelle
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: storage-admin
  apiGroup: rbac.authorization.k8s.io

=>kubectl apply -f clusterRoleBinding.yaml 
OR
=>kubectl api-resources

=>kubectl create clusterrole storage-admin --resource=persistentvolumes,storageclasses --verb=list,create,get,watch
=>kubectl describe clusterrole storage-admin 

=>kubectl get clusterrole storage-admin -o yaml
Check out two Role


Create ClusterroleBinding:
=>kubectl create clusterrolebinding michelle-storage-admin --user=michelle --clusterrole=storage-admin
=>kubectl describe clusterrolebinding michelle-storage-admin

=>kubectl get storageclass --as michelle






##Admission Controllers
--------------------------------------------------
kubectl -> Aluthentication -> Authorization  -> Create Pod
Usually Aluthentication done by certificate. may with role base Authorization.

Aluthentication->By .kube/config file
Authorization->By RoleBase


But, Role base Authorization are not able make restrection in more details like:
- Only permite image allow, and certain registry
- Do not permit runAs Root user
- Only permit certain capabilities
- Pod always has labels etc.

For make this kind of restraction AdmissionController come in to picture.



kubectl -> Aluthentication -> Authorization -> AdmissionController -> Create Pod
AdmissionController List:
 - AlwaysPullImages
 - DefaultStorageClass
 - EventRateLimit
 - NamespaceLifeCycle
and More..........

=>kub-apiserver -h | grep enable-admission-plugins
=>kubectl exec kube-apiserver-controlplane -n kube-system -- kube-apiserver -h | grep enable-admission-plugins

AdmissionController not only valaid and reject user request, It can also done back end operation.
and May change thei request its self.

NamespaceAutoProvision and NamespaceExists controller deprecated,
Now it NamespaceLifeCycle admission controller.






##Admission Controllers
--------------------------------------------------
1)What is not a function of admission controller?
Authencate a user.

2)Which admission controller is not enabled by default?
Check enable-admission-plugins in kube-apiserver help options
NamespaceAutoProvision

=>kubectl get pods -n kube-system
=>kubectl exec -it kube-apiserver-controlplane -n kube-system -- kube-apiserver -h | grep 'enable-admission-plugins'
OR
=>kubectl exec -it kube-apiserver-controlplane -n kube-system -- kube-apiserver -h
Check this:
--disable-admission-plugins strings                                                         


3)Which admission controller is enabled in this cluster which is normally disabled?
Check enable-admission-plugins in /etc/kubernetes/manifests/kube-apiserver.yaml

=>cat /etc/kubernetes/manifests/kube-apiserver.yaml
=>grep enable-admission-plugin /etc/kubernetes/manifests/kube-apiserver.yaml



4)The previous step failed because kubernetes have NamespaceExists admission controller enabled which rejects requests to namespaces that do not exist. So, to create a namespace that does not exist automatically, we could enable the NamespaceAutoProvision admission controller
Enable the NamespaceAutoProvision admission controller
Note: Once you update kube-apiserver yaml file, please wait for a few minutes for the kube-apiserver to restart completely.

=>kubectl run nginx --image nginx -n blue

Edit /etc/kubernetes/manifests/kube-apiserver.yaml 
and add NamespaceAutoProvision admission controller to --enable-admission-plugins list

=>vi /etc/kubernetes/manifests/kube-apiserver.yaml
Add this:
    - --enable-admission-plugins=NodeRestriction,NamespaceAutoProvision

Add NamespaceAutoProvision admission controller to --enable-admission-plugins list to /etc/kubernetes/manifests/kube-apiserver.yaml
It should look like below

    - --enable-admission-plugins=NodeRestriction,NamespaceAutoProvision
API server will automatically restart and pickup this configuration.

=>kubectl get ns
=>kubectl run mynginx --image=nginx -n blue

Note that the NamespaceExists and NamespaceAutoProvision admission controllers are deprecated and now replaced by 
NamespaceLifecycle admission controller.

The NamespaceLifecycle admission controller will make sure that requests
to a non-existent namespace is rejected and that the default namespaces such as
default, kube-system and kube-public cannot be deleted.



5)Disable DefaultStorageClass admission controller
This admission controller observes creation of PersistentVolumeClaim objects that do not request any specific storage class and automatically adds a default storage class to them. This way, users that do not request any special storage class do not need to care about them at all and they will get the default one.
Note: Once you update kube-apiserver yaml file then please wait few mins for the kube-apiserver to restart completely.

Add DefaultStorageClass to disable-admission-plugins in /etc/kubernetes/manifests/kube-apiserver.yaml


=>vi /etc/kubernetes/manifests/kube-apiserver.yaml
Add this under -command: section
- --disable-admission-plugins=DefaultStorageClass


Since the kube-apiserver is running as pod you can check the process to see enabled and disabled plugins.
=>ps -ef | grep kube-apiserver | grep admission-plugins






##Validating and Mutating Admission Controllers
--------------------------------------------------
Two type AdmissionController
- Mutation AdmissionController            | Change the Object, the request its self.
- Validation AdmissionController          | Validate the request, Allow or Dineis

MutationController working before ValidationController, Like: Change the request berore validate.
This is all builtin Controller.


For Custome Admission Controller:

First Create Webhook Server, then add this on. For this we can build our own webhook server any langualge.
just support validate amd mutate api as json data of request and response.

CKAD exam dont have to develop webhook server code, just need to deploy and what we cand do using this.

For working with custom webhook sever:
- First deploy the webhook server
- then Need a service to access this
- then need a configurarion to access the service in cluster, so create a config Object




##Solution6: 150. Validating and Mutating Admission Controllers
--------------------------------------------------
1)Which of the below combination is correct for Mutating and validating admission controllers ?
NamespaceAutoProvision- Mutating , NamespaceExists - Validating


2)What is the flow of invocation of admission controllers?
First Mutating then Validating


3)Create TLS secret webhook-server-tls for secure webhook communication in webhook-demo namespace.
We have already created below cert and key for webhook server which should be used to create secret.
Certificate : /root/keys/webhook-server-tls.crt
Key : /root/keys/webhook-server-tls.key

Create tls secret type in kubernetes with --cert and --key options

=>kubectl create secret tls webhook-server-tls --cert "/root/keys/webhook-server-tls.crt" --key "/root/keys/webhook-server-tls.key" -n webhook-demo
=>kubectl -n webhook-demo create secret tls webhook-server-tls \
--cert "/root/keys/webhook-server-tls.crt" \
--key "/root/keys/webhook-server-tls.key"


4) Now create a deployment.
=>k apply -f webhook-deployment.yaml 
cat webhook-deployment.yaml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: webhook-server
  namespace: webhook-demo
  labels:
    app: webhook-server
spec:
  replicas: 1
  selector:
    matchLabels:
      app: webhook-server
  template:
    metadata:
      labels:
        app: webhook-server
    spec:
      securityContext:
        runAsNonRoot: true
        runAsUser: 1234
      containers:
      - name: server
        image: stackrox/admission-controller-webhook-demo:latest
        imagePullPolicy: Always
        ports:
        - containerPort: 8443
          name: webhook-api
        volumeMounts:
        - name: webhook-tls-certs
          mountPath: /run/secrets/tls
          readOnly: true
      volumes:
      - name: webhook-tls-certs
        secret:
          secretName: webhook-server-tls


Create webhook service now so that admission controller can communicate with webhook.
We have already added sample service definition under /root/webhook-service.yaml so just create service with that definition.
=>k apply -f webhook-service.yaml 

cat /root/webhook-service.yaml 
apiVersion: v1
kind: Service
metadata:
  name: webhook-server
  namespace: webhook-demo
spec:
  selector:
    app: webhook-server
  ports:
    - port: 443
      targetPort: webhook-api



We have added MutatingWebhookConfiguration under /root/webhook-configuration.yaml.
If we apply this configuration which resource and actions it will affect?

Check operations and resources under rules in /root/webhook-configuration.yaml
Pod with create operation.

Create webhookConfiguration:
=>kubectl apply -f webhook-configuration.yaml 

In previous steps we have deployed demo webhook which does below
- Denies all request for pod to run as root in container if no securityContext is provided.
- If no value is set for runAsNonRoot, a default of true is applied, and the user ID defaults to 1234
- Allow to run containers as root if runAsNonRoot set explicitly to false in the securityContext


Deploy a pod with no securityContext specified.
We have added pod definition file under /root/pod-with-defaults.yaml

=>kubectl apply -f /root/pod-with-defaults.yaml
cat /root/pod-with-defaults.yaml
# A pod with no securityContext specified.
# Without the webhook, it would run as user root (0). The webhook mutates it
# to run as the non-root user with uid 1234.
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-defaults
  labels:
    app: pod-with-defaults
spec:
  restartPolicy: OnFailure
  containers:
    - name: busybox
      image: busybox
      command: ["sh", "-c", "echo I am running as user $(id -u)"]



5)What are runAsNonRoot and runAsUser values for previously created pods securityContext?
We did not specify any securityContext values in pod definition so check out the changes done by mutation webhook in pod

=>k get pod
=>kubectl get pod pod-with-defaults -o yaml
Check Security Context section:
 securityContext:
    runAsNonRoot: true
    runAsUser: 1234
  serviceAccount: default






6)Deploy pod with a securityContext explicitly allowing it to run as root
We have added pod definition file under /root/pod-with-override.yaml
Validate securityContext after you deploy this pod

Run below command

=>kubectl apply -f /root/pod-with-override.yaml
then Validate securityContext with
=>kubectl get po pod-with-override -o yaml | grep -A2 " securityContext:"

=>cat pod-with-override.yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-override
  labels:
    app: pod-with-override
spec:
  restartPolicy: OnFailure
  securityContext:
    runAsNonRoot: false
  containers:
    - name: busybox
      image: busybox
      command: ["sh", "-c", "echo I am running as user $(id -u)"]

=>k apply -f pod-with-override.yaml 


7)Deploy a pod with a conflicting securityContext i.e. pod running with a user id of 0 (root)
We have added pod definition file under /root/pod-with-conflict.yaml
Mutating webhook should reject the request as its asking to run as root user without setting runAsNonRoot: false

Run below command

=>kubectl apply -f /root/pod-with-conflict.yaml
You should get error like below
Error from server: error when creating "/root/pod-with-conflict.yaml": admission webhook "webhook-server.webhook-demo.svc" denied the request: runAsNonRoot specified, but runAsUser set to 0 (the root user)

=>cat pod-with-conflict.yaml 
# A pod with a conflicting securityContext setting: it has to run as a non-root
# user, but we explicitly request a user id of 0 (root).
# Without the webhook, the pod could be created, but would be unable to launch
# due to an unenforceable security context leading to it being stuck in a
# 'CreateContainerConfigError' status. With the webhook, the creation of
# the pod is outright rejected.
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-conflict
  labels:
    app: pod-with-conflict
spec:
  restartPolicy: OnFailure
  securityContext:
    runAsNonRoot: true
    runAsUser: 0
  containers:
    - name: busybox
      image: busybox
      command: ["sh", "-c", "echo I am running as user $(id -u)"]








##153. API Versions/API Deprecations
--------------------------------------------------
VersionName:
Alpha -> Beta ->GA(Stable)

Alpha: Not enable by default, have to ebale vi flag.
Alpha: May not come to next releae version or its can be remove with any notice.
After all of the bug fixed and testing it come to beta version or drop.

Beta:
Beta stage also have minor bug, it enable by defautl.

GA: Its ( v1 ), Ebable by default.


PraferedVersion:
For a single thisng mya has multiple api version, to see the prefered version use:
=>kubectl explain

StorageVersion:
Object save on ETCD storage may or may not to different version from prefered version.



If any apiGroup has multiple version, PreferredVersion will work by default. To check tis:
=>localhost:8002/apis/batch
Other version call: Storage Version

To enable other version:
add the line in :
ExecStart = /
  --runtime-config=batch/v2alpha1\\
and Restart thsi api server.





API Deprection
--------------------------------------------------
Which versin we have to user for this we can check API Deprection.and ists rule.

Rules:
RuleOne: Remove a version on nesxt increments version.
Rule2: For next version have to be same filed of previous version.
Role3: Beta and GA version must have to mantin in 9 to 12 month, alpha 0.
       alpha version not replace GA, and not to be depracate before GA is comming. v2 can be depricated v1 only.
       


=>kubectl convert -f oldFile --output-version newVersion
=>kubectl convert -f mydeployment.yaml --output-version apps/v1
For convert older version to new version.



  





##Solution7: 153. API Versions/API Deprecations
--------------------------------------------------

1)Identify the short names of the deployments, replicasets, cronjobs and customresourcedefinitions.
=>kubectl api-resources
=>kubectl api-resources | grep -e customresourcedefinitions -e cronjobs
=>k api-resources | grep -e deployments -e replicasets -e cronjobs -e customresourcedefinitions

2)What is the patch version in the given Kubernetes API version?
Kubernetes API version - 1.22.2
2, the last one

=>kubectl version --short



3)Identify which API group a resource called job is part of?

Run the command kubectl explain job and see the API Version in the top of the line.
At first will be the API group and second will be the version : <group>/<version>

=>kubectl explain job
Check
KIND:     Job
VERSION:  batch/v1


4)What is the preferred version for authorization.k8s.io api group?

It supports v1 and v1beta1 but the preferred version is v1.

=>kubectl proxy 8001&
=>curl localhost:8081/apis/authorization.k8s.io

Where & runs the command in the background and kubectl proxy command starts the proxy to the kubernetes API server.




5)Enable the v1alpha1 version for rbac.authorization.k8s.io API group on the controlplane node.
Note: If you made a mistake in the config file could result in the API server being unavailable and can break the cluster.

Add the --runtime-config=rbac.authorization.k8s.io/v1alpha1 option to the kube-apiserver.yaml file.

As a good practice, take a backup of that apiserver manifest file before going to make any changes.

In case, if anything happens due to misconfiguration you can replace it with the backup file.

root@controlplane:~# cp -v /etc/kubernetes/manifests/kube-apiserver.yaml /root/kube-apiserver.yaml.backup
Now, open up the kube-apiserver manifest file in the editor of your choice. It could be vim or nano.

root@controlplane:~# vi /etc/kubernetes/manifests/kube-apiserver.yaml
Add the --runtime-config flag in the command field as follows :-

 - command:
    - kube-apiserver
    - --advertise-address=10.18.17.8
    - --allow-privileged=true
    - --authorization-mode=Node,RBAC
    - --client-ca-file=/etc/kubernetes/pki/ca.crt
    - --enable-admission-plugins=NodeRestriction
    - --enable-bootstrap-token-auth=true
    - --runtime-config=rbac.authorization.k8s.io/v1alpha1 --> This one 
After that kubelet will detect the new changes and will recreate the apiserver pod.

It may take some time.

root@controlplane:~# kubectl get po -n kube-system
Check the status of the apiserver pod. It should be in running condition.

OR
=>cp /etc/kubernetes/manifests/kube-apiserver.yaml /root/kube-apiserver.yaml.backup
Add this line:
- --runtime-config=rbac.authorization.k8.io/v1alpha1

6)Install the kubectl convert plugin on the controlplane node.
If unsure how to install then refer to the official k8s documentation page which is available at the top right panel.

Download the latest release version from the curl command :-
=>curl -LO https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl-convert

Change the permission of the file and move to the /usr/local/bin/ directory.
=>chmod +x kubectl-convert 
=>mv kubectl-convert /usr/local/bin/kubectl-convert
Use the --help option to see more option.
=>kubectl-convert --help
If it'll show more options that means it's configured correctly if it'll give an error that means we haven't set up properly.


OR
=>curl -LO "https://dl.k8s.io/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl-convert.sha256"

=>chmod +x kubectl-convert
=>mv kubectl-convert /usr/local/bin/
=>kubectl-convert -h


7)Ingress manifest file is already given under the /root/ directory called ingress-old.yaml.
With help of the kubectl convert command, change the deprecated API version to the networking.k8s.io/v1 and create the resource.

Run the command: kubectl-convert -f ingress-old.yaml --output-version networking.k8s.io/v1 | kubectl apply -f -


Run the command kubectl-convert to change the deprecated API version as follows :-
=>kubectl-convert -f ingress-old.yaml --output-version networking.k8s.io/v1

# store new changes into a file 
=>kubectl-convert -f ingress-old.yaml --output-version networking.k8s.io/v1 > ingress-new.yaml
After changing the API version and storing into a file, use the kubectl create -f command to deploy the resource :-

=>kubectl create -f ingress-new.yaml
Inspect the apiVersion as follows :-

=>kubectl get ing ingress-space -oyaml | grep apiVersion
Note: Maybe you will not see the service and other resources mentioned in the ingress YAML on the controlplane node because we have to only deploy the ingress resource with the latest API version.

OR
=>cat ingress-old.yaml
=>kubectl-convert -f ingress-old.yaml --output-version networking.k8s.io/v1
=>kubectl-convert -f ingress-old.yaml --output-version networking.k8s.io/v1>newintrss.yaml
=>kubectl apply -f newintrss.yaml







##Custom Resource Definition
--------------------------------------------------
What is a custom resource?
It is an extension of the Kubernetes API that is not necessarily available in a default Kubernetes installation.
CRD Object can be either namespaced or cluster scoped.

What is kubernetes Resource:
A resource is object in kubernetes, like deployment,job,ReplicsSet etc.
How resource work:
Deployment(Resource run)-> ETCD(Database) Keep the Data-> Controller(Monitoring tools) Monitor the run as expectedly.

We not need to create controller for deployment, in builtin with kubernatis source code wirtten by Go langualge.


As it you can create your won Custome Resources in Kubernetes. For that you have to config Kubernetes API.
So, Where you defive apis, this is called CRD(Custome resource defination) file a yaml file with details of api 
for custome resource.

Now Create the CR by using the file and you able to create your resource.
But until now this resource do notheng its only create object and save info in ECTD no taken action.
For that you need a Controller ! the second part is this to watch and take action create Controller.



Custome Resouece Controller
--------------------------------------------------
Custome resource create doen and data in ETCD, Now we have to Monitor on data in ETCD and take action.
thats Why ed need a custome controller.
A Custome Controller is a any process what is contionuslly monitor the etcd to permorm action.
You can develop Custome controller by any langualge like: Go, Python etc.

It is better build a container and run int on kubernatis cluster and user it.



CKAD exam not have to build customer controller, May need to build a customer resource using defination file.
and working  with existing Custome resource Controller what is already may deploy.

Not so much possible to come in exam to create a custome resource, but it is possible. and better to know



crdExample:
cat im-crd.yaml 
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  # name must match the spec fields below, and be in the form: <plural>.<group>
  name: imranscrd.imranmadbar.com
spec:
  # group name to use for REST API: /apis/<group>/<version>
  group: imranmadbar.com
  # list of versions supported by this CustomResourceDefinition
  versions:
    - name: v1
      # Each version can be enabled/disabled by Served flag.
      served: true
      # One and only one version must be marked as the storage version.
      storage: true
      schema:
        openAPIV3Schema:
          type: object
          properties:
            spec:
              type: object
              properties:
                name:
                  type: string
                gender:
                  type: string
                age:
                  type: integer
  # either Namespaced or Cluster
  scope: Namespaced
  names:
    # plural name to be used in the URL: /apis/<group>/<version>/<plural>
    plural: imranscrd
    # singular name to be used as an alias on the CLI and for display
    singular: imrancrd
    # kind is normally the CamelCased singular type. Your resource manifests use this.
    kind: ImranCrd
    shortNames:
    - ic

 csExample:
 cat im-cs.yaml 
apiVersion: "imranmadbar.com/v1"
kind: ImranCrd 
metadata:
  name: imran-cs
spec:
  name: iman
  gender: Male
  age: 39






// Operator Framework
--------------------------------------------------
Until now we see that create and CRD and Custome Controller together for working.
Its cand be package this two as Framework called: Operator Framework.Its more only CRD and CC.

Operator can do as human Operatior like: manage app install, uninstall etc.
All Operator in avalialbelin operation Hub with documentation. etcd,graffena,myusql etc.


CKAD exam not commming Operaton only may come CRD.



##Solution | Custom Resource
--------------------------------------------------
1)CRD Object can be either namespaced or cluster scoped.
Is this statement true or false?

ans: True
A CRD can be either namespaced or cluster-scoped. When you create a CRD, you can specify its scope in the spec.scope field of the CRD manifest.

What is a custom resource?
It is an extension of the Kubernetes API that is not necessarily available in a default Kubernetes installation.


2)We have provided an incomplete CRDs manifest file called crd.yaml under the /root directory. Let’s complete it and create a custom resource definition from it.
Let’s create a custom resource definition called internals.datasets.kodekloud.com. Assign the group to datasets.kodekloud.com and the resource is accessible only from a specific namespace.

Make sure the version should be v1 and needed to enable the version so it’s being served via REST API.
So finally create a custom resource from a given manifest file called custom.yaml.


Note :- Make sure resources should be created successfully from the custom.yaml file.

In-complate CRD:
cat crd.yaml 
---
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name:  
spec:
  group: 
  versions:
    - name: v2
      served: false
      storage: true
      schema:
        openAPIV3Schema:
          type: object
          properties:
            spec:
              type: object
              properties:
                internalLoad:
                  type: string
                range:
                  type: integer
                percentage:
                  type: string
  scope: 
  names:
    plural: internal
    singular: internal
    kind: Internal
    shortNames:
    - int

The solution file for crd.yaml is pasted below:

---
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: internals.datasets.kodekloud.com 
spec:
  group: datasets.kodekloud.com
  versions:
    - name: v1
      served: true
      storage: true
      schema:
        openAPIV3Schema:
          type: object
          properties:
            spec:
              type: object
              properties:
                internalLoad:
                  type: string
                range:
                  type: integer
                percentage:
                  type: string
  scope: Namespaced 
  names:
    plural: internals
    singular: internal
    kind: Internal
    shortNames:
    - int


Now the resource will be like this:
cat custom.yaml 
---
kind: Internal
apiVersion: datasets.kodekloud.com/v1
metadata:
  name: internal-space
  namespace: default
spec:
  internalLoad: "high"
  range: 80
  percentage: "50"
  
=>kubectl create -f custom.yaml 
after correcting and creating CRD.


3)What are the properties given to the CRD’s called collectors.monitoring.controller?
=>kubectl describe crd collectors.monitoring.controller

Run the commad: kubectl describe crd collectors.monitoring.controller and inspect the given CRD.



4)Create a custom resource called datacenter and the apiVersion should be traffic.controller/v1.
Set the dataField length to 2 and access permission should be true.
To create a custom resource called datacenter :-


apiVersion: traffic.controller/v1
kind: Global
metadata:
  name: datacenter
spec:
  dataField: 2
  access: true



5)What is the short name given to the CRD globals.traffic.controller ?
Inspect the CRD globals.traffic.controller and identify the short names.
gb
=>kubectl describe crd globals.traffic.controller






##Deployment strategies
--------------------------------------------------
Currently Only RollingUpdate and Recreate strategies can be define on deployment file.
But thereis other deployment strategies aveilable: 
 - BlueGreen
   In this case all new service deploy and at a time switch traffic to news services. 

So, How to deploy blue green:
Suppose current running service run with tag blue(in pod template label), now Update Service new version app with green tag.
Now update the tage of deploy ment blue to green and all traffic go to greent pods.

 - Canary
   In this case Deploy all new version app and a few traffic flow to new app. 
   If every thisng ok then go for full traffic.

So, how to deploy Canary:
As Current deployment and service running with a label, and new version deploy with to a dirrent label.
Now add another common label on both deployment as result both version reecive the traffic.
To provide a few traffice in new version reduce the number of pod on new app version deployment like pod 1.

in kunbernetes Traffice to deployment % are the number of pod.
By using ServcieMess we can route traffice as required to target deployment.




##164. Solution8: Deployment strategies
--------------------------------------------------

1)A deployment has been created in the default namespace. What is the deployment strategy used for this deployment?
Run: kubectl describe deployments.apps frontend and inspect the StrategyType
=>kubectl describe deployment frontend

2)The deployment called frontend app is exposed on the NodePort via a service.
Identify the name of this service.
Inspect the services created in the default namespace.

=>kubectl describe deployment frontend
=>k get svc
=>k describe svc frontend-service 


3)A new deployment called frontend-v2 has been created in the default namespace using the image kodekloud/webapp-color:v2. This deployment will be used to test a newer version of the same app.
Configure the deployment in such a way that the service called frontend-service routes less than 20% of traffic to the new deployment.
Do not increase the replicas of the frontend deployment.


The frontend-v2 deployment currently has 2 replicas. The frontend service now routes traffic to 7 pods in total ( 5 replicas on the frontend deployment and 2 replicas from frontend-v2 deployment).
Since the service distributes traffic to all pods equally, in this case, approximately 29% of the traffic will go to frontend-v2 deployment.
To reduce this below 20%, scale down the pods on the v2 version to the minimum possible replicas = 1.
Run: kubectl scale deployment --replicas=1 frontend-v2
Once this is done, only ~17% of traffic should go to the v2 version.


=>kubectl get deployment
=>kubectl describe service frontend-service
=>kubectl describe deployment frontend-v2

=>kubectl scale deployment --replicas=1 frontend-v2

We have now established that the new version v2 of the application is working as expected.
We can now safely redirect all users to the v2 version.


4)Scale down the v1 version of the apps to 0 replicas and scale up the new(v2) version to 5 replicas.
=>kubectl scale deployment --replicas=5 frontend-v2
=>kubectl scale deployment --replicas=0 frontend

=>kubectl delete deployment frontend
You can now reload the Webapp tab to validate that all user traffic now uses the v2 version of the app.






##169.  Install Helm
--------------------------------------------------
https://helm.sh/docs/intro/install/#from-apt-debianubuntu

For Helm Install you need a fuctional kubernetes Cluster and with kubectl tools with propepr config/access.



Helm call package manger, In kunbernetes we bild our app with different type deployment file.
In Helm all make together build the app. Name of the package in know what and how many package need to deploy.

To Install the helm, Need a complate kubernetes cluster and run:

=>sudo snap install helm --classic
Or 
Useing package pkg Manager
Or may download package file using Curl




##Solution - | Install Helm
--------------------------------------------------
1)Identify the name of the Operating system installed.
=>cat /etc/*release

2)Install the helm package.

curl https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg > /dev/null
sudo apt-get install apt-transport-https --yes
echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list
sudo apt-get update
sudo apt-get install helm

=>helm version --short

3)Use the help page of helm command to identify the command used to retrieve helm client environment information.
=>helm -h
=>helm env


4)What is a command line flag used to enable verbose output?
 helm -h | grep verbose
=>helm --debug




##Helm Concept
--------------------------------------------------

ObjectYamlFile=>| ObjectTemplateYamlFile+valueYamlFile=>| HelmChart
It has Chart.yaml file for this chart

=>helm search hub wordpress

=>helm list
=>helm uninstall my-release

CKAD: This is all need on Exam.



##Solution  | Helm Concept
--------------------------------------------------
1)Which command is used to search for a wordpress helm chart package from the Artifact Hub?
=>helm search hub wordpress

2)Add a bitnami helm chart repository in the controlplane node.
name - bitnami
chart repo name - https://charts.bitnami.com/bitnami

=>helm repo add bitnami https://charts.bitnami.com/bitnami



=>helm repo add bitnami https://charts.bitnami.com/bitnami
=>helm repo list


3)Which command is used to search for the joomla package from the added repository?
=>helm search repo joomla


4)What is the app version of joomla in the bitnami helm repository?
=>helm search repo joomla
look at the APP VERSION



5)How many helm repositories are added in the controlplane node?
=>helm repo list


6)Install drupal helm chart from the bitnami repository.
Release name should be bravo.
Chart name should be bitnami/drupal.
Note: Ignore the state of the application now.

=>helm install bravo bitnami/drupal
=>helm list

7)Uninstall the drupal helm package which we installed earlier.
=>helm uninstall bravo


8)Download the bitnami apache package under the /root directory.
Note: Do not install the package. Just download it.
=>helm pull --untar bitnami/apache



9)Inspect the file values.yaml and make changes so that 2 replicas of the webserver are running and the http is exposed on nodeport 30080.
Note: You can read the Bitnami documentation for more.
https://github.com/bitnami/charts/tree/master/bitnami/apache/#installing-the-chart



10)Install the apache from the downloaded helm package.
Release name: mywebapp
Note: Do make changes accordingly so that 2 replicas of the webserver are running and the http is exposed on nodeport 30080.
Make sure that the pods are in the running state.

Edit the values.yaml file and change the parameters to replicaCount: 2 and http: 30080 under the nodePorts section. Then run helm install command to install the package.

  nodePorts:
    replicaCount: 2
    http: "30080"
    https: ""

Once you have modified the values.yaml file , run the below command to install the apache package on the controlplane node:
=>helm install mywebapp ./apache

After installation, run the below command to list the mywebapp release:
=>helm list 


=================================================
Helm:
=================================================

=>helm repo ls 
List the helm repositories.

=>helm ls -A
Deployed Helm charts releases list, Here lists all the releases of all the namespaces.


Official Helm Stable Charts: https://charts.helm.sh/stable
Prometheus Helm chart repository: https://prometheus-community.github.io/helm-charts
Rancher's Helm chart repository: https://releases.rancher.com/server-charts

=>helm repo add repoName https://charts.bitnami.com/bitnami
=>helm repo ls 
Add the repostiory to Helm.
It allows us to browse and install charts from the new repository using the Helm package manager.


=>helm search repo nginx
=>helm search hub nginx
Search Helm charts, Its return last lates version. for more

=>helm search repo bitnami/joomla -l | head -n10

When you run "helm search repo nginx", it will query the repositories you have added to your Helm configuration 
When you run "helm search hub nginx", it queries the Helm Hub and returns any matching charts related to nginx 
that are available on the Helm Hub. The Helm Hub is a public repository and can be accessed by anyone.


=>helm search repo polar | grep nginx
=>helm install nginx-server polar/nginx 
=>helm  install jom13 bitnami/joomla  --version=13.3.19
Search and Inatall for the nginx chart in a polar chart repository



cd /root/
=>helm lint ./newVersion
Validate the helm chart by using the helm lint command


=>helm uninstall oldVersioApp -n default
=>helm install -myNewApp ./new-version
=>helm install --generate-name ./new-version
Install/Uninstall application


=>helm search repo lvm-crystal-apd/nginx -l | head -n30
The helm search command searches for all the available charts in a specific Helm chart repository. 


=>helm upgrade nging ofc/nginx-ingress --version=1.41.1
=>helm upgrade lvm-crystal-apd lvm-crystal-apd/nginx -n crystal-apd-ns --version=13.2.12 --set replicaCount=2
Upgrade the helm chart and increase the replica count.

=>helm ls -n crystal-apd-ns
Look under the CHART column for the chart version for varify.


=>helm repo update myHelmChart
Now, update the helm repository with the following command: -
The above command updates the local cache of available charts from the configured chart repositories.



=>helm repo ls 
List the helm repositories.

=>helm ls -A
Deployed Helm charts releases list, Here lists all the releases of all the namespaces.


Official Helm Stable Charts: https://charts.helm.sh/stable
Prometheus Helm chart repository: https://prometheus-community.github.io/helm-charts
Rancher's Helm chart repository: https://releases.rancher.com/server-charts

=>helm repo add repoName https://charts.bitnami.com/bitnami
=>helm repo ls 
Add the repostiory to Helm.
It allows us to browse and install charts from the new repository using the Helm package manager.


=>helm search repo nginx
=>helm search hub nginx
Search Helm charts, Its return last lates version. for more

=>helm search repo bitnami/joomla -l | head -n10

When you run "helm search repo nginx", it will query the repositories you have added to your Helm configuration 
When you run "helm search hub nginx", it queries the Helm Hub and returns any matching charts related to nginx 
that are available on the Helm Hub. The Helm Hub is a public repository and can be accessed by anyone.


=>helm search repo polar | grep nginx
=>helm install nginx-server polar/nginx 
=>helm  install jom13 bitnami/joomla  --version=13.3.19
Search and Inatall for the nginx chart in a polar chart repository



cd /root/
=>helm lint ./newVersion
Validate the helm chart by using the helm lint command


=>helm uninstall oldVersioApp -n default
=>helm install -myNewApp ./new-version
=>helm install --generate-name ./new-version
Install/Uninstall application


=>helm search repo lvm-crystal-apd/nginx -l | head -n30
The helm search command searches for all the available charts in a specific Helm chart repository. 


=>helm upgrade nging ofc/nginx-ingress --version=1.41.1
=>helm upgrade lvm-crystal-apd lvm-crystal-apd/nginx -n crystal-apd-ns --version=13.2.12 --set replicaCount=2
Upgrade the helm chart and increase the replica count.

=>helm ls -n crystal-apd-ns
Look under the CHART column for the chart version for varify.


=>helm repo update myHelmChart
Now, update the helm repository with the following command: -
The above command updates the local cache of available charts from the configured chart repositories.







ExtraTopic
=======================================

How to Create statuc pod
yml file crete in /etc/kubernetes/manafiest/



Take a backup of the etcd cluster and save it to /opt/etcd-backup.db.
export ETCDCTL_API=3
etcdctl snapshot save --endpoints https://[127.0.0.1]:2379 --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key  /opt/etcd-backup.db


=>kubectl get nodes -o jsonpath='{.items[*].status.addresses[?(@.type=="InternalIP")].address}'
List the InternalIP of all nodes of the cluster.

=>vi /etc/kubernetes/kubelet.conf
=>systemctl restart kubelet
Kubeclt config file for node (master and slave bot)



=>systemctl status containerd
=>systemctl status kubelet
This can be corrected by updating the file /var/lib/kubelet/config.yaml as follows: -

Kubelet config file:
Check the kubelet.conf file at /etc/kubernetes/kubelet.conf.


=>kubectl get nodes -o jsonpath='{.items[*].status.nodeInfo.osImage}'
Use JSON PATH query to retrieve the osImage


=>kubectl get nodes -o json 
Get the list of nodes in JSON format



Node
--------------------------------------
=>journalctl -u kubelet -f
=>journalctl -u kubelet 
Investigation for cluster broken issue/master and workernode

Node config file
/etc/kubernetes/kubelet.conf 

config file
/var/lib/kubelet/config.yaml



