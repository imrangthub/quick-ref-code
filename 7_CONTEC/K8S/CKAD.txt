#################################################
#          CKAD EXAM-PREPARATION                #
#################################################



CNCF Certification
Certified Kubernetes Application Developer: http://kub.to/dev

Candidate Handbook: https://www.cncf.io/certification/candidate-handbook

Exam Tips: https://docs.linuxfoundation.org/tc-docs/certification/tips-cka-and-ckad

Keep the code – DEVOPS15 – handy while registering for the CKA or CKAD exams at Linux Foundation to get a 15% discount.




Certified Kubernetes Application Developer: https://www.cncf.io/certification/ckad/
Candidate Handbook: https://www.cncf.io/certification/candidate-handbook
Exam Tips: https://docs.linuxfoundation.org/tc-docs/certification/tips-cka-and-ckad
https://www.youtube.com/watch?v=wtKef83kmUA&list=PL0hSJrxggIQoKLETBSmgbbvE4FO_eEgoB

https://docs.linuxfoundation.org/tc-docs/certification/faq-cka-ckad-cks
FAQ

lAB
https://kodekloud.com/lessons/recap-core-concepts/



=>sudo apt-get update && apt-get install iputils-ping && sudo apt install net-tools
=================================================
#General                                
=================================================

=>kubectl api-resources

=>kubectl version
=>kubectl version --short 
=>kubectl get nodes

=>kubectl get pods
=>kubectl get pods -o wide
=>kubectl get -o json pod prodName

=>kubectl explain pod
=>kubectl explain deployment

=kubectl logs myPod

=>kubectl run --help
=>kubectl run nginx --image=nginx --dry-run=client
=>kubectl run nginx --image=nginx --dry-run=client -o yaml
=>kubectl run myng --image=nginx --dry-run=client -o yaml>labelpod.yaml
=>kubectl create deployment --image=nginx nginx --dry-run=client -o yaml
=>kubectl run ng3 --image=nginx -- /bin/sh -c "while true; do echo  $(date); sleep 1; done "

=>kubectl run pod1 --image=imranmadbar/nginx && sleep 2 && kubectl exec -it pod1 -- bash
=>echo -e "Welcome to nginx! \nHost Name: $(hostname -f)\nHost IP: $(hostname -i)">/usr/share/nginx/html/index.html

=>kubectl run mynginx --image=nginx
=>kubectl run  bu1 --image=busybox -- sh  -c "hostname -i"
=>kubectl run logpod --image=busybox -- sh -c "ping google.com"
=>kubectl run bx2 --image=busybox -- sh -c "while true; do echo $(date)>>/var/1.log; sleep 1; done"

=>kubectl run -it ubuntu1 --image=ubuntu --restart=Never -- bash -ec "apt update; apt install mysql-server; bash"
          =while true; do echo "infinity"; sleep 1; done

=>kubectl exec -it ub1 -- bash
=>kubectl exec -it mynginx -- bash
=>kubectl exec -it mynginx -- ls -l


=>kubectl apply -f sample.yaml
=>kubectl delete -f sample.yaml
=>kubectl delete pod my-pod1 

=>kubectl describe pod mynginx



KodeKloud
=================================================
#CoreConcept         
================================================= 

Pod
-------------------------------------------------
Edit current running pod, like change image
1) kubectl edit pod myng
OR
2) Edit then pod defanation yaml file and run again
=>Kubectl apply -f mypod.yaml


Generate another pod defenation yaml file from current pod
=>>kubectl get pod mypod -o yaml
=>kubectl get pod mypod -o yaml> pod.yaml


Remember, you CANNOT edit specifications of an existing POD other than the below.
spec.containers[*].image
spec.initContainers[*].image
spec.activeDeadlineSeconds
spec.tolerations

For edit other file generate another yaml file from current pod status and update it, delete perivious pod and run new one.
=>kubectl get pod mypod -o yaml> pod.yaml
=>kubectl apply -f pod.yaml





ReplicaSet
-------------------------------------------------
Simple ReplicaSet:
apiVersion:apps/v1
kind: ReplicaSet
metadata:
  name: replicaset-1
spec:
  replicas: 2
  selector:
    matchLabels:
      tier: frontend
  template:
    metadata:
      labels:
        tier: frontend
    spec:
      containers:
      - name: nginx
        image: nginx


For finding any issu in defination yaml file, first run the yaml
=>kubectl create -f myreplicaset.yaml
And now check the error message



Scale ReplicaSet
1) Update number of replica(replicas:5) in yaml file and run
=>kubectl replace -f myreplica.yaml
OR
2) Run
=>kubectl scale --replicas=5 -f myreplica.yaml
OR
3) Run with replical type(replicaset) and name (myreplicaset) 
=>kubectl scale --replicas=5 replicaset myreplicaset
2 and 3 Not made change the original defination file

Check ReplicaSet info like: uses image, etc
=>kubectl describe rs new-replica-set

Check the pod status of ReplicaSet
=>kubectl get rs new-replica-set


If need to Update ReplicaSet like Image change, then its does not automatically re-create the  pod.
First you can eidt current rs
=>kubectl edit rs new-replica-set 
Then delete existing pod
=>kubectl delete pod --all
and RS will create new pod with new Image
OR
Delete old rs and create again


=>kubectl get rs myreplicaset1 -o yaml
=>kubectl get rs myreplicaset1 -o yaml>myrs2.yaml


#Deployment
-------------------------------------------------
With Deployments you can easily edit any field/property of the POD template. 

=>kubectl create deployment --help
=>kubectl create deployment httpd-frontend --image=httpd:2.4-alpine --replicas=3 --dry-run=client -o yaml>mydeployment1.yaml


Generate a yaml file from current deployment
=>kubectl get deployment mydeployment -o yaml
=>kubectl get deployment httpd-frontend -o yaml>mydeployment2.yaml



#NameSpace
-------------------------------------------------
=>kubectl get all

=>kubectl get namespace
=>kubectl get pod --namespace=research
=>kubectl get pod -n=research
=>kubectl run redis --image=redis --namespace=research


=>kubectl get pod -A
=>kubectl get pod --all-namespaces
Show all pod with namespace

=>kubectl get pods -n=marketing
=>kubectl get svc -n=marketing
Show service of a specific namespace



Creating Kubernetes objects imperatively
-------------------------------------------------
There are two main ways to manage Kubernetes objects: imperative (with kubectl commands) and 
declarative (by writing manifests and then using kubectl apply).


=>kubectl run --help
=>kubectl run redis --image=redis:alpine --labels="tier=db"
=>get pod --show-labels


Create a service redis-service to expose the redis application within the cluster on port 6379.
Use imperative commands.
=>kubectl expose pod redis --port=6379 --name=redis-service
=>kubectl describe svc redis-service
OR
=>kubectl create service --help
=>kubectl create service clusterip --help
=>kubectl create service clusterip redis-service --tcp=6379:6379


=>kubectl create deployment --help
=>kubectl create deployment webapp --image=kodekloud/webapp-color --replicas=3

=>kubectl run custom-nginx --image=nginx --port=8080


=>kubectl create namespace dev-ns


=>kubectl create deployment redis-deploy --image=redis --replicas=2 -n=dev-ns



=>kubectl create service clusterip --help


=>kubectl create service clusterip httpd --tcp=5678:80
=>kubectl get pod --show-labels
=>kubectl edit svc  httpd
=>kubectl describe services httpd


expose:
=>kubectl expose --help
"expose" command user for expose existing pod for expose.
=>kubectl expose pod redis --port=6379 --name=redis-service
OR 
Create pod and service at a same time
=>kubectl run httpd --image=httpd:alpine --port=80 --expose=true
This command create pod as well as service with same name with labels and bind service with pod at a time



=================================================
#Configuration         
================================================= 


#Pod command and Args:
-------------------------------------------------



Practice:
================================================= 
What is the command used to run the pod ubuntu-sleeper?
=>kubectl describe pod ubuntu-sleeper

Create a pod with the ubuntu image to run a container to sleep for 5000 seconds.
apiVersion: v1 
kind: Pod 
metadata:
  name: ubuntu-sleeper-2 
spec:
  containers:
  - name: ubuntu
    image: ubuntu
    command:  ["sleep"]
    args: ["5000"]

apiVersion: v1 
kind: Pod 
metadata:
  name: ubuntu-sleeper-2 
spec:
  containers:
  - name: ubuntu
    image: ubuntu
    args: ["sleep","5000"]

apiVersion: v1 
kind: Pod 
metadata:
  name: pod3 
spec:
  containers:
  - name: ubuntu
    image: ubuntu
    args:
      - "sleep"
      - "1200"

apiVersion: v1 
kind: Pod 
metadata:
  name: ubuntu-sleeper-3 
spec:
  containers:
  - name: ubuntu
    image: ubuntu
    command:
      - "sleep"
      - "1200"

apiVersion: v1 
kind: Pod 
metadata:
  name: webapp-green
  labels:
      name: webapp-green 
spec:
  containers:
  - name: simple-webapp
    image: kodekloud/webapp-color
    command: ["python", "app.py"]
    args: ["--color", "pink"]




Update pod ubuntu-sleeper-3 to sleep for 2000 seconds.
=>kubectl get pod ubuntu-sleeper-3 -o yaml>newpod.yaml
Modify the newpod.yaml file and run again
OR
=>kubectl replace --force -f newpod.yaml
It will delete existing pod and create new one




Dockerfile with ENTRYPOINT and cmd
FROM python:3.6-alpine
RUN pip install flask
COPY . /opt/
EXPOSE 8080
WORKDIR /opt
ENTRYPOINT ["python", "app.py"]
CMD ["--color", "red"]

apiVersion: v1 
kind: Pod 
metadata:
  name: webapp-green
  labels:
      name: webapp-green 
spec:
  containers:
  - name: simple-webapp
    image: kodekloud/webapp-color
    command: ["python", "app.py"]
    args: ["--color", "pink"]


=>kubectl run webapp-green2 --image=kodekloud/webapp-color --command -- python app.py  -- --color=green

Create a pod with the given specifications. By default it displays a blue background. 
Set the given command line arguments to change it to green.


=>kubectl run webapp-green --image=kodekloud/webapp-color -- --color=green



#ConfigMap
-------------------------------------------------

#Practice:
=========
What is the environment variable name set on the container in the pod?
=>kubectl describe pod webapp-color


1)Update the environment variable on the POD to display a green background.
Note: Delete and recreate the POD. 
=>kubectl get pod webapp-color -o yaml>newpod.yaml
=>kubectl replace --force -f newpod.yaml


Identify the database host from the config map db-config?
=>kubectl describe configmap db-config


2)Create a new ConfigMap for the webapp-color POD. Use the spec given below?
  ConfigMap Name: webapp-config-map
  Data: APP_COLOR=darkblue
  Data: APP_OTHER=disregard


=>kubectl create configmap --help
=>kubectl create configmap my-config --from-literal=key1=config1 --from-literal=key2=config2

=>kubectl create configmap webapp-config-map --from-literal=APP_COLOR=darkblue --from-literal=APP_OTHER=disregard
apiVersion: v1
data:
  APP_COLOR: darkblue
  APP_OTHER: disregard
kind: ConfigMap
metadata:
  creationTimestamp: null
  name: webapp-config-map

=>kubectl get cm



3)Update the environment variable on the POD to use only the APP_COLOR key from the newly created ConfigMap.
Note: Delete and recreate the POD.
  Pod Name: webapp-color
  ConfigMap Name: webapp-config-map

=>k get po -o yaml>newpod.yaml
=>vi ewpod.yaml
add this:
  spec:
    containers:
    - env:
      - name: APP_COLOR
        valueFrom:
            configMapKeyRef:
              name: webapp-config-map
              key: APP_COLOR

=>kubectl replace --force -f newpod.yaml


#Secret
-------------------------------------------------
=>kubectl get secret
=>kubectl get secret db-secret -o yaml
=>echo "sfsdfrweewer" | base64 --decode

=>kubectl describe secret dashboard-token 
Show How many secret data on a secret, What is the type of the dashboard-token secret.


1)Create a new secret named db-secret with the data given below.
  Secret Name: db-secret
  Secret 1: DB_Host=sql01
  Secret 2: DB_User=root
  Secret 3: DB_Password=password123

=>kubectl create secret --help
=>kubectl create secret generic --help
=>kubectl create secret generic my-secret --from-literal=key1=supersecret --from-literal=key2=topsecret
=>kubectl create secret generic db-secret --from-literal=DB_Host=sql01 --from-literal=DB_User=root --from-literal=DB_Password=password123

=>kubectl get secret db-secret -o yaml
=>kubectl describe secret db-secret


=>kubectl describe pod webapp-pod
Show Is the pod has any secret

2)Configure webapp-pod to load environment variables from the newly created secret.
Delete and recreate the pod if required.

Pod name: webapp-pod
Image name: kodekloud/simple-webapp-mysql
Env From: Secret=db-secret


envFrom:
 - secretRef:
    name: db-secret

Add this under container like this:
spec:
  containers:
  - image: kodekloud/simple-webapp-mysql
    imagePullPolicy: Always
    name: webapp
    resources: {}
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    envFrom:
      - secretRef:
          name: db-secret


=>kubectl edit pod webapp-pod
=>kubectl replace --force -f /tmp/kubectl-edit-3803810208.yaml



#Solution: Security Contexts
-------------------------------------------------
1)What is the user used to execute the sleep process within the ubuntu-sleeper pod?
=>kubectl exec -it ubuntu-sleeper -- whoami

2)Edit the pod ubuntu-sleeper to run the sleep process with user ID 1010.

Pod Name: ubuntu-sleeper
Image Name: ubuntu
SecurityContext: User 1010

add this under containers: section
  securityContext:
    runAsUser:1010

Like this:
spec:
  containers:
  - command:
    - sleep
    - "4800"
    image: ubuntu
    securityContext:
      runAsUser: 1010

=>kubectl replace --force -f /tmp/kubectl-edit-1907445851.yaml

Pod and container lavel SecurityContext:
apiVersion: v1
kind: Pod
metadata:
  name: multi-pod
spec:
  securityContext:
    runAsUser: 1001
  containers:
  -  image: ubuntu
     name: web
     command: ["sleep", "5000"]
     securityContext:
      runAsUser: 1002
  -  image: ubuntu
     name: sidecar
     command: ["sleep", "5000"]


3)Update pod ubuntu-sleeper to run as Root user and with the SYS_TIME capability.
Pod Name: ubuntu-sleeper
Image Name: ubuntu
SecurityContext: Capability SYS_TIME

=>kubectl edit pod ubuntu-sleeper
add this under SecurityContext:
capabilities:
  add: ["SYS_TIME"]

=>kubectl replace --force -f /tmp/kubectl-edit-2505242802.yaml





#Solution: Service Accounts
-------------------------------------------------

=>kubectl get serviceaccount


1)What is the secret token used by the default service account?
=>kubectl describe sa default

2)Which account does the Dashboard application use to query the Kubernetes API?
Check the error:
pods is forbidden: User "system:serviceaccount:default:default" cannot list resource "pods" in API group "" in the namespace "default"

Its default account identify by "system:serviceaccount:default:default".


3)Inspect the Dashboard Application POD and identify the Service Account mounted on it.
Identify the service account name of the pod:

=>kubectl get pod mypod -o yaml
and Find this:
serviceAccount: default

4)what location is the ServiceAccount credentials available within the pod?
=>kubectl describe pod web-dashboard-65b9cf6cbb-vpkkz 
Now check the  Mounts: properties

5)Create a new ServiceAccount named dashboard-sa
=>kubectl create sa dashboard-sa


6)We just added additional permissions for the newly created dashboard-sa account using RBAC.

pod-reader-role.yaml
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  namespace: default
  name: pod-reader
rules:
- apiGroups:
  - ''
  resources:
  - pods
  verbs:
  - get
  - watch
  - list

dashboard-sa-role-binding.yaml 
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: read-pods
  namespace: default
subjects:
- kind: ServiceAccount
  name: dashboard-sa # Name is case sensitive
  namespace: default
roleRef:
  kind: Role #this must be Role or ClusterRole
  name: pod-reader # this must match the name of the Role or ClusterRole you wish to bind to
  apiGroup: rbac.authorization.k8s.io

=>kubectl get role
=>kubectl get rolebinding


7)Enter the access token in the UI of the dashboard application. Click Load Dashboard button to load Dashboard

Create an authorization token for the newly created service account, copy the generated token and paste it into the token field of the UI.
To do this, run kubectl create token dashboard-sa for the dashboard-sa service account, copy the token and paste it in the UI.


=>kubectl get sa
=>kubectl describe sa dashboard-sa
=>kubectl describe secret dashboard-sa-token-rdqw9


8)You shouldn't have to copy and paste the token each time. The Dashboard application is programmed to read token from the secret mount location. However currently, the default service account is mounted. Update the deployment to use the newly created ServiceAccount
Edit the deployment to change ServiceAccount from default to dashboard-sa

=>k get deployment
=>kubectl get deployment -o yaml>mydeployment.yaml
=>kubectl delete deploy web-dashboard 

=>vi mydeployment.yaml
Add serviceAccountName: dashboard-sa under templated spec

    template:
      metadata:
        creationTimestamp: null
        labels:
          name: web-dashboard
      spec:
        serviceAccountName: dashboard-sa
        containers:




#Solution: Resource Requirements
-------------------------------------------------
When a pod is created the containers are assigned a default CPU request of .5 and 
memory of 256Mi". 
For the POD to pick up those defaults you must have first set those as default values for request and limit by creating a LimitRange in that namespace.

apiVersion: v1
kind: LimitRange
metadata:
  name: mem-limit-range
spec:
  limits:
  - default:
      memory: 512Mi
    defaultRequest:
      memory: 256Mi
    type: Container

apiVersion: v1
kind: LimitRange
metadata:
  name: cpu-limit-range
spec:
  limits:
  - default:
      cpu: 1
    defaultRequest:
      cpu: 0.5
    type: Container 

1)A pod called rabbit is deployed. Identify the CPU requirements set on the Pod?
=>kubectl describe pod rabbit


2)Another pod called elephant has been deployed in the default namespace. It fails to get to a running state. Inspect this pod and identify the Reason why it is not running.

=>kubectl describe pod elephant

Check this section:
    Last State:     Terminated
      Reason:       OOMKilled
      Exit Code:    1

3)The status OOMKilled indicates that it is failing because the pod ran out of memory. Identify the memory limit set on the POD.

=>kubectl describe pod elephant
    Limits:
      memory:  10Mi

4)The elephant pod runs a process that consumes 15Mi of memory. Increase the limit of the elephant pod to 20Mi.

=>kubectl get pod -o yaml elephant >newpod.yaml
=>kubectl delete pod elephant 
=>vi newpod.yaml 
Update the memory limit from 10 20
=>kubectl apply -f newpod.yaml 




#Taints and Tolerations
-------------------------------------------------
Tain = Node
Toleration = Pod

=>kubectl describe node controlplane | grep Taints


#Solution: Taints and Tolerations
-------------------------------------------------


1)How many nodes exist on the system?
=>kubectl get node


2)Create a taint on node01 with key of spray, value of mortein and effect of NoSchedule
Key = spray
Value = mortein
Effect = NoSchedule

=>kubectl taint --help
=>kubectl taint nodes node01 spray=mortein:NoSchedule
=>kubectl describe node node01 | grep Taints


3) Create a pod name mosquito, Why pod on pending state ?
=>kubectl describe pod mosquito

4)Create another pod named bee with the nginx image, which has a toleration set to the taint mortein.

Image name: nginx
Key: spray
Value: mortein
Effect: NoSchedule
Status: Running

=> kubectl run bee --image=nginx --dry-run=client -o yaml>bee.yaml
=>vi bee.yaml

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: bee
  name: bee
spec:
  containers:
  - image: nginx
    name: bee
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
  tolerations:
    - key: "spray"
      value: "mortein"
      effect: "NoSchedule"
      operator: "Equal"
status: {}

=>k apply -f bee.yaml 


5)Notice the bee pod was scheduled on node node01 despite the taint?
=>kuebctl get pod -o wide

6)Remove the taint on controlplane, which currently has the taint effect of NoSchedule.
=>kubectl describe node controlplane
Check:
Taints:             node-role.kubernetes.io/control-plane:NoSchedule

Remove it:
=>kubectl taint node controlplane node-role.kubernetes.io/control-plane:NoSchedule-



##Node Selectors
-------------------------------------------------
=>kubectl label nodes node01 size=learge
Labeling a node

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: bee
  name: bee
spec:
  containers:
  - image: nginx
    name: bee
  nodeSelector:
    size: Large

=>kuebctl apply -f newpod.yaml



#Solution: Node Selectors
-------------------------------------------------

1)How many Labels exist on node node01?
=>kubectl describe node node01
Check  Labels:             beta.kubernetes.io/arch=amd64

2)Apply/Add a label color=blue to node node01
=>kubectl label node --help
=>kubectl label node node01 color=blue

3)Create a new deployment named blue with the nginx image and 3 replicas.
=>kubectl create deployment --help
=>kubectl create deployment blue --image=nginx --replicas=3

4)Which nodes can the pods for the blue deployment be placed on?
=>kubectl get pod -o wide
=>kubectl describe node node01 | grep Taints


5)Set Node Affinity to the deployment to place the pods on node01 only.
Name: blue
Replicas: 3
Image: nginx
NodeAffinity: requiredDuringSchedulingIgnoredDuringExecution
Key: color
value: blue

=>kubectl label node node01 color=blue

=>kubectl edit deployment blue
Add this:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
        - matchExpressions:
          - key: color
            operator: In
            values:
            - blue

Complate deployment yaml will like this:
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: blue
  name: blue
spec:
  replicas: 3
  selector:
    matchLabels:
      app: blue
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: blue
    spec:  
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: color
                operator: In
                values:
                - blue
      containers:
      - image: nginx
        name: nginx
        resources: {}
status: {}


6)Create a new deployment named red with the nginx image and 2 replicas, and ensure it gets placed on the controlplane node only.
Use the label key - node-role.kubernetes.io/control-plane - which is already set on the controlplane node.

apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: red
  name: red
spec:
  replicas: 2
  selector:
    matchLabels:
      app: red
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: red
    spec:  
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: node-role.kubernetes.io/control-plane
                operator: Exists
      containers:
      - image: nginx
        name: nginx
        resources: {}
status: {}




##Section 4: Multi-Container Pods
==================================================

##Multi-Container Pods
--------------------------------------------------

Design Pattern of MultiContainer Pod:
   - Sidecar Pattern
   - Adapter Pattern
   - Ambassador Pattern



##Solution - Multi-Container Pods
--------------------------------------------------

1)Create a multi-container pod with 2 containers.
Use the spec given below.
If the pod goes into the crashloopbackoff then add the command sleep 1000 in the lemon container.

Name: yellow
Container 1 Name: lemon
Container 1 Image: busybox
Container 2 Name: gold
Container 2 Image: redis

=>kubectl run yellow --image busybox --dry-run=client -o yaml

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: yellow
  name: yellow
spec:
  containers:
  - image: busybox
    name: lemon
    command: ["sleep", "1000"]
  - image: redis
    name: gold

=>kubectl describe pod laymon


2)We have deployed an application logging stack in the elastic-stack namespace. Inspect it.
Before proceeding with the next set of questions, please wait for all the pods in the elastic-stack namespace to be ready. This can take a few minutes.

=>kubectl get pod -n elastic-stack

Once the pod is in a ready state, inspect the Kibana UI using the link above your terminal. There shouldn't be any logs for now.
We will configure a sidecar container for the application to send logs to Elastic Search.
NOTE: It can take a couple of minutes for the Kibana UI to be ready after the Kibana pod is ready.

You can inspect the Kibana logs by running:
kubectl -n elastic-stack logs kibana


3)The application outputs logs to the file /log/app.log. View the logs and try to identify the user having issues with Login.
Inspect the log file inside the pod

=>kubectl logs app -n elastic-stack
=>kubectl -n elastic-stack exec -it app -- cat /log/app.log


4)Edit the pod to add a sidecar container to send logs to Elastic Search. Mount the log volume to the sidecar container.
Only add a new container. Do not modify anything else. Use the spec provided below.

Note: State persistence concepts are discussed in detail later in this course. For now please make use of the below documentation link for updating the concerning pod.
https://kubernetes.io/docs/tasks/access-application-cluster/communicate-containers-same-pod-shared-volume/

Name: app
Container Name: sidecar
Container Image: kodekloud/filebeat-configured
Volume Mount: log-volume
Mount Path: /var/log/event-simulator/
Existing Container Name: app
Existing Container Image: kodekloud/event-simulator


=>kubectl edit pod -n elastic-stack

- image: kodekloud/filebeat-configured
  name: sidecar
  volumeMounts:
    - mountPath: /var/log/event-simulator/
      name: log-volume


https://www.loom.com/share/c2ae70197e8340a0ba77fc1de8179182



##initContainers
--------------------------------------------------
An initContainer is configured in a pod like all other containers, except that it is specified inside a initContainers section, like this:


apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
spec:
  containers:
  - name: myapp-container
    image: busybox:1.28
    command: ['sh', '-c', 'echo The app is running! && sleep 3600']
  initContainers:
  - name: init-myservice
    image: busybox
    command: ['sh', '-c', 'git clone <some-repository-that-will-be-used-by-application> ;']


When a POD is first created the initContainer is run, and the process in the initContainer must run to a completion before the real container hosting the application starts.
You can configure multiple such initContainers as well, like how we did for multi-pod containers. In that case each init container is run one at a time in sequential order.
If any of the initContainers fail to complete, Kubernetes restarts the Pod repeatedly until the Init Container succeeds.

apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
spec:
  containers:
  - name: myapp-container
    image: busybox:1.28
    command: ['sh', '-c', 'echo The app is running! && sleep 3600']
  initContainers:
  - name: init-myservice
    image: busybox:1.28
    command: ['sh', '-c', 'until nslookup myservice; do echo waiting for myservice; sleep 2; done;']
  - name: init-mydb
    image: busybox:1.28
    command: ['sh', '-c', 'until nslookup mydb; do echo waiting for mydb; sleep 2; done;']


Read more about initContainers here. And try out the upcoming practice test.
https://kubernetes.io/docs/concepts/workloads/pods/init-containers/


##Solution – Init Containers
--------------------------------------------------

1)Identify the pod that has an initContainer configured.
=>kubectl describe pod

2)What is the state of the initContainer on pod blue?
=>kubectl describe pod
Cehck out this:
      State:          Terminated
      Reason:       Completed

3)We just created a new app named purple. How many initContainers does it have?
=>kubectl describe pod purple

4)What is the state of the POD?
=>kubectl describe pod purple
Status:           Pending
IP:               10.42.0.12


5)How long after the creation of the POD will the application come up and be available to users?
=>kubectl describe pod purple
add all the init container wait or sleep time then 


6)Update the pod red to use an initContainer that uses the busybox image and sleeps for 20 seconds
Delete and re-create the pod if necessary. But make sure no other configurations change.

Pod: red
initContainer Configured Correctly

=>kubectl get pod red -o yaml>red.yaml
=>kubectl delete pod red 
=>vi red.yaml

  initContainers:
    - image: busybox
      name: red-initcontainer
      command:
        - "sleep"
        - "20"

Add it like this:

spec:
  initContainers:
    - image: busybox
      name: red-initcontainer
      command:
        - "sleep"
        - "20"
  containers:
  - command:
    - sh
    - -c
    - echo The app is running! && sleep 3600
    image: busybox:1.28
    imagePullPolicy: IfNotPresent
    name: red-container

7)A new application orange is deployed. There is something wrong with it. Identify and fix the issue.
Once fixed, wait for the application to run before checking solution.

=>kubectl describe pod orange
Proble show in pod config properties like connadn, args image name typho etc.




##Section 5: Observability
==================================================
This chapter expose a service what is able to access from browser url.


##Readiness and Liveness Probes
--------------------------------------------------

Curl for serveice url:
curl-test.sh 
for i in {1..20}; do
   kubectl exec --namespace=kube-public curl -- sh -c 'test=`wget -qO- -T 2  http://webapp-service.default.svc.cluster.local:8080/ready 2>&1` && echo "$test OK" || echo "Failed"';
   echo ""
done

=>./curl-test.sh


crash-app.sh 
kubectl exec --namespace=kube-public curl -- wget -qO- http://webapp-service.default.svc.cluster.local:8080/crash


freeze-app.sh 
nohup kubectl exec --namespace=kube-public curl -- wget -qO- http://webapp-service.default.svc.cluster.local:8080/freeze &




##Solution: Readiness adn Liveness Probes
--------------------------------------------------

1)Update the newly created pod 'simple-webapp-2' with a readinessProbe using the given spec
Spec is given on the below. Do not modify any other properties of the pod.

Pod Name: simple-webapp-2
Image Name: kodekloud/webapp-delayed-start
Readiness Probe: httpGet
Http Probe: /ready
Http Port: 8080

=>kubectl get pod simple-webapp-2 -o yaml>newpod.yaml
=>kubectl delete pod simple-webapp-2 
=>vi newpod.yaml

Add this rediness probe under containers: section
readinessProbe:
  httpGet:
    path: /ready
    port:8080

=>kubectl get pod
Now this pod running but not ready, waiting for ready


=>./crash-app.sh
Crash the application, it will be restart again.


2)Update both the pods with a livenessProbe using the given spec
Delete and recreate the PODs.

Pod Name: simple-webapp-1
Image Name: kodekloud/webapp-delayed-start
Liveness Probe: httpGet
Http Probe: /live
Http Port: 8080
Period Seconds: 1
Initial Delay: 80
Pod Name: simple-webapp-2
Image Name: kodekloud/webapp-delayed-start
Liveness Probe: httpGet
Http Probe: /live
Http Port: 8080
Initial Delay: 80
Period Seconds: 1


=>kubectl get pod -o yaml>allwebapp.yaml
=>kubectl delete pod --all
=>vi allwebapp.yaml

Add this on containers: section for bot container

livenessProbe:
  httpGet:
    path: /live
    port: 8080
  periodSeconds: 1
  initialDelaySeconds: 80





##Container Logging and Monitor
--------------------------------------------------

=>kubectl logs mypod 

1)Let us deploy metrics-server to monitor the PODs and Nodes. Pull the git repository for the deployment files.
Run: git clone https://github.com/kodekloudhub/kubernetes-metrics-server.git

=>kubectl create -f .
Create matrice server.

=>kubectl top node
Check resource monitoring of nodes


2)Identify the node that consumes the most CPU(cores).
  Identify the node that consumes the most Memory(bytes).
=>kubectl top node

3)Identify the most memory consumes pod
  Identify the POD that consumes the least CPU(cores).
=>kubectl top pod







##Section 6: POD Design
==================================================

##Labels, Selectors and Annotations
--------------------------------------------------



##Solution: Labels, Selectors and Annotations
--------------------------------------------------

1)We have deployed a number of PODs. They are labelled with tier, env and bu. How many PODs exist in the dev environment (env)?
Use selectors to filter the output

=>kubectl get pod --selector env=dev
OR
=>kubectl get pod --selector env=dev --no-headers
=>kubectl get pod --selector env=dev --no-headers | wc -l

2)How many objects are in the prod environment including PODs, ReplicaSets and any other objects?
=>kubectl get all
=>kubectl get all --selector env=prod
OR
=>kubectl get all --selector env=prod | wc -l


3)Identify the POD which is part of the prod environment, the finance BU and of frontend tier?
=>kubectl get pod --selector env=prod,bu=finance,tier=frontend


4)A ReplicaSet definition file is given replicaset-definition-1.yaml. Try to create the replicaset. There is an issue with the file. Try to fix it.
=>kubectl apply -f replicaset-definition-1.yaml 
=>vi replicaset-definition-1.yaml 
=>kubectl apply -f replicaset-definition-1.yaml 

=>kubectl get rs





##Deployment
--------------------------------------------------

=>kubectl create -f mydeployment.yaml --record
=>kubectl rollout status deployment/mydeployment
=>kubectl rollout history deployment/mydeployment
Creating a deployment

Creating a deployment, checking the rollout status and history:
In the example below, we will first create a simple deployment and inspect the rollout status and the rollout history:

=>kubectl create deployment nginx --image=nginx:1.16
=>kubectl rollout status deployment nginx
=>kubectl rollout history deployment nginx


Using the --revision flag:
Here the revision 1 is the first version where the deployment was created.
You can check the status of each revision individually by using the --revision flag:

=>kubectl rollout history deployment nginx --revision=1


Using the --record flag:
You would have noticed that the "change-cause" field is empty in the rollout history output. We can use the --record flag to save the command used to create/update a deployment against the revision number.

=>kubectl set image deployment nginx nginx=nginx:1.17 --record
=> kubectl rollout history deployment nginx
=>kubectl edit deployments. nginx --record
=>kubectl set image deployment nginx nginx=nginx:1.17 --record=true
=>kubectl edit deployments. nginx --record=true
=>kubectl rollout history deployment nginx --revision=3


Undo a change:Lets now rollback to the previous revision:
=>kubectl rollout history deployment nginx
=>kubectl rollout history deployment nginx --revision=3
=>kubectl describe deployments. nginx | grep -i image:
With this, we have rolled back to the previous version of the deployment with the image = nginx:1.17.

=>kubectl rollout history deployment nginx --revision=1
=>kubectl rollout undo deployment nginx --to-revision=1
=>kubectl describe deployments. nginx | grep -i image:

To rollback to specific revision we will use the --to-revision flag.
With --to-revision=1, it will be rolled back with the first image we used to create a deployment







##Solution:Deployment Rolling Updte and Rollback
--------------------------------------------------
1)What container image is used to deploy the applications?

=>k describe deployment frontend 

2)Inspect the deployment and identify the current strategy?
=>k describe deployment frontend 
StrategyType:           RollingUpdate

3)Let us try that. Upgrade the application by setting the image on the deployment to kodekloud/webapp-color:v2

=>kubectl edit deployment frontend
Edit the yaml file
OR
=>kubectl set image --help
=>kubectl set image deploy frontend simple-webapp=kodekloud/webapp-color:v2
=>kubectl describe deployment frontend 


4)Up to how many PODs can be down for upgrade at a time
Consider the current strategy settings and number of PODs - 4

=>kubectl describe deploy frontend 
checkout:
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Its mean 1 pod what is 25%.


5)Change the deployment strategy to Recreate

=>kubectl get deployment frontend -o yaml>newdeployment.yaml
=>kuebctl delete deploy frontend
 strategy:
    type: Recreate
=>kubectl apply -f newdeployment.yaml
=>kubectl describe deploy frondend
Check:
StrategyType:       Recreate



##Job and CronJob
--------------------------------------------------



##Solution: Job and CronJob
--------------------------------------------------
throw-dice.yaml
apiVersion: v1
kind: Pod
metadata:
  name: throw-dice-pod
spec:
  containers:
  -  image: kodekloud/throw-dice
     name: throw-dice
  restartPolicy: Never

1)Create a Job using this POD definition file or from the imperative command and look at how many attempts does it take to get a '6'.
Use the specification given on the below.

Job Name: throw-dice-job
Image Name: kodekloud/throw-dice

apiVersion: batch/v1
kind: Job
metadata:
  name: throw-dice-job
spec:
  template:
    spec:
      containers:
      - name: throw-dice-job
        image: kodekloud/throw-dice
      restartPolicy: Never
  backoffLimit: 25

=>vi myjob.yaml
=>kubectl apply -f myjon.yaml

=>kubectl describe job throw-dice-job 



2)Update the job definition to run as many times as required to get 3 successful 6's.
Delete existing job and create a new one with the given spec. Monitor and wait for the job to succeed.

Job Name: throw-dice-job
Image Name: kodekloud/throw-dice
Completions: 3
Job Succeeded: True

=>vi jobn3.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: throw-dice-job
spec:
  template:
    spec:
      containers:
      - name: throw-dice-job
        image: kodekloud/throw-dice
      restartPolicy: Never
  backoffLimit: 30
  completions: 3

=>kubectl delete job throw-dice-job
=>kubectl apply -f job3.yaml

=> k describe job throw-dice-job 
Check:
Pods Statuses:    1 Active (0 Ready) / 2 Succeeded / 5 Failed


3)That took a while. Let us try to speed it up, by running upto 3 jobs in parallel.
Update the job definition to run 3 jobs in parallel.

Job Name: throw-dice-job
Image Name: kodekloud/throw-dice
Completions: 3
Parallelism: 3

=>vi jon4.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: throw-dice-job
spec:
  template:
    spec:
      containers:
      - name: throw-dice-job
        image: kodekloud/throw-dice
      restartPolicy: Never
  backoffLimit: 30
  completions: 3
  parallelism: 3


4)Let us now schedule that job to run at 21:30 hours every day.
Create a CronJob for this.
CronJob Name: throw-dice-cron-job
Image Name: kodekloud/throw-dice
Schedule: 30 21 * * *

apiVersion: batch/v1
kind: CronJob
metadata:
  name: throw-dice-cron-job
spec:
  schedule: "30 21 * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: throw-dice-cron-job
            image: kodekloud/throw-dice
          restartPolicy: OnFailure






























##Section 5: temp
==================================================
##temp
--------------------------------------------------
##Solution: temp
--------------------------------------------------



@@@
ZonVovg
=================================================
#D1:CoreConcept               
================================================= 
pod1:
apiVersion: v1
kind: Pod
metadata:
  name: my-webserver-pod
spec:
  containers:
    - name: mynginx
      image: nginx

pod2:
apiVersion: v1
kind: Pod
metadata:
  name: my-pod1
spec:
  containers:
    - name: mycont1
      image: busybox
      command: ["sleep","3360"]
   or args: ["3600"]
   or args: ["sleep","3360"]

pod3:
apiVersion: v1
kind: Pod
metadata:
  name: busybox
spec:
  containers:
  - name: busybox
    image: busybox
    command: ["/bin/sh"]

pod4:
apiVersion: v1 
kind: Pod 
metadata: 
  name: bu1 
spec: 
  containers: 
  - name: busybox 
    image: busybox 
    command: ["sh","-ec","ping google.com"] 

pod5:
apiVersion: v1
kind: Pod
metadata:
  name: ubuntu
spec:
  containers:
  - name: ubuntu
    image: ubuntu:latest
    # Just spin & wait forever
    command: [ "/bin/bash", "-c", "--" ]
    args: [ "while true; do sleep 30; done;"]


pod6:
kind: Pod
metadata:
  name: nginx-ports
spec:
  containers:
  - image: nginx
    name: nginx-ports
    ports:
    - containerPort: 80


=>kubectl apply -f myfile.yml
=>kubectl get pods
=>kubectl exec -it my-pod1 sh


#PracticeDomain1:
-----------------------------------------------------
Q1:
apiVersion: v1
kind: Pod
metadata:
  name: kplabs-nginx
spec:
  containers:
    - name: mycontainer
      image: nginx

Q2:
apiVersion: v1
kind: Pod
metadata:
  name: kplabs-cmdargs
spec:
  containers:
    - name: cmdcontainer
      image: busybox
      command: ["sleep"]
      args: ["3500"]

Q3:
apiVersion: v1
kind: Pod
metadata:
  name: kplabs-pods
spec:
  containers:
    - name: nginx
      image: nginx
      ports: 
      - containerPort: 80


Q4:
apiVersion: v1
kind: Pod
metadata:
  name: log-ng
spec:
  containers:
    - name: log-ngcont
      image: nginx
      args:
       - /bin/sh
       - -c
       - >
         i=0;
         while true;
         do
          echo "$i: $(date)" >> /var/log/1.log;
          echo "$(date) INFO $i" >> /var/log/2.log;
          i=$((i+1));
          sleep 1;
         done


=================================================
#D2:PodDesign               
================================================= 

#Label and Selectors:
-------------------------------------------------
=>kubectl label --help

=>kubectl run pod1 --image=nginx
=>kubectl get pods --show-labels
Show Labels

=>kubectl label pod pod1 env=prod
=>kubectl label  pods --all status=runing
Add Label

=>kubectl get pods -l env=prod
Filter by label

=>kubectl label pod pod2 env-
Delete label



#ReplicaSet:
-------------------------------------------------
=>kubectl create deployment myreplica --image=nginx --replicas 3 --dry-run=client -o yaml
Generate a deployment file and Update for ReplicaSet

=>kubectl get rs
=>kubectl get replicaset
=>kubectl apply -f rpset.yaml

=>kubectl describe replicaset myrpset

=>kubectl get pods --show-labels
=>kubectl delete rs myrpset
If a pod got sutdown then replicatSet re create it


repliSet1:
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: myrpset
spec:
  replicas: 3
  selector:
    matchLabels:
      tier: myrpset
  template:
    metadata:
      labels:
        tier: myrpset
    spec:
      containers:
      - name: myngcont
        image: nginx



#Deployment:
-------------------------------------------------
=>kubectl create deployment --help

deployment1:
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mydeployment
spec:
  replicas: 3
  selector:
    matchLabels:
      tier: myproj
  template:
    metadata:
      labels:
        tier: myproj
    spec:
      containers:
      - name: myng
        image: nginx


=>kubectl apply -f mydeploy.yaml
=>kubectl create deployment mydeployment --image=nginx --replicas 3
=>kubectl get deployment
=>kubectl describe deployment mydeployment

=>kubectl create deployment mydeployment --image=nginx --replicas 3 --dry-run=client -o yaml
Generate deployment yaml file

=>kubectl delete deployment mydeployment
Delete deployment


Edit yaml file for Update the deployment, like change the image version
then check the rollout history
=>kubectl rollout --help

=>kubectl rollout history deployment.v1.apps/mydeployment
=>kubectl rollout history deployment.v1.apps/mydeployment --revision 2
Get Rollout history and check revision details

=>kubectl get deployment my-dep -o yaml
=>kubectl describe deployment mydeployment
Check which revision version currently runing 

=>kubedtl rollout undo --help

=>kubectl rollout history deployment/dep1 --revision=3
Check rollout history data

=>kubectl rollout undo deployment/dep1 --to-revision=2

=>kubectl rollout history deployment.v1.apps/mydeployment
=>kubectl rollout undo deployment.v1.apps/mydeployment
=>kubectl rollout undo deployment.v1.apps/mydeployment --to-revision=2
=>kubectl describe deploy mydeployment
Deployment rolback



Form axSurge and maxUnavailable:
=>kubectl get deployment my-dep -o yaml
Check masSurge and maxUnavailable
=>kubectl create deployment mydeploy --images=nginx --replica 3

maxSurge: The number of pods that can be created above the desired amount of pods.
maxUnavailable: The number of pods that can be unavailable during the update process.


=>kubectl set --help
=>kubectl set image --help
=>kubectl set image deployment mydeploy nginx=httpd
Update  deployemnt


=>kubectl get deploy dep1 -o yaml>dep2.yaml
Then update the file of dep2.yaml 
OR
=>kubectl edit deployment mydeploy
=>kubectl get pods
Edit deoloyement



=>kubectl set image deployment mydeploy nginx=httpd
=>kubectl set image deployment mydeploy nginx=httpd --record
=>kubectl rollout history deployment mydeploy
=>kubectl rollout undo deployment/mydeploy
Roolout/Undo the deployemnt last deployment

=>kubectl scale deployment --help
=>kubectl scale deployment mydeploy --replicas 1
Scele up/down deployemnt


#InP
------------------------------------------------
1)How to set a new Image to deployment as part of rolling Update
2)Need to know --record Instruction
3)You should know how to rollback a deployment
4)you should be able to scale the deployment






#Batch Job:
-------------------------------------------------
Job are two type:
1)jobs (Run to completion)
2)CronJob

=>kubectl get jobs
=>kubectl create job --help
=>kubectl create job my-job --image=busybox --dry-run=client -o yaml


simpldJob:
apiVersion: batch/v1
kind: Job
metadata: 
  name: myjob
spec:
  template:
    spec:
      containers:
        - name: myjoncont
          image: busybox
          command: ["/bin/sh"]
          args: ["-c","echo Hello Imran"]
      restartPolicy: Never


=>kubectl apply -f myjob.yml
=>kubectl logs myjob-pod 
After complate the job task pod are not exit, stay with complate Status.

=>kubectl get jobs
=>kubectl delete job myjob
When delete the job all related pos will be delete



=>kubectl create cronjob --help
=>kubectl create cronjob my-job --image=busybox --schedule="*/1 * * * *"  --dry-run=client -o yaml

cronJob:
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: mycronjob
spec:
  schedule: "*/1 * * * *"
  jonTemplate:
    spec:
      template:
        spec:
          containers:
            - name: mycronjob-pod
              image: busybox
              args: 
              - /bin/sh
              - -c
              - date; echo Hello Imran, This is from CronJob.
          restartPolicy: OnFailure


=>kubectl get cronjob
=>kubectl get job
=>kubectl get job -w

=>kubectl describe cronjob cron1
=>kubectl delete cronjob mycronjob



#PracticeDomain02:
-----------------------------------------------------
Question 1: Labels
Create a pod named kplabs-label. The pod should be launched from nginx image. The name of container should be nginx-container. Attach following label to the pod.
env=production
app=webserver

Solution:
=>kubectl run --help
=>kubectl run kplabs-label --image=nginx --labels="app=webservr,env=production" --dry-run -o yaml>pod1.yaml

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    env: production
    app: webserver
  name: kplabs-label
spec:
  containers:
  - image: nginx
    name: kplabs-container



Question 2: Deployments
Create a deployment named kplabs-deployment. The deployment should be launched from nginx image. The deployment should have three replicas. The selector should be based on the label of app=nginx

=>kubectl create deployment --help
=>kubectl create deployment kplabs --image=nginx --replicas=3 --dry-run=client -o yaml>dep2.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: kplabs-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - image: nginx
        name: nginx


Question 3: Deployments - Rolling Updates and Rollbacks
Create a deployment named kplabs-updates. The deployment should be launched from nginx image. There should be two  replicas. Verify the status of the deployment. As part of rolling update, update the image to nginx2:alpine. Verify the status of deployment. Perform a rollback to the previous version. Verify the status of deployment.

apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-deploy
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - image: nginx
        name: nginx-cont




=>kubectl apply -f mydeployment.yaml
=>kubectl set image deployment my-deploy nginx-cont=nginx2:alpine --record=true
=>kubectl get pods
  If show ImagePullBackOff then its deployment update failed

=>kubectl rollout undo deployment/my-deploy
Undo deploument and rollback


Question 4: Labels and Selectors
Create a deployment named kplabs-selector. The pods should be launched from nginx image.The pods should only be launched in a node which has a label of disk=ssd. Observe the status of deployment. Add the appropriate label to the worker node and then observe the status of the deployment.

=>kubectl create deployment --help
=>kubectl create deployment my-dep --image=nginx --replicas=3  --dry-run=client -o yaml>dep4.yaml

Edit yaml file and add this under spec as containers
      nodeSelector:
        disktype: ssd

apiVersion: apps/v1
kind: Deployment
metadata:
  name: node-select-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx-node-select
  template:
    metadata:
      labels:
        app: nginx-node-select
    spec:
      containers:
      - image: nginx
        name: node-select-cont
      nodeSelector:
        disktype: ssd

=>kubectl get pods
Pod will show panding status becouse disktype: ssd not found on node
Now need to add this on node 

=>kubectl get node
=>kubectl get node --show-labels
=>kubectl label nodes node01 disktype=ssd
=>kubectl get deployment


Question 5:  CronJob
Create a job named kplabs-job. The job should run every minute and should print out the current date.

=>kubectl create cronjob --help
=>kubectl create cronjob kplabs-job --image=busybox --schedule="*/1 * * * *" --dry-run=client -o yaml>cron5.yaml

apiVersion: batch/v1
kind: CronJob
metadata:
  name: mycronjob
spec:
  schedule: "*/1 * * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: mycroncont
            image: busybox
            args:
            - /bin/sh
            - -c
            - date
          restartPolicy: OnFailure

=>kubectl get pods
=>kubectl logs mycronjob

=>kubectl get cronjob
=>kubectl get job
=>kubectl get job -w
=>kubectl delete cronjob mycronjob


Question 6:  CronJob
Create a job named kplabs-cron. The job should run every minute and should run following command "curl kplabs.in/ping". Terminate the container within 10 seconds if it does not run.

apiVersion: batch/v1
kind: CronJob
metadata:
  name: mycron2
spec:
  schedule: "*/1 * * * *"
  jobTemplate:
    spec:
      activeDeadlineSeconds: 15
      template:
        spec:
          containers:
          - name: hello
            image: busybox
            command: ["curl",  "hello.in/ping"]
          restartPolicy: OnFailure


Question 7:  Deployment Configuration
Create a deployment named kplabs-configuration. The deployment should have 3 replicas of nginx image. Once the deployment is created, verify the maxSurge and maxUnavailable parameters. Edit the the maxUnavailable to 0 and maxSurge to 30% on the live deployment object. Once those two parameters are modified, change the image of the deployment to nginx:alpine. Make sure to use the record instruction on rolling updates.

apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    run: mydeploy-label
  name: mydeploy
spec:
  replicas: 3
  selector:
    matchLabels:
      run: mydeploy-slt
  template:
    metadata:
      labels:
        run: mydeploy-slt
    spec:
      containers:
      - image: nginx
        name: mydeploy-cont

=>kubectl rollout history deployment mydeploy
=>kubectl rollout undo deployment/mydeploy
Roolout/Undo the deployemnt last deployment

=>kubectl set image --help
=>kubectl set image deployment mydeploy mydeploy-cont=httpd
=>kubectl set image deployment mydeploy mydeploy-cont=nginx --record









=================================================
#D3:Service and Networking               
================================================= 

Services act as a Gateway of variable amount pods in different node.

ServiceType:
  NodePort
  ClusterIP
  LoadBalancer
  ExternalName


serviceExample1:
Step 1: Creating Backend and Frontend PODS
=>kubectl run bkpod1 --image=nginx
=>kubectl run bkpod1 --image=nginx
=>kubectl run fndpod --image=ubuntu --command -- sleep 1h
=>echo -e "Welcome to nginx! \nHost Name: $(hostname -f)\nHost IP: $(hostname -i)">/usr/share/nginx/html/index.html
=>kubectl exec mypod1 -- ls /usr/share/nginx/html

Step 2: Test the Connection between Frontend and Backend PODs
=>kubectl get pods -o wide
=>kubectl exec -it fndpod -- bash
=>apt-get update && apt-get -y install curl
Curl to backend IP



Step 3: Create a new Service (clusterIp by default)
vi myservice.yaml

=>kubectl create service --help
=>kubectl create service clusterip --help
=>kubectl create service clusterip svc1 --tcp=80:80 --dry-run=client -o yaml

apiVersion: v1
kind: Service
metadata:
   name: app-service
spec:
   ports:
   - port: 8181
     targetPort: 80

=>kubectl apply -f myservice.yaml
=>kubectl get service
=>kubectl describe service app-service

Step 4: Associate Endpoints with Service
vi myendpoint.yaml
apiVersion: v1
kind: Endpoints
metadata:
  name: app-service
subsets:
  - addresses:
      - ip: 10.244.1.3
      - ip: 10.244.1.2
    ports:
      - port: 80

=>kubectl apply -f myendpoint.yaml



Step 5: Test the Connection
=>kubectl exec -it fndpod -- bash
curl to service IP with port


Create Service with Selector:
-------------------------------------------------

Step 1: Creating Service
apiVersion: v1
kind: Service
metadata:
   name: sltsvc1
spec:
   selector:
     env: webapp
   ports:
   - port: 80
     targetPort: 80

Step 2: Creating Deployments

=>kubectl get node --show-labels
=>kubectl label nodes node01 disktype=ssd

apiVersion: apps/v1
kind: Deployment
metadata:
  name: mydep
  labels:
    env: mydep-lb
spec:
  replicas: 5
  selector:
    matchLabels:
      env: webapp
  template:
    metadata:
      labels:
        env: webapp
    spec:
      containers:
      - name: mydep-cont
        image: imranmadbar/nginx
        ports:
        - containerPort: 80
      nodeSelector:
        disktype: ssd

=>kubectl apply -f mydeployment.yaml
=>kubectl get pods --show-labels


=>kubectl apply -f myselector-service.yaml
=>kubectl describe service myselector-service

=>kubectl scale deployment/mydeployment --replicas=5
=>kubectl describe service myselector-service

=>kubectl run manual-added-pod --image=nginx
=>kubectl label pods manual-added-pod  env=backend-service

=>kubectl describe service myselector-service
=>kubectl describe endpoints kplabs-service-selector

=>kubectl get endpoints
=>kubectl describe endpoints myselector-service




#NodePort Service
-------------------------------------------------
=>kubectl create service nodeport --help
Step 1: Create Sample POD with Label
=>kubectl run nppod --labels="type=publicpod" --image=nginx
=>kubectl get pods --show-labels

Step 2: Create NodePort service
apiVersion: v1
kind: Service
metadata:
   name: npsvc
spec:
   selector:
     type: publicpod
   type: NodePort
   ports:
   - port: 80
     targetPort: 80

=>kubectl apply -f mynodeport.yaml
=>kubectl get service

Step 3: Fetch the Worker Node Public IP

=>kubectl get nodes -o wide
=>curl 192.2.145.12:32613
Copy the Public IP of Worker Node and Paste it in browser along with NodePort


=>kubectl delete pod nppod
=>kubectl delete -f mynodeport.yaml



#LoadBalancer Service
-------------------------------------------------
Step 1: Create Sample POD with Label
=>kubectl run lb-pod --labels="type=loadbalanced" --image=nginx
=>kubectl get pods --show-labels

Step 2: Create LoadBalancer service
apiVersion: v1
kind: Service
metadata:
  name: elb-service
spec:
  type: LoadBalancer
  ports:
  - port: 80
    protocol: TCP
  selector:
    type: loadbalanced
    
=>kubectl apply -f elb-service.yaml

Step 3: Verify Service Logs
=>kubectl describe service elb-service

=>kubectl delete pod lb-pod
=>kubectl delete -f elb-service.yaml



#Service generated by CLI
-------------------------------------------------
=>kubectl run mynginx --image=nginx
=>kubectl expose pod mynginx --name nginx-service --port=80 --target-port=80 --dry-run=client -o yaml
=>kubectl expose pod mynginx --name nginx-service --port=80 --target-port=80 --dry-run=client -o yaml > service2.yaml

=>kubectl expose pod mynginx --name nginx-nodeport-service --port=80 --target-port=80 --type=NodePort --dry-run=client -o yaml
=>kubectl get service
=>kubectl expose deployment mydeployment --name nginx-deployment-service --port=80 --target-port=8000
=>kubectl describe service nginx-deployment-service




#Name Space
-------------------------------------------------
=>kubectl get namespace
=>kubectl get pod --namespace kube-system
=>kubectl create namespace prod-namespace

=>kubectl run prod-ng --image=nginx --namespace prod-namespace



#Service Account
-------------------------------------------------
=>cat /run/secrets/kubernetes.io/serviceaccount/token
Pod token

=>kubectl get sa
=>kubectl get serviceaccount
=>kubectl get serviceaccount -n my-mynmspc
=>kubectl get secret -n my-mynmspc

=>kubectl get pod -o yaml
Chace current pod service account

=>kubectl get secrets -n prod-mynmspc 
=>kubectl get sa default -o yaml

=>kubectl get pod myng -o yaml

=>kubectl create sa my-saacc
=>kubectl get secret
=>kubectl run myng-sa --image=nginx --serviceaccount="mysaacc"
=>kubectl get pod
=>kubectl get pod myng-sa -o yaml

=>kubectl get pod deployment-sa-8585d89b57-46rtb -o yaml


#Network Security Policies
-------------------------------------------------

=>kubectl run mybx1 --image=busybox --sh
=>kubectl run mybx2 --image=busybox --sh
Run two Pod and ping each other


policy1:
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: test-network-policy
  namespace: default
spec:
  podSelector:
    matchLabels:
      run: mybx2
  policyTypes:
    - Ingress

=>kubectl apply -f my-net-poilicy





#PracticeDomain3:
-----------------------------------------------------

Q1: Create a deployment named kplabs-service. 
The deployment should have three replicas and the image should be based on nginx. 
Create a service based on NodePort. 
The service port should be 8080. The website should be accessible from port 32001 from all hosts.

=>kubectl create deployment kplabs-service --image=nginx --replicas=1 --dry-run=client -o yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: kplabs-service
spec:
  replicas: 3
  selector:
    matchLabels:
      app: kplabs-service
  template:
    metadata:
      labels:
        app: kplabs-service
    spec:
      containers:
      - image: nginx
        name: nginx


=>kubectl create service nodeport --help
=>kubectl create service nodeport my-ns --tcp=32001:8080 --node-port=32001 --dry-run=client -o yaml

apiVersion: v1
kind: Service
metadata:
  name: myservice
  labels:
    run: myservice
spec:
  type: NodePort
  ports:
  - port: 8080
    targetPort: 80
    nodePort: 32001
    protocol: TCP
  selector:
    run: kplabs-service

SSH to node and curl with nodeport

=>curl serviceIP:8080
=>curl nodeIP:32001



Q2: Rung this services and access nginx from service IP, this is with port mapping issue.
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    run: kplabs-service
  name: kplabs-fix
spec:
  replicas: 2
  selector:
    matchLabels:
      run: kplabs-fix
  template:
    metadata:
      labels:
        run: kplabs-fix
    spec:
      containers:
      - image: nginx
        name: kplabs-service
---
apiVersion: v1
kind: Service
metadata:
  name: fix-service
  labels:
    run: fix-service
spec:
  ports:
  - port: 8080
    targetPort: 80
    protocol: TCP
  selector:
    run: kplabs-fix

Question 3: Namespace
Create a pod named redis-pod . The pod should be part of the namespace my-namespace.
The pod should make use of redis image. Expose port 6379.

=>kubectl create namespace --help
=>kubectl create namespace my-namespace

=>kubectl run pod --help
=>kubectl run kplabs-namespace --image=redise --help
=>kubectl run redis-pod --image=redis --port=6379 -n my-namespace



Question 4: Service Account
Create a new service account named kplabs. Launch a new pod named kplabs-sa from nginx image. 
The pod should be launched from the kplabs service account. Verify whether the token has been mounted inside the pod.

=>kubectl create serviceaccount myseracc
=>kubectl get sa
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: kplabns-sa
  name: kplabns-sa
spec:
  containers:
  - image: nginx
    name: kplabns-sa
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
  serviceAccountName: kplabs
status: {}


Question 5: Deployments and Service Account
Create a deployment named deployment-sa. The deployment should have 2 replicas of nginx image. 
After the deployment has been created, check the service account associated with the pods. 
Modify the deployment so that all pods shall use the service account of kplabs.

=>kubectl create deployment --help
=>kubectl create deployment deployment-sa  --image=nginx --replicas=3
=>kubectl edit deployment deployment-sa

Add this two property below  securityContext: {}
      serviceAccount: kplabs
      serviceAccountName: kplabs

=>kubectl get pod deployment-sa-75dd877cbf-ddjjd -o yaml
Check updated service acc



Q6:
=>kubectl apply -f troubleshoot-deployment.yaml

apiVersion: v1
kind: Namespace
metadata:
  name: newkplabs
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    run: troubleshoot-deployment
  name: troubleshoot-deployment
  namespace: newkplabs
spec:
  replicas: 2
  selector:
    matchLabels:
      run: troubleshoot-deployment
  template:
    metadata:
      labels:
        run: troubleshoot-deployment
    spec:
      containers:
      - image: ninx
        name: troubleshoot-deployment

=>kubectl get deployment --all-namespaces
=>kubectl get pod -n newkplabs
=>kubectl edit deployment troubleshoot-deployment  -n newkplabs
Fixed the image name ninx to nginx



================================================= 
Section 5: Domain 4 - Configuration
================================================= 
Three Secret type:
  1)Generic
    File, directory, literal value
  2)Docker Registry
  3)TLS


=>kubectl get secret

GenericType:
=>kubectl create secret generic --help
=>kubectl create secret generic mysec --from-literal=dbpass=dbpass12345
=>kubectl create secret generic mysec1 --from-literal=key1=supersecret --from-literal=key2=topsecret
=>kubectl describe secret mysec

=>kubectl get secret mysec -o yaml
=>echo ZGJwYXNzMTIzNDU= | base64 -d

apiVersion: v1
kind: Secret
metadata:
  name: mysec2
type: Opaque
data:
  #key1: root
  #key2: 54321
  key1: cm9vdAo=
  key2: NTQzMjEK


FileType:
=>echo key1=supersec>secFile.txt
=>kubectl create secret generic mysec3 --from-file=./secFile.txt
=>kubectl get secret
=>kubectl describe secret mysec3


apiVersion: v1
kind: Secret
metadata:
  name: mysec4
type: Opaque
stringData:
  config.yaml: | 
    username: root
    password: 54321

=>kubectl get secret -o yaml


MountingSecret:
apiVersion: v1
kind: Pod
metadata:
  name: myngpod
spec:
  containers:
  - name: myngpod
    image: nginx
    volumeMounts:
      - name: mymount
        mountPath: "/etc/secmount"
        readOnly: true
  volumes: 
  - name: mymount
    secret:
      secretName: mysec1

=>kubectl apply -f pod1.yaml
=>kubectl get pod
=>kubectl exec -it myngpod -- bash
=>cd /etc/mymount/


MountingEnvVariableSecret:
apiVersion: v1
kind: Pod
metadata:
  name: pod2
spec:
  containers:
  - name: myngpod-env
    image: nginx
    env:
      - name: SECRET_USERNAME
        valueFrom:
          secretKeyRef:
            name: mysec2
            key: key1

apiVersion: v1
kind: Pod
metadata:
  name: pod4
spec:
  containers:
  - name: myngpod-env
    image: nginx
    env:
      - name: SECRET_USERNAME
        valueFrom:
          secretKeyRef:
            name: mysec4
            key: key1

=>kubectl apply -f pod1.yaml
=>kubectl get pod
=>kubectl exec -it myngpod -- bash
=>cd /etc/mymount/


ResourceLimites:
apiVersion: v1
kind: Pod
metadata:
  name: pod2
spec:
  containers:
  - name: pod2-cont
    image: nginx
    resources:
      requests:
        memory: "64Mi"
        cpu: "250m"
      limits:
        memory: "128Mi"
        cpu: "500m"

=>kubectl get pod2 -o wide
=>kubectl describe node node01
=>kubectl delete -f pod2.yaml

apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
  - image: nginx
    name: my-cont
    resources:
      requests:
        memory: "64Mi"
      limits:
        memory: "128Mi"
        cpu: "1"




#Practice Test - Domain 4
-------------------------------------------------
Question 1: Resource Quotas
Create a pod named kplabs-quota. The pod should have following configuration:
a. Should run with nginx image.
b. It should use maximum of 512 MiB of memory.
c. It should use maximum of 2 core CPU.
d. The POD should require a minimum of 128 MiB of memory before it is 
scheduled.

apiVersion: v1
kind: Pod
metadata:
  name: kplabs-quota
spec:
  containers:
  - name: kplabs-quota-cont
    image: nginx
    resources:
      requests:
        memory: "128Mi"
      limits: 
        memory: "512Mi"
        cpu: "2"



Question 2: Secrets
Create a secret named kplabs-secret. The secret should have content where user=admin and pass=12345. Create a pod from the nginx image. Mount the secret as environment variables in the pod. The username should be available as DB_USER and password should be available as DB_PASSWORD inside the pod

=>kubectl create secret generic --help
=>kubectl create secret generic mysec1 --from-literal=user=admin --from-literal=pass=12345

apiVersion: v1
kind: Pod
metadata:
  name: pod3
spec:
  containers:
  - name: myngpod-env
    image: nginx
    env:
      - name: DB_USER
        valueFrom:
          secretKeyRef:
            name: mysec1
            key: user
      - name: DB_PASSWORD
        valueFrom:
          secretKeyRef:
            name: mysec1
            key: pass




================================================= 
Section 6: Domain 5 - Observability
================================================= 

Liveness:
-------------------------------------------------
livenessProbe three type:
  http, command, tcp
=>kubectl run -it ubuntu --image=ubuntu
=>kubectl exec -it ubuntu -- service nginx status
=>echo $?
=>kubectl exec -it ubunty -- bash
  =apt-get update && apt-get install nginx -y
  =service nginx start
  =service nginx status
  =echo $?

apiVersion: v1
kind: Pod
metadata:
  name: lnpod
spec:
  containers:
  - image: ubuntu
    name: lnpod
    tty: true
    livenessProbe:
      exec:
        command: 
        - service
        - nginx
        - status
      initialDelaySeconds: 10
      periodSeconds: 5

=>kubectl apply -f pod.yaml
=>kubectl exec -it lnpod -- service nginx status
=>kubectl get pods



Readiness:
-------------------------------------------------
apiVersion: v1
kind: Pod
metadata:
  name: rdpod
spec:
  containers:
  - image: ubuntu
    name: rdpod
    tty: true
    readinessProbe:
      exec:
        command: 
        - cat
        - /tmp/healthy
      initialDelaySeconds: 10
      periodSeconds: 5


=>kubectl apply -f pod2.yaml 
=>kubectl get pods
=>kubectl exec -it rdpod -- touch /tmp/healthy
=>kubectl exec -it rdpod -- rm /tmp/healthy




Appliation logs:
-------------------------------------------------
Fache multi container podlog:
apiVersion: v1
kind: Pod
metadata:
  name: mlpod
spec:
  containers:
  - image: busybox
    name: cont1
    command: ["ping"]
    args: ["google.com"]
  - image: busybox
    name: cont2
    command: ["ping"]
    args: ["8.8.8.8"]


=>kubectl logs mlpod cont2



Monitoring Components 
-------------------------------------------------
Install Metric server:
=>kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

=>kubectl top pods
=>kubectl top nodes

=>kubectl top pods --all-namespaces




#Event
-------------------------------------------------
=>kubectl get events -n kube-system
=>kubectl get events


#Field Selector
-------------------------------------------------
=>kubectl get pods --all-namespaces --field-selector metadata.namespace!=default
=>kubectl get events --field-selector involvedObject.name=mynginx
=>kubectl get pods --field-selector ""

=>kubectl create namespace myns
=>kubectl  get namespace
=>kubectl run mung --image=nginx -n myns
=>kubectl get pods --all-namespaces --field-selector metadata.namespace=myns



#Practice Test - Domain 5
---------------------------------------------------
Question 1: Probes
Create a POD from the nginx image. Pod should be named kplabs-probe. The pod should be created in such a way that if the application inside is not responding to HTTP requests made on port 8080, then Kubernetes should restart the POD.

apiVersion: v1
kind: Pod
metadata:
  name: kplabs-probe
spec:
  containers:
  - name: liveness
    image: nginx
    livenessProbe:
      httpGet:
        path: /
        port: 8080
      initialDelaySeconds: 3
      periodSeconds: 3


Question 2: Probes
Create a POD named newprobe. Pod should run from nginx image. The Pod should run with arguments defined below. Create a probe that checks if a file on that path /tmp/myfile exists. If it does not exist, the POD should be restarted.

    - /bin/sh
    - -c
    - touch /tmp/myfile; 3600

apiVersion: v1
kind: Pod
metadata:
  labels:
    test: liveness
  name: pod3
spec:
  containers:
  - name: liveness
    image: registry.k8s.io/busybox
    args:
    - /bin/sh
    - -c
    - touch /tmp/healthy; sleep 10; rm -f /tmp/healthy; sleep 600
    livenessProbe:
      exec:
        command:
        - cat
        - /tmp/healthy
      initialDelaySeconds: 2
      periodSeconds: 2



Question 4: Events
Store all the events associated with the pod liveness-http and store it to /tmp/podlog.txt. Make sure to use the kubectl command and do not modify anything in the output.

=>kubectl get events --field-selector involvedObject.name=liveness-http
=>echo | kubectl get events --field-selector involvedObject.name=newprobe > myoutput.txt




================================================= 
#Section 7: Domain 6 - State Persistence
================================================= 
There are many type volume: hostPath:
apiVersion: v1
kind: Pod
metadata:
  name: mypod-vol
spec:
  containers:
  - name: mypod-cont
    image: nginx
    volumeMounts:
    - mountPath: /data
      name: podvol
  volumes:
  - name: podvol
    hostPath:
      path: /mydata
      type: DirectoryOrCreate



apiVersion: v1
kind: Pod
metadata:
  name: mypod-vol
spec:
  containers:
  - name: mypod-cont
    image: nginx
    volumeMounts:
    - mountPath: /data
      name: podvol
  volumes:
  - name: podvol
    hostPath:
      path: /root/mydata
      type: DirectoryOrCreate



#PersistentVolume and PersistentVolumeClaim | pv
-------------------------------------------------
Persistent Volume:pv.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: my-pv
spec:
  storageClassName: manual
  capacity:
    storage: 10Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: /tmp/data


=>kubectl apply -f pv.yaml 
=>kubectl get pv
This pv create in host node

PersistentVolumeClaim: pvc.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-pvc
spec:
  storageClassName: manual
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi

=>kubectl apply -f pvc.yaml
=>kubectl get pvc

PVC pod: pod-pvc.yaml
apiVersion: v1
kind: Pod
metadata:
  name: pvcpod
spec:
  containers:
    - name: mypvccont
      image: nginx
      volumeMounts:
      - mountPath: "/data"
        name: my-volume
  volumes:
    - name: my-volume
      persistentVolumeClaim:
        claimName: my-pvc

=>kubectl apply -f pvc-pod.yaml
=>df -h
=>kubectl exec -it mycont -- bash



#ConfigMap
-----------------------------------------------
=>kubectl create configmap --help
=>kubectl get configmap
=>kubectl create configmap my-dev-config --from-literal=app.mem=1024m

=>kubectl get configmap my-config -o yaml


File Base configMap:
app.user=imran
app.pass=root
app.dburl=http://some-url.com
=>kubectl create configmap my-file-base-config --from-file=dev.properties
=>kubectl get configmap -o yaml

ConfigMap Mounted to Pod:
apiVersion: v1
kind: Pod
metadata:
  name: pod2
spec:
  containers:
    - name: conmap-cont
      image: nginx
      volumeMounts:
      - name: configmap-vol
        mountPath: "/data"
  volumes:
    - name: configmap-vol
      configMap: 
        name: my-configmap
  restartPolicy: Never

=>kubectl exec -it myconfigmap-pod2 -- bash
=>cd data
=>cat dev.properties




#Security Contex
-----------------------------------------------
Three Important Permission Aspects:
  runAsUser, runAsGroup, fsGroup

  =>kubectl run my-bubx --image=busybox -it sh

apiVersion: v1
kind: Pod
metadata:
  name: my-bubx2
spec:
  containers:
  - image: busybox
    name: my-bubx2
    command: ["sh", "-c", "sleep 1h"]
  securityContext: 
    runAsUser: 1000
    runAsGroup: 3000

=>kubectl apply -f pod3.yaml
=>kubectl exec my-bubx2 -it -- sh
=>cd /tmp && touch myfile.txt
=>ls -l

With fsGroup:
apiVersion: v1
kind: Pod
metadata:
  name: my-bubx
spec:
  containers:
  - image: busybox
    name: my-bubx-cont
    command: ["sh", "-c", "sleep 1h"]
    volumeMounts:
    - name: my-sec-ctx-vol
      mountPath: /mydata
  volumes:
    - name: my-sec-ctx-vol
      emptyDir: {}
  securityContext: 
    runAsUser: 1000
    runAsGroup: 3000
    fsGroup: 4000




#Practice Test - Domain 6
-----------------------------------------------
Question 1 - ConfigMap
  Create a configmap named kplabs-config which contains all the contents of that file.
  course: kubernetes 2020
  instructor: zeal
  type: certification

Mount the configmap to a pod named configmap-pod based on nginx image in such a way that all contents are available at /etc/config/kplabs.config

Ans:
Create a properties file with this:
=>vi app.properties
=>kubectl create configmap my-file-base-config --from-file=app.config
=>kubectl get configmap

apiVersion: v1
kind: Pod
metadata:
  name: configmappod
spec:
  containers:
    - name: conmap-cont
      image: nginx
      volumeMounts:
      - name: myfconfigmap-vol
        mountPath: /etc/config
  volumes:
    - name: myfconfigmap-vol
      configMap: 
        name: app.config 
  restartPolicy: Never



Question 2 - PV and PVC
Create a persistent volume with the name kplabs-pv. The size should be 2Gi and hostpath should be /tmp/mydata. It should have access mode of ReadWriteOnce
Create a persistent volume claim that will make use of the PV created earlier.
Create a Pod named kplabs-pv-pod. The POD should have the volume mounted at /mydata directory.

Create pv: 
apiVersion: v1
kind: PersistentVolume
metadata:
  name: my-pv
spec:
  capacity:
    storage: 2Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: "/tmp/mydata"

Create a Persistent Volume Claim
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 2Gi

apiVersion: v1
kind: Pod
metadata:
  name: mypvcpod
spec:
    containers:
    - name: pod
      image: nginx
      volumeMounts:
      - mountPath: /mydata
        name: myvolume
    volumes:
    - name: myvolume
      persistentVolumeClaim:
        claimName: my-pvc

=>ssh node01
=>ls /tmp/mydata
=>cat /tmp/mydata/myFile.txt


Question 3 - Security Context
Create a POD named busybox-security. The pod should run a command sleep 3600.  The primary process in POD should run with UID of 1000 and GID of 2000 all newly created contents of volume should have the group ID of 3000.

apiVersion: v1
kind: Pod
metadata:
  name: sctpod
spec:
  securityContext:
    runAsUser: 1000
    runAsGroup: 2000
    fsGroup: 3000
  containers:
  - name: busybox-container
    image: busybox
    command: ["sleep","3600"]

=>kubectl exec -it sctpod -- sh
=>id



Question 4 - Secrets and Environment Variables
a. Create a secret name db-creds which has following data:
user: dbreadonly
pass: myDBPassword#%
b. Create a pod from nginx image. The pod should be named secret-pod
c. Mount the secret to the POD in such a way that the contents of the database user are available in the form of DB_USER environment variable and database password is available in the form of DB_PASSWORD environment variable inside the container.

Create a secret
=>kubectl create secret generic mydbsec --from-literal=user=dbreadonly --from-literal=pass=myDBPassword

apiVersion: v1
kind: Pod
metadata:
  name: mysecpod
spec:
  containers:
  - image: nginx
    name: mysec-cont
    env:
      - name: DB_USER
        valueFrom:
          secretKeyRef:
            name: mydbsec
            key: user
      - name: DB_PASSWORD
        valueFrom:
          secretKeyRef:
            name: mydbsec
            key: pass

=>kubectl exec -it mysecpod -- bash
=>env

Question 5 - Secrets and Volumes
a. Create a secret name app-creds that has the following data:
appuser: dbreadonly
apppass: myDBPassword
b. Create a pod based on nginx image with the name of app-pod
c. Mount the secret to the pod so that it is available in the path of /etc/secret


Create a secret
=>kubectl create secret generic mydbsec --from-literal=user=dbreadonly --from-literal=pass=myDBPassword

apiVersion: v1
kind: Pod
metadata:
  name: podsec2
spec:
  containers:
  - image: nginx
    name: app-pod
    volumeMounts:
    - name: myvolume
      mountPath: "/etc/secret"
      readOnly: true
  volumes:
  - name: myvolume
    secret:
      secretName: mydbsec







================================================= 
#Section 8: Domain 7 - Multi Continer
================================================= 

apiVersion: v1
kind: Pod
metadata:
  name: app-pod
spec:
  containers:
  - image: nginx
    name: cont1
  - image: busybox
    name: cont2
    command: 
     - sleep
     - "1h"

=>kubectl get pods
=>kubectl describe pods
=>kubectl exec -it app-pod -c cont2 -- sh

=>wget 10.244.1.2




Multi-Container POD Patterns
-----------------------------------------------

SideCare Pattern:


Ambassador Pattern( a type of sidecare pattern):



Adapter Pattern
---------------------------------------------------






#Practice Test - D7
-------------------------------------------------
apiVersion: v1
kind: Pod
metadata:
  name: pod1
spec:
  containers:
  - name: first-container
    image: nginx
  - name: second-container
    image: mykplabs/kubernetes:nginx
  - name: third-container
    image: busybox
    command: ["sleep", "3600"]



2) Ambassador Pattern:
Create a pod named kplabs-ambassador-pod from the legacy application image. 
The image is mykplabs/kubernetes:nginx
Create configmap called as kplabs-ambassador-config which has the following data:

    global
        daemon
        maxconn 256
 
    defaults
        mode http
        timeout connect 5000ms
        timeout client 50000ms
        timeout server 50000ms
 
    listen http-in
        bind *:80
        server server1 127.0.0.1:9080 maxconn 32


Create an ambassador container named haproxy-container from the image of haproxy:1.7
Expose the port 80 from Haproxy container.

Mount the configmap to the haproxy in such a way that HAProxy config is 
available at the following file:
/usr/local/etc/haproxy/haproxy.cfg



Create a Busybox pod from following pod definition:
apiVersion: v1
kind: Pod
metadata:
  name: kplabs-busybox-curl
spec:
  containers:
  - name: curl-container
    image: yauritux/busybox-curl
    command: ['sh', '-c', 'while true; do sleep 3600; done']

Verify if you can perform CURL from busybox pod towards the ambassador pod on port 80.

Solution:
----------------------------------------------

Create a file with config
=>kubectl create configmap kplabs-ambassador-config --from-file ./haproxy.cfg    


apiVersion: v1
kind: Pod
metadata:
  name: ambspod
spec:
  containers:
  - name: first-container
    image: mykplabs/kubernetes:nginx
  - name: haproxy-container
    image: haproxy:1.7
    ports:
       - containerPort: 80
    volumeMounts:
     - name: config-volume
       mountPath: /usr/local/etc/haproxy/haproxy.cfg
  volumes:
    - name: config-volume
      configMap:
        name: kplabs-ambassador-config

=>kubectl get pods -o wide       
=>kubectl run mybusybox -it --image=busybox -- sh
=>wget ip of ambspod


Question  : Adapter Pattern
Create a pod named kplabs-adapter-logging.
The Pod should have a container running from the busybox image with the following arguments:

    - /bin/sh
    - -c
    - >
      i=0;
      while true;
      do
        echo "$i: $(date)" >> /var/log/1.log;
        echo "$(date) INFO $i" >> /var/log/2.log;
        i=$((i+1));
        sleep 1;
      done


Create and mount a volume under the mount path of /var/log of the first container. The volume should be removed as soon as the pod is deleted.

Create a second container in the pod. It should be launched from the following image.

k8s.gcr.io/fluentd-gcp:1.30



The container should have an environment variable named FLUENTD_ARGS with following values:

-c /etc/fluentd-config/fluentd.conf



The second container should also have the same volume as the first container mounted on the path of /var/log



The second container should also have a fluentd configuration (mentioned in below configmap) available in the following path:
/etc/fluentd-config/fluentd.conf


Create a ConfigMap object with the name of fluentd-config. The ConfigMap should have the following configuration:
    <source>
      type tail
      format none
      path /var/log/1.log
      pos_file /var/log/1.log.pos
      tag PHP
    </source>
 
    <source>
      type tail
      format none
      path /var/log/2.log
      pos_file /var/log/2.log.pos
      tag JAVA
    </source>
 
    <match **>
       @type file
       path /var/log/fluent/access
    </match>


Verify if you can see log files with the tag of PHP and JAVA under the following directory
/var/log/fluent/access


Adapter Pattern Solution:
------------------------------------------------

apiVersion: v1
kind: ConfigMap
metadata:
  name: fluentd-config
data:
  fluentd.conf: |
    <source>
      type tail
      format none
      path /var/log/1.log
      pos_file /var/log/1.log.pos
      tag PHP
    </source>
    <source>
      type tail
      format none
      path /var/log/2.log
      pos_file /var/log/2.log.pos
      tag JAVA
    </source>
    <match **>
       @type file
       path /var/log/fluent/access
    </match>
---
apiVersion: v1
kind: Pod
metadata:
  name: adppod
spec:
  containers:
  - name: adppod-count
    image: busybox
    args:
    - /bin/sh
    - -c
    - >
      i=0;
      while true;
      do
        echo "$i: $(date)" >> /var/log/1.log;
        echo "$(date) INFO $i" >> /var/log/2.log;
        i=$((i+1));
        sleep 1;
      done
    volumeMounts:
    - name: varlog
      mountPath: /var/log
  - name: count-agent
    image: k8s.gcr.io/fluentd-gcp:1.30
    env:
    - name: FLUENTD_ARGS
      value: -c /etc/fluentd-config/fluentd.conf
    volumeMounts:
    - name: varlog
      mountPath: /var/log
    - name: config-volume
      mountPath: /etc/fluentd-config
  volumes:
  - name: varlog
    emptyDir: {}
  - name: config-volume
    configMap:
      name: fluentd-config


=>kubectl apply -f mypod.yaml
=>kubectl exec -it adppod -- sh

=>tail -f /var/log/1.log
=>tail -f /var/log/2.log
=>tail -f /var/log/fluent/access.20230408.b5f8ccf3468ae28ef





================================================= 
#New Update | Exam Preparation
================================================= 

#Deployment  Strategy:
-------------------------------------------------
=>kubectl explain deployment.spec.strategy
=>kubectl create deployment my-deployment --image=nginx --dry-run=client -o yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: my-deployment
  name: my-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: my-deployment
  strategy: 
    type: Recreate
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: my-deployment
    spec:
      containers:
      - image: nginx
        name: nginx
        resources: {}
status: {}


=>kubectl apply -f dpl1.yaml
=>kubectl get deployment
=>kubectl get pods
=>kubectl get rs

Now change deployment dpl1.yaml with a invalid image name 
Deploy again thne:

=>kubectl apply -f dpl1.yaml
Or
=>kubectl edit deployment my-deployment

=>kubectl get deployment
=>kubectl get pods
=>kubectl get rs

Now show all pod now sutdown, now container are running
this is of Recreate stategy.
Fixed the proper deployment config (image) then pod will be OutsideOfWork
Or
=>kubectl rollout undo deployment/my-deployment

Samelair process for RollingUpdate:

apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: my-deployment
  name: my-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: my-deployment
  strategy: 
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: my-deployment
    spec:
      containers:
      - image: nginx
        name: nginx
        resources: {}
status: {}



#Blue Green deployment:
------------------------------------------------------

=>kubectl run blue-pod --image=nginx
=>kubectl run green-pod --image=nginx

=>kubectl label pod blue-pod app=app-v1
=>kubectl label pod green-pod app=app-v2

=>kubectl get pods --show-labels

Create a service
=>kubectl create service nodeport my-service --tcp=80:80 --dry-run=client -o yaml
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec: 
  type: NodePort
  ports:
  - name: http
    port: 80
    targetPort: 80
  selector:
    app: app-v1

=>kubectl get svc
=>kubectl describe service my-service

Update service "selector" value app-v2
and deploy again, then check the end point

Or 
Can Edit directly 
=>kubectl edit svc



#Canary deployment:
------------------------------------------------------
1. Create Deployment Manifests:

kubectl create deployment deployment1 --image=nginx --replicas 3 --dry-run=client -o yaml
kubectl create deployment deployment2 --image=httpd --replicas 1 --dry-run=client -o yaml
2. Store these manifests in v1-canary.yaml and v2-canary.yaml

3. Add a common label of (deptype:canary ) to both of these manifest files for the pod template section
v1-canary.yaml:
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deployment1
spec:
  replicas: 3
  selector:
    matchLabels:
      app: app-nginx
  template:
    metadata:
      labels:
        app: app-nginx
        deptype: mycanary 
    spec:
      containers:
      - image: nginx
        name: nginx-cont
v1-canary.yaml:
Same as two only change replica and image
apiVersion: apps/v1
kind: Deployment
metadata:
  name: deployment2
spec:
  replicas: 2
  selector:
    matchLabels:
      app: app-httpd
  template:
    metadata:
      labels:
        app: app-httpd
        deptype: mycanary 
    spec:
      containers:
      - image: httpd
        name: nginx-cont

kubectl apply -f deployment1.yaml
kubectl apply -f deployment2.yaml
5. Verify if PODS are created with appropriate labels:

kubectl get pods --show-labels

6. Create a Canary Service

canary-svc.yaml

apiVersion: v1
kind: Service
metadata:
 name: canary-svc 
spec:
 type: NodePort
 ports:
 - name: http
   port: 80
   targetPort: 80
 selector:
   deptype: mycanary
kubectl apply -f canary-svc.yaml

7. Verify if Service have 4 Endpoint IPs:

kubectl describe svc canary-deployment
8. Make CURL request to see if requests are distributed among v1 and v2 apps
curl PUBLIC-IP:NODEPORT


#Custom Resource
------------------------------------------
Kubernetes Documentation Referred for Base CRD manifests:

https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/

=>kubectl proxy --port=8080
=>curl 127.0.0.1:8080

Manifests File (Step 1 and Step 2)

crd.yaml

apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: crontabs.kplabs.internal
spec:
  group: kplabs.internal
  versions:
    - name: v1
      served: true
      storage: true
      schema:
        openAPIV3Schema:
          type: object
          properties:
            spec:
              type: object
              properties:
                cronSpec:
                  type: string
                image:
                  type: string
                replicas:
                  type: number
  scope: Namespaced
  names:
    plural: crontabs
    singular: crontab
    kind: CronTab
    shortNames:
    - ct

=>k apply -f crd.yaml 


crd-object.yaml

apiVersion: "kplabs.internal/v1"
kind: CronTab
metadata:
  name: my-new-cron-object
spec:
  cronSpec: "* * * * */5"
  image: my-awesome-cron-image
  replicas: 3

=>kubectl apply -f crdo.yaml 
=>kubectl get crontab



AWS Service Operator Manifest File:
https://raw.githubusercontent.com/awslabs/aws-service-operator/master/configs/aws-service-operator.yaml










#Authentication
----------------------------------------------
=>cat .kube/config
=>curl -k https://controlplane:6443

=>cd /etc/kubernetes/manifests/
=>ls
=>cat kube-apiserver.yaml 


=>curl -k https://controlplane:6443 --header "Authorization: 
Bearer ewrwfsdlkfjdlf="
Call with token

=>kubectl create serviceaccount myservicacc

apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: ngpod
  name: ngpod
spec:
  serviceAccountName: myservicacc
  containers:
  - image: nginx
    name: ngpod
    resources: {}
  dnsPolicy: ClusterFirst


=>kubectl get pod -o yaml
=>k exec -it ngpod -- bash
=>cat /run/secrets/kubernetes.io/serviceaccount/token


=>curl -k https://controlplane:6443/api/v1 --header "Authorization: 
Bearer sfsfsf"






#Authorization
----------------------------------------------

Create a Role:
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: default
  name: pod-reader
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["list"]

=>kubectl apply -f myrole.yaml 

=>kubectl get role
=>kubectl describe role pod-reader


Role banding:

apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: read-pods
  namespace: default
subjects:
- kind: User
  name: system:serviceaccount:default:myadmin
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role 
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io


=>kubectl apply -f role-banding.yaml
=>kubectl get rolebinding
=>kubectl describe rolebinding read-pods



ClusterRole:
---------------------------------------------

Create a Cluster role:
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: pod-reader
rules:
- apiGroups: [""]
  resources: ["secrets"]
  verbs: ["get", "watch", "list"]


=>kubectl get clusterrole
=>kubectl describe clusterrole pod-role

Cluster Role banding:

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: pod-list-global
subjects:
- kind: User
  name: system:serviceaccount:default:myadmin
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io

  =>kubectl apply -f cls-role-banign.yaml
  =>kubectl get clusterrolbinding 
  =>kubectl describne clusterrolbinding pod-list-global



Cluster role to Role Banding:
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: read-pods
  namespace: default
subjects:
- kind: User
  name: system:serviceaccount:default:myadmin
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole 
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io





#Exam Preparation
----------------------------------------------------
=>kubectl api-resources
=>alias k=kubectl
Create alias for code
=>k get pods

=>k run mynginx --image=nginx --port=80 --dry-run=client -o yaml
Generate a POD  yaml file

=>k create deployment -h
How to crearte a deployment get Help
=>kubectl create deployment my-dep --image=nginx --replicas=3 --dry-run=client -o yaml



Domain1-Core concept:
1) Vary well to create pod
=>kubectl run mynginx --image=nginx --port=80 --restart=Never --dry-run -o yaml>newpod.yaml
On this command, It will creatre a pod if not mentation (--restart=Never) it will create a deployemnt
2) Have to very femilary with tis cli command
3) Can not modify all config/expect in live pod


=>kubectl eidt pod mybusybox
  add or update label on the pod is ok.
But all Properties are not able to update on live pod. It will a Forbidden error

For update any properties flow this step:
=>kubectl get pod mybusybox -o yaml>myupdatedbusybox.yaml
Save current pod config and delete it then run update pod.
=>kubectl apply -f myupdatedbusybox.yaml



Domain2-Pod Design:
1) Very familier with labels and selectors.
2) How can apply to and object both vi manifest file and live modification.
3) Network policy with label

4) Very familary deployment.
5) Deploymetn is the key of CKAD exam.
6) Have to undestandi ths deployment from screate.
7) Create and modify live deployment.
8) Understand the Important of labels and selectors in deployment.
9) Have a good idea about masSurge and maxUnavailable in deployment and how to modify it
10) Be good at with rollign update and rollbacks, its very Important.

11) Jobs and CronJob familary.
12) Dont worry about cron date/time, find the exmple fron docuemtation and use it.
13) Understanding of activeDeadlineSeconds with in CronJob



Domain3-Service and Neworkign:
1) NodePort service 
2) Network policy
3) Diffeetnt between servie Port and targetPort
4) How to change live service port
5) Be sure services and Pod must be in same namespaces.

6) Network policy with label and troubleshooting question.



Domain4-configuration:
1) ConfigMap
2) How to mount configMap to a specific path vi volume
3) SecurityContext (runAsUser, runAsGroup, fsGroup)
4) Good idea about Resource Quotas
5) Request and limits for Resource Quotas and set at Pod level
6) What happen if nodes not match the minum request value
7) May no direct question for Resource Quotas.
8) Undestranding of Kubernetes Secret
9) Create Mount it as env variable and volume fo Pod
10) Familiar with environment variables

11) How can associate service account with existing deployment and existing POD
12) Name space has its own default service account
13) Add secrect on live deployment and Pod



Domain5- Liveness and Readiness:
1) Exma may not asky for liveness and readyness 
2) Liveness/Readiness not be able to update on live pod, save pod a new yaml and apply it.
3) Diffeetnt between bot and some Troubleshooting question for this relevent.
4) Like: deploy a pod by dont ready yet, what is problm fixed.

5) Have to konw only kubectl logs command about it.
6) Question like: save pod log a file.
7) Event is inport Topics

8) kubectl top pods/ kubectl top nodes 
9) Metric server will pre-install in exam.
10) Have to familier to see CPU/Memory usege of a specific pod or nods.
11) Queston like: on 3 pod wihich pod use most cpu and save this on a file

12) Many Debugging related question will be with:
  Network Policies
  Services
  Liveness/Readiness Probes
  Deployment etc



Domain6-PV and PVC:
1) Be familier with PV and PVC
2) Why use hostPath and emptyDir, be clear for this defferent.
3) Question Like: Create a pod attach a voliume ton path of mount is a such way
volume should be deleted once pod is removed.



Domain7-MultiContainer:
1) Be prepared for the question based on Sidecar Pattern
2) Practice queston related to HAProxy and FluentD 10 time before exam
3) HAProxy and FluentD  have to vary family with tis two.
4) HAProxy and FluentD all configuration provide on exam dont worry about congig.




  




















================================================= 
#Debug
================================================= 
=>kubectl api-resources
Details about api and with short name

Pod Details:
-----------------------------------------------
=>kubectl describe pod -n default
=>systemctl status kubelet


Create a deployment
=>kubectl create deployment mydeployment --image=nginx --dry-run=client -o yaml
=>kubectl explain deployment.spec.strategy


Edit Pod Information
=>kubectl edit pods mybusybox
=>kubectl get pods --show-laels


























Install Kubernetes
-------------------------------------------------
1)Mamage Kubernetes Services
2)MiniKube
3)Install Kubernetes Manually

Three 03 Thing need in a typical Kubernetes server:
     1)kubectl
     2)Kubernetes Master
     3)Worker Node Agents


Extract pod definition to a file using the below command:
=>kubectl get pod <pod-name> -o yaml > pod-definition.yaml


=>kubectl create namespace test-123 --dry-run -o json/yaml
Formatting Output




=================================================
#  Pre-Requisites                                          
=================================================

Topics Covered on the Exam
-------------------------------------------------

Cluster Architecture, Installation, and Configuration (25%)
A big part of the exam will focus on the Kubernetes setup and configuration. The tutorial, “Kubernetes The Hard Way” is a very helpful tool as you prepare for this section. I’ll talk more about this tutorial later.

Workloads and Scheduling (15%)
You’ll be expected to create robust deployments..

Storage (10%)
A small section will test your knowledge about volumes and volume claims.

Troubleshooting (30%)
The biggest section of the exam will test you on troubleshooting a Kubernetes cluster. This is a task you can only improve at through practice.



Certified Kubernetes Administrator (CKA)
-------------------------------------------------
The CKA tests your ability to deploy and configure a Kubernetes cluster as well as your understanding of core concepts. Candidates have three hours to take the exam and must score 74% or higher to earn the certification.

The CKA exam tests the following areas:

8% – Application lifecycle management
12% – Installation, configuration & validation
19% – Core concepts
11% – Networking
5% – Scheduling
12% – Security
11% – Cluster maintenance
5% – Logging/monitoring
7% – Storage
10% – Troubleshooting


Certified Kubernetes Application Developer (CKAD)
-------------------------------------------------
The CKAD tests your ability to deploy and configure applications running on the Kubernetes cluster and your understanding of some core concepts. You’ll have two hours to complete the CKAD exam. Scoring a 66% or higher means you’ve passed.

For the CKAD exam, you will be tested in the following areas:

13% – Core concepts
18% – Configuration
10% – Multi-container pods
18% – Observability
20% – Pod design
13% – Services & networking
8% – State persistence









Linux
-------------------------------------------------------
https://www.youtube.com/watch?v=Q8Nh8r6_tkQ&list=PLd3UqWTnYXOnar-GXf1taqzw5Z8nAAxod&index=1
https://www.youtube.com/watch?v=mzMD5duBA-A&list=PLd3UqWTnYXOkCdIbrnfB7A51jOlsP4i3w&index=1
https://www.youtube.com/watch?v=UoJ94MirYmw&list=PLd3UqWTnYXOny6ntfCKt9S4mwDM4GDaAG&index=1



CKA
===============================================================================


Domains & Competencies
-------------------------------------------------
Storage10%
Understand storage classes, persistent volumes
Understand volume mode, access modes and reclaim policies for volumes
Understand persistent volume claims primitive
Know how to configure applications with persistent storage

Troubleshooting30%
Evaluate cluster and node logging
Understand how to monitor applications
Manage container stdout & stderr logs
Troubleshoot application failure
Troubleshoot cluster component failure
Troubleshoot networking

Workloads & Scheduling15%
Understand deployments and how to perform rolling update and rollbacks
Use ConfigMaps and Secrets to configure applications
Know how to scale applications
Understand the primitives used to create robust, self-healing, application deployments
Understand how resource limits can affect Pod scheduling
Awareness of manifest management and common templating tools

Cluster Architecture, Installation & Configuration25%
Manage role based access control (RBAC)
Use Kubeadm to install a basic cluster
Manage a highly-available Kubernetes cluster
Provision underlying infrastructure to deploy a Kubernetes cluster
Perform a version upgrade on a Kubernetes cluster using Kubeadm
Implement etcd backup and restore

Services & Networking20%
Understand host networking configuration on the cluster nodes
Understand connectivity between Pods
Understand ClusterIP, NodePort, LoadBalancer service types and endpoints
Know how to use Ingress controllers and Ingress resources
Know how to configure and use CoreDNS
Choose an appropriate container network interface plugin





ExamTips
------------------------------------------------------------------------------
After 2 years of procrastination, finally booked the CKA certification exam.

Happy to share that I passed Certified Kubernetes Administrator (CKA) Exam today with 91% score.

I can never thank enough Mr. Mumshad Mannambeth for the excellent course, keeping concepts simple yet informative. His solution walkthroughs on KodeKloud labs were so helpful. Also I thank Udemy for bringing the best lecturer to the platform.

Tips for CKA exam prep:
💡 1. Check your understanding on below competencies (source: https://t.ly/gtXI)
     -  Storage : Persistent Volumes, mountpoints, storage classes etc.
     - Troubleshooting: debugging services on nodes, debugging error logs, understanding node logs, using metrics server
     -  Workloads & Scheduling: Understanding deployments, manifest files, RBAC
     -  Cluster Architecture, Installation & Configuration: Cluster setup, Kubeadm, k8 version upgrades
     - Services & Networking: Ingress, Network policies, Network interface plugins, CoreDNS, connectivity between pods and services, various ports.   
💡2. k8 documentation:
     - Practice all tasks in the documentation (https://lnkd.in/gCsHrdjp)
     - Make a habit of checking k8 documentation for manifest formats, it comes handy during exam.
     - Only official documentation is allowed on the exam remote desktop.
💡3. Mock exams:
     - Experience the exam environment which is remote desktop, in killerKoda (https://t.ly/1wYz)
     - KodeKloud mock exams that comes with Udemy course (https://t.ly/4Vf8) to get an idea how questions would be.
     - I highly reccommed to take exam simulator by Killer.sh only before 1 week to your exam. (you will get 2 free sessions on purchase of exam), the questions you face in this are lot tougher than the real exam.
💡4. Commands:
     - Practice imperative commands for pods, deployments and services. Instead of writing manifest files from scratch.
     - Try to make use of short commands using alias.
             
💡5. Main exam:
     - CKA exam time limit of 2 hours with 17 questions, each with different weights (4%, 5%, 7%, 13%), you need to get 66% to pass the exam.
     - All 7% looks simple in plain, but you have to pay attention to each detail. It may have two or three steps in it.
     - After implementing a task, take some time to test it.
     - Don’t stick to the hard question more than 10mins. Just flag it, you can visit back after completing easy ones.
     - Always you have one free retake, if you fail first time dont worry. Practice well to score well.

💡6. Patience, Preparation and Practice :
     - It was not an easy thing to crack this test, needs a lot of patience to give time to understand and prepare notes on each concept. I took nearly a month for practicing all scenarios


Lab:
CKA-LAB:
https://kodekloud.com/lessons/core-concepts-4/
Docker-lab:
https://kodekloud.com/lessons/hands-on-labs-2/
DockerSwarmLab
https://kodekloud.com/lessons/labs-5/
PythonLab
https://kodekloud.com/lessons/quizzes/
DevPOpsLan
https://kodekloud.com/lessons/labs-6/


