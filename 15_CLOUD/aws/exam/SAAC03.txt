##################################################
##               AWS | SAACO3                   ##
##################################################


Latency:বিলম্ব, দেরী,  বাধা, 
Availability:প্রাপ্যতা, লভ্যতা
Durability:স্থায়িত্ব, টেকসইতা
Resilient:স্থিতিস্থাপক
Resilience:সহনশীলতা,স্বাভাবিক অবস্থায় প্রত্যাবর্তন
Reliable:নির্ভরযোগ্য
Elasticity:স্থিতিস্থাপকতা, নমনীয়তা

Provision:বন্দোবস্ত,ব্যবস্থা
Consistent:সামঞ্জস্যপূর্ণ,সমান
Retained:ধরে রাখা
Leverage:উদ্দেশ্যসাধনের উপায়
Sole:একমাত্র
Exploits:শোষণ
Proprietary:মালিকানা
Regardless:মনোযোগ বা পাত্তা না দিয়ে
Potentially:সম্ভাব্য
Compliance:সম্মতি,অনুরোধ বা আদেশ পালন
Successor:উত্তরাধিকারী
Assume:গ্রহণ করা, দায়িত্বগ্রহণ করা
Tenancy:ভাড়াটিয়া রূপে অধিকার
ComplianceRequirements:মেনে চলার প্রয়োজনীয়তা
Contemplating:চিন্তা করা
Legacy:উত্তরাধিকার
Deteriorated:অবনতি
Discrepancies :অসঙ্গতি
Prone:প্রবণ
Contemplating:চিন্তা করা
Adequate:পর্যাপ্ত
Withstand disruptions:বাধা সহ্য করা
Ingest:গ্রহণ করা
Sustainability:স্থায়িত্ব
Spare: অতিরিক্ত
prohibits:নিষেধ করে
mitigate:নিরসন করা
comply:মেনে চলা
obsolete:অপ্রচলিত
sophisticated :ভেজালমিশ্রিত
circumstances:পরিস্থিতি


=================================================
#General | Basic          
================================================= 
Tags=>This helps to identify,Categorious more quickly. 
Stopping => Temporarily shutting down the system
Terminating => Returning control to Amazon

Private key: The user downloads the private key
Public key: AWS uses the public key to confirm the identity of the user. 

Inbound network-> from other to the instance
Outbound network-> from the instance to other


Classic Ports to know:
• 22 	= SSH (Secure Shell) - log into a Linux instance
• 21 	= FTP (File Transfer Protocol) – upload files into a file share
• 22 	= SFTP (Secure File Transfer Protocol) – upload files using SSH
• 80 	= HTTP – access unsecured websites
• 443 	= HTTPS – access secured websites
• 3389  = RDP (Remote Desktop Protocol) – log into a Windows instance



AWS History:
2002: Internally launched
2003: Amazon infrastructure is one of their core strength. Idea to market
2004: Launched publicly with SQS
2006: Re -launched publicly with SQS, S3 & EC2
2007:Launched in Europe
=================================================
##Contain | Element | Object | Keyworkd     
=================================================
IAM-Identity and Access Management
EC2-Elastic Compute Cloud
ECR-Elastic Container Registry 
ECS-Elastic Container Service
KMS-AWS Key Management Service
S3-Amazon Simple Storage Service
AWS Lambda
SES-Simple Email Service
RDS-Relational Database Service
Amazon DynamoDB
Amazon ElastiCache
SQS-Simple Queue Service
SNS-Simple Notification Service
Auto Scaling
Route53
CloudFront
CloudWatch
CloudTrail
CloudFormation
Step Functions
Amazon Kinesis
Amazon API Gateway
Elastic Load Balancing
Elastic Beanstalk
RPO: Recovery Point Objective
RTO: Recovery Time Objective
SSM: AWS Systems Manager Agent 
SCP: Service control policies
Route53, IAM, CloudFront, WAF are Global.
SAML:Security Assertion Markup Language 



1
##Cloud Computing    
-------------------------------------------------
Cloud computing is a computing service made available over the internet.
Cloud computing is a pay-as-you-go model for delivering IT resources.
   


Throttling is the process of limiting the number of requests an authorized program can submit to a given operation in a given amount 
of time.


2
##AWS BasicInfo
-------------------------------------------------
Amazon web service is an online platform that provides scalable and cost-effective cloud computing solutions.
AWS has Regions all around the world.

Every AWS service is publicly exposed (public URL)

AWS:
-Security: AWS provides a secure and durable platform that provides end-to-end security and storage.
-Flexibility: It allows users to select the operating systems, language, database, and other services as per their requirements.
-Easy to use: AWS lets you host your applications quickly and securely, regardless of whether 
 it’s an existing or new application.
-Scalable: The applications you use can be scaled up or down, depending on your requirements.
-Cost savings: You only pay for the compute power, storage, and other resources that you use, without any long-term commitments.
-Scheduling: This enables you to start and stop AWS services at predetermined times
-Reliability: AWS takes multiple backups at servers at multiple physical locations.


AWS Use cases:
• Enterprise IT, Backup & Storage, Big Data analytics
• Website hosting, Mobile & Social Apps
• Gaming




3
##Region        
-------------------------------------------------
AWS Region is a separate geographic area.
A region is a cluster of data centers, Most AWS services are region-scoped.

How to choose  and AWS Region?
• Compliance with data governance and legal requirements: data never leaves a region without your explicit permission.
• Proximity to customers: reduced latency
• Available services within a Region: new services and new features aren’t available in every Region
• Pricing: pricing varies region to region.
 
 
 
 
 
4
##Availabiulity Zones |  AZ | az      
-------------------------------------------------
Availability zones (AZs) are isolated data centers located within specific regions.
AZ is the logical building block that makes up an AWS Region.

Each region hase many availabuility zone.
Usually 3, Min 2 and Max 6;

They’re separate from each other, so that they’re isolated from disasters.
Each availability zone (AZ) is one or more discrete data centers with redundant power, networking, and connectivity.





5
##Edge Locations
-------------------------------------------------
Edge locations are AWS data centers designed to deliver services with the lowest latency possible.
Edge location is a physical location where AWS has its servers that are used to stores cdn caching.

CloudFront delivers your content through a worldwide network of data centers called edge locations.

An edge location is the nearest point to the consumer(user) who is consuming the AWS service. 
In these locations, the server is not present but a small setup is there. 
They are located in major cities around the world.


Uses:
	CloudFront: It makes use of edge locations to cache versions of the data it provides, 
	            allowing the content to be delivered to users more quickly.
	Route 53: It delivers DNS responses from edge locations, allowing DNS queries to be resolved more quickly.
	AWS Shield and Web Application Firewall: It screens traffic in edge locations to prevent undesired traffic.


AWS has Global Services:
• Identity and Access Management (IAM)
• Route 53 (DNS service)
• CloudFront (Content Delivery Network)
• WAF (Web Application Firewall)


Most AWS services are Region-scoped:
• Amazon EC2 (Infrastructure as a Service)
• Elastic Beanstalk (Platform as a Service)
• Lambda (Function as a Service)
• Rekognition (Software as a Service)
RegionalServices:https://aws.amazon.com/about-aws/global-infrastructure/regional-product-services/





==| IAM Section
6
##IAM
-------------------------------------------------
AWS Identity and Access Management (IAM) is a web service that helps you securely control access to AWS resources. 

IAM enables organizations with many employees to create and manage multiple users under a single AWS account.

Three types of Principals — Root Users, IAM Users and Instance Principals
IAM deals with four Principle Entities: Users, Groups, Roles and Policies.

A principal is a person or application that can make a request for an action or operation on an AWS resource.

IAM roles and IAM users are separate IAM entities and should not be mixed. 
Only IAM roles can be used to access cross-account resources.
 
IAM Policies Structure:
• Version: policy language version, always include “2012-10-17”
• Id: an identifier for the policy (optional) 
• Statement: one or more individual statements (required)
• Statements consists of:
	• Sid: an identifier for the statement (optional) 
	• Effect: whether the statement allows or denies access(Allow, Deny)
	• Principal: account/user/role to which this policy applied to 
	• Action: list of actions this policy allows or denies 
	• Resource: list of resources to which the actions applied to 
	• Condition: conditions for when this policy is in effect (optional)

Custom inline policy single uses for, only for one role. cant able to add multiple role.
When we create a custom policy:
	-Select a resource which for this policy 
	  like S3
	-We can create this policy for specific resource 
	  like we can set ARN of a spesific backet or all backer.

Multi Factor Authentication - MFA:
MFA = Password+Scurity device you own.
Main benefit of MFA: if a password is stolen or hacked, the account is not compromised.



How can access AWS, Three options:
	• AWS Management Console (protected by password + MFA)
	• AWS Command Line Interface (CLI): protected by access keys
	• AWS Software Developer Kit (SDK) - for code: protected by access keys
	
	
• Access Keys are secret, just like a password. Don’t share them
	• Access Key ID ~= username
	• Secret Access Key ~= password


IAM Roles for Services: Some AWS service will need to perform actions on your behalf
To do so, we will assign permissions to AWS services with IAM Roles
Common roles: 
	• EC2 Instance Roles 
	• Lambda Function Roles 
	• Roles for CloudFormation 
	
	

IAM Security Tools:
	IAM Credentials Report (account-level)
		• a report that lists all your account's users and the status of their various credentials.

	IAM Access Advisor (user-level)
	IAM access advisor helps you audit service access, remove unnecessary permissions, and set 
		zappropriate permissions providing the last timestamp when an 
		IAM entity (e.g., user, role, or a group) accessed an AWS service.
	• Access advisor shows the service permissions granted to a user and when those services were last accessed.
	• You can use this information to revise your policies.

When you use the AWS Management Console to create a user, you must choose to include at least a 
console password or access keys. 
By default, a brand new IAM user created using the AWS CLI or AWS API has no credentials of any kind. 
You must create the type of credentials for an IAM user based on the needs of your user.


IAM: Permissions:
• Users or Groups can be assigned JSON documents called policies.
• These policies define the permissions of the users.


You can assign an IAM Role to the users or groups from your Active Directory once it is integrated 
with your VPC via the AWS Directory Service AD Connector.


Area:
=====
IAM are a Global service.
Users, Groups, Roles, Accounts – Global
	Same AWS accounts, users, groups and roles can be used in all regions
Key Pairs – Global or Regional
	Amazon EC2 created key pairs are specific to the region
	RSA key pair can be created and uploaded that can be used in all regions



UseCase:
========
-Shared access to your AWS account
-Granular permissions
-Secure access to AWS resources for applications that run on Amazon EC2
-Multi-factor authentication (MFA)
-Identity federation
-Identity information for assurance
-PCI DSS Compliance
-Integrated with many AWS services


BillCost:
=========
No additional charge for IAM and STS. 


Security:
=========
MFA + Password Policy.




7
##User
-------------------------------------------------
An IAM user is an entity that you create in AWS. 
An IAM user with administrator permissions is not the same thing as the AWS account root user.

A user in AWS consists of a name, a password to sign into the AWS Management Console, 
and up to two access keys that can be used with the API or CLI. 





8
##Group
-------------------------------------------------
An IAM user group is a collection of IAM users. 
You can use user groups to specify permissions for a collection of users, which can make those permissions 
easier to manage for those users. 


UseCase:
========
A user group is a way to attach policies to multiple users at one time. 
When you attach an identity-based policy to a user group, all of the users in the user group receive the 
permissions from the user group.




9
##Policy
-------------------------------------------------
A policy is an JSON document or object in AWS that, when associated with an identity or resource, defines their permissions.

For Centralized management of permissions Policies can be attached to users, groups, or Amazon S3 buckets.
 

AWS supports six types of policies: 
	Identity-based policies, 
	resource-based policies, 
	permissions boundaries, 
	Organizations SCPs, ACLs, and 
	Session policies.

Identity-based policies: are attached to an IAM user, group, or role to specify what that identity 
         can do (its permissions).
		 Identity-based policies grant permissions to an identity.
		 
Resource-based policies: Attach inline policies to resources, you can specify who has access to the resource 
         and what actions they can perform on it.  

Permissions boundaries: Use a managed policy as the permissions boundary for an IAM entity (user or role). 
						
						
Organizations SCPs: Organizations service control policy (SCP) to define the maximum permissions 
		for account members of an organization or organizational unit (OU). 
	    SCPs limit permissions that identity-based policies or resource-based policies grant to entities 
	    (users or roles) within the account, but do not grant permissions.
The effective permissions are the logical intersection between what is allowed by the 
SCP and what is allowed by the IAM and resource-based policies.


S3-ACL:
Access control lists (ACLs) – Use ACLs to control which principals in other accounts can access the 
    resource to which the ACL is attached.
	ACLs are cross-account permissions policies that grant permissions to the specified principal. 
	ACLs cannot grant permissions to entities within the same account.
	
With ACLs, you can only grant other AWS accounts (not specific users) access to your 
Amazon S3 resources(buckets or objects).


VPC-ACL:
A network access control list (ACL) allows or denies specific inbound or outbound traffic at the subnet level. 
You can use the default network ACL for your VPC, or you can create a custom network ACL for your VPC with rules that are 
similar to the rules for your security groups in order to add an additional layer of security to your VPC.




Session policies: 
    Pass advanced session policies when you use the AWS CLI or AWS API to assume a role or a federated user. 
	Session policies limit the permissions that the role or user's identity-based policies grant to the session. 
	Session policies limit permissions for a created session, but do not grant permissions.



Area:
=====
Global Service.


UseCase:
========
You manage access in AWS by creating policies and attaching them to 
AWS (IAM) principals (roles, users, or groups) or AWS resources. 

AWS evaluates these policies when an IAM principal makes a request, such as uploading an object.




10
##IAM Role
-------------------------------------------------
Some AWS service will need to perform actions on your behalf. 
To do so, we will assign permissions to AWS services with IAM Roles.


Common roles: 
• EC2 Instance Roles 
• Lambda Function Roles 
• Roles for CloudFormation 


Area:
=====
Global Service.



UseCase:
========
1)Giving permissions to non-humans, such as services/applications.
Ex. EC2 of  needs access to S3.

2)Giving permissions to federated/outside users & groups.
An IAM user can assume a role to temporarily take on different permissions for a specific task.




11
#IAM Section – Summary:
-------------------------------------------------
• Users: mapped to a physical user, has a password for AWS Console
• Groups: contains users only
• Policies: JSON document that outlines permissions for users or groups
• Roles: for EC2 instances or AWS services
• Security: MFA + Password Policy
• Access Keys: access AWS using the CLI or SDK
• Audit: IAM Credential Reports & IAM Access Advisor





12
##AWS CLI | SDK | AccessKeys | CloudShell
-------------------------------------------------
CLI: A tool that enables you to interact with AWS services using commands in your command-line shell.
Direct access to the public APIs of AWS services.

AWS SDK: Language-specific APIs (set of libraries), Enables you to access and manage AWS services programmatically.
Embedded within your application.


AWS CloudShell:
	AWS CloudShell is a browser-based shell that makes it easy to securely manage, explore, and interact with your AWS resources. 
	CloudShell is pre-authenticated with your console credentials.
	Common development and operations tools are pre-installed, so no local installation or configuration is required.


AccessKey:
	Access keys consist of an access key ID and secret access key, which are used to sign programmatic 
	requests that you make to AWS. 


Programmatic access:
	You must provide your AWS access keys to make programmatic calls to AWS or to use the 
	AWS Command Line Interface or AWS Tools for PowerShell.








==| EC2 Basics
13
##EC2
-------------------------------------------------
EC2 = Elastic Compute Cloud = Infrastructure as a Service.
Its provides scalable computing capacity in the  Cloud. 

When Launch an instance using the Amazon EC2 API or a command line tool:
	Don’t specify a security group, the instance is automatically 
	assigned to the default security group for the VPC.

Using the Amazon EC2 console, you have an option to create a new security group.


EC2 sizing & configuration options: 
• Operating System (OS): Linux, Windows or Mac OS 
• How much compute power & cores (CPU) 
• How much random-access memory (RAM) 
• How much storage space: 
	• Network-attached (EBS & EFS) 
	• hardware (EC2 Instance Store) 
• Network card: speed of the card, Public IP address 
• Firewall rules: security group 
• Bootstrap script (configure at first launch): EC2 User Data

AWS has a limit of 20 instances per region. You can request a limit increase.


m5.2xlarge-
m: instance class
5: generation (AWS improves them over time)
2xlarge: size within the instance class


TypeOfEC2:
	-General purpose
	-Compute optimized
	-Memory-optimized
	-Storage optimized
	-Accelerated Computing

Storage optimized instances are designed for workloads that require high, sequential read and write access to very large data sets 
on local storage.

Memory Optimized Instances is  are designed to deliver fast performance for workloads that process large data sets in memory, 
which is quite different from handling high read and write capacity on local storage.
These instances are well suited for the following: High-performance, relational (MySQL) and NoSQL (MongoDB, Cassandra) databases.


Compute Optimized Instances are ideal for compute-bound applications that benefit from high-performance processors, 
such as batch processing workloads and media transcoding.

General Purpose Instances are the most basic type of instances. 
They provide a balance of compute, memory, and networking resources, and can be used for a variety of workloads.


You can put an instance that is in the InService state into the Standby state, update some software or 
troubleshoot the instance, and then return the instance to service.

Instances that are on standby are still part of the Auto Scaling group, 
but they do not actively handle application traffic.
 
Security Groups:
	• Can be attached to multiple instances
	• Locked down to a region / VPC combination
	• Does live “outside” the EC2 – if traffic is blocked the EC2 instance won’t see it
	• It’s good to maintain one separate security group for SSH access
	• If your application is not accessible (time out), then it’s a security group issue
	• If your application gives a “connection refused“ error, then it’s an application
	  error or it’s not launched
	• All inbound traffic is blocked by default.
	• All outbound traffic is authorised by default.






On-Demand Capacity Reservations enable you to reserve compute capacity for your 
Amazon EC2 instances in a specific Availability Zone for any duration.
 
This gives you the ability to create and manage Capacity Reservations independently 
from the billing discounts offered by Savings Plans or Regional Reserved Instances.


There is a vCPU-based On-Demand Instance limit per region which is why subsequent requests failed. 
Just submit the limit increase form to AWS and retry the failed requests once approved.


By default, the vCPU-based On-Demand Instance limit is set per region and not per Availability Zone. 
This can be increased after submitting a request form to AWS.
 
By default, AWS allows you to provision a maximum of 20 instances per region. 
Select a different region and retry the failed request is incorrect. 
There is no need to select a different region since this limit can be increased after 
submitting a request form to AWS.



A company needs to deploy at least 2 EC2 instances to support the normal workloads of its application and 
automatically scale up to 6 EC2 instances to handle the peak load. 
The architecture must be highly available and fault-tolerant as it is processing mission-critical workloads.

ANS:
Create an Auto Scaling group of EC2 instances and set the 
minimum capacity to 4 and the maximum capacity to 6. 
Deploy 2 instances in Availability Zone A and another 2 instances in Availability Zone B.



Area:
=====
Resource Identifiers – Regional
	Each resource identifier, such as an AMI ID, instance ID, EBS volume ID, or EBS snapshot ID, 
	is tied to its region and can be used only in the region where you created the resource.
Instances – Availability Zone
	An instance is tied to the Availability Zones in which you launched it. 
	However, note that its instance ID is tied to the region.
EBS Volumes – Availability Zone
	Amazon EBS volume is tied to its Availability Zone and can be attached only to instances in the same Availability Zone.
EBS Snapshot – Regional
	An EBS snapshot is tied to its region and can only be used to create volumes in the same region and has to 
    be copied from One region to other if needed.





UseCase:
========
Renting virtual machines (EC2)
Auto-scaling, Pay-as-you-go, Increased Reliability, Elasticity.
Run cloud-native and enterprise applications.
Scale for HPC applications, Access the on-demand infrastructure and capacity you need to run HPC applications.
Develop for Apple platforms.
Train and deploy ML applications



BillCost:
=========
EC2 in FreeTier, 750 hours per month for 12 months with the AWS Free Tier.
Stopped EC2 instances do not incur any charges. 


EC2 Instances Purchasing Options:
• On-Demand Instances – short workload, predictable pricing, pay by second
• Reserved (1 & 3 years)
	• Reserved Instances – long workloads
	• Convertible Reserved Instances – long workloads with flexible instances
• Savings Plans (1 & 3 years) –commitment to an amount of usage, long workload
• Spot Instances – short workloads, cheap, can lose instances (less reliable)
• Dedicated Instances – no other customers will share your hardware
	Instances run on hardware that’s to you
	May share hardware with otherinstances in same account
	No control over instance placement(can move hardware after Stop / Start)\
• Dedicated Hosts – book an entire physical server, control instance placement
• Capacity Reservations – reserve capacity in a specific AZ for any duration
  No need for 1 or 3-year commitment, The Availability Zone in which to reserve the capacity (only one).



Reserved Instances:
-------------------------------------------------
Reserved Instances do not renew automatically.
Keep in mind that you can always refer to On-Demand Instances to suffice any additional resource requirements.

Reserved Instances are not physical instances, but rather a billing discount applied to the 
use of On-Demand Instances in your account.

You can also sell your unused instance for Standard RIs but 
not Convertible RIs on the Reserved Instance Marketplace.

You can reserve capacity to a specific AWS Region (regional Reserved Instance) or specific Availability Zone (zonal Reserved Instance) 
only. You cannot reserve capacity to multiple AWS Regions in a single RI purchase.


Capacity Reservations:
On-Demand Capacity Reservations enable you to reserve compute capacity for your 
Amazon EC2 instances in a specific Availability Zone for any duration. 
This gives you the ability to create and manage Capacity Reservations independently 
from the billing discounts offered by Savings Plans or Regional Reserved Instances.

When you create a Capacity Reservation, you specify:
– The Availability Zone in which to reserve the capacity
– The number of instances for which to reserve capacity
– The instance attributes, including the instance type, tenancy, and platform/OS

In addition, you can use Savings Plans and Regional Reserved Instances with your Capacity Reservations to benefit from billing discounts.




Security:
=========
Security is a shared responsibility between AWS and you.
Managing the guest operating system and software deployed to the guest operating system, 
including updates and security patches.





##Amazon EC2 Instance Purchasing Options
-------------------------------------------------
https://enlear.academy/aws-ec2-purchasing-options-and-spot-instances-overview-44dd5fb0d8f7

1) On-Demand Instances: short workload, predictable pricing.
2) Reserved Instances: long workloads (≥ 1 year).
	Convertible Reserved Instances: long workloads with flexible instances.
	Scheduled Reserved Instances: launch within time window you reserve.
3) Spot Instances: short workloads, for cheap, can lose instances.
4) Dedicated Instances: no other customers will share your hardware.
5) Dedicated Hosts: book an entire physical server, control instance placement.




Spot Instances:
===============
Spot blocks can only be used for a span of up to 6 hours.
ASpot Price is the hourly rate for a Spot instance. 

 You can use Spot Instances for various stateless, fault-tolerant, or flexible applications.
 Spot instances can be taken back by AWS with two minutes of notice, so spot instances cannot be 
 reliably used for running the dev application (which can be up and running for up to 8 hours).
 
 
Spot instances can be used for applications flexible in the timing when they can run and also able to 
handle interruption by storing the state externally for e.g. they are 
well-suited for data analysis, batch jobs, background processing, and optional tasks

Spot instances differ from the On-Demand instances:
	they are not launched immediately
	they can be terminated anytime
	price varies as per the demand and supply of spot instances
	
Spot instances can also be launched with a required duration (max 6 hour) (also known as Spot blocks), 
which are not interrupted due to changes in the Spot price.

A Spot Instance request is either one-time or persistent.


There are two types of spot requests:
-A one-time spot request stays active until  EC2 runs the spot instance, you cancel the request, or it expires. 
	If the spot price goes above your bid or capacity is unavailable, 
	Amazon terminates the spot instance and closes the spot request.

-A persistent spot instance request stays active until you cancel it or until it expires—even if, 
	Amazon carried out the request. 
	If the spot price goes above your bid or capacity becomes unavailable, the spot instance is interrupted. 
	Amazon keeps the request open, and if conditions change and a spot instance does become available, it starts or 
	resumes the spot instance


What are Spot Instance Interruptions?	
	EC2 can interrupt the Spot instance when the Spot price rises above the bid price, 
	when the demand for Spot instances rises, or when the supply of Spot instances decreases.
	not available on the Zone.

EC2 Instance Rebalance Recommendation: is a signal that notifies when a Spot Instance is at elevated risk 
of interruption. The signal provides an opportunity to proactively manage the Spot Instance
in advance of the two-minute Spot Instance interruption notice.

		
When an instance is interrupted, you can select one of 
	Three possible actions:
		Terminating the spot instance (default), 
		Stopping the spot instance (making it possible to restart it with the same launch specifications), or 
		Hibernating the instance.


EC2 tries to maintain the target capacity, adding spot instances if available, 
based on the request details. 
When spot instances are terminated, 
Spot Fleet attempts to replace them using the lowest price option available.

Spot blocks:
Spot Instances with a defined duration (also known as Spot blocks) are designed not to be interrupted and will run continuously 
for the duration you select. This makes them ideal for jobs that take a finite time to complete, 
such as batch processing, encoding and rendering, modeling and analysis, and continuous integration.


What is AWS Spot Fleet?
Spot Fleet is a collection of spot instances.


Spot Fleet requests type:
Request
	Spot Fleet places an asynchronous one-time request for the desired capacity.
	If capacity is diminished because of Spot interruptions, the fleet does not attempt
	to replenish Spot Instances, nor does it submit requests in alternative Spot capacity pools if 
	capacity is unavailable.
Maintain
	Spot Fleet places an asynchronous request for the desired capacity and maintains capacity 
	by automatically replenishing any interrupted Spot Instances.





 
13
##Security Group
-------------------------------------------------
Security groups are stateful,You can specify allow rules, but not deny rules.
An AWS security group acts as a virtual firewall for your EC2 instances to control incoming and outgoing traffic.
You can specify one or more security groups for each EC2 instance, with a maximum of five per network interface. 

All inbound traffic is blocked by default.
All outbound traffic is authorised by default.

Can be attached to multiple instances.




14
##EC2 Instance Role
-------------------------------------------------
When you launch an Amazon EC2 instance, you specify an IAM role to associate with the instance 
to securely access aws Resources.

Use an instance profile to pass an IAM role to EC2 instance.
An instance profile can contain only one IAM role, although a role can be included in multiple instance profiles. 
This limit of one role per instance profile cannot be increased.




15
##Elastic IP | PublicVsPrivate
-------------------------------------------------
PublicVsPrivate:
A private IPv4 address is an IP address that's not reachable over the Internet. 
You can use private IPv4 addresses for communication between instances in the same VPC.

A public IP address is an IPv4 address that's reachable from the Internet. 
You can use public addresses for communication between your instances and the Internet.

An Elastic IP address is a static (doesn't change) public IPv4 address associated with your AWS account. 
AWS accounts are limited to five (5) Elastic IP addresses per Region.



Area:
=====
Elastic IP Address – Regional



UseCase:
========
You can mask the failure of an instance or software by rapidly remapping the address to another 
instance in your account.
 
Alternatively, you can specify the Elastic IP address in a DNS record for your domain, 
so that your domain points to your instance.
 
 
 
BillCost:
=========
To ensure efficient use of Elastic IP addresses, aws impose a small hourly charge if an Elastic IP address is 
not associated with a running instance, or if it is associated with a stopped instance or 
an unattached network interface. 

While your instance is running, you are not charged for one Elastic IP address associated with the instance, 
but you are charged for any additional Elastic IP addresses associated with the instance.




16
##Placement Group
-------------------------------------------------
Placement Groups are logical groupings or clusters of instances in the selected AWS region. 
Specifically used for launching cluster compute instance types.

Placement type:
Cluster(Same Rack Same AZ ) – clusters instances into a low-latency group in a single Availability Zone.
	This strategy enables workloads to achieve the low-latency network performance.
	:If the rack fails, all instances fails at the same time.

Use case:
• Big Data job that needs to complete fast
• Application that needs extremely low latency and high network throughput.



Partition(Not Same Rack but Same AZ ) — spreads instances across many different partitions (which rely on
	different sets of racks) within a single AZ.
	Do not share the same rack.  
	Can span across multiple AZs in the same region.
	
A partition placement group can have a maximum of seven partitions per Availability Zone. 
Since a partition placement group can have partitions in multiple Availability Zones in the same region, 
therefore instances will not have low-latency network performance. 

Scales to 100s of EC2 instances per group (Hadoop, Cassandra, Kafka)
The instances in a partition do not share racks with the instances in the other partitions.
:A partition failure can affect many EC2 but won’t affect other partitions.




Spread(Not Same Rack Not Same AZ ) – strictly places a small group of instances across distinct 
underlying hardware to reduce correlated failures.
    For Each instance each different Rack or Hardware.
	It spread the instances across multiple AZ.
	It reduces the risk of failure of the network since other instances are active in other zones.
	:Limited to 7 instances per AZ per placement group

Use case:
• Application that needs to maximize high availability.
• Critical Applications where each instance must be isolated from failure from each other.







17
##ENI (Elastic Network Interfaces)
-------------------------------------------------
AWS Network Interface or Elastic Network Interfaces (ENIs) are virtual network cards attached to EC2 
instances that help facilitate network connectivity for instances. 
Having two or more of them connected to an instance permits it to communicate on two separate subnets.

Attaching a secondary network interface allows you to connect your EC2 instance to two networks at once.

Has the following attributes:
Description
Private IP Address
Elastic IP Address
MAC Address
Security Group(s)
Source/Destination Check Flag
Delete on Termination Flag



Area:
=====
Bound to a specific availability zone (AZ)
Each ENI lives within a particular subnet of the VPC.



UseCase:
========
A common use case for ENIs is the creation of management networks. 
This allows you to have public-facing applications like web servers in a public subnet but lock down 
SSH access down to a private subnet on a secondary network interface.



BillCost:
=========
ENIs are entirely free to use, it has standard AWS data charges.



Security:
=========
ENIs have security groups, just like EC2 instances, which act as a built in firewall.







18
##EC2 Hibernate
-------------------------------------------------
Hibernation gives you the ability to launch EC2 instances, set them up as desired, hibernate them, 
and then quickly  bring them back to life when you need them. 

Hibernation saves the contents from the instance memory (RAM) to your Amazon EBS root volume.
Root Volume – must be EBS, encrypted, not instance store, and large
An instance can NOT be hibernated more than 60 day


UseCase:
========
Long-running processing
Saving the RAM state
Services that take time to initialize



BillCost:
=========
While the instance is in hibernation, you pay only for the EBS storage and any 
Elastic IP addresses attached to the instance.



Security:
=========
It is a aws managed service, security is a shared responsibility.






19
##EC2 Instance Storage Section
-------------------------------------------------
You can only use block-level storage with an EC2 instance where the OS runs.
There are two block-level storage options for EC2 instances.

Instance store:
An instance store volume is a disk that is physically attached to the host computer that runs the 
EC2 virtual machine (VM).

EBS volume:
EBS volume, which provides off-instance storage that will persist independently from the life of the instance. 


Instance store is ideal for the temporary storage of information that changes frequently such as buffers, 
	caches, scratch data, and other temporary content, or for data that is replicated across a 
	fleet of instances, such as a load-balanced pool of web servers.

If an instance reboots (intentionally or unintentionally), data in the instance store persists.
When you stop, hibernate, or terminate an instance, every block of storage in the instance store is reset, data loss.




21
##EBS
-------------------------------------------------
AWS Elastic Block Store (EBS) is Amazon’s block-level storage solution used with the 
EC2 cloud service to store persistent data. 
This means that the data is kept on the AWS EBS servers even when the EC2 instances are shutdown. 

EBS volume is a storage area network (SAN) storage and not a POSIX-compliant shared file system. 
You have to use EFS instead.

• An EBS (Elastic Block Store) Volume is a network drive you can attach to your instances while they run
• It allows your instances to persist data, even after their termination
• They can only be mounted to one instance at a time (at the CCP level)
• They are bound to a specific availability zone


Amazon EBS volume types:
	Solid state drives (SSD)
	General Purpose SSD volumes
	Provisioned IOPS SSD volumes

Hard disk drives (HDD) 
Previous generation 




Area:
=====
EBS volume in a specific Availability Zone



UseCase:
========
Preserve root volume when instance is terminated
Run relational or NoSQL databases
Migrate mid-range, on-premises storage area network (SAN) workloads to the cloud.



BillCost:
=========
30 GB of storage for 12 months with the AWS Free Tier 
Volume storage for all EBS volume types is charged by the amount of GB you provision per month 
until you release the storage. 
Costs increase for EBS volumes that support additional input/output 
operations per second (IOPS) and throughput beyond baseline performance.



Security:
=========
Ensure EBS volumes are encrypted with KMS CMKs in order to have full control over data encryption and decryption.
Encrypted Volumes and snapshots  with CMK




22
++EBS Shapshort
-------------------------------------------------
• Make a backup (snapshot) of your EBS volume at a point in time
• Not necessary to detach volume to do snapshot, but recommended
• Can copy snapshots across AZ or Region


EBS Snapshots Features:
• EBS Snapshot Archive
	• Move a Snapshot to an ”archive tier” that is 75% cheaper
	• Takes within 24 to 72 hours for restoring the archive
• Recycle Bin for EBS Snapshots
	• Setup rules to retain deleted snapshots so you can recover them after an accidental deletion
	• Specify retention (from 1 day to 1 year)
• Fast Snapshot Restore (FSR)
	• Force full initialization of snapshots.





23
##AMI
-------------------------------------------------
• AMI = Amazon Machine Image
• AMI are a customization of an EC2 instance
	• You add your own software, configuration, operating system, monitoring…
	• Faster boot / configuration time because all your software is pre-packaged
• AMI are built for a specific region (and can be copied across regions)
• You can launch EC2 instances from:
	• A Public AMI: AWS provided
	• Your own AMI: you make and maintain them yourself
	• An AWS Marketplace AMI: an AMI someone else made (and potentially sells)


An AMI includes the following:
	One or more Amazon Elastic Block Store (Amazon EBS) snapshots, or, for instance-store-backed AMIs, 
	a template for the root volume of the instance 
		(for example, an operating system, an application server, and applications).
    Launch permissions that control which AWS accounts can use the AMI to launch instances.
	A block device mapping that specifies the volumes to attach to the instance when it's launched.



AMI to EC2 Instance:
	• Start an EC2 instance and customize it
	• Stop the instance (for data integrity)
	• Build an AMI – this will also create EBS snapshots
	• Launch instances from other AMIs


During the AMI-creation process, Amazon EC2 creates snapshots of your instance's root volume and any other 
EBS volumes attached to your instance. 

You're charged for the snapshots until you deregister the AMI and delete the snapshots. 
If any volumes attached to the instance are encrypted, the new AMI only launches successfully on 
instances that support Amazon EBS encryption.


 
AMIs – Regional
AMI provides templates to launch EC2 instances

When the new AMI is copied from region A into region B, it also creates a snapshot in region 
B because AMIs are based on the underlying snapshots. 




 
24
##EC2 Instance Store
-------------------------------------------------
• EBS volumes are network drives with good but “limited” performance
• If you need a high-performance hardware disk, use EC2 Instance Store
• Better I/O performance
• EC2 Instance Store lose their storage if they’re stopped (ephemeral)
• Good for buffer / cache / scratch data / temporary content
• Risk of data loss if hardware fails
• Backups and Replication are your responsibility





25
##EBS Volume Types | Use cases
-------------------------------------------------
Volume Types:
	General Purpose SSD volumes
	Provisioned IOPS SSD volumes
	Throughput Optimized HDD and Cold HDD volumes
	Previous generation Magnetic volumes

• Only gp2/gp3 and io1/io2 can be used as boot volumes

General Purpose SSD: Maximum 10,000 IOPS/Volume
Provisioned IOPS SSD: Maximum 20,000 IOPS/Volume
Throughput Optimized HDD: Maximum throughput 500 MiB/s (Optimized for throughput rather than IOPS, good for large, contiguous reads)


EBS Volumes come in 6 types:
• gp2 / gp3 (SSD): General purpose SSD volume that balances price and performance 
  for a wide variety of workloads.
• io1 / io2 (SSD): Highest-performance SSD volume for mission-critical low-latency or 
  high-throughput workloads
• st1 (HDD): Low cost HDD volume designed for frequently accessed, throughput- intensive workloads
• sc1 (HDD): Lowest cost HDD volume designed for less frequently accessed workloads

EBS Volumes are characterized in Size | Throughput | IOPS (I/O Ops Per Sec)

General Purpose SSD:
• Cost effective storage, low-latency
• System boot volumes, Virtual desktops, Development and test environments


Provisioned IOPS (PIOPS) SSD:
• Critical business applications with sustained IOPS performance
• Or applications that need more than 16,000 IOPS
• Great for databases workloads (sensitive to storage perf and consistency)
  Supports EBS Multi-attach 

Hard Disk Drives (HDD):
• Cannot be a boot volume 
• 125 GiB to 16 TiB



EBS Multi-Attach – io1/io2 family:
• Attach the same EBS volume to multiple EC2 instances in the same AZ
• Each instance has full read & write permissions to the volume

• Use case:
	• Achieve higher application availability in clustered Linux applications (ex: Teradata)
	• Applications must manage concurrent write operations
	• Up to 16 EC2 Instances at a time (ExamTips)
	• Must use a file system that’s cluster-aware (not XFS, EX4, etc…) 


You can attach multiple EBS volumes to a single instance. The volume and instance must be in the same Availability Zone. 
Depending on the volume and instance types, you can use Multi-Attach to mount a volume to multiple instances at the same time.

Amazon EBS Multi-Attach enables you to attach a single Provisioned IOPS SSD ( io1 or io2 ) volume to multiple instances that 
are in the same Availability Zone.



EBS Encryption:When you create an encrypted EBS volume, you get the following:
• Data at rest is encrypted inside the volume
• All the data in flight moving between the instance and the volume is encrypted
• All snapshots are encrypted
• All volumes created from the snapshot
• Encryption and decryption are handled transparently (you have nothing to do)
• Encryption has a minimal impact on latency
• EBS Encryption leverages keys from KMS (AES-256)
• Copying an unencrypted snapshot allows encryption
• Snapshots of encrypted volumes are encrypted


Encrypt an unencrypted EBS volume:
• Create an EBS snapshot of the volume
• Encrypt the EBS snapshot ( using copy )
• Create new ebs volume from the snapshot ( the volume will also be encrypted )
• Now you can attach the encrypted volume to the original instance




Data Lifecycle Manager (Amazon DLM):
Amazon Data Lifecycle Manager (Amazon DLM) is an automated procedure to back up the data stored on 
your Amazon EBS volumes. Use Amazon DLM to create lifecycle policies to automate snapshot management.

You can use Amazon Data Lifecycle Manager (Amazon DLM) to automate the creation, retention, and deletion of snapshots taken 
to back up your Amazon EBS volumes. Automating snapshot management helps you to:

– Protect valuable data by enforcing a regular backup schedule.
– Retain backups as required by auditors or internal compliance.
– Reduce storage costs by deleting outdated backups.


Combined with the monitoring features of Amazon CloudWatch Events and AWS CloudTrail,
Amazon DLM provides a complete backup solution for EBS volumes at no additional cost.


There is no such thing as EBS-cycle policy in Amazon S3.


EBS volumes support live configuration changes while in production which means that you 
can modify the volume type, volume size, and IOPS capacity without service interruptions.


EBS Volumes:
– When you create an EBS volume in an Availability Zone, it is automatically replicated 
  within that zone to prevent data loss due to a failure of any single hardware component.
– After you create a volume, you can attach it to any EC2 instance in the same Availability Zone
– Amazon EBS Multi-Attach enables you to attach a single Provisioned IOPS SSD (io1) volume to 
	multiple Nitro-based instances that are in the same Availability Zone. However, other 
	EBS types are not supported.
– An EBS volume is off-instance storage that can persist independently from the life of an instance. 
  You can specify not to terminate the EBS volume when you terminate the EC2 instance during instance creation.
– EBS volumes support live configuration changes while in production which means that you 
  can modify the volume type, volume size, and IOPS capacity without service interruptions.
– Amazon EBS encryption uses 256-bit Advanced Encryption Standard algorithms (AES-256)
– EBS Volumes offer 99.999% SLA.




26
##EFS
-------------------------------------------------
Compatible with Linux based AMI (not Windows)
Amazon EFS only supports file locking. Object lock is a feature of Amazon S3 and not Amazon EFS.


Amazon EFS provides a simple, serverless, set-and-forget elastic file system. 
With Amazon EFS, you can create a file system, 
mount the file system on an Amazon EC2 instance, and then read and write data to and from your file system.

• Managed NFS (network file system) that can be mounted on many EC2
• EFS works with EC2 instances in multi-AZ
• Highly available, automatically scalable, expensive (3x gp2), pay per use 


Amazon EFS offers four storage classes: 
	Two standard storage classes(Multi-AZ):
		Amazon EFS Standard 
		Amazon EFS Standard-Infrequent Access (EFS Standard-IA). 

	Two One Zone storage classes:
		Amazon EFS One Zone
		Amazon EFS One Zone-Infrequent Access (EFS One Zone-IA).



Can copy snapshots across AZ or Region.



Amazon EFS is a regional service storing data within and across multiple Availability Zones (AZs) 
for high availability and durability. 
Amazon EC2 instances can access your file system across AZs, 
regions, and VPCs, while on-premises servers can access using AWS Direct Connect or AWS VPN.


Use Amazon EFS and create a lifecycle policy that will move the objects to 
Amazon EFS-IA after 2 years is incorrect because the maximum days for the 
EFS lifecycle policy is only 90 days. The requirement is to move the files that are 
older than 2 years or 730 days.


Create an Amazon Elastic Filesystem (EFS) file system and enable encryption. 
Configure AWS Transfer for SFTP to securely upload files to the EFS file system. 

Apply an EFS lifecycle policy to delete files after 30 days is incorrect. 
This may be possible, however, the EFS lifecycle management doesn’t delete objects. 
It can only transition files in and out of the “Infrequent Access” tier.


Area:
=====
Amazon EFS is a regional service storing data within and across multiple Availability Zones (AZs) 
for high availability and durability.
 
 
 
UseCase:
========
Content management, web serving, data sharing, Wordpress.

Share code and other files in a secure, organized way to increase DevOps agility and 
respond faster to customer feedback.
Persist and share data from your AWS containers and serverless applications with zero management required.


BillCost:
=========
AWS Free Tier
You pay only for the storage you use, for read and write access to data stored in 
Infrequent Access storage classes, and for any provisioned throughput.



Security:
=========
Uses security group to control access to EFS
Encryption at rest using KMS

When using Amazon EFS, you specify Amazon EC2 security groups for your EC2 instances and 
security groups for the EFS mount targets associated with the file system.






==| AWS Fundamentals– Part II

27
##Scalability & High Availability
-------------------------------------------------
• Scalability means that an application / system can handle greater loads by adapting.
• There are two kinds of scalability:
• Vertical Scalability
• Horizontal Scalability (= elasticity)
• Scalability is linked but different to High Availability


Vertical Scaling: Increase instance size (= scale up / down)
• From: t2.nano - 0.5G of RAM, 1 vCPU
• To: u-12tb1.metal – 12.3 TB of RAM, 448 vCPUs

Horizontal Scaling: Increase number of instances (= scale out / in)
• Auto Scaling Group
• Load Balancer

High Availability: Run instances for the same application across multi AZ
• Auto Scaling Group multi AZ
• Load Balancer multi AZ



Load Balancing:
Load Balances are servers that forward traffic to multiple servers (e.g., EC2 instances) downstream.


Why use a load balancer? 
• Spread load across multiple downstream instances 
• Expose a single point of access (DNS) to your application 
• Seamlessly handle failures of downstream instances 
• Do regular health checks to your instances 
• Provide SSL termination (HTTPS) for your websites 
• Enforce stickiness with cookies 
• High availability across zones 
• Separate public traffic from private traffic





28
##ELB (Elastic Load Balancer) 
-------------------------------------------------
Elastic Load Balancing automatically distributes your incoming traffic across multiple targets, such as 
EC2 instances, containers, and IP addresses, in one or more Availability Zones.
 
It monitors the health of its registered targets, and routes traffic only to the healthy targets. 

Elastic Load Balancing supports the following load balancers: 
	Application Load Balancers, 
	Network Load Balancers, 
	Gateway Load Balancers,
	Classic Load Balancers.



It is integrated with many AWS offerings / services:
• EC2, EC2 Auto Scaling Groups, Amazon ECS
• ACM, CloudWatch
• Route53, AWS WAF, GlobaAccelerator

Some load balancers can be setup as internal (private) or external (public) ELBs



Health Checks:
• They enable the load balancer to know if instances it forwards traffic to are available to reply to requests
• The health check is done on a port and a route (/health is common)
• If the response is not 200 (OK), then the instance is unhealthy


Elastic Load Balancing provides access logs that capture detailed information about requests 
sent to your load balancer. Each log contains information such as the time the request was 
received, the client’s IP address, latencies, request paths, and server responses. 
You can use these access logs to analyze traffic patterns and troubleshoot issues.



Area:
=====
Elastic Load Balancer – Regional
Elastic Load Balancer distributes traffic across instances in multiple Availability Zones in the same region


UseCase:
========
Using a load balancer increases the availability and fault tolerance of your applications.



BillCost:
=========
AWS Free Tier.

ELB pricing also follows an AWS rule: “You only pay for what you use with your setup.” 
To properly bill you for your ELB usage, AWS has introduced a component called the Load Balancer Capacity Unit (LCU), 
which is an integral part of your billing calculation, along with the region you host your infrastructure.



Security:
=========
Security Group of Load Balancer => Application Security Group, Allow traffic only from Load Balancer.

Ensure your ELBs do not use insecure or deprecated SSL ciphers.
Ensure that your AWS ELBs listeners are using a secure protocol (HTTPS or SSL).
Ensure there are valid security groups associated with your Elastic Load Balancer.
Ensure that your AWS Elastic Load Balancers use access logging to analyze traffic patterns and 
identify and troubleshoot security issues.






29
##CLB (Classic Load Balancer)
-------------------------------------------------
Load balancer routes traffic between clients and backend servers based on IP address and TCP port. 
For example, an ELB at a given IP address receives a request from a client on TCP port 80 (HTTP).

• Supports TCP (Layer 4), HTTP & HTTPS (Layer 7)
• Health checks are TCP or HTTP based
• Fixed hostname XXX.region.elb.amazonaws.com


Area:
=====
Regional scope.



UseCase:
========
Support for EC2-Classic
Support for TCP and SSL listeners
Support for sticky sessions using application-generated cookies



BillCost:
=========
You are charged for each hour or partial hour that a 
Classic Load Balancer is running and for each GB of data transferred through your load balancer.



Security:
=========
Security is a shared responsibility.
Elastic Load Balancing provides a security group.






29
##ALB (Application Load Balancer)
-------------------------------------------------
An Application Load Balancer (ALB) only works at layer 7 (HTTP).
A listener checks for connection requests from clients, using the protocol and port that you configure. 

ALBs can also route and load balance gRPC traffic between microservices or between 
gRPC-enabled clients and services, gRPC protocol is at Layer 7.
Network Load Balancers do not support gRPC.


Gateway Load Balancer operates as a Layer 3 Gateway and a Layer 4 Load Balancing service.

To connect to an ALB, a client needs to resolve its DNS name, No IP address.

So when you need static IP addresses for inbound traffic, the ALB is not an option. 
Luckily, that’s something the NLB comes with out-of-the-box.

• Load balancing to multiple HTTP applications across machines(target groups)
• Load balancing to multiple applications on the same machine(ex: containers)
• Support for HTTP/2 and WebSocket
• Support redirects (from HTTP to HTTPS for example)

Routing tables to different target groups:
• Routing based on path in URL (example.com/users & example.com/posts)
• Routing based on hostname in URL (one.example.com & other.example.com)
• Routing based on Query String, Headers (example.com/users?id=123&order=false)



Target Groups:
• EC2 instances (can be managed by an Auto Scaling Group) – HTTP
• ECS tasks (managed by ECS itself) – HTTP
• Lambda functions – HTTP request is translated into a JSON event
• IP Addresses – must be private IPs
• ALB can route to multiple target groups.
• Health checks are at the target group leve.


• Fixed hostname (XXX.region.elb.amazonaws.com)
• The application servers don’t see the IP of the client directly
	• The true IP of the client is inserted in the header X-Forwarded-For
	• We can also get Port (X-Forwarded-Port) and proto (X-Forwarded-Proto)



Routing different types:
	Host-based Routing:
	You can route a client request based on the Host field of the HTTP header allowing you to 
	route to multiple domains from the same load balancer.

	Path-based Routing:
	You can route a client request based on the URL path of the HTTP header.

	HTTP header-based routing:
	You can route a client request based on the value of any standard or custom HTTP header.

	HTTP method-based routing:
	You can route a client request based on any standard or custom HTTP method.

	Query string parameter-based routing:
	You can route a client request based on the query string or query parameters.

	Source IP address CIDR-based routing:
	You can route a client request based on source IP address CIDR from where the request originates.


Host-based routing defines rules that forward requests to different target groups based on the 
hostname in the host header instead of the URL.

ALB only HTTP or HTTPS health check.
ICMP health check and FTP health check are incorrect as these are not supported.
A TCP health check is only offered in Network Load Balancers and Classic Load Balancers.
 
 
Area:
=====
Regional scope.


UseCase:
========
IP address registration as targets
Path-based/host-based routing
Calling Lambda functions to serve HTTP(S) requests
AWS WAF load balancer
SNI
Enhanced containers via load balancing between multiple ports of a single instance


BillCost:
=========
You are charged for each hour or partial hour that an Application Load Balancer is running, 
and the number of Load Balancer Capacity Units (LCU) used per hour.


Security:
=========
It is a aws managed service, security is a shared responsibility.
Elastic Load Balancing provides a security group.





30
##NLB (Network Load Balancer)
-------------------------------------------------
Network Load Balancer cannot facilitate content-based routing, ALB does it.

Network load balancers (Layer 4) allow to:
	• Forward TCP & UDP traffic to your instances
	• Handle millions of request per seconds
	• Less latency ~100 ms (vs 400 ms for ALB)
	
• NLB has one static IP per AZ, and supports assigning Elastic IP(helpful for whitelisting specific IP)
• NLB are used for extreme performance, TCP or UDP traffic

Provide IP address - Static, Elastic, not DNS name.
The load balancer rewrites the destination IP address before forwarding it to the target.

Network Load Balancer –Target Groups:
• EC2 instances
• IP Addresses – must be private IPs
• Application Load Balancer
• Health Checks support the TCP, HTTP and HTTPS Protocols


ExamTips: If you see UDP or TCP, its NLB;
          If your application(nlb) access with one,two or three ips then its NLB as option.
		  If see Extreme performance, UPD,TCP or static IP its NLB;
		  Back end application Health Checks support the TCP, HTTP and HTTPS Protocols only.
		  


The NLB uses HTTP, HTTPS, and TCP as possible protocols when performing health checks on targets. 
The default is the TCP protocol. If the target type is ALB, the supported health check protocols are 
HTTP and HTTPS. 


Area:
=====
Regional scope.



UseCase:
========
NLB are used for extreme performance, TCP or UDP traffic
This type of load balancer works at the transport layer(TCP/SSL) of the OSI model. 
It’s capable of handling millions of requests per second.  
It is mainly used for load balancing TCP traffic.



BillCost:
=========
No AWS free tier
You are charged for each hour or partial hour that a Network Load Balancer is running, and the number of 
Network Load Balancer Capacity Units (NLCU) used by Network Load Balancer per hour.



Security:
=========
It is a aws managed service, security is a shared responsibility.
Network Load Balancers do not have security groups.





31
##GWLB (Gateway Load Balaner)
-------------------------------------------------
A Gateway Load Balancer operates at the 3 layer. 
It listens for all IP packets across all ports and forwards traffic to the target group that's 
specified in the listener rule, using the GENEVE protocol on port 6081.


Source and destination  are unaware the traffic is inspected and do somthing. 
(Vertual application like Friewall, IDP)
Operates at Layer 3 of the OSI model, the network layer.
Uses the GENEVE protocol on port 6081


Gateway Load Balancer –Target Groups:
• EC2 instances
• IP Addresses – must be private IPs

Gateway Load Balancer endpoint:
A Gateway Load Balancer endpoint is a VPC endpoint that provides private connectivity between 
virtual appliances in the service provider VPC and application servers in the service consumer VPC. 

You deploy the Gateway Load Balancer in the same VPC as the virtual appliances.


https://docs.aws.amazon.com/vpc/latest/privatelink/vpce-gateway-load-balancer.html
https://aws.amazon.com/blogs/aws/introducing-aws-gateway-load-balancer-easy-deployment-scalability-and-high-availability-for-partner-appliances/

Area:
=====
Runs within one AZ and is recommended to be deployed in multiple AZs for greater availability.



UseCase:
========
Deploy third-party virtual appliances faster.
Improve virtual appliance availability.
Centralize your third-party virtual appliances
Add third-party security appliances to your network
Integration with orchestration and deployment tools.
Gateway Load Balancers provides you the facility to deploy, scale, and manage virtual appliances like firewall. 
Gateway Load Balancers combines a transparent network gateway and then distributes the traffic.



BillCost:
=========
You are charged for each hour or partial hour that a Gateway Load Balancer is running, and the number of 
Gateway Load Balancer Capacity Units (GLCU) used by Gateway Load Balancer per hour. 
Gateway Load Balancer uses Gateway Load Balancer Endpoint (GWLBE),GWLBE is priced and billed separately.
You pay charges for inter-AZ data transfer if enabled.

Gateway Load Balancer + Gateway Load Balancer endpoints + virtual appliances + data processing charges.



Security:
=========
It is a aws managed service, security is a shared responsibility.
Gateway Load Balancers do not have associated security groups. Therefore, the security groups for your
targets must use IP addresses to allow traffic from the load balancer






32
##Sticky Sessions (Session Affinity)
-------------------------------------------------
It is possible to implement stickiness so that the same client is always redirected to the 
  same instance behind a load balancer.
• This works for Classic Load Balancers & Application Load Balancers
• The “cookie” used for stickiness has an expiration date you control
• Enabling stickiness may bring imbalance to the load over the backend EC2 instances 

Use case: 
Make sure the user doesn’t lose his session data.


Sticky Sessions – Cookie:

• Application-based Cookies
	Custom cookie
		• Generated by the target
		• Can include any custom attributes required by the application
		• Cookie name must be specified individually for each target group
		• Don’t use AWSALB, AWSALBAPP, or AWSALBTG (reserved for use by the ELB)
	Application cookie
		• Generated by the load balancer
		• Cookie name is AWSALBAPP

• Duration-based Cookies
	• Cookie generated by the load balancer
	• Cookie name is AWSALB for ALB, AWSELB for CLB



Cross-Zone Load Balancing:
	With Cross Zone Load Balancing  each load balancer instance distributes evenly
	across all registered instances in all AZ.(All Host==Host)

	Without Cross Zone Load Balancing: Requests are distributed in the instances of the
	node of the Elastic Load Balancer (All AZ == AZ)


Cross-Zone Load Balancing:
• Application Load Balancer 
	• Always on (can’t be disabled) 
	• No charges for inter AZ data 
• Network Load Balancer 
	• Disabled by default 
	• You pay charges ($) for inter AZ data if enabled 
• Classic Load Balancer 
	• Disabled by default 
	• No charges for inter AZ data if enabled



SSL/TLS - Basics:
• An SSL Certificate allows traffic between your clients and your load balancer
  to be encrypted in transit (in-flight encryption)
• SSL refers to Secure Sockets Layer, used to encrypt connections
• TLS refers to Transport Layer Security, which is a newer version
• Nowadays, TLS certificates are mainly used, but people still refer as SSL
• Public SSL certificates are issued by Certificate Authorities (CA)
• Comodo, Symantec, GoDaddy, GlobalSign, Digicert, Letsencrypt, etc…
• SSL certificates have an expiration date (you set) and must be renewed

You can manage certificates using ACM (AWS Certificate Manager)
You can create upload your own certificates alternatively



Load Balancer - SSL Certificates:

Classic Load Balancer (v1)
• Support only one SSL certificate
• Must use multiple CLB for multiple hostname with multiple SSL certificates

Application Load Balancer (v2)
• Supports multiple listeners with multiple SSL certificates
• Uses Server Name Indication (SNI) to make it work

Network Load Balancer (v2)
• Supports multiple listeners with multiple SSL certificates
• Uses Server Name Indication (SNI) to make it work


HTTPS listener-multiple SSL certificates:
• You must specify a default certificate
• You can add an optional list of certs to support multiple domains
• Clients can use SNI (Server Name Indication) to specify the hostname they reach
• Ability to specify a security policy to support older versions of SSL / TLS (legacy clients)


SSL-Server Name Indication (SNI):
• SNI solves the problem of loading multiple SSL certificates onto one web server (to serve multiple websites)
• The server will then find the correct certificate, or return the default one
Note:
• Only works for ALB & NLB (newer generation), CloudFront
• Does not work for CLB (older gen)







Connection Draining:
Stops sending new requests to the EC2 instance which is willing to de-registering.
Between 1 to 3600 seconds (1hr) (default: 300 seconds 5mm)


When Connection Draining is enabled and configured, the process of deregistering an instance from an 
Elastic Load Balancer gains an additional step. 

For the duration of the configured timeout, the load balancer will allow existing, in-flight requests 
made to an instance to complete, but it will not send any new requests to the instance.

AWS ELB connection draining prevents breaking open network connections while taking an instance out of service.



Feature naming (Call same feature different way ! ):
• Connection Draining – for CLB
• Deregistration Delay – for ALB & NLB





33
##ASG (Auto Scaling Group)
-------------------------------------------------
AWS Auto Scaling monitors your applications and automatically adjusts capacity.

If exactly 10, then desired capacity to 10.
If Range of Instance then minMix.

Amazon EC2 Auto Scaling to provision new resources takes minutes. Lambda within seconds.


The goal of an Auto Scaling Group (ASG) is to:
• Scale out (add EC2 instances) to match an increased load
• Scale in (remove EC2 instances) to match a decreased load
• Ensure we have a minimum and a maximum number of EC2 instances running
• Automatically register new instances to a load balancer
• Re-create an EC2 instance in case a previous one is terminated (ex: if unhealthy)

Use predictive scaling:
You can also combine predictive scaling and dynamic scaling (proactive and reactive approaches, respectively) 
to scale your EC2 capacity faster.


Auto Scaling Group Attributes:
	• A Launch Template (older “Launch Configurations” are deprecated)
	• AMI + Instance Type
	• EC2 User Data
	• EBS Volumes
	• Security Groups
	• SSH Key Pair
	• IAM Roles for your EC2 Instances
	• Network + Subnets Information
	• Load Balancer Information
	• Min Size / Max Size / Initial Capacity
	• Scaling Policies


It is possible to scale an ASG based on CloudWatch alarms
An alarm monitors a metric (such as Average CPU, or a custom metric)


Dynamic Scaling Policies:
	Target Tracking Scaling:
	• Most simple and easy to set-up
	• Example: I want the average ASG CPU to stay at around 40%

	Simple / Step Scaling:
	• When a CloudWatch alarm is triggered (example CPU > 70%), then add 2 units
	• When a CloudWatch alarm is triggered (example CPU < 30%), then remove 1

	Scheduled Actions:
	• Anticipate a scaling based on known usage patterns
	• Example: increase the min capacity to 10 at 5 pm on Fridays


Manual scaling:
	At any time, you can change the size of an existing Auto Scaling group manually.

Predictive scaling: 
	Continuously forecast load and schedule scaling ahead.


Cooldown period:
After a scaling activity happens, you are in the cooldown period (default 300 seconds = 5M).
During the cooldown period, the ASG will not launch or terminate additional instances 
(to allow for metrics to stabilize)
Advice: Use a ready-to-use AMI to reduce configuration time in order to be serving 
request fasters and reduce the cooldown period.

ASG tries the balance the number of instances across AZ by default.


ASG Lifecycle Hooks:
• You have the ability to perform extra steps before the instance goes in service (Pending state)
• You have the ability to perform some actions before the instance is terminated (Terminating state)


Launch Configuration (legacy,deprecated):
	• Must be re-created every time
Launch Template (newer):
	• Can have multiple versions
	• Create parameters subsets (partial configuration for re-use and inheritance)


Suspend the ReplaceUnhealthy process type for the Auto Scaling group and apply the maintenance 
patch to the instance. 

Once the instance is ready, you can manually set the instance's health status 
back to healthy and activate the ReplaceUnhealthy process type again.

Taking the snapshot of the existing instance to create a new AMI and then creating a new instance 
in order to apply the maintenance patch, is not time/resource optimal.


ASG cannot directly use a Cloudwatch alarm as the source for a scale-in or scale-out event. Based on the alarm:
	• We can create scale-out policies (increase the number of instances)
	• We can create scale-in policies (decrease the number of instances)

You can put an instance that is in the InService state into the Standby state, 
update some software or troubleshoot the instance, and then return the instance to service.

Instances that are on standby are still part of the Auto Scaling group, but they do not actively 
handle application traffic.



Step scaling policies increase or decrease the current capacity of a scalable target 
based on a set of scaling adjustments, known as step adjustments.

The option that says: create a new target group and create a new target group and 
launch configuration are both incorrect because you only want to change the 
AMI being used by your instances, and not the instances themselves. 
Target groups are primarily used in ELBs and not in Auto Scaling. 
The scenario didn’t mention that the architecture has a load balancer. 
Therefore, you should be updating your launch configuration, not the target group.


Deploy eight EC2 instances with Auto Scaling in one Availability Zone behind an 
Amazon Elastic Load Balancer is incorrect because this architecture is not highly available. 
If that Availability Zone goes down then your web application will be unreachable.

T3 instances also have a Burstable Performance capability to burst or go beyond the 
current compute capacity of the instance to higher performance as required by your workload. 
So your 4 servers will be able to manage 110% compute capacity for a short period of time. 
This is the power of cloud computing versus our on-premises network architecture. 
It provides elasticity and unparalleled scalability.


Default termination policy:

1. If there are instances in multiple Availability Zones, choose the Availability Zone with the most instances and at least 
one instance that is not protected from scale in. If there is more than one Availability Zone with this number of instances, 
choose the Availability Zone with the instances that use the oldest launch configuration.

2. Determine which unprotected instances in the selected Availability Zone use the oldest launch configuration. 
If there is one such instance, terminate it.

3. If there are multiple instances to terminate based on the above criteria, determine which unprotected instances are 
closest to the next billing hour. (This helps you maximize the use of your EC2 instances and manage your Amazon EC2 usage costs.) 
If there is one such instance, terminate it.

4. If there is more than one unprotected instance closest to the next billing hour, choose one of these instances at random.


There are three main parts in a distributed messaging system: the components of your distributed 
system which can be hosted on EC2 instance; your queue (distributed on Amazon SQS servers); 
and the messages in the queue.
To improve the scalability of your distributed system, you can add Auto Scaling group to your EC2 instances.



Area:
=====
Auto Scaling – Regional
Auto Scaling spans across multiple Availability Zones within the same region.


UseCase:
========
SETUP SCALING QUICKLY
AUTOMATICALLY MAINTAIN PERFORMANCE
PAY ONLY FOR WHAT YOU NEED
IMPROVE FAULT TOLERANC



BillCost:
=========
ASG are free (you only pay for the underlying EC2 instances).



Security:
=========
It is a aws managed service, security is a shared responsibility.






==| RDS, Aurora & ElastiCache
34
##RDS 
-------------------------------------------------

RDS stands for Relational Database Service
• It’s a managed DB service for DB use SQL as a query language.
• It allows you to create databases in the cloud that are managed by AWS
• Postgres
• MySQL
• MariaDB
• Oracle
• Microsoft SQL Server
• Aurora (AWS Proprietary database)

Oracle RMAN and RAC are not supported in RDS.
 
 
RDS is a managed service:
• Automated provisioning, OS patching
• Continuous backups and restore to specific timestamp (Point in Time Restore)!
• Monitoring dashboards
• Read replicas for improved read performance
• Multi AZ setup for DR (Disaster Recovery)
• Maintenance windows for upgrades
• Scaling capability (vertical and horizontal)
• Storage backed by EBS (gp2 or io1)
• BUT you can’t SSH into your instances

Supports all RDS database engines (MariaDB, MySQL,PostgreSQL, SQL Server, Oracle)

Multi-AZ follows synchronous replication and spans at least two Availability Zones within a single region. 
Read replicas follow asynchronous replication and can be within an Availability Zone, Cross-AZ, or Cross-Region.

When you provision a Multi-AZ DB Instance, Amazon RDS automatically creates a primary 
DB Instance and synchronously replicates the data to a standby instance in a different 
Availability Zone (AZ). Multi-AZ spans at least two Availability Zones within a single region.


RDS – From Single-AZ to Multi-AZ, Zero downtime operation (no need to stop the DB)


RDS Backups:
• Automated backups:
	• Daily full backup of the database (during the maintenance window)
	• Transaction logs are backed-up by RDS every 5 minutes
	• => ability to restore to any point in time (from oldest backup to 5 minutes ago)
	• 1 to 35 days of retention, set 0 to disable automated backups
• Manual DB Snapshots:
	• Manually triggered by the user
	• Retention of backup for as long as you want


ExamTrick: in a stopped RDS database, you will still pay for storage. If you plan on
stopping it for a long time, you should snapshot & restore instead.



IAM database authentication provides the following benefits:
	Network traffic to and from the database is encrypted using Secure Sockets Layer (SSL).
	You can use IAM to centrally manage access to your database resources, instead of managing access individually on each DB instance.
	For applications running on Amazon EC2, you can use profile credentials specific to your EC2 instance to access your database 
	instead of a password, for greater security.

Although STS is used to send temporary tokens for authentication, this is not a compatible use case for RDS.

RDS– Storage Auto Scaling:
• Helps you increase storage on your RDS DB instance dynamically
• When RDS detects you are running out of free database storage, it scales automatically


RDS Read Replicas for read scalability:
• Up to 5 Read Replicas
• Within AZ, Cross AZ or Cross Region
• Replication is ASYNC, so reads are eventually consistent
• Replicas can be promoted to their own DB
• Applications must update the connection string to leverage read replicas
• Read replicas are used for SELECT =read only kind of statements(not INSERT, UPDATE, DELETE)


RDS Read Replicas – Network Cost:
• In AWS there’s a network cost when data goes from one AZ to another
• For RDS Read Replicas within the same region, you don’t pay that fee.


RDS Multi AZ (Disaster Recovery):
• SYNC replication
• One DNS name – automatic app failover to standby
• Increase availability
• Failover in case of loss of AZ, loss of network, instance or storage failure
• No manual intervention in apps
• Not used for scaling
• Multi-AZ replication is free
• Note:The Read Replicas be setup as Multi AZ for Disaster Recovery (DR)


++RDS Custom for Oracle and Microsoft SQL Server
-------------------------------------------------
• Managed Oracle and Microsoft SQL Server Database with OS and database customization
• RDS: Automates setup, operation, and scaling of database in AWS
• Custom: access to the underlying database and OS so you can
	• Configure settings
	• Install patches
	• Enable native features
	• Access the underlying EC2 Instance using SSH or SSM Session Manager
• De-activate Automation Mode to perform your customization, better to take a DB snapshot before

• RDS vs. RDS Custom
	• RDS: entire database and the OS to be managed by AWS
	• RDS Custom: full admin access to the underlying OS and the database


Failover support:
-------------------------------------------------
Amazon RDS provides high availability and failover support for DB instances using Multi-AZ deployments. 
Amazon RDS uses several different technologies to provide failover support. 

Multi-AZ deployments for Oracle, PostgreSQL, MySQL, and MariaDB DB instances use Amazon’s failover technology. 
SQL Server DB instances use SQL Server Database Mirroring (DBM).
Multi-AZ deployments for Microsoft SQL Server by using either Database Mirroring (DBM) or Always On Availability Groups (AGs). 


Amazon RDS automatically performs a failover in the event of any of the following:
	Loss of availability in primary Availability Zone.
	Loss of network connectivity to primary.
	Compute unit failure on primary.
	Storage failure on primary.





RDS Security – Summary:
• Encryption at rest:
	• Is done only when you first create the DB instance
	• or: unencrypted DB => snapshot => copy snapshot as encrypted => create DB from snapshot
• Your responsibility:
	• Check the ports /IP/ security group inbound rules in DB’s SG
	• In-database user creation and permissions or manage through IAM
	• Creating a database with or without public access
	• Ensure parameter groups or DB is configured to only allow SSL connections
• AWS responsibility:
	• No SSH access
	• No manual DB patching
	• No manual OS patching
	• No way to audit the underlying instance


Basic Schema Copy will not migrate secondary indexes, foreign keys or stored procedures. 

When you need to use a more customizable schema migration process (e.g. when you are migrating your 
production database and need to move your stored procedures and secondary database objects), 
you must use the AWS Schema Conversion Tool.


When you create or modify your DB instance to run as a Multi-AZ deployment, Amazon RDS automatically provisions and maintains 
a synchronous standby replica in a different Availability Zone. 
Updates to your DB Instance are synchronously replicated across Availability Zones to the standby in order to keep both in sync 
and protect your latest database updates against DB instance failure.



Encryption of Data in Transit:
-------------------------------------------------
Encrypt communications between your application and your DB Instance using SSL/TLS.

Amazon RDS creates an SSL certificate and installs the certificate on the DB instance 
when the instance is provisioned. 


For MySQL, you launch the mysql client using the --ssl_ca parameter 
to reference the public key in order to encrypt connections. 

For SQL Server, download the public key 
and import the certificate into your Windows operating system. 

RDS for Oracle uses Oracle native network encryption with a DB instance. 



IAM database authentication works with MySQL and PostgreSQL.
IAM authentication is just another way to authenticate the user's credentials while accessing 
the database.

It would not significantly enhance the security in a way that SSL does by   in-transit encryption for the database.

With this authentication method, you don’t need to use a password when you connect to a DB instance. 
Instead, you use an authentication token.


Primary database instance fails:
The canonical name record (CNAME) is switched from the primary to standby instance.


Read Replicas:
It elastically scales out beyond the capacity constraints of a single DB instance for read-heavy database workloads.
Provides asynchronous replication and improves the performance of the primary database by taking read-heavy database workloads from it.

You can also create read replicas within a Region or between Regions for your Amazon RDS for MySQL, MariaDB, PostgreSQL, and 
Oracle database instances encrypted at rest with AWS Key Management Service (KMS).


There are 2 ways to use SSL to connect to your SQL Server DB instance:
– Force SSL for all connections — this happens transparently to the client, and the client doesn’t have to do any work to use SSL.
– Encrypt specific connections — this sets up an SSL connection from a specific client computer, and you must do work on the 
  client to encrypt connections.


Enhanced Monitoring is a feature of Amazon RDS. 
By default, Enhanced Monitoring metrics are stored for 30 days in the CloudWatch Logs.

Differences between CloudWatch and Enhanced Monitoring Metrics:  
	CloudWatch gathers metrics about CPU utilization from the hypervisor for a DB instance, and 
	Enhanced Monitoring gathers its metrics from an agent on the instance.

Why Enhanced Monitoring for RDS: 
You do not have direct access to the instances/servers of your RDS database instance, unlike with your 
EC2 instances where you can install a CloudWatch agent or a custom script to get 
CPU and memory utilization of your instance.




Amazon RDS supports change data capture (CDC) for your DB instances running Microsoft SQL Server. 
	CDC captures changes that are made to the data in your tables. 
	It stores metadata about each change, which you can access later.
	After CDC is enabled on the database, you can start tracking specific tables. 




IAM database authentication works with MySQL and PostgreSQL. With this authentication method, you don’t need to use a password 
when you connect to a DB instance. Instead, you use an authentication token Each token has a lifetime of 15 minutes.


RDS events only provide operational events such as DB instance events, DB parameter group events, 
DB security group events, and DB snapshot events. What we need in the scenario is to capture 
data-modifying events (INSERT, DELETE, UPDATE) which can be achieved thru native functions or stored procedures.



Area:
=====
Multi-AZ deployment, which replicates data synchronously across AZs.
By default, the Multi-AZ feature is not enabled for the RDS instance.

In an Amazon RDS Multi-AZ deployment, Amazon RDS automatically creates a primary database (DB) instance and 
synchronously  replicates the data to an instance in a different AZ.




UseCase:
========
Build web and mobile applications.
Move to managed databases.



BillCost:
=========
AWS Free Tier with Amazon RDS
When you use Amazon RDS, you can choose to use on-demand DB instances or reserved DB instances.

It is comprised of 3 parts:
DB instance hours (per hour) . You can choose from different types of hosting depending on your need
Storage (per GiB per month),I/O requests (per 1 million requests per month): Storage and Operations. 
Storage is billed per gigabyte per month, and I/O is billed per million-request.
Provisioned IOPS (per IOPS per month) 
Backup storage (per GiB per month) –

Data transfer (per GB) – Data transfer in and out of your DB instance from or to the internet and other Regions.

For RDS Read Replicas within the same region, you don’t pay that fee but cross - region must pay.



Security:
=========
Possibility to encrypt the master & read replicas with AWS KMS - AES-256 encryption
Transparent Data Encryption (TDE) available for Oracle and SQL Server
Provide SSL options with trust certificate when connecting to database
Security group and IAM controls the access to a DB instance.

RDS databases are usually deployed within a private subnet, not in a public one
IAM policies help control who can manage AWS RDS (through the RDS API)
Traditional Username and Password can be used to login into the database
IAM-based authentication can be used to login into RDS MySQL & PostgreSQL






35
##Aurora
-------------------------------------------------
Amazon Aurora (Aurora) is a fully managed relational database engine that's compatible with MySQL and PostgreSQL.
The underlying storage grows automatically as needed. 
An Aurora cluster volume can grow to a maximum size of 128 tebibytes (TiB). 

5 times faster than standard MySQL
3 times faster than standard PostgreSQL

Amazon Aurora provides built-in security, continuous backups, serverless compute, 
up to 15 read replicas, automated multi-Region replication, and integrations with other AWS services.

Non-Serverless DB cluster for Aurora is called a provisioned DB cluster. 
Aurora Serverless clusters and provisioned clusters both have the same kind of 
high-capacity, distributed, and highly available storage volume.

When you work with Amazon Aurora without Aurora Serverless (provisioned DB clusters), you can choose your DB instance class 
size and create Aurora Replicas to increase read throughput. 
If your workload changes, you can modify the DB instance class size and change the number of Aurora Replicas. 
This model works well when the database workload is predictable, because you can adjust capacity manually based on the expected workload.

With Aurora Serverless , you can create a database endpoint without specifying the DB instance class size. 
You set the minimum and maximum capacity. 



• Aurora costs more than RDS (20% more) – but is more efficient


Features of Aurora:
• Automatic fail-over
• Backup and Recovery
• Isolation and security
• Industry compliance
• Push-button scaling
• Automated Patching with Zero Downtime
• Advanced Monitoring
• Routine Maintenance
• Backtrack: restore data at any point of time without using backups



Aurora Replicas - Auto Scaling


Aurora – Custom Endpoints:
• Define a subset of Aurora Instances as a Custom Endpoint
• Example: Run analytical queries on specific replicas
• The Reader Endpoint is generally not used after defining Custom Endpoints


Aurora Serverless:
• Automated database instantiation and auto - scaling based on actual usage
• Good for infrequent, intermittent or unpredictable workloads
• No capacity planning needed
• Pay per second, can be more cost -effective


Amazon Aurora and Aurora Serverless are two distinct products from AWS.
Amazon Aurora Serverless is an on-demand, autoscaling configuration for Amazon Aurora.



Aurora Multi-Master:
• In case you want immediate failover for write node (HA) –
• Every node does R/W - vs promoting a RR as the new master

Multi-master Aurora limitations.
	Currently, you can have a maximum of four DB instances in a multi-master cluster.
	Currently, all DB instances in a multi-master cluster must be in the same AWS Region.
	You can't enable cross-Region replicas from multi-master clusters.
	Multi-master clusters are not available in all Regions.


Amazon Aurora Global Database is designed for globally distributed applications, 
allowing a single Amazon Aurora database to span multiple AWS Regions.


Global Aurora:
Aurora Cross Region Read Replicas:
	• Useful for disaster recovery
	• Simple to put in place
• Aurora Global Database (recommended):
	• 1 Primary Region (read / write)
	• Up to 5 secondary (read-only) regions, replication lag is less than 1 second
	• Up to 16 Read Replicas per secondary region
	• Helps for decreasing latency
	• Promoting another region (for disaster recovery) has an RTO of < 1 minute
	• Typical cross-region replication takes less than 1 second





Aurora Machine Learning:
• Enables you to add ML-based predictions to your applications via SQL
• Simple, optimized, and secure integration between Aurora and AWS ML services
• Supported services
	• Amazon SageMaker (use with any ML model)
	• Amazon Comprehend (for sentiment analysis) 
• Use cases: fraud detection, ads targeting, sentiment analysis, product recommendations


Aurora Replicas have two main purposes:
	for read operations for your application. 
    Aurora can spread the load for read-only connections across as many 
	Aurora Replicas as you have in the cluster. 

	Aurora Replicas also help to increase availability. 
	If the writer instance in a cluster becomes unavailable, Aurora automatically 
	promotes one of the reader instances to take its place as the new writer. 
	Up to 15 Aurora Replicas can be distributed across the Availability 
	Zones that a DB cluster spans within an AWS Region.

Amazon RDS for MySQL, MariaDB and PostgreSQL allow you to add up to 15 read replicas to each DB Instance. 
Amazon RDS for Oracle and SQL Server allow you to add up to 5 read replicas to each DB Instance.
An Aurora DB cluster can contain up to 15 Aurora Replicas. The Aurora Replicas can be distributed across the Availability Zones 
that a DB cluster spans within an AWS Region.



Failover rules priority tier:
	Amazon Aurora features a distributed, fault-tolerant, self-healing storage system that 
	auto-scales up to 128TB per database instance. 
	It delivers high performance and availability with up to 15 low-latency read replicas, 
	point-in-time recovery, continuous backup to Amazon S3, and 
	replication across three Availability Zones (AZs).

For Amazon Aurora, each Read Replica is associated with a priority tier (0-15). 
In the event of a failover, Amazon Aurora will promote the Read Replica that has the highest priority 
(the lowest numbered tier). If two or more Aurora Replicas share the same priority, then 
Amazon RDS promotes the replica that is largest in size.
 
If two or more  Aurora Replicas share the same priority and size, then 
Amazon Aurora promotes an ramdomly replica in the same promotion tier.


creating a custom endpoint in Aurora based on the specified criteria for the production traffic and 
another custom endpoint to handle the reporting queries.


Area:
=====
Multiple AWS Regions. 
It replicates your data with no impact on database performance, 
enables fast local reads with low latency in each Region, and provides disaster recovery from Region-wide outages.


UseCase:
========
Automatic fail-over
Aurora Cross Region Read Replicas, Useful for disaster recovery.



BillCost:
=========
Prices vary according to the region.
With Amazon Aurora Global Database, you 
pay for replicated write I/Os between the primary Region and each secondary Region.



Security:
=========
It is a aws managed service, security is a shared responsibility.

• Encryption at rest using 
• Automated backups, snapshots and replicas are also encrypted
• Encryption in flight using SSL (same process as MySQL or Postgres)
• Possibility to authenticate using IAM token (same method as RDS)
• You are responsible for protecting the instance with security groups
• You can’t SSH



RDS/Aurora - Backup and Monitoring
-------------------------------------------------

RDS Backups:
	• Automated backups
	• Daily full backup of the database (during the maintenance window)
	• Transaction logs are backed-up by RDS every 5 minutes
	• => ability to restore to any point in time (from oldest backup to 5 minutes ago)
	• 1 to 35 days of retention, set 0 to disable automated backups
	• Manual DB Snapshots:
		• Manually triggered by the user
		• Retention of backup for as long as you want
	• Trick: in a stopped RDS database, you will still pay for storage. 
	  If you plan on stopping it for a long time, you should snapshot & restore instead
	  

RDS & Aurora Restore options:
• Restoring a RDS / Aurora backup or a snapshot creates a new database
	• Restoring MySQL RDS database from S3
		• Create a backup of your on-premises database
		• Store it on Amazon S3 (object storage)
		• Restore the backup file onto a new RDS instance running MySQL
	• Restoring MySQL Aurora cluster from S3
		• Create a backup of your on-premises database using Percona XtraBackup
		• Store the backup file on Amazon S3
		• Restore the backup file onto a new Aurora cluster running MySQL



Aurora Database Cloning:
• Create a new Aurora DB Cluster from an existing one
• Faster than snapshot & restore
• The new DB cluster uses the same cluster volume and data as the original but will change when data updates are made.
• Very fast & cost-effective
• Useful to create a “staging” database from a “production” database without impacting the production database




++RDS & Aurora Security:
-------------------------------------------------
• At-rest encryption:
	• Database master & replicas encryption using AWS KMS – must be defined as launch time
	• If the master is not encrypted, the read replicas cannot be encrypted
	• To encrypt an un-encrypted database, go through a DB snapshot & restore as encrypted
• In-flight encryption: TLS-ready by default, use the AWS TLS root certificates client-side
• IAM Authentication: IAM roles to connect to your database (instead of username/pw)
• Security Groups: Control Network access to your RDS and Aurora DB
• No SSH available except on RDS Custom
• Audit Logs can be enabled and sent to CloudWatch Logs for longer retention






++RDS Proxy
-------------------------------------------------
Amazon RDS Proxy:
	• Fully managed database proxy for RDS
	• Allows apps to pool and share DB connections established with the database.
	• Improving database efficiency by reducing the stress on database resources (e.g., CPU, RAM) and
	  minimize open connections (and timeouts)
	• Serverless, autoscaling, highly available (multi-AZ)
	• Reduced RDS & Aurora failover time by up 66%
	• Supports RDS (MySQL, PostgreSQL, MariaDB) and Aurora (MySQL, PostgreSQL)
	• No code changes required for most apps
	• Enforce IAM Authentication for DB, and securely store credentials in AWS Secrets Manager
	• RDS Proxy is never publicly accessible (must be accessed from VPC)





36
##ElastiCache
-------------------------------------------------
ElastiCache supports two open-source in-memory caching engines: Memcached and Redis.
Using ElastiCache involves heavy application code changes.

ElastiCache is to get managed Redis or Memcached as RDS for other enign lke MySQL.
Distributed in-memory key-value environment.

Amazon ElastiCache for Redis is a great choice for real-time transactional and analytical 
processing use cases such as caching, chat/messaging, gaming leaderboards, geospatial, machine learning, 
media streaming, queues, real-time analytics, and session store.

Amazon ElastiCache for Memcached is a great choice for implementing an in-memory 
cache to decrease access latency, increase throughput, and ease the load off your relational or NoSQL database.
ElastiCache Memcached cannot be used as a cache to serve static content from S3.

Although you can integrate Redis with DynamoDB.

• Helps reduce load off of databases for read intensive workloads
• Helps make your application stateless
• AWS takes care of OS maintenance / patching, optimizations, setup, configuration, 
  monitoring, failure recovery and backups
• Using ElastiCache involves heavy application code changes



REDIS:
	• Multi AZ with Auto-Failover
	• Read Replicas to scale reads and have high availability
	• Data Durability using AOF persistence
	• Backup and restore features


Use Case:
	• Gaming Leaderboards are computationally complex
	• Redis Sorted sets guarantee both uniqueness and element ordering
	• Each time a new element added, it’s ranked in real time, then added in correct order


MEMCACHED:
• Multi-node for partitioning of data (sharding)
• No high availability (replication)
• Non persistent
• No backup and restore
• Multi-threaded architecture



ElastiCache (available as a managed service) can cache the results from anything. 
The difference here is that you need to manage invalidation and you must adjust your code to 
check the cache before querying the main query store.

Most fo the time, you’ll want to go w/DAX if you’re using DyanmoDB as a data store. 
Otherwise, use ElastiCache.

Elasticache is used as a caching layer in front of relational databases. 
It is not a good fit to store data in key-value pairs from the IoT sources.

 
Amazon ElastiCache for Memcached is an ideal front-end for data stores like Amazon RDS or 
Amazon DynamoDB, providing a high-performance middle tier for applications with extremely 
high request rates and/or low latency requirements.


Area:
=====
Amazon ElastiCache is available in multiple AWS Regions. This means that you can launch ElastiCache clusters in 
locations that meet your requirements.
The new Global Datastore feature is available with Amazon ElastiCache for Redis 5.0. 6 or above, and is 
supported on M5 and R5 nodes.



UseCase:
========
Gamming Leadboard
Accelerate application performance.
Ease backend database load.
Build low-latency data stores.
Primary data store for use cases that don't require durability like 
session stores, gaming leaderboards, streaming, and analytics



BillCost:
=========
AWS Free Tier.
You are charged hourly based on the number of nodes, node type, and pricing model you select. 
ElastiCache supports both on-demand nodes, which allow you to pay by hour with no long-term commitments, 
and reserved nodes, which offer discounts for customers who commit to either one-year or three-year terms.



Security:
=========
ElastiCache Securiry:
• Do not support IAM authentication
• IAM policies on ElastiCache are only used for AWS API-level security

Redis AUTH
• You can set a “password/token” when you create a Redis cluster
• This is an extra level of security for your cache (on top of security groups)
• Support SSL in flight encryption

Memcached
• Supports SASL-based authentication (advanced)

A security group controls the access to a cluster. It does so by allowing access to IP address ranges or 
Amazon EC2 instances that you specify. 







==| Route 53 Section
37
##Route53
-------------------------------------------------
Amazon Route 53 is a highly available and scalable Domain Name System (DNS) web service. 
Route 53 connects user requests to internet applications running on AWS or on-premises.



Geolocation routing lets you choose the resources that serve your traffic based on the 
geographic location of your users, meaning the location that DNS queries originate from.


Routing Policies:
Simple routing policy — Use for a single resource that performs a given function for your domain, 
	for example, a web server that serves content for the example.com website.
Weighted routing policy — Use to route traffic to multiple resources in (%)percentage base that you specify.
Latency routing policy — Use when you have resources in multiple AWS Regions and you want to 
	route traffic to the region that provides the lest latency.
	Redirect to the resource that has the least latency close to user.
Failover routing policy — Use when you want to configure active-passive failover.
Geolocation routing policy — Use when you want to route traffic based on the location of your users.
Geoproximity routing policy — Use when you want to route traffic based on the location of your resources and, 
	optionally, shift traffic from resources in one location to resources in another by bias number(0-99).

Multivalue answer routing policy — Use when you want Route 53 to respond to DNS queries with up to eight healthy 
	records selected at random.


• Route 53 supports the following DNS record types:
• (must know) A / AAAA / CNAME / NS
• (advanced) CAA / DS / MX / NAPTR / PTR / SOA / TXT / SPF / SRV



In summary: Browser -> TLD -> NS -> SOA -> DNS record. 
The pipeline reverses when the correct DNS record is found.


A Record:
	A record (or Address record) value is always an IP address. The IP should be static. it 
	should not change frequently. For example, Elastic IPs in AWS are static IPs.

CNAME Record:
	CNAME record maps a name/ to another name value. It can never be an IP address. 

An A record for example.com (i.e. root/naked domain) points to the server IP address
A CNAME record for www.example.com points to example.com(A or AAAA). 


CNAME vs Alias:
	• AWS Resources (Load Balancer, CloudFront...) expose an AWS hostname:
	• lb1-1234.us-east-2.elb.amazonaws.com and you want myapp.mydomain.com
CNAME:
		• The target or value is a domain name which must have an A or AAAA record.
		• Points a hostname to any other hostname. (app.mydomain.com => anything.com)
		• ONLY FOR NON ROOT DOMAIN (aka.something.mydomain.com) Destination.
		
Alias   : Alias record is an Amazon Route 53-specific virtual record. 
		• Maps a hostname to an AWS resource
		• An extension to DNS functionality
		• Automatically recognizes changes in the resource’s IP addresses
		• Unlike CNAME, it can be used for the top node of a DNS namespace (Zone Apex), e.g.: example.com
		• Alias Record is always of type A/AAAA for AWS resources (IPv4 / IPv6)
		• You can’t set the TTL
		
		
A     =  11.221.32.143 (IP)             ->example.com      Not for www.example.com 
CNAME =  example.com(A/AAAA)           ->www.example.com or app.mydomain.com  (DNS name not for A/AAAA)
Alias =  elb.amazonaws.com            ->example.com(ROOT DOMAIN ) or myapp.mydomain.com(NON ROOT DOMAIN )

To simplify it we can say that example.com points to the server IP address using 
A record, and www.example.com points to the same address via example.com using a CNAME record. (Read Now – CNAME vs Alias)


Now a AWS service • lb1-1234.us-east-2.elb.amazonaws.com and you want myapp.mydomain.com, how ?

CNAME Record Target only A (example.com i.e. root/naked domain) Record not IP or AWS services DNS name.
Alias Records Targets Aws Resource or Other Host.
A CNAME can’t be used for naked/root domain names. Root domain names must be mapped with either 
an A record or an Alias record (in Route 53).




A CNAME can’t be used providing for naked/root domain names ( (Zone Apex), e.g.: example.com). 
    A Record can providing root(example.com) domain but only support IP.
	Root domain names (example.com) must be mapped with either an A record or an Alias record (in Route 53).


ANS:  Alias record in Route 53 which handles this issue. 
	  Alias record will allow ELB DNS name (or any other DNS names produced by AWS services 
	  like CloudFront, S3 etc) to be mapped with the root domain (example.com) name in Route53.
	  
      Alias = lb1-1234.us-east-2.elb.amazonaws.com ->example.com

Amazon Route53 supports both forward (AAAA) and reverse (PTR) IPv6 records. 


Route53-TTL:
The amount of time, in seconds, that you want DNS recursive resolvers to cache information about this record.

High TTL – e.g., 24 hr
	• Less traffic on Route 53
	• Possibly outdated records
• Low TTL – e.g., 60 sec.
	• More traffic on Route 53 ($$)
	• Records are outdated for less time
	• Easy to change records
Except for Alias records, TTL is mandatory for each DNS record


Route53 - Health Check:
Amazon Route 53 health checks monitor the health and performance of your web applications, 
web servers, and other resources. 
You can optionally configure Amazon CloudWatch alarms for your health checks, so that you receive notification 
when a resource becomes unavailable.


Types of Amazon Route 53 health checks:

Health checks that monitor an endpoint
Health checks that monitor other health checks (calculated health checks)
Health checks that monitor CloudWatch alarms
Amazon Route 53 Application Recovery Controller


Route 53 – Hosted Zones:
• A container for records that define how to route traffic to a domain and its subdomains
	• Public Hosted Zones – contains records that specify how to route traffic on the 
	  Internet (public domain names) application1.mypublicdomain.com
	• Private Hosted Zones – contain records that specify how you route traffic within one or more 
	  VPCs (private domain names)application1.company.interna



Alias Records Targets:
	Elastic Load Balancers
	CloudFront distributions
	API Gateway
	Elastic Beanstalk
	S3 Website
	Global Accelerator
	VPC Interface Endpoints
	Route 53 record in the same hosted zone
Alias records do not support
	EC2 DNS endpoint
	RDS DNS endpoint
CNAME record can redirect DNS queries to any DNS record



Enable DNS Hostnames in your VPC — this creates and manages an invisible Route 53 Private Hosted zone. 

https://www.whizlabs.com/blog/dns-records/




Area:
=====
Route 53 is primarily a global service, but the following features support AWS Regions:
If you're using Route 53 Resolver to set up hybrid configurations, you create endpoints in
AWS Regions that you choose, 
and you specify IP addresses in multiple Availability Zones. 
For outbound endpoints, you create rules in the same Region where you created the endpoint.



UseCase:
========
Create, visualize, and scale complex routing relationships between records and policies 
with easy-to-use global DNS features.
Set routing policies to pre-determine and automate responses in case of failure, 
like redirecting traffic to alternative 

Availability Zones or Regions.
Assign and access custom domain names in your Amazon Virtual Private Cloud (VPC).


BillCost:
=========

Route 53 doesn’t charge for alias queries to AWS resources
Route 53 charges for CNAME queries

Managing hosted zones: You pay a monthly charge for each hosted zone managed with Route 53.
Managing domain names: You pay an annual charge for each domain name registered via or transferred 
 into Route 53.
Serving DNS queries: You incur charges for every DNS query answered by the Amazon Route 53 service, 
 except for queries to Alias A records.


Security:
=========
Filter and control outbound DNS traffic for your VPCs
Amazon Route 53 Resolver DNS Firewall
	
AWS Shield Standard automatically protects your Amazon Route 53 Hosted Zones from infrastructure 
layer DDoS attacks at no additional cost.

You can protect your domain from this type of attack, known as DNS spoofing or a man-in-the-middle attack, 
by configuring Domain Name System Security Extensions (DNSSEC), a protocol for securing DNS traffic. 
Amazon Route 53 supports DNSSEC signing as well as DNSSEC for domain registration.





==| Classic Solutions Architecture
-------------------------------------------------


• WhatIsTheTime.Com
• MyClothes.Com
• MyWordPress.Com
• Instantiating applications quickly
• Beanstalk 






38
##AWS Elastic Beanstalk 
-------------------------------------------------
ElasticBeanstalk  is PaaS (platform as a service) while CloudFormation,EC2 is IaC (infrastructure as code).

AWS Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications and services developed with 
Java, .NET, PHP, Node.js, Python, Ruby, Go, and 
Docker on familiar servers such as Apache, Nginx, Passenger, and IIS.

If not supported, you can write your custom platform (advanced)


Elastic Beanstalk takes care of the hosting infrastructure, coding language interpreter, operating system, 
security, https service and application layer. 


You can simply upload your code and Elastic Beanstalk automatically handles the deployment, from capacity provisioning, 
load balancing, auto-scaling to application health monitoring. 


Elastic Beanstalk isn't great if you need a lot of environment variables. 
The simple reason is that Elastic Beanstalk has a hard limit of 4KB to store all key-value pairs.






Area:
=====
Regional.



UseCase:
======== 
AWS Elastic Beanstalk provides an environment that makes it easy to deploy and run applications in the cloud.
Complete resource control: You have the freedom to select the AWS resources, such as Amazon EC2 instance type and processor 
type to run the workload on, that are optimal for your application. 

AWS Elastic Beanstalk is combined with the developer tools to help you manage the lifecycle of your applications.



BillCost:
=========
There is no additional charge for Elastic Beanstalk - you pay only for the AWS resources needed to 
store and run your applications.



Security:
=========
Security is a shared responsibility.
By default, Elastic Beanstalk creates a security group for your environment, but you can also add your own 
security group to your environment.





==| S3 Storage and Data Management

39
##S3
-------------------------------------------------
An Amazon S3 bucket is a public cloud storage.
S3 is an object storage service. 

You can only add 1 SQS or SNS at a time for Amazon S3 events notification.

S3 Versioning-enabled buckets, help you to recover objects from accidental deletion or overwrite.

API calls can be logged in AWS CloudTrail.
S3 can host static websites and have them accessible on the www.

S3 in Amazon has two entities- called buckets and objects. 
Objects are stored inside buckets.
By default, the maximum number of buckets that can be created per account is 100.

Object values are the content of the body:
• Max Object Size is 5TB (5000GB)
• If uploading more than 5GB, must use “multi-part upload” cecommmand for obj>100MB

You can version your files in Amazon S3, It is enabled at the bucket level.


After a successful write of a new object or an overwrite of an existing object, any subsequent 
read request immediately receives the latest version of the object. S3 also provides strong 
consistency for list operations, so after a write, you can immediately perform a listing of the 
objects in a bucket with any changes reflected.


When you need to access S3 using a file system protocol, you should use File Gateway. 
You get a local cache in the gateway that provides high throughput and low latency over SMB.


• Metadata (list of text key / value pairs – system or user metadata)
• Tags (Unicode key / value pair – up to 10) – useful for security / lifecycle
• Version ID (if versioning is enabled)


Buckets can be in one of three states:
	Unversioned (the default)
	Versioning-enabled
	Versioning-suspended

• Any file that is not versioned prior to enabling versioning will have version “null”
• Suspending versioning does not delete the previous versions
	
After you version-enable a bucket, it can never return to an unversioned state. 
But you can suspend versioning on that bucket.
You enable and suspend versioning at the bucket level.

There are two types of Lifecycle Management actions:
	Transaction Actions: This action defines objects’ transition from one storage class to another.
	Expiration Actions: This action deletes objects in the Amazon S3 bucket.


Amazon S3 Storage Classes:
	S3 Standard
	S3 Standard-Infrequent Access
	S3 Intelligent-Tiering
	S3 One Zone-Infrequent Access
	S3 Glacier Instant Retrieval
	S3 Glacier Flexible Retrieval
	S3 Glacier Deep Archive

Amazon S3 supports a waterfall model for transitioning between storage classes.

There are 4 methods of encrypting objects in S3:
	• SSE-S3: encrypts S3 objects using keys handled & managed by AWS
	• SSE-KMS: AWS Key Management Service to manage encryption keys
	• SSE-C: when you want to manage your own encryption keys
	• Client Side Encryption
	
EmaxTips:It’s important to understand which ones are adapted to which situation for the exam


SSE-S3:
	• SSE-S3: encryption using keys handled & managed by AmazonS3 its self.
	• Object is encrypted server side
	• AES-256 encryption type
	• Must set header: “x-amz-server-side-encryption": "AES256"

SSE-KMS:
	• SSE-KMS: everage AWS Key Management Service to manage encryption keys
	• KMS Advantages: user control + Audit trail
	• Object is encrypted server side
	• Must set header: “x-amz-server-side-encryption": ”aws:kms"

SSE-C:
	• SSE-C: when you want to manage your own encryption keys
	• SSE-C: server-side encryption using data keys fully managed by the customer outside of AWS
	• Amazon S3 does not store the encryption key you provide
	• HTTPS is mandatory for SSE-C
	• Encryption key must provided in HTTP headers, for every HTTP request made

Client Side Encryption:
	• Client library such as the Amazon S3 Encryption Client 
	• Clients must encrypt data themselves before sending to S3 
	• Clients must decrypt data themselves when retrieving from S3 
	• Customer fully manages the keys and encryption cycle





S3 Security:
	IAM policies                 = Manage multiple Users to do under a single AWS account about S3 bucket or objects .
	Bucket policies,             = With Bucket policies and Access point policies, define rules  which apply all requests to S3 resources.
	Access point policies        = Do
	Access Control Lists         = With ACLs only to other AWS accounts, not to users in your account. grant specific permissions (i.e. READ, WRITE, FULL_CONTROL) to specific users (customers who prefer to use exclusively policies for access control)for an individual bucket or object.
	Query String Authentication  = With Query String Authentication, create a URL to an S3 object which is only valid for a limited time. 
	VPC endpoint policies,       = withh VPC endpoint, they can attach an endpoint policy to it that controls access to the S3 resources to which will are connecting. Also S3 bucket policies to control access to buckets from specific endpoints or specific VPCs.
	SCPs in AWS Organizations    = SCPs are a type of AWS Organizations policy that customers can use to manage permissions in their organization. SCPs offer central control over the maximum available permissions for all accounts in an organization.
	S3 Block Public Access       = S3 Block Public Access provides settings for- access points, buckets, and accounts to  manage public access to  S3 resources.

With MFA Delete, Encryption, Cross-Account Access, Requester Pays Buckets and Monitoring.

User based
	• IAM policies - which API calls should be allowed for a specific user from IAM console(Web).
Resource Based
	• Bucket Policies(JSON based policies) - bucket wide rules from the S3 console - allows cross account.
	• Object Access Control List (ACL) – finer grain
	• Bucket Access Control List (ACL) – less common
	
Note: an IAM principal can access an S3 object if
	• the user IAM permissions allow it OR the resource policy ALLOWS it
	• AND there’s no explicit DENY


Use S3 bucket policy to:- 
	• Grant public access to the bucket 
	• Force objects to be encrypted at upload 
	• Grant access to another account (Cross Account)



S3 Bucket Policies: 
	• JSON based policies 
	• Resources: buckets and objects 
	• Actions: Set of API to Allow or Deny 
	• Effect: Allow / Deny 
	• Principal: The account or user to apply the policy to

Use S3 bucket for policy to: 
	• Grant public access to the bucket 
	• Force objects to be encrypted at upload 
	• Grant access to another account (CrossAccount)



CORS:
• An origin is a scheme (protocol), host (domain) and port
• Web Browser based mechanism to allow requests to other origins while visiting the main origin
	The requests won’t be fulfilled unless the other origin allows for the
	requests, using CORS Headers (ex: Access-Control-Allow-Origin)
	
• If a client does a cross-origin request on our S3 bucket, we need to enable the correct CORS headers
• You can allow for a specific origin or for * (all origins)	
ExamTip:It’s a popular exam question


S3 Sync command:
	The aws S3 sync command uses the CopyObject APIs to copy objects between S3 buckets. 
	The sync command lists the source and target buckets to identify objects that are in the 
	source bucket but that aren't in the target bucket.
	 
	The command also identifies objects in the source bucket that have different LastModified dates 
	than the objects that are in the target bucket. The sync command on a versioned bucket copies only the 
	current version of the object—previous versions aren't copied.



There are no S3 data transfer charges when data is transferred in from the internet. 
Also with S3TA, you pay only for transfers that are accelerated. Therefore the junior 
scientist does not need to pay any transfer charges for the image upload because S3TA did not 
result in an accelerated transfer.


If you have workloads that cater to a global client base, AWS recommends that you use AWS Global Accelerator. 
If you have workloads hosted in a single AWS Region and used by clients in and around the same 
Region, you can use an Application Load Balancer or Network Load Balancer to manage your resources.

If you create 10 prefixes in an Amazon S3 bucket to parallelize reads, you could scale your read 
performance to 55,000 read requests per second.



When you use bucket default settings, you don't specify a Retain Until Date. 
Instead, you specify a duration, in either days or years.
ByDefault its a period like: 1 year.

When you apply a retention period to an object version explicitly, you specify a Retain Until Date 
for the object version.





Encryption:
	Server-side Encryption using
		Amazon S3-Managed Keys (SSE-S3 with AES-256)
		AWS KMS-Managed Keys (SSE-KMS with Audit-trails)
		Customer-Provided Keys (SSE-C)
	Client-side Encryption using
		AWS KMS-managed customer master key
		client-side master key



When using an AWS KMS-Managed customer master key to enable client-side data encryption, you provide an 
AWS KMS customer master key ID (CMK ID) to AWS. 

On the other hand, when you use client-side master key for client-side data encryption, your client-side master keys 
and your unencrypted data are never sent to AWS.


S3  replicating the objects to the destination bucket takes about 15 minutes. 
Take note that the requirement in the scenario is to aggregate the data in the fastest way.
Use Transfer Acclearation.


Area:
=====
Global but Data is Regional.
The user interface shows all your buckets, in all regions. 
But buckets exist in a specific region and you need to specify that region when you create a bucket.





UseCase:
========
S3 can host static websites and have them accessible on the www.
Build a data lake:Run big data analytics, artificial intelligence (AI), machine learning (ML), 
and high performance computing (HPC) applications to unlock data insights.
Back up and restore critical data: Meet Recovery Time Objectives (RTO), Recovery Point Objectives (RPO).
Archive data at the lowest cost.
Run cloud-native applications.
 
 
 
BillCost:
=========
AWS Free Tier
Amazon S3 charges you only for what you actually use, with no hidden fees and no overage charges.
Bucket pricing varies by region.



Security:
=========
IAM to create users and manage their respective access; 
ACLs (Resource base) to make individual objects accessible to authorized users; 
Bucket policies to configure permissions  for all objects within a single S3 bucket;

-Amazon S3 also supports Audit Logs.
-Enabling detective controls such as AWS CloudTrail or Amazon GuardDuty for Amazon S3.



S3 Security - Other:
Networking:
	• Supports VPC Endpoints (for instances in VPC without www internet)
Logging and Audit:
	• S3 Access Logs can be stored in other S3 bucket
	• API calls can be logged in AWS CloudTrail
	• GuardDuty monitors threats against your Amazon S3 resources by analyzing AWS CloudTrail management events and 
	  CloudTrail S3 data events.
User Security:
	• MFA Delete: MFA can be required in versioned buckets to delete objects.
	• Pre-Signed URLs: URLs that are valid only for a limited time 
	  (ex: premium video service for logged in users)

Encryption in-flight is also called SSL / TLS.







==Developing on AWS
40
##Instance Metadata
-------------------------------------------------
EC2 Instance metadata is data about your instance that you can use to configure or manage the running instance. 
AWS EC2 Instance Metadata is powerful but one of the least known features to developers.

• It allows AWS EC2 instances to ”learn about themselves” without using an IAM Role for that purpose.
• The URL is http://169.254.169.254/latest/meta-data
• You can retrieve the IAM Role name from the metadata, but you CANNOT retrieve the IAM Policy.

Metadata = Info about the EC2 instance
Userdata = launch script of the EC2 instance




AWS SDK: Good to know: if you don’t specify or configure a default region, then us-east-1 will be chosen by default.






==| Advanced S3
41
##S3 MFA
-------------------------------------------------
MFA (multi factor authentication) forces user to generate a code on a device 
(usually a mobile phone or hardware) before doing important operations on S3.

• To use MFA-Delete, enable Versioning on the S3 bucket
• You will need MFA to
	• permanently delete an object version
	• suspend versioning on the bucket
• You won’t need MFA for
	• enabling versioning
	• listing deleted versions
• Only the bucket owner (root account) can enable/disable MFA-Delete
• MFA-Delete currently can only be enabled using the CLI





42
##S3 Access Logs
-------------------------------------------------
S3 Access Logs provides detailed records for the requests that are made to an Amazon S3 bucket. 
Provide the name of the target bucket. 
This bucket is where you want Amazon S3 to save the access logs as objects. 

Both the source and target buckets must be in the same AWS Region and owned by the same account.
That data can be analyzed using data analysis tools.

S3 Access Logs: Warning
• Do not set your logging bucket to be the monitored bucket
• It will create a logging loop, and your bucket will grow in size exponentially

There is no extra charge for enabling server access logging on an Amazon S3 bucket.




43
##S3 Replication 
-------------------------------------------------
Must enable versioning in source and destination.

Replication enables automatic, asynchronous copying of objects across  
Amazon S3 buckets same AWS account or by different accounts. 
You can replicate objects to a single destination bucket or to multiple destination buckets. 

If you specify an object version ID to delete, Amazon S3 deletes that object version in the source bucket. 
But it doesn't replicate the deletion in the destination bucket. In other words, 
it doesn't delete the same object version from the destination bucket. 
This protects data from malicious deletions.

When you add a replication rule to a bucket, the rule is enabled by default, 
so it starts working as soon as you save it.

S3 Replication supports two-way replication between two or more buckets in the same or different AWS Regions. 


• CRR(Cross Region Replication) - Use cases: compliance, lower latency access, replication across accounts
• SRR(Same Region Replication)  – Use cases: log aggregation, live replication between production and test.


After activating, only new objects are replicated
• Optionally, you can replicate existing objects using S3 Batch Replication
• Replicates existing objects and objects that failed replication


For DELETE operations:
• Can replicate delete markers from source to target (optional setting)
• Deletions with a version ID are not replicated (to avoid malicious deletes)
• There is no “chaining” of replication


S3 Transfer Acceleration is a bucket-level feature that enables fast, easy, and secure 
transfers of files over long distances between your client and an S3 bucket. 

You cannot use Transfer Acceleration to copy objects across S3 buckets in different Regions using S3 console.



UseCase:
========
S3 Replication for:
Data redundancy – If you need to maintain multiple copies of your data in the same, 
or different AWS Regions, with different encryption types, or across different accounts.
Replicate objects to more cost-effective storage classes.
Maintain object copies under a different account.





44
##S3 Pre-signed URLs
-------------------------------------------------
S3 objects are private. Only the object owner has permission to access them. 

Can generate pre-signed URLs using SDK or CLI
• For downloads (easy, can use the CLI)
• For uploads (harder, must use the SDK)
• Users given a pre-signed URL inherit the permissions of the person who generated the URL for GET / PUT
• Valid for a default of 3600 seconds (60mm or 1Hour), can change timeout with --expires-in [TIME_BY_SECONDS] argument

UseCase :
• Allow only logged-in users to download a premium video on your S3 bucket
• Allow an ever changing list of users to download files by generating URLs dynamically
• Allow temporarily a user to upload a file to a precise location in our bucket






45
##S3 Storage Class
-------------------------------------------------

S3 Storage Classes can be configured at the object level, and a single bucket can contain all Class type object.

Amazon S3 Storage Classes:
	S3 Standard
	S3 Standard-Infrequent Access
	S3 Intelligent-Tiering
	S3 One Zone-Infrequent Access
	S3 Glacier Instant Retrieval
	S3 Glacier Flexible Retrieval
	S3 Glacier Deep Archive

S3 standard has 99.99% availability = not available 53 minutes a year


S3 Standard – General Purpose:
	• 99.99% Availability
	• Used for frequently accessed data
	• Low latency and high throughput
	• Sustain 2 concurrent facility failures
Use Cases: Big Data analytics, mobile & gaming applications, content distribution


•S3 Standard-Infrequent Access (S3 Standard-IA)
	• For data that is less frequently accessed, but requires rapid access when needed
	• Lower cost than S3 Standard
	• 99.9% Availability
	• Use cases: Disaster Recovery, backups

S3 Intelligent-Tiering:
	• Small monthly monitoring and auto-tiering fee
	• Moves objects automatically between Access Tiers based on usage
	• There are no retrieval charges in S3 Intelligent-Tiering
	• Frequent Access tier (automatic): default tier
	• Infrequent Access tier (automatic): objects not accessed for 30 days
	• Archive Instant Access tier (automatic): objects not accessed for 90 days
	• Archive Access tier (optional): configurable from 90 days to 700+ days
	• Deep Archive Access tier (optional): config. from 180 days to 700+ days



•S3 One Zone-Infrequent Access (S3 One Zone-IA for re-creatable data) 
	• In a single AZ; data lost when AZ is destroyed
	• 99.5% Availability
    • Use Cases: Storing secondary backup copies of on-premises data, or data you can recreate
S3 One Zone-IA is for data that is accessed less frequently, but requires rapid access when needed.
 
S3 One Zone-IA is ideal for customers who want a lower-cost option for infrequently accessed and 
re-creatable data but do not require the availability and resilience of S3 Standard or S3 Standard-IA. 
The minimum storage duration is 30 days before you can transition objects from S3 Standard to S3 One Zone-IA.


S3 Glacier Storage Classes:
	• Low-cost object storage meant for archiving / backup
	• Pricing: price for storage + object retrieval cost
	• S3 Glacier Instant Retrieval
		• Millisecond retrieval, great for data accessed once a quarter
		• Minimum storage duration of 90 days
	• S3 Glacier Flexible Retrieval (formerly Amazon S3 Glacier):
		• Expedited (1 to 5 minutes), Standard (3 to 5 hours), Bulk (5 to 12 hours) – free
		• Minimum storage duration of 90 days
	• S3 Glacier Deep Archive – for long term storage:
		• Standard (12 hours), Bulk (48 hours)
		• Minimum storage duration of 180 days

You can transition objects between storage classes 
Moving objects can be automated using a lifecycle configuration


The minimum storage duration is 30 days before you can transition objects from 
S3 Standard to S3 One Zone-IA or S3 Standard-IA.


No retraival free for S3Standerd and InteletencyTaring


45
##S3 Lifecycle | Rule
-------------------------------------------------
An S3 Lifecycle configuration is a set of rules that define actions that S3 applies to a group of objects. 
S3 Object Can move between classes manually or using S3 Lifecycle configurations

Transition actions: It defines when objects are transitioned to another storage class.
	• Move objects to Standard IA class 60 days after creation
	• Move to Glacier for archiving after 6 months
Expiration actions: configure objects to expire (delete) after some time
	• Access log files can be set to delete after a 365 days
	• Can be used to delete old versions of files (if versioning is enabled)
	• Can be used to delete incomplete multi-part uploads
• Rules can be created for a certain prefix (ex - s3://mybucket/mp3/*)
• Rules can be created for certain objects tags (ex - Department: Finance)



Amazon S3 supports a waterfall model for transitioning between storage classes.

Supported lifecycle transitions:
	The S3 Standard storage class to any other storage class.
	The S3 Standard-IA storage class to the S3 Intelligent-Tiering, S3 One Zone-IA, S3 Glacier Instant Retrieval, S3 Glacier Flexible Retrieval, or S3 Glacier Deep Archive storage classes.
	The S3 Intelligent-Tiering storage class to the S3 One Zone-IA, S3 Glacier Instant Retrieval, S3 Glacier Flexible Retrieval, or S3 Glacier Deep Archive storage classes.
	The S3 One Zone-IA storage class to the S3 Glacier Flexible Retrieval or S3 Glacier Deep Archive storage classes.
	The S3 Glacier Instant Retrieval storage class to the S3 Glacier Flexible Retrieval or S3 Glacier Deep Archive storage classes.
	The S3 Glacier Flexible Retrieval storage class to the S3 Glacier Deep Archive storage class.
	Any storage class to the S3 Glacier Deep Archive storage class.

Unsupported lifecycle:
	Any storage class to the S3 Standard storage class.
	Any storage class to the Reduced Redundancy Storage (RRS) class.
	The S3 Intelligent-Tiering storage class to the S3 Standard-IA storage class.
	The S3 One Zone-IA storage class to the S3 Intelligent-Tiering, S3 Standard-IA, and  S3 Glacier Instant Retrieval storage classes.
	
https://docs.aws.amazon.com/AmazonS3/latest/userguide/lifecycle-transition-general-considerations.html




46
##S3 Analytics
-------------------------------------------------
• You can setup S3 Analytics to help determine when to transition objects from Standard to Standard_IA
• Does not work for ONEZONE_IA or GLACIER
• Report is updated daily
• Takes about 24h to 48h hours to first start
• Good first step to put together Lifecycle Rules (or improve them)!


Storage class analysis does not give recommendations for transitions to the ONEZONE_IA or 
S3 Glacier storage classes. Only Standard to Standard-IA


S3 – Baseline Performance
-------------------------------------------------
• Amazon S3 automatically scales to high request rates, latency 100-200 ms
• Your application can achieve at least 3,500 PUT/COPY/POST/DELETE and
  5,500 GET/HEAD requests per second per prefix in a bucket

3500 and 5500 for Each S3 prefix can support these request rates, making it simple to increase 
performance significantly. 
  
Performance scales per prefix, so you can use as many prefixes as you need in parallel to achieve the required throughput. 
There are no limits to the number of prefixes.



S3 – KMS Limitation:
• If you use SSE-KMS, you may be impacted by the KMS limits
• When you upload, it calls the GenerateDataKey KMS API
• When you download, it calls the Decrypt KMS API


Multi-Part upload:
• recommended for files > 100MB, must use for files > 5GB
• Can help parallelize uploads (speedup transfers)

S3 Transfer Acceleration:
• Increase transfer speed by transferring file to an AWS edge location which will forward the data to the 
  S3 bucket in the target region.
• Compatible with multi-part upload



There are no limits to the number of prefixes in a bucket. 

If you spread reads across all four prefixes evenly, you can achieve 22,000
requests per second for GET and HEAD (4*5500)

S3 – KMS Limitation:
• Count towards the KMS quota per second (5500, 10000, 30000 req/s based on region)
• You can request a quota increase using the Service Quotas Console



47
##S3 Select & Glacier Select
-------------------------------------------------
Retrive subset of object date. Get Data in a object by object key.
• Retrieve less data using SQL by performing server side filtering
• Can filter by rows & columns (simple SQL statements)
• Less network transfer, less CPU cost client-side






48
##S3 Event Nofifications
-------------------------------------------------
Can create as many “S3 events” as desired
S3 event notifications typically deliver events in seconds but can sometimes take a minute or longer.

You can use the Amazon S3 Event Notifications feature to receive notifications when certain 
events happen in your S3 bucket. 
To enable notifications, add a notification configuration that identifies the events that you want 
Amazon S3 to publish. 
Make sure that it also identifies the destinations where you want Amazon S3 to send the notifications. 

You store this configuration in the notification subresource that's associated with a bucket


S3 Event Notifications with Amazon EventBridge:
	• Advanced filtering options with JSON rules (metadata, object size, name...)
	• Multiple Destinations – ex Step Functions, Kinesis Streams / Firehose…
	• EventBridge Capabilities – Archive, Replay Events, Reliable delivery


UseCase:
========
Amazon S3 supports For Event:Amazon SNS, Amazon SQS, Lambda.

S3 cannot directly write data into SNS, although it can certainly use S3 event notifications to 
send an event to SNS.



49
##S3 Requester pays
-------------------------------------------------
In general, bucket owners pay for all Amazon S3 storage and data transfer costs that are 
associated with their bucket. 

However, you can configure a bucket to be a Requester Pays bucket. With Requester Pays buckets, 
the requester instead of the bucket owner pays the cost of the request and the data download from the bucket. 
The bucket owner always pays the cost of storing data.


Helpful when you want to share large datasets with other accounts
The requester must be authenticated in AWS (cannot be anonymous)


UseCase:
========
Typically, you configure buckets to be Requester Pays buckets when you want to share data but not incur 
charges associated with others accessing the data. 





51
##S3 Lock | Vault Lock | Glacier Vault Lock
-------------------------------------------------
S3 Glacier Vault Lock helps you to easily deploy and enforce compliance controls for individual 
S3 Glacier vaults with a Vault Lock policy. 

You can specify controls such as "write once read many" (WORM) in a Vault Lock policy  and 
lock the policy from future edits.

After a Vault Lock policy is locked, the policy can no longer be changed or deleted.


S3 Object Lock (versioning must be enabled):
• Adopt a WORM (Write Once Read Many) model
• Block an object version deletion for a specified amount of time

	• Retention mode - Compliance:
		• Object versions can't be overwritten or deleted by any user, including the root user
		• Objects retention modes can't be changed, and retention periods can't be shortened
	• Retention mode - Governance:
		• Most users can't overwrite or delete an object version or alter its lock settings
		• Some users have special permissions to change the retention or delete the object
	• Retention Period: protect the object for a fixed period, it can be extended
	• Legal Hold:
		• protect the object indefinitely, independent from retention period
		• can be freely placed and removed using the s3:PutObjectLegalHold IAM permission




S3 – Access Points:
	• Each Access Point gets its own DNS and policy to limit who can access it
	• A specific IAM user / group
	• One policy per Access Point => Easier to manage than complex bucket policies



S3 Object Lambda:
S3 Object Lambda supports only GetObject requests. 
Use AWS Lambda Functions to change the object before it is retrieved by the caller application.
Only one S3 bucket is needed, on top of which we create S3 Access Point and S3 Object Lambda Access Points.

Use Cases:
	Redacting personally identifiable information for analytics or non-production environments.
	Converting across data formats, such as converting XML to JSON.
	Augmenting data with information from other services or databases.
	Compressing or decompressing files as they are being downloaded.
	Resizing and watermarking images on the fly using caller-specific details, such as the user who 
	requested the object.
	Implementing custom authorization rules to access data.

Element:
The S3 bucket
An S3 Bucket Access Point
An S3 Object Lambda Access Point

ClientGetReqRes->S3ObjectLambdaAccessPoint->S3BucketAccessPoint->S3


S3 Batch Operations:
• Perform bulk operations on existing S3 objects with asingle request, example:
	• Modify object metadata & properties
	• Copy objects between S3 buckets
	• Encrypt un-encrypted objects
	• Modify ACLs, tags
	• Restore objects from S3 Glacier
	• Invoke Lambda function to perform custom action on each object
• A job consists of a list of objects, the action to perform, and optional parameters
• S3 Batch Operations manages retries, tracks progress, sends completion notifications, generate reports …
• You can use S3 Inventory to get object list and use S3 Select to filter your objects




Bucket Policies:
-------------------------------------------------
Bucket policies in Amazon S3 can be used to add or deny permissions across some or all of the objects 
within a single bucket. 
Policies can be attached to users, groups, or Amazon S3 buckets, enabling centralized management of permissions. 

With bucket policies, you can grant users within your 
AWS Account or other AWS Accounts access to your Amazon S3 resources.

IAM				->AccNot->UserYes
ACL				->AccYes->UserNot
BucketPolicies	->AccYes->UserYes

With IAM policies, you can only grant users within your own AWS account permission to access your Amazon S3 resources.
With ACLs, you can only grant other AWS accounts (not specific users) access to your Amazon S3 resources. 





S3 Transfer Acceleratio
-------------------------------------------------
Use Amazon S3 Transfer Acceleration to enable faster file uploads into the destination S3 bucket.

Amazon S3 Transfer Acceleration enables fast, easy, and secure transfers of files over 
long distances between your client and an S3 bucket. 

Transfer Acceleration takes advantage of 
Amazon CloudFront’s globally distributed edge locations. 

As the data arrives at an edge location, 
data is routed to Amazon S3 over an optimized network path.

Use multipart uploads for faster file uploads into the destination S3 bucket.



You can place a retention period on an object version either explicitly or through a bucket default setting. 

When you apply a retention period to an object version explicitly, 
you specify a Retain Until Date for the object version.


S3 Standard-IA and S3 One Zone-IA have a minimum storage duration charge of 30 days 
(so instead of 24 hours, you end up paying for 30 days). 

S3 Standard-IA and S3 One Zone-IA also have 
retrieval charges (as the results are heavily referenced by other parts of the analytics pipeline, 
so the retrieval costs would be pretty high).





==| Global Infrastructure
52
##CloudFront 
-------------------------------------------------
UserReq->CloudFront->EdgeLocation->WebServer


AWS CloudFront is a globally-distributed network.
CloudFront delivers your content through a worldwide network of data centers called EdgeLocation.

CloudFront works from edge locations and doesn't belong to a VPC.


CloudFront is Amazon’s content delivery network that is primarily used to speed up websites. 
It’s particularly useful for large, static assets—like images and videos. 

AWS CloudFront is a globally-distributed network offered by Amazon Web Services, which securely 
transfers content such as software, SDKs, videos, etc., to the clients, with high transfer speed.


Amazon CloudFront uses standard cache control headers you set on your files to identify 
static and dynamic content. You can use different origins for different types of content on 
a single site – e.g. Amazon S3 for static objects, Amazon EC2 for dynamic content, and 
custom origins for third-party content.



• Content Delivery Network (CDN)
• Improves read performance, content is cached at the edge
• 216 Point of Presence globally (edge locations)
• DDoS protection, integration with Shield, AWS Web Application Firewall
• Can expose external HTTPS and can talk to internal HTTPS backends


Performance
CloudFront improves performance for both cacheable content (such as images and videos) and 
dynamic content (such as API acceleration and dynamic site delivery).

Global Accelerator improves performance for a wide range of applications over TCP or UDP by 
proxying packets at the edge to applications running in one or more AWS Regions.
It use dedicated AWS Global Network.


CloudFront– Origins:
S3 bucket 
	• For distributing files and caching them at the edge 
	• Enhanced security with CloudFront Origin Access Identity (OAI) 
	  Private S3 bucket file can be access by ClourFront using OAI
	• CloudFront can be used as an ingress (to upload files to S3) 
Custom Origin (HTTP) 
	• Application Load Balancer 
	• EC2 instance 
	• S3 website (must first enable the bucket as a static S3 website) 
	• Any HTTP backend you want



CloudFront Geo Restriction:
	• You can restrict who can access your distribution
	• Whitelist: Allow your users to access your content only if they're in one of the 
	  countries on a list of approved countries.
	• Blacklist: Prevent your users from accessing your content if they're in one of the countries on a 
	  blacklist of banned countries.
	• The “country” is determined using a 3rd party Geo-IP database
	• Use case: Copyright Laws to control access to content


CloudFront vs S3 Cross Region Replication:
CloudFront:
	• Global Edge network
	• Files are cached for a TTL (maybe a day)
	• Great for static content that must be available everywhere
S3 Cross Region Replication:
	• Must be setup for each region you want replication to happen
	• Files are updated in near real-time
	• Read only
	• Great for dynamic content that needs to be available at low-latency in few regions


CloudFront Signed URL / Signed Cookies:
• You want to distribute paid shared content to premium users over the world
• We can use CloudFront Signed URL / Cookie. We attach a policy with:
	• Includes URL expiration
	• Includes IP ranges to access the data from
	• Trusted signers (which AWS accounts can create signed URLs)
• How long should the URL be valid for?
	• Shared content (movie, music): make it short (a few minutes)
	• Private content (private to the user): you can make it last for years
	
• Signed URL = access to individual files (one signed URL per file)
• Signed Cookies = access to multiple files (one signed cookie for many files)


CloudFront Signed URL vs S3Pre-Signed URL:

Only S3 refers to them as Pre-signed URLs; 
CloudFront refers to them as Signed URLs and Signed Cookies.

-Pre-signed URLs use the owner’s security credentials to grant.

-Restrict access to files in CloudFront edge caches
-Restrict access to files in your Amazon S3 bucket (unless you’ve configured it as a website endpoint)
-You can configure CloudFront to require that users access your files 
 using either signed URLs or signed cookies.
-OAI prevents users from viewing your S3 files by simply using the direct URL for the file. 
 Instead, they would need to access it through a CloudFront URL.


Lambda Http HTTP  errors:
Set up an origin failover by creating an origin group with two origins. 
Specify one as the primary origin and the other as the second origin which 
CloudFront automatically switches to when the primary origin returns specific 
HTTP status code failure responses.


When you create or update Lambda functions that use environment variables, 
AWS Lambda encrypts them using the AWS Key Management Service. 
When your Lambda function is invoked, those values are decrypted and made available to the Lambda code.


Although Lambda encrypts the environment variables in your function by default, 
the sensitive information would still be visible to other users who have access to the Lambda console. 
This is because Lambda uses a default KMS key to encrypt the variables, which is 
usually accessible by other users. 

Use Signed Cookies to control who can access the private files in your CloudFront distribution by modifying your application to 
determine whether a user should have access to your content. 
For members, send the required Set-Cookie headers to the viewer which will unlock the content only to them.




Use signed URLs for the following cases:
– You want to use an RTMP distribution. Signed cookies aren’t supported for RTMP distributions.
– You want to restrict access to individual files, for example, an installation download for your application.
– Your users are using a client (for example, a custom HTTP client) that doesn’t support cookies.

Use signed cookies for the following cases:
– You want to provide access to multiple restricted files, for example, all of the files for a video in HLS format or 
  all of the files in the subscribers’ area of a website.
– You don’t want to change your current URLs.



Create an Origin Access Identity (OAI) for CloudFront and grant access to the objects in your S3 bucket to that OAI.


The Cache-Control and Expires headers control how long objects stay in the cache. 
The Cache-Control max-age directive lets you specify how long (in seconds) you want an object to remain in the cache before 
CloudFront gets the object again from the origin server. 
The minimum expiration time CloudFront supports is 0 seconds for web distributions and 3600 (60 mm or 1har) seconds for RTMP distributions.


CloudFront Functions are just lightweight functions in JavaScript for high-scale, latency-sensitive CDN customizations and 
not for enforcing security. 

A CloudFront Function runtime environment offers submillisecond startup times which allows your application to scale 
immediately to handle millions of requests per second. But again, this can’t be used to restrict access to your files.


Serve the private content via CloudFront only:
	A better solution is to set up an origin access identity (OAI) then use Signed URL or Signed Cookies in your 
	CloudFront web distribution.
 

Amazon S3 Storage Lens feature just provides a single view of object storage usage and activity across your entire Amazon S3 storage.
S3 Storage Lens is the first cloud storage analytics solution to provide a single view of object storage usage and 
activity across hundreds, or even thousands, of accounts in an organization, with drill-downs to generate insights 
at the account, bucket, or even prefix level.




Area:
=====
Global services.
Global Accelerator and CloudFront both use the AWS global network and its edge locations around the world.


UseCase:
========
-Deliver fast, secure websites
-Accelerate dynamic content delivery and APIs
-Stream live and on-demand video
-Distribute patches and updates


CloudFront is a good fit for HTTP use cases
Global Accelerator is a good fit for non-HTTP use cases, such as gaming (UDP), IoT (MQTT), or VoIP, 
as well as for HTTP use cases that require static IP addresses or deterministic, fast regional failover.

Caching:
CloudFront supports Edge caching
Global Accelerator does not support Edge Caching.

Geo-Targeting:
Geo-Targeting is a concept where businesses can show personalized content to their audience 
based on their geographic location without changing the URL. 

This helps you create customized content for the audience of a specific 
geographical area, keeping their needs in the forefront.




BillCost:
=========
AWS CloudFront is in Free Tier
Pricing varies by usage type, geographical region, and feature selection.

Charge for storage in an S3 bucket.
	Charge for serving objects from edge locations.
	Data Transfer Out
	Charge for submitting data to your origin.
	Data Transfer Out
	HTTP/HTTPS Requests
	Invalidation Requests,
	Dedicated IP Custom SSL certificates associated with a CloudFront distribution.
You also incur a surcharge for HTTPS requests, and an additional surcharge for requests that 
also have field-level encryption enabled




Security:
=========
Security is a shared responsibility.
Amazon CloudFront is a highly secure CDN that provides both network and application level protection. 
AWS Shield Standard, at no additional charge.






53
++CloudFront - price Classes
-------------------------------------------------
• CloudFront Edge locations are all around the world 
• The cost of data out per edge location varies

CloudFront – Price Classes:
• You can reduce the number of edge locations for cost reduction
Three price classes:
	1. Price Class All: all regions – best performance
	2. Price Class 200: most regions, but excludes the most expensive regions
	3. Price Class 100: only the least expensive regions






++CloudFront Cache Invalidation
-------------------------------------------------
• In case you update the back-end origin, CloudFront doesn’t know about it and will only get the
  refreshed content after the TTL has expired
• However, you can force an entire or partial cache refresh (thus bypassing the TTL) 
  by performing a CloudFront Invalidation.
• You can invalidate all files (*) or a special path (/images/*)


Global users for our application:
• You have deployed an application and have global users who want to access it directly.
• They go over the public internet, which can add a lot of latency due to many hops
• We wish to go as fast as possible through AWS network to minimize latency


Unicast IP vs Anycast IP:
• Unicast IP: one server holds one IP address.
• Anycast IP: all servers hold the same IP address and the client is routed to the nearest one.






54
##Global Accelerator 
-------------------------------------------------
In summary, Global Accelerator is a fast/reliable pipeline between user and application.

• Leverage the AWS internal network to route to your application
• 2 Anycast IP are created for your application(Two IP for availability !)
• The Anycast IP send traffic directly to Edge Locations
• The Edge locations send the traffic to your application


AWS Global Accelerator is a service in which you create accelerators to improve the performance of your applications 
for local and global users.

With Global Accelerator, you are provided two global static public IPs that act as a 
fixed entry point to your application,  improving availability. 

On the back end, add or remove your AWS application endpoints, 
such as Application Load Balancers, Network Load Balancers, EC2 Instances, 
and Elastic IPs without making user-facing changes. 

Global Accelerator automatically re-routes your traffic to your nearest healthy available endpoint 
to mitigate endpoint failure.

Consistent Performance:
• Intelligent routing to lowest latency and fast regional failover
• No issue with client cache (because the IP doesn’t change)
• Internal AWS network


AWS Global Accelerator vs CloudFront:
	• They both use the AWS global network and its edge locations around the world
	• Both services integrate with AWS Shield for DDoS protection.
• CloudFront
	• Improves performance for both cacheable content (such as images and videos)
	• Dynamic content (such as API acceleration and dynamic site delivery)
• Global Accelerator
	• Improves performance for a wide range of applications over TCP or UDP
	• Proxying packets at the edge to applications running in one or more AWS Regions.
	• Good fit for non-HTTP use cases, such as gaming (UDP), IoT (MQTT), or Voice over IP
	• Good for HTTP use cases that require static IP addresses
	• Good for HTTP use cases that required deterministic, fast regional failover


AWS Global Accelerator uses endpoint weights to determine the proportion of traffic 
that is directed to endpoints in an endpoint group, and traffic dials to control 
the percentage of traffic that is directed to an endpoint group (an AWS region where 
your application is deployed).


With AWS Global Accelerator, you can shift traffic gradually or all at once between the blue and the 
green environment and vice-versa without being subject to DNS caching on client devices and internet 
resolvers, traffic dials and endpoint weights changes are effective within seconds.



AWS Global Accelerator for availability and performance of your applications with local or global users.
CloudFront is better for improving application resiliency to handle spikes in traffic.

Direct Connect cannot be used to improve application resiliency to handle spikes in traffic.
 
 
AWS Global Accelerator is incorrect because this service is more suitable for non-HTTP use cases, 
such as gaming (UDP), IoT (MQTT), or Voice over IP, as well as for HTTP use cases that specifically 
require static IP addresses or deterministic, fast regional failover.
 
Moreover, there is no direct way that you can integrate AWS Global Accelerator with Amazon S3.

Area:
=====
Global Accelerator is a global service.



UseCase:
========
Global Accelerator also provides fast regional failover.



BillCost:
=========
In AWS Global Accelerator, you are charged for each accelerator that is provisioned and the amount of traffic in the dominant 
direction that flows through the accelerator.



Security:
=========
only 2 external IP need to be whitelisted
DDoS protection  AWS Shield.







==| Advanced Storage on AWS
55
##Snow Family
-------------------------------------------------
AWS Snow Family: offline devices to perform data migrations
If it takes more than a week to transfer over the network, use Snowball devices.

We setup a Snowball Edge/Snowcone device to do edge computing.


Highly-secure, portable devices to collect and process data at the edge, and migrate data into and out of AWS. 
It is a cost-effective way to transfer data offline. 


AWS snow family service model is of three types: 

AWS snow cone: 
	Snow cone is the most compact and portable device weighing in at 4.5 pounds (2.1kg). 
	It is available in HDD and SDD options. Snowcone is ruggedized, secure, and purpose-built for use outside of a 
	traditional data centre. 
	Can be sent back to AWS offline, or connect it to internet and use AWS DataSync to send data.


AWS snowball: 
	Snowball is available as compute optimized device and storage optimized device. 
	These devices are suited for extreme conditions, tamper-proof, and highly secure. 
	Use cases: large data cloud migrations, DC decommission, disaster recovery
	Pay per data transfer job.

AWS snowmobile: 
	AWS snowmobile is an Exabyte-scale data migration device which is used to move extremely large data. 
	It migrates up to 100PB in a 45-foot-long ruggedized shipping container, pulled by a semi-trailer truck. 
	• High security: temperature controlled, GPS, 24/7 video surveillance
	• Better than Snowball if you transfer more than 10 PB


Snow Family – Edge Computing:
• Process data while it’s being created on an edge location
• A truck on the road, a ship on the sea, a mining station underground...
• These locations may have
	• Limited / no internet access
	• Limited / no easy access to computing power
• We setup a Snowball Edge / Snowcone device to do edge computing
• Use cases of Edge Computing:
	• Preprocess data
	• Machine learning at the edge
	• Transcoding media streams
	
• Snowcone (smaller,  online and offline)
	• 2 CPUs, 4 GB of memory, wired or wireless access
	• USB-C power using a cord or the optional battery
	• Edge computing
	• 8 TBs of usable storage
	Can be sent back to AWS offline, or connect it to internet and use AWS DataSync to send data
• Snowball Edge(offline) – Compute Optimized
	• 52 vCPUs, 208 GiB of RAM
	• Optional GPU (useful for video processing or machine learning)
	• 42 TB usable storage, 
	• 42 TB of HDD capacity for block volume and S3 compatible object storage
• Snowball Edge(offline) – Storage Optimized
	• Up to 40 vCPUs, 80 GiB of RAM
	• 80 TB of HDD capacity for block volume and S3 compatible object storage
	• Object storage clustering available, 80 TB HDD
• All: Can run EC2 Instances & AWS Lambda functions (using AWS IoT Greengrass)
• Long-term deployment options: 1 and 3 years discounted pricing


For datasets less than 10PB or distributed in multiple locations, you should use Snowball. 


AWS OpsHub (a software you install on your computer / laptop) to manage your Snow Family Device.
Snowball cannot import to Glacier directly.
You must use Amazon S3 first, in combination with an S3 lifecycle policy.


Snow Family – Usage Process:
1. Request Snowball devices from the AWS console for delivery
2. Install the snowball client / AWS OpsHub on your servers
3. Connect the snowball to your servers and copy files using the client
4. Ship back the device when you’re done (goes to the right AWS facility)
5. Data will be loaded into an S3 bucket
6. Snowball is completely wiped.



Area:
=====
Global.


UseCase:
========
Process and analyze data locally: Run Amazon Machine Images (AMIs) on Amazon EC2 and distribute 
AWS Lambda code using machine learning (ML) or other applications on Snowball Edge devices.


BillCost:
=========
Snowmobile pricing is based on the amount of data stored on the truck per month.

With AWS Snowball, you pay only for your use of the device and for data transfer out of AWS. 
There are two ways to pay for AWS Snowball: on-demand and committed upfront pricing.



Security:
=========
All data moved to AWS Snow Family devices is automatically encrypted with 256-bit encryption keys that are 
managed by the AWS Key Management Service (KMS). 
Encryption keys are never stored on the device so your data stays secure during transit.







56
##FSx | ++Amazon FSx
-------------------------------------------------
Launch 3rd party high-performance file systems on AWS
Four widely-used file systems: 
 - NetApp ONTAP, 
 - OpenZFS, 
 - Windows File Server 
 - Lustre.
 

FSx for Windows(Service Message Block (SMB) protocol):
FSx for Window Supports Microsoft's Distributed File System (DFS).
AWS Managed Microsoft AD does not support Microsoft’s Distributed File System (DFS).
Microsoft SQL Server on AWS does not support Microsoft’s Distributed File System (DFS).

	Amazon FSx for Windows is accessible from Windows, Linux, and MacOS compute instances and devices. 
	Thousands of compute instances and devices can access a file system concurrently. 
	Amazon FSx File Gateway provides low-latency, on-premises access to fully managed file shares in the cloud.

	• FSx for Windows is a fully managed Windows file system share drive
	• Supports SMB protocol & Windows NTFS
	• Microsoft Active Directory integration, ACLs, user quotas
	• Can be mounted on Linux EC2 instances
	• Supports Microsoft's Distributed File System (DFS) Namespaces (group files across multiple FS)

	• Scale up to 10s of GB/s, millions of IOPS, 100s PB of data
	• Storage Options:
		• SSD – latency sensitive workloads (databases, media processing, data analytics, …)
		• HDD – broad spectrum of workloads (home directory, CMS, …)
	• Can be accessed from your on-premises infrastructure (VPN or Direct Connect)
	• Can be configured to be Multi-AZ (high availability)
	• Data is backed-up daily to S3


FSx for Windows does not allow you to present S3 objects as files and does not allow you to 
write changed data back to S3. 
Therefore you cannot reference the "cold data" with quick access for reads and updates at low cost.

 

Amazon FSx for Lustre:
 FSx for Lustre does not support Microsoft’s Distributed File System (DFS), so this option is incorrect.
	• Lustre is a type of parallel distributed file system, for large-scale computing
	• The name Lustre is derived from “Linux” and “cluster
	• Machine Learning, High Performance Computing (HPC)
	• Video Processing, Financial Modeling, Electronic Design Automation
	• Scales up to 100s GB/s, millions of IOPS, sub-ms latencies
	• Storage Options:
		• SSD – low-latency, IOPS intensive workloads, small & random file operations
		• HDD – throughput-intensive workloads, large & sequential file operations
		• Seamless integration with S3, r/w (through FSx)
		• Can be used from on-premises servers (VPN or Direct Connect)

FSx Lustre - File System Deployment Options:
•Scratch File System
	• Temporary storage
	• Data is not replicated (doesn’t persist if file server fails)
	• High burst (6x faster, 200MBps per TiB)
	• Usage: short-term processing, optimize costs
• Persistent File System
	• Long-term storage
	• Data is replicated within same AZ
	• Replace failed files within minutes
	• Usage: long-term processing, sensitive data.

FSx for Lustre integrates with Amazon S3.
FSx for Lustre provides the ability to both process the 'hot data' in a parallel and distributed fashion 
as well as easily store the 'cold data' on Amazon S3.


Amazon FSx for NetApp ONTAP:
	• Managed NetApp ONTAP on AWS
	• File System compatible with NFS, SMB, iSCSI protocol
	• Move workloads running on ONTAP or NAS to AWS
	• Works with:
		• Linux
		• Windows
		• MacOS
		• VMware Cloud on AWS
		• Amazon Workspaces & AppStream 2.0
		• Amazon EC2, ECS and EKS
	• Storage shrinks or grows automatically
	• Snapshots, replication, low-cost, compression and data de-duplication
	• Point-in-time instantaneous cloning (helpful for testing new workloads)



Amazon FSx for OpenZFS:
	• Managed OpenZFS file system on AWS
	• File System compatible with NFS (v3, v4, v4.1, v4.2)
	• Move workloads running on ZFS to AWS
	• Works with:
		• Linux
		• Windows
		• MacOS
		• VMware Cloud on AWS
		• Amazon Workspaces & AppStream 2.0
		• Amazon EC2, ECS and EKS
	• Up to 1,000,000 IOPS with < 0.5ms latency
	• Snapshots, compression and low-cost
	• Point-in-time instantaneous cloning (helpful for testing new workloads)



Hybrid Cloud for Storage:
• S3 is a proprietary storage technology (unlike EFS / NFS), so how do you expose the S3 data on-premises?
  Ans: AWS Storage Gateway.


AWS Storage Cloud Native Options:
	Block: Amazon EBS, EC2 Instance Store
	File: Amazon EFS, Amazon FSx 
	Object:Amazon S3, Amazon Glacier




Area:
=====
Regional Service.



UseCase:
========
Migrate Windows file servers to AWS
Accelerate hybrid workloads
Simplify virtual desktops and streaming



BillCost:
=========
With Amazon FSx for Windows File Server, you pay only for the resources you use and there are no minimum 
fees or setup charges. 
While pricing is quoted on a monthly basis, your usage is prorated by the hour, and you are billed for 
your average usage over a month.



Security:
=========
You can configure security group rules and control network access to your Amazon FSx file systems.
Amazon FSx is integrated with AWS Identity and Access Management (IAM).
You can also tag your Amazon FSx resources and control the actions that your 
IAM users and groups can take based on those tags.

AWS FSx was designed to meet high security standards. 
The service has ISO, PCI-DSS, and SOC certifications, and is also HIPAA eligible.







57
##Storage Gateway
-------------------------------------------------
Bridge between on-premises data to AWS, no aws-to-aws

Storage Gateway provides a standard set of storage protocols such as 
iSCSI, SMB, and NFS, which allow you to use AWS storage without rewriting your existing applications. 



SMB or NFS interface->FileGateway
iSCSI block storage-> Volume Gateway

File gateway offers SMB or NFS-based access to data in Amazon S3 with local caching.

Use cases: disaster recovery, backup & restore, tiered storage.

3 types of Storage Gateway:
	• File Gateway
	• Volume Gateway
	• Tape Gateway
	
Exam Tip: You need to know the differences between all 3!
AWS Storage Gateway doesn’t support EFS filesystem.

++
• S3 File Gateway
• FSx File Gateway
• Volume Gateway
• Tape Gateway



S3 File Gateway:
Used for flat files only, stored directly on S3.
	• Configured S3 buckets are accessible using the NFS and SMB protocol
	• Most recently used data is cached in the file gateway
	• Supports S3 Standard, S3 Standard IA, S3 One Zone A, S3 Intelligent Tiering
	• Transition to S3 Glacier using a Lifecycle Policy
	• Bucket access using IAM roles for each File Gateway
	• SMB Protocol has integration with Active Directory (AD) for user authentication

You can manage your Amazon S3 data using lifecycle policies, cross-region replication, and versioning. 
You can think of a file gateway as a file system mount on S3.




FSx File Gateway:
	File Gateway synchronizes changed data to FSx for Windows File Server in the background. 
	With these capabilities, you can consolidate all of your on-premises file share data in AWS on FSx for 
	Windows File Server and benefit from protected, resilient, fully managed file systems.
		• Native access to Amazon FSx for Windows File Server
		• Local cache for frequently accessed data
		• Windows native compatibility (SMB, NTFS, Active Directory...)
		• Useful for group file shares and home directories




Volume Gateway:
	• Block storage using iSCSI protocol backed by S3
	• Backed by EBS snapshots which can help restore on-premises volumes!
		• Cached volumes: low latency access to most recent data
		• Stored volumes: entire dataset is on premise, scheduled backups to S3
Cached Volume mode – the entire dataset is stored on S3, and a cache of the most frequently 
accessed data is cached on-site.
Stored Volume mode – the entire dataset is stored on-site and is asynchronously backed up to 
S3 (EBS point-in-time snapshots). 
Snapshots are incremental and compressed.



Tape Gateway:
	• Some companies have backup processes using physical tapes (!)
	• With Tape Gateway, companies use the same processes but, in the cloud
	• Virtual Tape Library (VTL) backed by Amazon S3 and Glacier
	• Back up data using existing tape-based processes (and iSCSI interface)
	• Works with leading backup software vendors 

It is supported by NetBackup, Backup Exec, Veeam, etc. 
Instead of using physical tape, they are using virtual tape,  and these virtual tapes are 
further stored in Amazon S3.


Storage Gateway – Hardware appliance:
	• Using Storage Gateway means you need on-premises virtualization
	• Otherwise, you can use a Storage Gateway Hardware Appliance
	• You can buy it on amazon.com

	• Works with File Gateway, Volume Gateway, Tape Gateway
	• Has the required CPU, memory, network, SSD cache resources
	• Helpful for daily NFS backups in small data centers

Needs additional setup on-premises



File gateway offers SMB or NFS-based access to data in Amazon S3 with local caching.

Volume Gateway to present cloud-based iSCSI block storage volumes to your on-premises applications. 
Volume Gateway does not support NFS interface.
Tape Gateway does not support NFS interface.


Area:
=====
Storage Gateway – Regional
AWS Storage Gateway stores volume, snapshot, and tape data in the AWS region in which the gateway is activated



UseCase:
========
Provide cloud-based backup for on-premises files and database applications for low-cost, 
virtually unlimited scale.



BillCost:
=========
This pricing is based on data transferred "in" to and "out" of the AWS Storage Gateway Service by  gateway, 
and it varies by region and gateway host. 



Security:
=========
All data transferred between the gateway and AWS storage is encrypted using SSL.







58
##Transfer Famamily 
-------------------------------------------------
A fully-managed service for file transfers into and out of Amazon S3 or Amazon EFS using the FTP protocol


The data transfer between AWS Transfer Family servers and Amazon S3 happens over internal AWS networks and doesn’t 
traverse the public internet. 

Because of this, you do not need to use AWS PrivateLink for data transfered from the 
AWS Transfer Family server to Amazon S3.

• Supported Protocols
	• AWS Transfer for FTP (File Transfer Protocol (FTP))
	• AWS Transfer for FTPS (File Transfer Protocol over SSL (FTPS))
	• AWS Transfer for SFTP (Secure File Transfer Protocol (SFTP))
• Managed infrastructure, Scalable, Reliable, Highly Available (multi-AZ)
• Pay per provisioned endpoint per hour + data transfers in GB
• Store and manage users’ credentials within the service
• Integrate with existing authentication systems (Microsoft Active Directory, LDAP, Okta, 
  Amazon Cognito, custom)


Seamlessly migrate, automate, and monitor your file transfer workflows into and out of 
Amazon S3 and Amazon EFS using the SFTP, FTPS, and FTP protocols.


AWS Transfer Family supports the following clients:
OpenSSH (macOS and Linux)
WinSCP (Microsoft Windows only)
Cyberduck (Windows, macOS, and Linux)
FileZilla (Windows, macOS, and Linux)


File Transfer Workflows – MFTW:
	1)File Transfer Workflows – MFTW is a fully managed, serverless File Transfer Workflow service 
	    that makes it easy to set up, run, automate, and monitor the processing of uploaded files.
	2)MFTW can be used to automate various processing steps such as copying, tagging, scanning, filtering, 
		compressing/decompressing, and encrypting/decrypting the data that is transferred using Transfer Family.
	3)MFTW provides end-to-end visibility for tracking and auditability.



Area:
=====
Regional.


UseCase:
========
sharing files, public datasets, CRM, ERP,


BillCost:
=========
You pay only for access-enabled protocols, the amount of data (gigabytes) uploaded and downloaded 
over SFTP/FTPS/FTP, or the number of messages sent or received over AS2. 

Additional charges:
You are charged standard storage request and data transfer rates to store, read, and write 
to and from Amazon S3. 
You are also charged standard rates for domain name lookups using Amazon Route53, Amazon CloudTrail, and 
Amazon CloudWatch Logs and Events.



Security:
=========
Security is a shared responsibility.






##DataSync
-------------------------------------------------
Schedule data sync from on-premises to AWS, or AWS to AWS.
DataSync makes it easier to move data over the network between storage systems and services.

S3-to-S3
AWS DataSync copy data and metadata between AWS Storage Services

• Move large amount of data to and from
	• On-premises / other cloud to AWS (NFS, SMB, HDFS, S3 API…) – needs agent
	• AWS to AWS (different storage services) – no agent needed
• Can synchronize to:
	• Amazon S3 (any storage classes – including Glacier)
	• Amazon EFS
	• Amazon FSx (Windows, Lustre, NetApp, OpenZFS...)
• Replication tasks can be scheduled hourly, daily, weekly
• File permissions and metadata are preserved (NFS POSIX, SMB…)
• One agent task can use 10 Gbps, can setup a bandwidth limit

In order to move files directly into your Amazon VPC, DataSync supports VPC endpoints 
(powered by AWS PrivateLink).

When you use DataSync with S3, it supports storing data directly into any S3 storage class. 
You do not need to use lifecycle policies or manually transfer data from S3.

ExamTips:AWS DataSync copy data and metadata between AWS Storage Services



S3 to EFS then AWS Datasync is the service to go for Datasync, as AWS Storage Gateway doesn’t support 
transfer within cloud.

If you want to transfer on-premises block/tapes to AWS, then AWS Storage Gateway is the option to go with.

 When the requirement is to transfer files from on premises to AWS EFS (elastic file system) 
 then AWS Datasync is the option as AWS Storage Gateway doesn’t support EFS.


Datasync can also be used to move or sync data between AWS storage services like S3, 
EFS or FSx in the same or different accounts.


You can connect Datasync to NFS (Network File System) shares, SMB (Server Message Block) shares, 
Hadoop distributed file systems, self managed object storage, AWS Snowcone and move data to S3, 
Amazon elastic file systems, FSx for Windows file server and FSx for Lustre file systems.


++All Aws Storage Comparison
-------------------------------------------------
Storage Comparison:
• S3: Object Storage
• S3 Glacier: Object Archival
• EBS volumes: Network storage for one EC2 instance at a time
• Instance Storage: Physical storage for your EC2 instance (high IOPS)
• EFS: Network File System for Linux instances, POSIX filesystem
• FSx for Windows: Network File System for Windows servers
• FSx for Lustre: High Performance Computing Linux file system
• FSx for NetApp ONTAP: High OS Compatibility
• FSx for OpenZFS: Managed ZFS file system
• Storage Gateway: S3 & FSx File Gateway, Volume Gateway (cache & stored), Tape Gateway
• Transfer Family: FTP, FTPS, SFTP interface on top of Amazon S3 or Amazon EFS
• DataSync: Schedule data sync from on-premises to AWS, or AWS to AWS
• Snowcone / Snowball / Snowmobile: to move large amount of data to the cloud, physically
• Database: for specific workloads, usually with indexing and querying






==| AWS Integration & Messaging
59
##Decouple your applications:
-------------------------------------------------
There are two patterns of application communication:
	1) Synchronous communications (application-application)
	2) Asynchronous / Event based (application-queue-application)
	
• Synchronous between applications can be problematic if there are sudden spikes of traffic
• What if you need to suddenly encode 1000 videos but usually it’s 10?


In that case, it’s better to decouple your applications,
	• using SQS: queue model
	• using SNS: pub/sub model
	• using Kinesis: real-time streaming model
These services can scale independently from our application!

SQS, SNS are “cloud-native” services, and they’re using proprietary protocols from AWS.
Traditional applications running from on-premises may use open protocols such as: MQTT, AMQP, STOMP, 
Openwire, WSS



60
##SQS Queues
-------------------------------------------------
Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and 
scale microservices, distributed systems, and serverless applications.

You cannot set a priority to individual items in the SQS queue.

Attributes:
	• Unlimited throughput, unlimited number of messages in queue
	• Default retention of messages: 4 days, maximum of 14 days
	• Low latency (<10 ms on publish and receive)
	• Limitation of 256KB per message sent
• Can have duplicate messages (at least once delivery, occasionally)
• Can have out of order messages (best effort ordering)


Produced:
	• Produced to SQS using the SDK (SendMessage API)
	• The message is persisted in SQS until a consumer deletes it
	• Message retention: default 4 days, up to 14 days


Consuming:
Consumers receive and process messages in parallel
We can scale consumers horizontally to improve throughput of processing

	• Consumers (running on EC2 instances, servers, or AWS Lambda)…
	• Poll SQS for messages (receive up to 10 messages at a time)
	• Process the messages (example: insert the message into an RDS database)
	• Delete the messages using the DeleteMessage API


Message Visibility Timeout:
	• After a message is polled by a consumer, it becomes invisible to other consumers
	• By default, the “message visibility timeout” is 30 seconds
	• That means the message has 30 seconds to be processed consumer need more time to process then consumer 
	  clud call ChangeMessageVisible API to  get more time.
	• After the message visibility timeout is over, the message is “visible” in SQS again for every consumer



Dead Letter Queue:
	• We can set a threshold of how many times a message can go back to the queue
	• After the MaximumReceives threshold is exceeded, the message goes into a dead letter queue (DLQ)
	• Useful for debugging!  Good to set a retention of 14 days in the DLQ



Delay Queue:
	• Delay a message (consumers don’t see it immediately) up to 15 minutes
	• Default is 0 seconds (message is available right away)
	• Can set a default at queue level
	• Can override the default on send using the DelaySeconds parameter



Long Polling:
	• When a consumer requests messages from the queue, it can optionally “wait” for messages to
	  arrive if there are none in the queue
	• This is called Long Polling
	• LongPolling decreases the number of API calls made to SQS while increasing the efficiency and
	  reducing latency of your application
	• The wait time can be between 1 sec to 20 sec (20 sec preferable)



SQS – Request-Response Systems:
• To implement this pattern: use the SQS Temporary Queue Client
• It leverages virtual queues instead of creating / deleting SQS queues (cost-effective)


FIFO Queue:
	• FIFO = First In First Out (ordering of messages in the queue)
	• Limited throughput: 300 msg/s without batching, 3000 msg/s with batching
	• Exactly-once send capability (by removing duplicates)
	• Messages are processed in order by the consumer


You can't convert an existing standard queue into a FIFO queue.
 
By default 300 Opration per second, if a 4 msg can process per operation then its will be 1200 Op per second !

By default, FIFO queues support up to 300 messages per second (300 send, receive, or delete operations per second). 

When you batch 10 messages per operation (maximum), FIFO queues can support up to 
3,000 messages per second. Therefore you need to process 4 messages per operation so that the 
FIFO queue can support up to 1200 messages per second, which is well within the peak rate.

If you require higher throughput, you can enable high throughput mode for FIFO on the 
Amazon SQS console, which will support up to 30,000 messages per second with batching, or up to 
3,000 messages per second without batching.



Amazon SQS has automatically deleted the messages that have been in a queue for more than the maximum message retention period.
Default message retention period is 4 days.


Amazon SQS uses short polling by default.
Short polling works for scenarios that require higher throughput. 
However, you can also configure the queue to use Long polling instead, to reduce cost.

The ReceiveMessageWaitTimeSeconds is the queue attribute that determines whether you are using 
Short or Long polling. By default, its value is zero which means it is using Short polling. 
If it is set to a value greater than zero, then it is Long polling.

The visibility timeout is a period of time during which Amazon SQS prevents other 
consuming components from receiving and processing a message.
The default visibility timeout for a message is 30 seconds. The maximum is 12 hours.
 
Amazon SQS doesn’t automatically delete the message

Area:
=====
SQS is a regional service, that is highly available within a single region. 
There is no cross-region replication capability.



UseCase:
========
SQS as a buffer to database writes
SQS to decouple between application tiers

Eliminate administrative overhead
Reliably deliver messages
Scale elastically and cost-effectively



BillCost:
=========
Amazon SQS Free Tier
Amazon SQS has no upfront costs. The first million monthly requests are free. 
After that, you pay based on the number and content of requests, and the interactions with 
Amazon S3 and the AWS Key Management Service.



Security:
=========
•Encryption:
	• In-flight encryption using HTTPS API
	• At-rest encryption using KMS keys
	• Client-side encryption if the client wants to perform encryption/decryption itself
• Access Controls: IAM policies to regulate access to the SQS API
• SQS Access Policies (similar to S3 bucket policies)
	• Useful for cross-account access to SQS queues
	• Useful for allowing other services (SNS, S3…) to write to an SQS queue
As a managed service, Amazon SQS is protected by the AWS global network security.







++SQS+Auto Scaling Group
-------------------------------------------------
CloudWatch Metric – Queue Length ApproximateNumberOfMessages and scale EC2




61
##SNS (Simple Notification Service)
--------------------------------------------------
Anytime multiple services need to receive the same event, you should consider SNS rather than SQS.

SNS cannot directly send messages to Kinesis Data Streams.
You can use to deliver messages to Amazon Kinesis Data Firehose. 

You would need to write a Lambda function that would send the message to a Kinesis stream.
Lambda function will not "fetch from SNS". Rather, the Lambda function will be triggered by SNS, 
with the message being passed as input. 
Your Lambda function will then need to send the message to Kinesis.

It provides much more functionality than just the ability to send push notifications (emails, SMS, and mobile push). 
In fact, it’s a serverless publish-subscribe messaging system allowing to send events to multiple 
applications (subscribers) at the same time (fan-out), 
including SQS queues, Lambda functions, Kinesis Data Streams, and generic HTTP endpoints. 

In order to use the service, we only need to:
create a topic,
subscribe to a topic,
confirm the subscription,
start sending events to a topic to deliver them to all subscribers (potentially multiple applications and people).


The event producer only sends a message to one SNS topic at a time
Many event receivers can be created to listen to an SNS topic notification

• The “event producer” only sends message to one SNS topic
• As many “event receivers” (subscriptions) as we want to listen to the SNS topic notifications
• Each subscriber to the topic will get all the messages (note: new feature to filter messages)
• Up to 12,500,000 subscriptions per topic
• 100,000 topics limit


Many AWS services can send data directly to SNS for notifications:
	CloudWatch Alarms
	S3 Bucket (Events)
	Auto Scaling Group (Notifications)
	CloudFormation (State Changes)
	AWS Budgets Lambda
	AWS DMS (New Replic)
	DynamoDB
	RDS Events
	 

62
##SNS and SQS | FanOut Pattern
-------------------------------------------------
Using Amazon SNS topics, your publisher systems can fanout messages to a 
large number of subscriber systems, including Amazon SQS queues, AWS Lambda functions, HTTPS endpoints, and 
Amazon Kinesis Data Firehose,  for parallel processing. 
The A2P functionality enables you to send messages to users at scale via SMS, mobile push, and email.

Take note that you should use SNS instead of SES (Simple Email Service) when you want to monitor your EC2 instances.


Fan Out:
• Push once in SNS, receive in all SQS queues that are subscribers
• Fully decoupled, no data loss
• Make sure your SQS queue access policy allows for SNS to write

By default, an Amazon SNS topic subscriber receives every message published to the topic. 
You can use Amazon SNS message filtering to assign a filter policy to the topic subscription, 
and the subscriber will only receive a message that they are interested in. 



SNS – FIFO Topic:
	• FIFO = First In First Out (ordering of messages in the topic).
	• Ordering by Message Group ID (all messages in the same group are ordered).
	• Deduplication using a Deduplication ID or Content Based Deduplication.
	• Can only have SQS FIFO queues as subscribers.
	• Limited throughput (same throughput as SQS FIFO).
	
You can use Amazon SNS FIFO (first in, first out) topics and Amazon Simple Queue Service 
(Amazon SQS) FIFO queues together to provide strict message ordering and message deduplication.


Message Filtering:
	• JSON policy used to filter messages sent to SNS topic’s subscriptions
	• If a subscription doesn’t have a filter policy, it receives every message





Area:
=====
Regioinal.



UseCase:
========
Application integration
Application alerts
User notifications:Amazon SNS can send push email messages and text messages (SMS messages) to individuals or groups.
Mobile push notifications.

Simplify and reduce costs with message filtering and batching
Amazon SNS FIFO topics work with Amazon SQS FIFO queues to ensure messages are delivered in a strictly-ordered 
manner and are only processed once.
Increase security with message encryption and privacy.



BillCost:
=========
The Free Tier 
You pay based on the number of messages that you publish, the number of notifications that you 
deliver, and any additional API calls for managing topics and subscriptions. 
Delivery pricing varies by endpoint type.



Security:
=========

Amazon SNS provides encrypted topics to protect your messages from unauthorized access. 
The encryption uses a 256-bit AES-GCM algorithm and a customer master key (CMK) issued 
with AWS Key Management Service (KMS). 
Amazon SNS also supports VPC endpoints via AWS PrivateLink, so you can privately publish messages 
to Amazon SNS topics, from an 
Amazon Virtual Private Cloud (VPC) subnet, without traversing the Internet.







63
##Kinesis
-------------------------------------------------
A Kinesis data stream stores records from 24 hours by default to a maximum of 8760 hours (365 days).

Amazon Kinesis is the streaming data platform of AWS and has four distinct services under it: 
Kinesis Data Firehose, Kinesis Data Streams, Kinesis Video Streams, and Amazon Kinesis Data Analytics. 


Makes it easy to collect, process, and analyze streaming data in real-time and that can be run on EC2 instances.
Kinesis Data Streams is a serverless.
Ingest real-time data such as: Application logs, Metrics, Website clickstreams,IoT telemetry data.

• Kinesis Data Streams: capture, process, and store data streams
• Kinesis Data Firehose: load data streams into AWS data stores
• Kinesis Data Analytics: analyze data streams with SQL or Apache Flink
• Kinesis Video Streams: capture, process, and store video streams


The producers continually push data to Kinesis Data Streams, and the consumers process the data in real time. 
Consumers (such as a custom application running on Amazon EC2 or an Amazon 
Kinesis Data Firehose delivery stream) 
can store their results using an AWS service such as Amazon DynamoDB, Amazon Redshift, or Amazon S3.


Kinesis Data Streams:
Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. 
By default, the 2MB/second/shard output is shared between all of the applications consuming data from the stream.
consumers to use enhanced fan-out and receive their own 2MB/second pipe of read throughput per shard.


Kinesis Data Streams cannot directly write the output to S3. 
Unlike Firehose, KDS does not offer a ready-made integration via an intermediary 
Lambda function to reliably dump data into S3. 

You will need to do a lot of custom coding to get the Lambda function to process the 
incoming stream and then store the transformed output to S3.

• Retention between 1 day to 365 days
• Ability to reprocess (replay) data,
  you can replay messages, or have multiple consumers that are subscribing to your Kinesis Stream.
• Once data is inserted in Kinesis, it can’t be deleted (immutability)
• Data that shares the same partition goes to the same shard (ordering)
	• Producers: AWS SDK, Kinesis Producer Library (KPL), Kinesis Agent
	• Consumers:
		• Write your own: Kinesis Client Library (KCL), AWS SDK
		• Managed: AWS Lambda, Kinesis Data Firehose, Kinesis Data Analytics.


Provisioned mode:
	• You choose the number of shards provisioned, scale manually or using API
	• Each shard gets 1MB/s in (or 1000 records per second)
	• Each shard gets 2MB/s out (classic or enhanced fan-out consumer)
	• You pay per shard provisioned per hour
On-demand mode:
	• No need to provision or manage the capacity
	• Default capacity provisioned (4 MB/s in or 4000 records per second)
	• Scales automatically based on observed throughput peak during the last 30 days
	• Pay per stream per hour & data in/out per GB



Kinesis Data Streams Security:
	• Control access/authorization using IAM policies
	• Encryption in-flight using HTTPS endpoints
	• Encryption at rest using KMS
	• You can implement encryption/decryption of data on client side (harder)
	• VPC Endpoints available for Kinesis to access within VPC
	• Monitor API calls using CloudTrail


Kinesis Data Firehose:
	 Kinesis Data Firehose is the easiest way to reliably load streaming data into data lakes, 
	 data stores, and analytics tools, Not application.

Amazon Kinesis Data Firehose captures and loads data in near real time. It loads new data into Amazon S3, 
Amazon Redshift, and Amazon OpenSearch Service within 60 seconds after the data is sent to the service. 

Load streaming data into S3 / Redshift / ElasticSearch / 3rd party / custom HTTP.
	• Fully managed
	• Near real-time (buffer time min. 60 sec)
	• Automatic scaling
	• No data storage
	• Doesn’t support replay capability

Kinesis Data Firehose can only write to S3, Redshift, ElasticSearch or Splunk. 
You can't have applications consuming data streams from Kinesis Data Firehose, 
that's the job of Kinesis Data Streams (Write your own: Kinesis Client Library (KCL), AWS SDK).

Pay for data going through Firehose
Perform real-time analytics on Kinesis Streams using SQL
 
It can also batch, compress, and encrypt the data before loading it, 
minimizing the amount of storage used at the destination and increasing security.


Kinesis Data Analytics (SQL application):
	• Perform real-time analytics on Kinesis Streams using SQL
	• Fully managed, no servers to provision
	• Automatic scaling
	• Real-time analytics
	• Pay for actual consumption rate
	• Can create streams out of the real-time queries
• Use cases:
	• Time-series analytics
	• Real-time dashboards
	• Real-time metrics
	
Kinesis Data Analytics cannot directly ingest data from the source, as it ingests data either from 
Kinesis Data Streams or Kinesis Data Firehose.

The four most common use cases are streaming 
	extract-transform-load (ETL), 
	continuous metric generation, 
	responsive real-time analytics, and 
	interactive querying of data streams.


Ordering data into Kinesis:
• Imagine you have 100 trucks (truck_1, truck_2, … truck_100) on the road sending their GPS positions regularly into AWS.
• You want to consume the data in order for each truck, so that you can track their movement accurately.
• How should you send that data into Kinesis?
	• Answer: send using a “Partition Key” value of the “truck_id”
	• The same key will always go to the same shard.



Ordering data into SQS:
• For SQS standard, there is no ordering.
• For SQS FIFO, if you don’t use a Group ID, messages are consumed in the order they are sent, 
  with only one consumer
• You want to scale the number of consumers, but you want messages to be “grouped” when they 
  are related to each other.
• Then you use a Group ID (similar to Partition Key in Kinesis)



SQS vs SNS vs Kinesis:

SQS:
• Consumer “pull data”
• Data is deleted after being consumed
• Can have as many workers (consumers) as we want
• No need to provision throughput
• Ordering guarantees only on FIFO queues
• Individual message delay capability

As multiple applications are consuming the same stream concurrently, both SQS Standard and 
SQS FIFO are not the right.

Message Replayability:
Because consumers usually will delete the message once it processed, there is no message replayability in SQS.

At-least-once delivery guarantee:multiple attempts are made at delivering it.
In more casual terms this means that messages may be duplicated but not lost.



SNS:
• Push data to many subscribers
• Up to 12,500,000 subscribers
• Data is not persisted (lost if not delivered)
• Pub/Sub
• Up to 100,000 topics
• No need to provision throughput
• Integrates with SQS for fan- out architecture pattern
• FIFO capability for SQS FIFO



Kinesis:
	• Standard: pull data
	• 2 MB per shard
	• Enhanced-fan out: push data
	• 2 MB per shard per consumer
	• Possibility to replay data
	• Meant for real-time big data, analytics and ETL
	• Ordering at the shard level
	• Data expires after X days
	• Provisioned mode or on-demand capacity mode



However, the user is expected to manually provision an appropriate number of shards to 
process the expected volume of the incoming data stream. 
The throughput of an 
Amazon Kinesis data stream is designed to scale without limits via increasing the number of 
shards within a data stream. 

Area:
=====
Regional.


UseCase:
========
Kinesis Video Streams
Kinesis Data Streams
Real-time

• Time-series analytics
• Real-time dashboards
• Real-time metrics


BillCost:
=========
You pay only for the volume of data you ingest, store, and consume in your video streams.



Security:
=========
It is a Fully-managed, serverless service.
Security is a shared responsibility.





64
##Amazon MQ
-------------------------------------------------
Amazon MQ is a managed message broker service for Apache ActiveMQ and RabbitMQ that makes it easy to set up and operate 
message brokers on AWS.

• SQS, SNS are “cloud-native” services, and they’re using proprietary protocols from AWS.
• Traditional applications running from on-premises may use open protocols such as: MQTT, AMQP, STOMP, Openwire, WSS
• When migrating to the cloud, instead of re-engineering the application to use SQS and SNS, we can use Amazon MQ
• Amazon MQ = managed Apache ActiveMQ
• Amazon MQ doesn’t “scale” as much as SQS / SNS
• Amazon MQ runs on a dedicated machine, can run in HA with failover
• Amazon MQ has both queue feature (~SQS) and topic features (~SNS)

How do I migrate if I'm using a different message broker instead of ActiveMQ or RabbitMQ?
Amazon MQ provides compatibility with the most common messaging APIs, such as Java Message Service (JMS) and 
.NET Message Service (NMS), and protocols, including AMQP, STOMP, MQTT, and WebSocket. 

This makes it easy to switch from any standards-based message broker to Amazon MQ without rewriting the messaging code 
in your applications. 
In most cases, you can simply update the endpoints of your Amazon MQ broker to connect to your existing applications, 
and start sending messages.



A cluster deployment is a logical grouping of three RabbitMQ broker nodes behind a Network Load Balancer, 
each sharing users, queues, and a distributed state across multiple Availability Zones (AZ). 
In a cluster deployment, Amazon MQ automatically manages broker policies to enable classic mirroring across all nodes, 
ensuring high availability (HA).


Area:
=====
Regional.



UseCase:
========
Migration MQ service to AWS.


BillCost:
=========
Amazon MQ is free to try.
You pay for the time your message broker instance runs, the storage you use monthly, and standard data transfer fees**

Broker Pricing Hourly instance usage
Broker Storage Pricing Monthly storage usag


Security:
=========
It is a Fully-managed, serverless service.
Security is a shared responsibility.





==| Container Section
65
##ECS
-------------------------------------------------
Amazon ECS is a fully managed container orchestration service that makes it easy for you to 
deploy, manage, and  scale containerized applications.

Launch Docker containers on AWS = Launch ECS Tasks on ECS Clusters


You can’t directly set CloudWatch Alarms to update the ECS task count.


Amazon ECS lets you run batch workloads with managed or custom schedulers on  
EC2 On-Demand Instances, Reserved Instances, or Spot Instances.

You can launch Reserved EC2 instances to process the mission-critical data and 
Spot EC2 instances for processing non-essential batch jobs.

There are two different charge models for Amazon Elastic Container Service (ECS): 
Fargate Launch Type Model and EC2 Launch Type Model. 
	With Fargate, you pay for the amount of vCPU and memory resources that your containerized application requests.
	EC2 launch type model, there is no additional charge. 
	You pay for AWS resources (e.g., EC2 instances or EBS volumes) you create to store and run your application. 
	You only pay for what you use, as you use it; there are no minimum fees and no upfront commitments.


	EC2 Launch Type: you must provision & maintain the infrastructure (the EC2 instances)
		• Each EC2 Instance must run the ECS Agent to register in the ECS Cluster
		• AWS takes care of starting / stopping containers

	Fargate launch type: This is a serverless pay-as-you-go option. 
	     You do not provision the infrastructure (no EC2 instances to manage)
		• It’s all Serverless!
		• You just create task definitions
		• AWS just runs ECS Tasks for you based on the CPU / RAM you need
		• To scale, just increase the number of tasks. Simple - no more EC2 instances



The EC2 launch type is suitable:
	Workloads that require consistently high CPU core and memory usage
	Large workloads that need to be optimized for price
	Your applications need to access persistent storage
	You must directly manage your infrastructure
	
	
The Fargate suitable:
	Large workloads that need to be optimized for low overhead
	Small workloads that have occasional burst
	Tiny workloads
	Batch workloads


TaskDefinition:
	TaskDefinition resource describes the container and volume definitions of an  ECS task. You can specify which Docker images to use, 
	the required resources, and other configurations related to launching the task 
	definition through an Amazon ECS service or task.
You can define multiple containers in a task definition.



AWS Fargate is a serverless option.
	-Service: Allows you to run and maintain a specified number (the “desired count”) of tasks
	-ECS cluster: Grouping of one or more container instances (EC2 instances) where you run your tasks
	-Container Instance - EC2 instance in the cluster running a container agent 
	 (helps it communicate with the cluster)
	-AWS provides ECS ready AMIs with container agents pre-installed.


Remember:
-AWS Fargate does NOT give you visibility into the EC2 instances in the cluster.
-You can use On-Demand instances or Spot instances to create your cluster.
-You can load balance using Application Load Balancers

Two features of ALB are important for ECS:
	-Dynamic host port mapping: Multiple tasks from the same service are allowed per EC2 (container) instance
	-Path-based routing: Multiple services can use the same listener port on same ALB/NLB and be routed based 
	 on path (www.app.com/microservice-a and www.app.com/microservice-b)




Elastic Beanstalk:
Single container or multiple containers in same EC2 instance
Recommended for simple web applications


Amazon Fargate:
Serverless version of Amazon ECS
You want to run microservices and you don’t want to manage the cluster


Amazon EKS:
AWS managed service for Kubernetes
Recommended if you are already using Kubernetes and would want to move the workload to AWS


Amazon ECS – Load Balancer Integrations:
	• Application Load Balancer supported and works for most use cases
	• Network Load Balancer recommended only for high throughput / high performance use cases, 
	  or to pair it with AWS Private Link.
	• Elastic Load Balancer supported but not recommended (no advanced features – no Fargate)

ECS Service Auto Scaling
• Automatically increase/decrease the desired number of ECS tasks
  ECS Service Auto Scaling (task level) ≠ EC2 Auto Scaling (EC2 instance level)

• Target Tracking – scale based on target value for a specific CloudWatch metric
• Step Scaling – scale based on a specified CloudWatch Alarm
• Scheduled Scaling – scale based on a specified date/time (predictable changes)



ECS Rolling Updates:
• When updating from v1 to v2, we can control how many tasks can be started and stopped, and in which order.


Amazon ECS – IAM Roles for ECS:
	• EC2 Instance Profile (EC2 Launch Type only):
		• Used by the ECS agent
		• Makes API calls to ECS service
		• Send container logs to CloudWatch Logs
		• Pull Docker image from ECR
		• Reference sensitive data in Secrets Manager or SSM Parameter Store
	• ECS Task Role:
		• Allows each task to have a specific role
		• Use different roles for the different ECS Services you run
		• Task Role is defined in the task definition


***
ECS Task Role is the IAM Role used by the ECS task itself. 
Use when your container wants to call other AWS services like S3, SQS, etc.


***
Amazon ECS – Data Volumes (EFS):
	• Mount EFS file systems onto ECS tasks
	• Works for both EC2 and Fargate launch types
	• Tasks running in any AZ will share the same data in the EFS file system
	• Fargate + EFS = Serverless
	• Use cases: persistent multi-AZ shared storage for your containers
	• Amazon S3 cannot be mounted as a file system


ECS tasks can invoked by EventBridge.
ECS tasks can invoked by EventBridge Schedule.
ECS task can invoke by  SQS Queue 

Fargate task storage: 
When provisioned, each Amazon ECS task that are hosted on AWS Fargate receives the following 
ephemeral storage for bind mounts. This can be mounted and shared among containers that use 
the volumes, mountPoints, and volumesFrom parameters in the task definition.

The ephemeral storage is encrypted with an AES-256 encryption algorithm, which uses an AWS owned encryption key.


By default, Amazon ECS tasks that are hosted on Fargate using platform version 1.0.0 or 
later receive a minimum of 20 GiB of ephemeral storage.


Fargate tasks using Linux platform version 1.3.0 or earlier
10 GB of Docker layer storage
An additional 4 GB for volume mounts. This can be mounted and shared among containers using the volumes, mountPoints, 
and volumesFrom parameters in the task definition.



Fargate tasks using Linux platform version 1.4.0 or later
By default, Amazon ECS tasks that are hosted on Fargate using platform version 1.4.0 
or later receive a minimum of 20 GiB of ephemeral storage. The total amount of 
ephemeral storage can be increased, up to a maximum of 200 GiB.


20 GB of ephemeral storage is available for all Fargate Tasks and Pods by default—you only pay for any additional storage 
that you configure.




Area:
=====
Amazon ECS is a regional service that simplifies running containers in a highly available manner across 
multiple Availability Zones within a Region



UseCase:
========
Deploy in a hybrid environment: Build container-based applications on-premises or in the cloud with 


BillCost:
=========
Fargate is not on the free tier list. 
Amazon ECS is always free. 
You pay for AWS resources (e.g. EC2 instances, EBS volumes, and memory) you 
create to store and run your application.

Amazon ECS pricing is dependent on whether you use AWS Fargate or Amazon EC2 infrastructure to host your 
containerized workloads.



Security:
=========
We can add security group for ECS and ECS Task.

Your EC2 instances use an IAM role to access the ECS/ECR service.
ECS Task Role use to access services and resources.
Security Groups and networks ACLs allow you to control inbound and outbound network access to and 
from your instances.





66
##Amazon ECR
-------------------------------------------------
Amazon ECR is a Fully-managed Docker container registry provided by AWS. Its an alternative to Docker Hub.

Amazon ECR is a fully managed container registry offering high-performance hosting, 
so you can reliably deploy application images and artifacts anywhere.

Amazon ECR:
• Private repository
• Public repository (Amazon ECR Public Gallery https://gallery.ecr.aws)


Area:
=====
Amazon ECR is a Regional service



UseCase:
========
Manage software vulnerabilities
Manage image lifecycle policies



BillCost:
=========
AWS Free Tier *
You pay only for the amount of data you store in your public or private repositories and 
for data transferred to the internet.



Security:
=========
Amazon ECR automatically encrypts images at rest using Amazon S3 server-side encryption or 
AWS KMS encryption and transfers your container images over HTTPS.
You can configure policies to manage permissions and control access to your images using AWS Identity and 
Access Management (IAM).

It is a Fully-managed, serverless service.
Security is a shared responsibility.




##EKS
-------------------------------------------------
Amazon EKS is integrated with many AWS services to provide scalability and security for your applications. 
These services include Elastic Load Balancing for load distribution, IAM for authentication, 
Amazon VPC for isolation, and AWS CloudTrail for logging.


Amazon EKS = Amazon Elastic Kubernetes Service (cloud-agnostic and open-source platforms = EKS)
• It is a way to launch managed Kubernetes clusters on AWS
• Kubernetes is an open-source system for automatic deployment, scaling and management of 
  containerized (usually Docker) application
• It’s an alternative to ECS, similar goal but different API
• EKS supports EC2 if you want to deploy worker nodes, or Fargate to deploy serverless containers
• Use case: if your company is already using Kubernetes on-premises or in another cloud, and wants 
  to migrate to AWS using Kubernetes.
• Kubernetes is cloud-agnostic (can be used in any cloud – Azure, GCP…)
• For multiple regions, deploy one EKS cluster per region
• Collect logs and metrics using CloudWatch Container Insights


Amazon EKS – Node Types:
• Managed Node Groups
	• Creates and manages Nodes (EC2 instances) for you
	• Nodes are part of an ASG managed by EKS
	• Supports On-Demand or Spot Instances
• Self-Managed Nodes
	• Nodes created by you and registered to the EKS cluster and managed by an ASG
	• You can use prebuilt AMI - Amazon EKS Optimized AMI
	• Supports On-Demand or Spot Instances
• AWS Fargate
	• No maintenance required; no nodes managed


Amazon EKS – Data Volumes:
• Need to specify StorageClass manifest on your EKS cluster
• Leverages a Container Storage Interface (CSI) compliant driver
• Support for…
• Amazon EBS
• Amazon EFS (works with Fargate)
• Amazon FSx for Lustre
• Amazon FSx for NetApp ONTAP




##AppRunner
-------------------------------------------------
Fully managed service that makes it easy to deploy web applications and APIs at scale
You can choose the programming language of your choice since App Runner can deploy directly from source code 
(in GitHub for example) or a Docker container image (from private or public repo in ECR) — 
all this without worrying about provisioning and managing the underlying infrastructure.

	• No infrastructure experience required
	• Start with your source code or container image
	• Automatically builds and deploy the web app
	• Automatic scaling, highly available, load balancer, encryption
	• VPC access support
	• Connect to database, cache, and message queue services
	• Use cases: web apps, APIs, microservices, rapid production deployments 




==| Serverless Overview
67
##AWS Serverless
-------------------------------------------------
Serverless on AWS Build and run applications without thinking about servers.

Serverless in AWS: 
• AWS Lambda 
• DynamoDB 
• AWS Cognito 
• AWS API Gateway 
• Amazon S3 
• AWS SNS & SQS 
• AWS Kinesis Data Firehose 
• Aurora Serverless 
• Step Functions 
• Fargate



Compute:
	AWS Lambda:AWS Lambda is an event-driven, pay-as-you-go compute service that lets you run code without 
	provisioning or managing servers.

	AWS Fargate:AWS Fargate is a serverless compute engine that works with ECS and EKS.

Application integration:
	EventBridge: Amazon EventBridge is a serverless event bus that lets you build event-driven applications at scale 
	across AWS and existing systems.

	Step Functions: AWS Step Functions is a visual workflow orchestrator that makes it easy to sequence multiple
	AWS services into business-critical applications.
	 
	SQS: Amazon Simple Queue Service (SQS) is a message queuing service enabling you to decouple and scale 
	microservices, distributed systems, and serverless applications.

	SNS: Amazon SNS is a fully managed messaging service for both ]
	application-to-application (A2A) and application-to-person (A2P) communication.

API Gateway: 
	Amazon API Gateway is a fully managed service that makes it easy to create and publish APIs at any scale.

AppSync: 
	AWS AppSync is a fully managed service that accelerates application development with scalable GraphQL APIs.


Data store:
	S3: Amazon S3 is an object storage service designed to store and protect any amount of data.

	DynamoDB: Amazon DynamoDB is a key-value and document database service, delivering single-digit millisecond 
	performance at any scale.

	RDS Proxy: Amazon RDS Proxy is a managed database proxy for Amazon Relational Database Service (RDS) 
	that makes applications more scalable and secure.

	Aurora Serverless: Amazon Aurora Serverless is a MySQL and PostgreSQL-compatible relational database that automatically scales 
	capacity based on your application's needs.




Area:
=====
May Regional.


UseCase:
========



BillCost:
=========
Different cost about different services.



Security:
=========
It is a Fully-managed, serverless service.
Security is a shared responsibility.





67
##Lambda
-------------------------------------------------
You can invoke an AWS Lambda function from an Aurora MySQL DB cluster with a native function or a stored procedure.


AWS Lambda is a serverless, event-driven compute service that lets you run code for virtually any type of 
application or backend service without provisioning or managing servers. 

The default timeout is 3 seconds, and the maximum execution duration per request in 
AWS Lambda is 900 seconds, which is equivalent to 15 minutes.

By default, the AWS Lambda limits the total 
concurrent executions across all functions within a given region to 1000.

You can trigger Lambda from over 200 AWS services and software as a service (SaaS) applications, 
and only pay for what you use.

Lambda allows you to add custom logic to AWS resources such as Amazon S3 buckets and Amazon DynamoDB tables.

AWS Lambda Limits to Know - per region:
Execution:
	• Memory allocation: 128 MB – 10GB (1 MB increments)
	• Maximum execution time: 900 seconds (15 minutes)
	• Environment variables (4 KB)
	• Disk capacity in the “function container” (in /tmp): 512 MB
	• Concurrency executions: 1000 (can be increased)
Deployment:
	• Lambda function deployment size (compressed .zip): 50 MB
	• Size of uncompressed deployment (code + dependencies): 250 MB
	• Can use the /tmp directory to load other files at startup
	• Size of environment variables: 4 KB

Lambda functions always operate from an AWS-owned VPC.
You should only enable your functions for VPC access when you need to interact with a private resource 
located in a private subnet. 
An RDS instance is a good example.


Once your function is VPC-enabled, all network traffic from your function is subject 
to the routing rules of your VPC/Subnet.
 
If your function needs to interact with a public 
resource, you will need a route through a NAT gateway in a public subnet.


Replace the Kinesis Data Streams with an Amazon SQS queue. 
Create a Lambda function that will asynchronously process the requests.

AWS Lambda function use to process messages in  SQS queue. 
Lambda event source mappings support standard queues and first-in, first-out (FIFO) queues.

Kinesis Data Streams is a real-time data streaming service that requires the provisioning of shards. 
Amazon SQS is a cheaper option because you only pay for what you use. 
Since there is no requirement for real-time processing in the scenario given, replacing 
Kinesis Data Streams with Amazon SQS would save more costs.


For Error like EC2ThrottledException:
Lambda function automatically scales based on the number of events it processes. 
If your Lambda function accesses a VPC, you must make sure that your VPC has sufficient 
ENI capacity to support the scale requirements of your Lambda function.




Area:
=====
AWS Lambda is a regional service.



UseCase:
========
Process data at scale
Run interactive web and mobile backends
Enable powerful ML insights
Create event-driven applications



BillCost:
=========
Pay per request and compute time
Free tier of 1,000,000 AWS Lambda requests and 400,000 GBs of compute time



Security:
=========
Security and Compliance is a shared responsibility.





68
##Labmbda@Edge | Lambda@Edge 
-------------------------------------------------
Lambda@Edge is a feature of Amazon CloudFront that lets you run code closer to users of your application, 
which improves performance and reduces latency.


With Lambda@Edge, you can enrich your web applications by making them globally distributed and 
improving their performance — all with zero server administration. 

Lambda@Edge runs your code in response to events generated by the CloudFront.
Just upload your code to AWS Lambda, which takes care of everything required to run and scale your code with high 
availability at an AWS location closest to your end user.


	After CloudFront receives a request from a viewer (viewer request)
	Before CloudFront forwards the request to the origin (origin request)
	After CloudFront receives the response from the origin (origin response)
	Before CloudFront forwards the response to the viewer (viewer response)

Lambda are regional service. 
Lambda@Edge is a global service. 

Lambda@Edge allows you to execute the logic across multiple location (Node.js and Paython only)

You can use Lambda@Edge to change CloudFront requests and responses.
You can also generate responses to viewers without ever sending the request to the origin.


Customize the content that the CloudFront  delivers to your users using Lambda@Edge, 
which allows your Lambda functions to execute the authentication process in AWS locations closer to the users.


User getting HTTP 504 errors:
Set up an origin failover by creating an origin group with two origins. 
Specify one as the primary origin and the other as the second origin which 
CloudFront automatically switches to when the primary origin returns specific 
HTTP status code failure responses.



By using Lambda@Edge and Kinesis together, you can process real-time streaming data 
so that you can track and analyze globally-distributed user activity on your website and 
mobile applications, including clickstream analysis.

UseCase:
A/B Testing
Real-time Image Transformation
User Authentication and Authorization
Intelligently Route Across Origins and Data Centers
User Prioritization
User Tracking and Analytics


By using Lambda@Edge to dynamically route requests to different origins based on different viewer characteristics, 
you can balance the load on your origins, while improving the performance for your users. 
For example, you can route requests to origins within a home region, based on a viewer's location.



Area:
=====
Lambda@Edge is a global service.



UseCase:
========
• Website Security and Privacy 
• Dynamic Web Application at the Edge 
• Search Engine Optimization (SEO) 
• Intelligently Route Across Origins and Data Centers 
• Bot Mitigation at the Edge 
• Real-time Image Transformation 
• A/B Testing 
• User Authentication and Authorization 
• User Prioritization 
• User Tracking and Analytics



BillCost:
=========
You pay only for the compute time you consume - there is no charge when your code is not running.



Security:
=========
It is a Fully-managed, serverless service.
Security is a shared responsibility.




++Lambda in VPC
-------------------------------------------------
• By default, your Lambda function is launched outside your own VPC (in an AWS -owned VPC)
• Therefore, it cannot access resources in your VPC (RDS, ElastiCache,internal ELB…)

• You must define the VPC ID, the Subnets and the Security Groups
• Lambda will create an ENI (Elastic Network Interface) in your subnets



++Lambda with RDS Proxy 
-------------------------------------------------
• If Lambda functions directly access your database, they may open too many connections under high load
• RDS Proxy:
	• Improve scalability by pooling and sharing DB connections
	• Improve availability by reducing by 66% the failover time and preserving connections
	• Improve security by enforcing IAM authentication and storing credentials in Secrets Manager
• The Lambda function must be deployed in your VPC, because RDS Proxy is never publicly accessible





69
##DynamoDB
-------------------------------------------------
Amazon DynamoDB is a fully managed, serverless, key-value NoSQL database designed to run high-performance 
applications at any scale and single-digit millisecond performance.
DynamoDB offers built-in security, continuous backups, automated multi-Region replication, in-memory caching, 
and data import and export tools.

In DynamoDB, you can only find by primary key or indexes.

A key-value store holds for each key a single value. Arguably, if the value can be an entire document, 
you can call this database a "document store". In this sense, DynamoDB is a document store.


The main components of DynamoDB are:
• DynamoDB is made of Tables
• Each table has a Primary Key (must be decided at creation time)
• Each table can have an infinite number of items (= rows)
• Each item has attributes (can be added over time – can be null)
• Maximum size of an item is 400KB
-Data types supported are:
	• Scalar Types – String, Number, Binary, Boolean, Null
	• Document Types – List, Map
	• Set Types – String Set, Number Set, Binary Set

DynamoDB is accessible via an HTTP API and performs authentication & authorization via IAM roles, 
making it a perfect fit for building Serverless applications.


DynamoDB – Read/Write Capacity Modes:
• Control how you manage your table’s capacity (read/write throughput)
Provisioned Mode (default)
	• You specify the number of reads/writes per second
	• You need to plan capacity beforehand
	• Pay for provisioned Read Capacity Units (RCU) & Write Capacity Units (WCU)
	• Possibility to add auto-scaling mode for RCU & WCU
On-Demand Mode
	• Read/writes automatically scale up/down with your workloads
	• No capacity planning needed
	• Pay for what you use, more expensive ($$$)
	• Great for unpredictable workloads


Time To Live (TTL):
• Automatically delete items after an expiry timestamp
• Use cases: reduce stored data by keeping only current items, adhere to regulatory obligations.



DynamoDB Accelerator (DAX):
	• Fully-managed, highly available, seamless in-memory cache for DynamoDB
	• Help solve read congestion by caching
	• Microseconds latency for cached data
	• Doesn’t require application logic modification(compatible with existing DynamoDB APIs)
	• 5 minutes TTL for cache (default)

DAX fetches the key-value pair from DynamoDB.
The key-value pair then returns the value back to the application.

DynamoDB Accelerator (DAX) delivers fast response times for accessing eventually consistent data. 
Certain usecases requires microseconds response times.

DAX:
You can't invalidate cache, it only invalidates when ttl is 
reached or nodes memory is full and it is dropping old items.

ElastiCache is not intended for use with DynamoDB.

ElastiCache (available as a managed service) can cache the results from anything. 
The difference here is that you need to manage invalidation and you must adjust your code to 
check the cache before querying the main query store.

Most fo the time, you’ll want to go DAX if you’re using DyanmoDB as a data store. 
Otherwise, use ElastiCache.



What are DynamoDB streams?
	A DynamoDB Stream is a time-ordered sequence of events recording all the modifications for 
	DynamoDB tables in near real-time. Similar to change data capture, DynamoDB Streams consist 
	of multiple Insert, Update, and Delete events. Each record has a unique sequence number 
	which is used for ordering. 
https://www.macrometa.com/event-stream-processing/dynamodb-streams

DymanoDB Stream:
	DymanoDB Stream is kind of event to interact with other servies like:lambda,s3,sns etc to doing some thing.
	as: User complated their registration and with DymanoDB Stream we cand send a welcome email using Lambda and SNS.
	For Global table Multi Region replication Need to enable DymanoDB Stream.


DynamoDB Streams:
	• 24 hours retention
	• Limited # of consumers
	• Process using AWS Lambda Triggers, or DynamoDB Stream Kinesis adapter

Kinesis Data Streams (newer):
	• 1 year retention
	• High # of consumers
	• Process using AWS Lambda, Kinesis DataAnalytics, Kineis Data Firehose, AWS GlueStreaming ETL… 


DynamoDB – Backups for disaster recovery:
• Continuous backups using point-in-time recovery (PITR)
	• Optionally enabled for the last 35 days
	• Point-in-time recovery to any time within the backup window
	• The recovery process creates a new table
• On-demand backups
	• Full backups for long-term retention, until explicitely deleted
	• Doesn’t affect performance or latency
	• Can be configured and managed in AWS Backup (enables cross-region copy)
	• The recovery process creates a new table



DynamoDB – Integration with Amazon S3:
	• Export to S3 (must enable PITR)
		• Works for any point of time in the last 35 days
		• Doesn’t affect the read capacity of your table
		• Perform data analysis on top of DynamoDB
		• Retain snapshots for auditing
		• ETL on top of S3 data before importing back into DynamoDB
		• Export in DynamoDB JSON or ION format
	• Import to S3:
		• Import CSV, DynamoDB JSON or ION format
		• Doesn’t consume any write capacity
		• Creates a new table
		• Import errors are logged in CloudWatch Logs


DynamoDB Global Tables:(two-way-replication)
	• Make a DynamoDB table accessible with low latency in multiple-regions
	• Active-Active replication
	• Applications can READ and WRITE to the table in any region
	• Must enable DynamoDB Streams as a pre-requisite



Enable DynamoDB Stream and create an AWS Lambda trigger, as well as the IAM role which contains 
all of the permissions that the Lambda function will need at runtime. The data from the stream 
record will be processed by the Lambda function which will then publish a message to SNS Topic 
that will notify the subscribers via email.

Also working with Kinesis Client Library (KCL).


Mobile game that has a serverless backend:
– Enable DAX and ensure that the Auto Scaling is enabled and increase the maximum provisioned read and write capacity.
– Use API Gateway in conjunction with Lambda and turn on the caching on frequently accessed data and enable DynamoDB global replication.

Since Auto Scaling is enabled by default, the provisioned read and write capacity will adjust automatically. 
Also enable DynamoDB Accelerator (DAX) to improve the performance 
from milliseconds to microseconds is incorrect because by default, 
Auto Scaling is not enabled in a DynamoDB table which is created using the AWS CLI.



Integrate DynamoDB table with CloudFront as these two are incompatible.


SimpleDB is incorrect. Although SimpleDB is also a highly available and scalable NoSQL database, 
it has a limit on the request capacity or storage size for a given table, unlike DynamoDB.

DynamoDB is durable, scalable, and highly available data store which can be used for 
real-time tabulation. 
You can also use AppSync with DynamoDB to make it easy for you to build collaborative apps that 
keep shared data updated in real-time.


Improve the database performance:
Use partition keys with high-cardinality attributes, which have a large number of distinct values for each item.


Avoid using a composite primary key, which is composed of a partition key and a sort key is 
incorrect because as mentioned, a composite primary key will provide more partition for the table 
and in turn, improves the performance. 
Hence, it should be used and not avoided.

Point-in-Time Recovery (PITR) feature is not capable of restoring a DynamoDB table to a particular 
point in time in a different AWS account.

DynamoDB on-demand backups are available at no additional cost beyond the normal pricing that’s associated with backup storage size. 
DynamoDB on-demand backups cannot be copied to a different account or Region. 
To create backup copies across AWS accounts and Regions and for other advanced features, you should use AWS Backup.


Area:
=====
Regional.



UseCase:
========
Develop software applications
Scale gaming platforms



BillCost:
=========
DynamoDB charges for reading, writing, and storing data in your DynamoDB tables, along with any optional 
features you choose to enable.

DynamoDB has two capacity modes,:
Pricing for on-demand capacity mode

On-demand capacity mode might be best if you:
Create new tables with unknown workloads
Have unpredictable application traffic
Prefer the ease of paying for only what you use

Provisioned capacity mode might be best if you:
Have predictable application traffic
Run applications whose traffic is consistent or ramps gradually
Can forecast capacity requirements to control costs



Security:
=========
Security is a shared responsibility.
DynamoDB encrypts data at rest by default and also in transit using the keys stored in 
AWS Key Management Service (or customer-provided keys).
DynamoDB also provides access control via AWS IAM roles.






70
##API Gateway
-------------------------------------------------
Amazon API Gateway is a fully managed service that makes it easy for developers to 
create, publish, maintain, monitor, and secure APIs at any scale.

You can create RESTful APIs and WebSocket APIs that enable real-time two-way communication applications.

API Types;
RESTful:   Build RESTful APIs optimized for serverless workloads and HTTP backends using HTTP APIs.
WebSocket: Build real-time two-way communication applications, such as chat apps and streaming dashboards, 
with WebSocket APIs.


Features:
	-Integrates with AWS Lambda, Amazon EC2, Amazon ECS or any web application
	-Supports HTTP(S) and WebSockets (two way communication - chat apps and streaming dashboards)
	-Serverless. Pay for use (API calls and connection duration)
	-Provides API Lifecycle Management for RESTful APIs and WebSocket APIs
	-You can Run multiple versions of the same API
	-Supports Rate Limits(request quota limits), throttling and fine-grained access permissions 
	 using API Keys for Third-Party Developers
	-Lifecycle management for REST APIs
	-Versioning and multiple environments
	-API keys - Generate API keys to monitor usage
	-Implement plans and quota limits for external applications (or developer)
	-WARNING - Do NOT use API keys for Authorization
	-Enable caching for API calls with TTL
	-Protect backends by throttling requests
	-Integrates with
	-Amazon CloudWatch - Performance metrics, API calls, latency data and error rates
	-Amazon CloudWatch Logs - Debug logging
	-AWS CloudTrail - Complete history of changes to your REST API



Endpoint Types:
	Edge-Optimized (default): For global clients
		• Requests are routed through the CloudFront Edge locations (improves latency)
		• The API Gateway still lives in only one region
	Regional:
		• For clients within the same region
		• Could manually combine with CloudFront (more control over the caching strategies and the distribution)
	Private:
		• Can only be accessed from your VPC using an interface VPC endpoint (ENI)
		• Use a resource policy to define access



API Gateway – Security – Summary:
IAM:
	• Great for users / roles already within your AWS account
	• Handle authentication + authorization
	• Leverages Sig v4
Custom Authorizer:
	• Great for 3rd party tokens
	• Very flexible in terms of what IAM policy is returned
	• Handle Authentication + Authorization
	• Pay per Lambda invocation
Cognito User Pool:
	• You manage your own user pool (can be backed by Facebook, Google login etc…)
	• No need to write any custom code
	• Only authentication, must implement authorization in the backend



API Gateway – Security:
	• User Authentication through
	• IAM Roles (useful for internal applications)
	• Cognito (identity for external users – example mobile users)
	• Custom Authorizer (your own logic)
• Custom Domain Name HTTPS security through integration with AWS Certificate Manager (ACM)
	• If using Edge-Optimized endpoint, then the certificate must be in us-east-1
	• If using Regional endpoint, the certificate must be in the API Gateway region
	• Must setup CNAME or A-alias record in Route 53


Amazon API Gateway, Amazon SQS and Amazon Kinesis:
To prevent your API from being overwhelmed by too many requests, Amazon API Gateway 
throttles requests to your API using the token bucket algorithm, where a token counts for a request. 
Specifically, API Gateway sets a limit on a steady-state rate and a burst of request submissions 
against all APIs in your account. In the token bucket algorithm, the burst is the maximum bucket size.


Enable throttling limits and result caching in API Gateway.
Amazon API Gateway provides throttling at multiple levels including global and by service call. 
Throttling limits can be set for standard rates and bursts. 

You can add caching to API calls by provisioning an Amazon API Gateway cache and specifying its 
size in gigabytes. The cache is provisioned for a specific stage of your APIs. 
This improves performance and reduces the traffic sent to your back end. 

Cache settings allow you to control the way the cache key is built and the time-to-live (TTL) 
of the data stored for each method. 
Amazon API Gateway also exposes management APIs that help you invalidate the cache for each stage.


Area:
=====
May Regional.



UseCase:
========
Efficient API development
Performance at any scale
Flexible security controls
Cost savings at scale



BillCost:
=========
The API Gateway free tier includes.
You pay only for the API calls you receive and the amount of data transferred out.



Security:
=========
Authorize access to your APIs with AWS Identity and Access Management (IAM) and Amazon Cognito. 
If you use OAuth tokens, API Gateway offers native OIDC and OAuth2 support. 
To support custom authorization requirements, you can execute a Lambda authorizer from AWS Lambda.






71
##AWS Step Functions
-------------------------------------------------
• Build serverless visual workflow to orchestrate your Lambda functions
• Features: sequence, parallel, conditions,timeouts, error handling, …
• Can integrate with EC2, ECS, On -premises servers, API Gateway, SQS queues, etc
• Possibility of implementing human approval feature
• Use cases: order fulfillment, data processing, web applications, any workflow






==|Serverless Architectures
72
##Serverless Architectures
-------------------------------------------------
MyTodoList:
• We want to create a mobile application with the following requirements
• Expose as REST API with HTTPS
• Serverless architecture
• Users should be able to directly interact with their own folder in S3
• Users should authenticate through a managed serverless service
• The users can write and read to-dos, but they mostly read them
• The database should scale, and have some high read throughput

Ans:
• Serverless REST API: HTTPS, API Gateway, Lambda, DynamoDB
• Using Cognito to generate temporary credentials with STS to access S3 bucket with restricted policy. 
  App users can directly access AWS resources this way. Pattern can be applied to DynamoDB, Lambda…
• Caching the reads on DynamoDB using DAX
• Caching the REST requests at the API Gateway level
• Security for authentication and authorization with Cognito, STS




MyBlog.com:
• This website should scale globally
• Blogs are rarely written, but often read
• Some of the website is purely static files, the rest is a dynamic REST API
• Caching must be implement where possible
• Any new users that subscribes should receive a welcome email
• Any photo uploaded to the blog should have a thumbnail generated

Ans:
• We’ve seen static content being distributed using CloudFront with S3
• The REST API was serverless, didn’t need Cognito because public
• We leveraged a Global DynamoDB table to serve the data globally
• (we could have used Aurora Global Database)
• We enabled DynamoDB streams to trigger a Lambda function
• The lambda function had an IAM role which could use SES
• SES (Simple Email Service) was used to send emails in a serverless way
• S3 can trigger SQS / SNS / Lambda to notify of events





72
##AWS MicroServices Architecture | Micro Services
-------------------------------------------------
Microservices are an architectural and organizational approach to software development created to 
speed up deployment cycles, foster innovation and ownership, improve maintainability and scalability of 
software applications.



• You are free to design each micro-service the way you want
• Synchronous patterns: API Gateway, Load Balancers
• Asynchronous patterns: SQS, Kinesis, SNS, Lambda triggers (S3)
• Challenges with micro-services:
	• repeated overhead for creating each new microservice,
	• issues with optimizing server density/utilization
	• complexity of running multiple versions of multiple microservices simultaneously
	• proliferation of client-side code requirements to integrate with many separate services.
• Some of the challenges are solved by Serverless patterns:
	• API Gateway, Lambda scale automatically and you pay per usage
	• You can easily clone API, reproduce environments
	• Generated client SDK through Swagger integration for the API Gateway



73
##Software updates offloading
-------------------------------------------------
• We have an application running on EC2, that distributes software updates once in a while
• When a new software update is out, we get a lot of request and the content is distributed in mass over the network. 
  It’s very costly
• We don’t want to change our application, but want to optimize our cost and CPU, how can we do it?

Ans:CloudFront
	• No changes to architecture
	• Will cache software update files at the edge
	• Software update files are not dynamic, they’re static (never changing)
	• Our EC2 instances aren’t serverless
	• But CloudFront is, and will scale for us
	• Our ASG will not scale as much, and we’ll save tremendously in EC2
	• We’ll also save in availability, network bandwidth cost, etc
	• Easy way to make an existing application more scalable and cheaper!


Area:
=====
Global



UseCase:
========
Software update and paching.



BillCost:
=========
Distributor is priced on a pay-per-use model.



Security:
=========
It is a aws managed service, security is a shared responsibility.
 






==|Databases
76
##Databases in AWS
-------------------------------------------------
Choosing the right database based on your architecture:
	• Read-heavy, write-heavy, or balanced workload? Throughput needs? Will it
	  change, does it need to scale or fluctuate during the day?
	• How much data to store and for how long? Will it grow? Average object size?
	  How are they accessed?
	• Data durability? Source of truth for the data ?
	• Latency requirements? Concurrent users?
	• Data model? How will you query the data? Joins? Structured? Semi-Structured?
	• Strong schema? More flexibility? Reporting? Search? RDBMS / NoSQL?
	• License costs? Switch to Cloud Native DB such as Aurora?


Database Types:
	• RDBMS (= SQL/OLTP): RDS, Aurora – great for joins
	• NoSQL database – no joins, no SQL : DynamoDB (~JSON), ElastiCache (key / value pairs), 
	  Neptune (graphs), DocumentDB (for MongoDB), Keyspaces (for Apache Cassandra)
	• Object Store: S3 (for big objects) / Glacier (for backups / archives)
	• Data Warehouse (= SQL Analytics / BI): Redshift (OLAP), Athena, EMR
	• Search: OpenSearch (JSON) – free text, unstructured searches
	• Graphs: Amazon Neptune – displays relationships between data
	• Ledger: Amazon Quantum Ledger Database
	• Time series: Amazon Timestream
	
Amazon Timestream is a fast, scalable, and serverless time series database service for IoT and operational 
	applications that makes it easy to store and analyze trillions of events per day up to 1,000 times 
	faster and at as little as 1/10th the cost of relational databases.
Amazon Timestream saves you time and cost in managing the lifecycle.


Amazon RDS – Summary:
	• Managed PostgreSQL / MySQL / Oracle / SQL Server / MariaDB / Custom
	• Provisioned RDS Instance Size and EBS Volume Type & Size
	• Auto-scaling capability for Storage
	• Support for Read Replicas and Multi AZ
	• Security through IAM, Security Groups, KMS , SSL in transit
	• Automated Backup with Point in time restore feature (up to 35 days)
	• Manual DB Snapshot for longer-term recovery
	• Managed and Scheduled maintenance (with downtime)
	• Support for IAM Authentication, integration with Secrets Manager
	• RDS Custom for access to and customize the underlying instance (Oracle & SQL Server)
	• Use case: Store relational datasets (RDBMS / OLTP), perform SQL queries, transactions


RDS for Solutions Architect:
	• Operations: small downtime when failover happens, when maintenance
	  happens, scaling in read replicas / ec2 instance / restore EBS 
	  implies manual intervention, application changes.
	• Security: AWS responsible for OS security, we are responsible for setting
	  up KMS, security groups, IAM policies, authorizing users in DB, using SSL
	• Reliability: Multi AZ feature, failover in case of failures
	• Performance: depends on EC2 instance type, EBS volume type, ability to
	  add Read Replicas. Storage auto-scaling & manual scaling of instances
	• Cost: Pay per hour based on provisioned EC2 and EBS.



Aurora Summary:
	• Compatible API for PostgreSQL / MySQL, separation of storage and compute
	• Storage: data is stored in 6 replicas, across 3 AZ – highly available, self-healing, auto-scaling
	• Compute: Cluster of DB Instance across multiple AZ, auto-scaling of Read Replicas
	• Cluster: Custom endpoints for writer and reader DB instances
	• Same security / monitoring / maintenance features as RDS
	• Know the backup & restore options for Aurora
	• Aurora Serverless – for unpredictable / intermittent workloads, no capacity planning
	• Aurora Multi-Master – for continuous writes failover (high write availability)
	• Aurora Global: up to 16 DB Read Instances in each region, < 1 second storage replication
	• Aurora Machine Learning: perform ML using SageMaker & Comprehend on Aurora
	• Aurora Database Cloning: new cluster from existing one, faster than restoring a snapshot
	• Use case: same as RDS, but with less maintenance / more flexibility / more performance / more features



Aurora for Solutions Architect:
	• Operations: less operations, auto scaling storage
	• Security: AWS responsible for OS security, we are responsible for setting
	  up KMS, security groups, IAM policies, authorizing users in DB, using SSL
	• Reliability: Multi AZ, highly available, possibly more than RDS, Aurora
	  Serverless option, Aurora Multi-Master option
	• Performance: 5x performance (according to AWS) due to architectural
	  optimizations. Up to 15 Read Replicas (only 5 for RDS)
	• Cost: Pay per hour based on EC2 and storage usage. Possibly lower
	  costs compared to Enterprise grade databases such as Oracle



Amazon ElastiCache – Summary:
	• Managed Redis / Memcached (similar offering as RDS, but for caches)
	• In-memory data store, sub-millisecond latency
	• Must provision an EC2 instance type
	• Support for Clustering (Redis) and Multi AZ, Read Replicas (sharding)
	• Security through IAM, Security Groups, KMS, Redis Auth
	• Backup / Snapshot / Point in time restore feature
	• Managed and Scheduled maintenance
	• Requires some application code changes to be leveraged
	• Use Case: Key/Value store, Frequent reads, less writes, cache results for DB queries, 
	  store session data for websites, cannot use SQL. 


ElastiCache for Solutions Architect:
	• Operations: same as RDS
	• Security: AWS responsible for OS security, we are responsible for setting up KMS, security groups, 
	  IAM policies, users (Redis Auth), using SSL.
	• Reliability: Clustering, Multi AZ
	• Performance: Sub-millisecond performance, in memory, read replicas for sharding, 
	  very popular cache option.
	• Cost: Pay per hour based on EC2 and storage usage


DynamoDB Overview:
	• AWS proprietary technology, managed NoSQL database
	• Serverless, provisioned capacity, auto scaling, on demand capacity (Nov 2018)
	• Can replace ElastiCache as a key/value store (storing session data for example)
	• Highly Available, Multi AZ by default, Read and Writes are decoupled, DAX for read cache
	• Reads can be eventually consistent or strongly consistent
	• Security, authentication and authorization is done through IAM
	• DynamoDB Streams to integrate with AWS Lambda
	• Backup / Restore feature, Global Table feature
	• Monitoring through CloudWatch
	• Can only query on primary key, sort key, or indexes
	• Use Case: Serverless applications development (small documents 100s KB), distributed serverless cache, 
	  doesn’t have SQL query language available, has transactions capability from Nov 2018.


DynamoDB for Solutions Architect:
	• Operations: no operations needed, auto scaling capability, serverless
	• Security: full security through IAM policies, KMS encryption, SSL in flight
	• Reliability: Multi AZ, Backups
	• Performance: single digit millisecond performance, DAX for caching reads, 
	  performance doesn’t degrade if your application scales.
	• Cost: Pay per provisioned capacity and storage usage (no need to guess in advance any 
	  capacity – can use auto scaling).


S3 Overview:
	• S3 is a… key / value store for objects
	• Great for big objects, not so great for small objects
	• Serverless, scales infinitely, max object size is 5 TB
	• Strong consistency
	• Tiers: S3 Standard, S3 IA, S3 One Zone IA, Glacier for backups
	• Features: Versioning, Encryption, Cross Region Replication, etc…
	• Security: IAM, Bucket Policies, ACL
	• Encryption: SSE-S3, SSE-KMS, SSE-C, client side encryption, SSL in transit
	• Use Case: static files, key value store for big files, website hosting


S3 for Solutions Architect:
	• Operations: no operations needed
	• Security: IAM, Bucket Policies, ACL, Encryption (Server/Client), SSL
	• Reliability: 99.999999999% durability / 99.99% availability, Multi AZ, CRR
	• Performance: scales to thousands of read / writes per second, transfer
	  acceleration / multi-part for big files
	• Cost: pay per storage usage, network cost, requests number



##DocumentDB
-------------------------------------------------
Migrating from MongoDB to Amazon DocumentDB: offline, online, and hybrid.
For online approach, you may use AWS DMS to minimize downtime. 
AWS DMS continually reads from the source MongoDB oplog and applies those changes 
in near-real time on the source Amazon DocumentDB cluster.

	• Aurora is an “AWS-implementation” of PostgreSQL / MySQL …
	• DocumentDB is the same for MongoDB (which is a NoSQL database)
	• MongoDB is used to store, query, and index JSON data
	• Similar “deployment concepts” as Aurora
	• Fully Managed, highly available with replication across 3 AZ
	• Aurora storage automatically grows in increments of 10GB, up to 64 TB.
	• Automatically scales to workloads with millions of requests per seconds

ExamTips:If any thing related to MongoDB this is Document DB.




##Amazon Neptune 
-------------------------------------------------
• Fully managed graph database 
• A popular graph dataset would be a social network 
	• Users have friends 
	• Posts have comments 
	• Comments have likes from users 
	• Users share and like posts… 
• Highly available across 3 AZ, with up to 15 read replicas 
• Build and run applications working with highly connected datasets – optimized for these complex and hard queries
• Can store up to billions of relations and query the graph with milliseconds latency
• Highly available with replications across multiple AZs 
• Great for knowledge graphs (Wikipedia), fraud detection, recommendation engines, social networking



++Keyspaces (for Apache Cassandra)
-------------------------------------------------
• Apache Cassandra is an open-source NoSQL distributed database
• A managed Apache Cassandra-compatible database service
• Serverless, Scalable, highly available, fully managed by AWS
• Automatically scale tables up/down based on the application’s traffic
• Tables are replicated 3 times across multiple AZ
• Using the Cassandra Query Language (CQL)
• Single-digit millisecond latency at any scale, 1000s of requests per second
• Capacity: On-demand mode or provisioned mode with auto-scaling
• Encryption, backup, Point-In-Time Recovery (PITR) up to 35 days
• Use cases: store IoT devices info, time-series data,

ExamTips:any thing about apache Cassandra, its Amazon Keyspace.



++QLDB
-------------------------------------------------
• QLDB stands for ”Quantum Ledger Database”
• A ledger is a book recording financial transactions
• Fully Managed, Serverless, High available, Replication across 3 AZ
• Used to review history of all the changes made to your application data over time
• Immutable system: no entry can be removed or modified, cryptographically verifiable

ExamTips:Any time you see Financial transaction or Ledger, its QLDB.




++Timestream
-------------------------------------------------
• Fully managed, fast, scalable, serverless time series database 
• Automatically scales up/down to adjust capacity 
• Store and analyze trillions of events per day 
• 1000s times faster & 1/10th the cost of relational databases 
• Scheduled queries, multi-measure records, SQL compatibility 
• Data storage tiering: recent data kept in memory and historical data kept in a cost-optimized storage
• Built-in time series analytics functions (helps you identify patterns in your data in near real-time)
• Encryption in transit and at rest 
• Use cases: IoT apps, operational applications, real-time analytics,






==|Data & Analytics
##Athena
-------------------------------------------------
Amazon Athena is an interactive query service that makes it easy to analyze data 
directly in Amazon S3 using standard SQL.
Athena is not a database but rather a query engine.

Athena cannot be used to build a REST API to consume data from the source, 
No APIGatwaay support to create rest api.

• Fully Serverless database with SQL capabilities
• Used to query data in S3
• Pay per query
• Output results back to S3
• Secured through IAM
• Use Case: one time SQL queries, serverless queries on S3, log analytics


Athena for Solutions Architect:
• Operations: no operations needed, serverless
• Security: IAM + S3 security
• Reliability: managed service, uses Presto engine, highly available
• Performance: queries scale based on data size
• Cost: pay per query / per TB of data scanned, serverless


Serverless query service to perform analytics against S3 objects
• Uses standard SQL language to query the files
• Supports CSV, JSON, ORC, Avro, and Parquet (built on Presto)



Exam Tip: analyze data in S3 using serverless SQL, use Athena



Amazon Athena – Performance Improvement:
	• Use columnar data for cost-savings (less scan)
	• Apache Parquet or ORC is recommended
	• Huge performance improvement
	• Use Glue to convert your data your Parquet or ORC



Serverless query service to analyze data stored in Amazon S3
• Uses standard SQL language to query the files (built on Presto)
• Supports CSV, JSON, ORC, Avro, and Parquet
• Pricing: $5.00 per TB of data scanned
• Commonly used with Amazon Quicksight for reporting/dashboards
• Use cases: Business intelligence / analytics / reporting, analyze &
  query VPC Flow Logs, ELB Logs, CloudTrail trails, etc...

ExamTip: analyze data in S3 using serverless SQL, use Athena


Area:
=====
Global.
Athena supports the ability to query Amazon S3 data in an AWS Region that is different from the Region in 
which you are using Athena.


UseCase:
========
Use cases: Business intelligence / analytics / reporting, analyze &
query VPC Flow Logs, ELB Logs, CloudTrail trails.

Athena helps you analyze unstructured, semi-structured, and structured data stored in Amazon S3. 
Examples include CSV, JSON, or columnar data formats such as Apache Parquet and Apache ORC. 
You can use Athena to run ad-hoc queries using ANSI SQL, without the need to aggregate or load the data into Athena.


BillCost:
=========
You are charged based on the amount of data scanned by each query.
Pricing: $5.00 per TB of data scanned




Security:
=========
Security is a shared responsibility.







77
##Redshift ( Data Warehouse)
-------------------------------------------------
Fastest, easiest, and most widely used cloud data warehouse.

Redshift cluster as their data warehouse region outage:
	Enable Cross-Region Snapshots Copy in your Amazon Redshift Cluster.


Redshift data-warehouse is a collection of computing resources known as nodes, and these nodes are organized in a 
group known as a cluster. 
Each cluster runs in a Redshift Engine which contains one or more databases.

When you launch a Redshift instance, it starts with a single node of size 160 GB. When you want to grow, 
you can add additional nodes to take advantage of parallel processing. You have a leader node that manages 
the multiple nodes. Leader node handles the client connection as well as compute nodes. It stores the data 
in compute nodes and performs the query.


Amazon Redshift is a fully managed data warehouse service in the cloud. Its datasets range from 
100s of gigabytes to a petabyte. 

What are the differences between a database and a data warehouse? 
A database is any collection of data organized for storage, accessibility, and retrieval. 

A data warehouse is a type of database the integrates copies of transaction data 
from disparate source systems and provisions them for analytical use.


Redshift for Solutions Architect:
	• Redshift is based on PostgreSQL, but it’s not used for OLTP
	• It’s OLAP – online analytical processing (analytics and data warehousing)
	• 10x better performance than other data warehouses, scale to PBs of data
	• Columnar storage of data (instead of row based) & parallel query engine
	• Pay as you go based on the instances provisioned
	• Has a SQL interface for performing the queries
	• BI tools such as Amazon QuickSight or Tableau integrate with it
	• vs Athena: faster queries / joins / aggregations thanks to indexes

Remember: Redshift = Analytics / BI / Data Warehouse



Redshift – Snapshots & DR:
	• Redshift has no “Multi-AZ” mode
	• Snapshots are point-in-time backups of a cluster, stored internally in S3
	• Snapshots are incremental (only what has changed is saved)
	• You can restore a snapshot into a new cluster
	• Automated: every 8 hours, every 5 GB, or on a schedule. Set retention between 1 to 35 days
	• Manual: snapshot is retained until you delete it
	• You can configure Amazon Redshift to automatically copy snapshots (automated or manual) 
	  of a cluster to another AWS Region



Loading data into Redshift: Large inserts are MUCH better
	Amazon Kinesis -> Kinesis Firehose->Redshift
	S3 using -> COPY command ->Redshift
	EC2 Instance -> JDBC driver ->Redshift





Redshift Spectrum:
With Redshift Spectrum, an analyst can perform SQL queries on data stored in Amazon S3 buckets. 
This can save time and money because it eliminates the need to move data from a storage service 
to a database, and instead directly queries data inside an S3 bucket. 

Redshift Spectrum also expands the scope of a given query because it 
extends beyond a user's existing Redshift data 
warehouse nodes and into large volumes of unstructured S3 data lakes.


• Query(SQL) data that is already in S3 without loading it
• Must have a Redshift cluster available to start the query
• The query is then submitted to thousands of Redshift Spectrum nodes

***
Today, there are three primary ways to access and analyze data in S3:
	Elastic MapReduce (EMR): EMR uses Hadoop-style queries to access and process large data sets in S3.
	Athena: Athena offers a console to query S3 data with standard SQL and no infrastructure to manage. 
	Athena also has an API.
	Redshift: You can load data from S3 into an Amazon Redshift cluster for analysis.
	
Redshift Spectrum is a logical extension of Redshift to query the data from Redshift as well as 
Amazon S3 data lakes whereas Athena is an exclusive tool to query data from Amazon S3 only.

If you already RedShift then go for ReadshiftSpectrum other wose Amazon Athena.

Athena and Redshift Spectrum provide compelling, cost-effective solutions to 
query the contents of your Datalake.

You cannot use Redshift to capture data in key-value pairs from the IoT sources.

Area:
=====
Redshift is not multi-AZ, if you want multi-AZ you will need to spin up a separate cluster 
ingesting the same input. 
You can also manually restore snapshots to a new AZ in the event of an outage.


UseCase:
========
Improve financial and demand forecasts
Collaborate and share data
Increase developer productivity



BillCost:
=========
AWS Free Tier
Customers can use the Redshift for just $0.25 per hour with no commitments or upfront costs and scale to a petabyte 
or more for $1,000 per terabyte per year.



Security:
=========
With a couple of parameter settings, you can set the Redshift to use SSL to secure your data. 
You can also enable encryption, all the data written to disk will be encrypted.





78
##OpenSearch Service|ElasticSearch now OpenSearch
-------------------------------------------------

Amazon OpenSearch Service makes it easy for you to perform interactive log analytics, 
real-time application monitoring,  website search, and more. 
OpenSearch is an open source, distributed search and analytics suite derived from Elasticsearch.

Amazon OpenSearch is successor to Amazon ElasticSearch (For licence issue ElasticSearch now Open search)
	• Example: In DynamoDB, you can only find by primary key or indexes
	• With OpenSearch, you can search any field, even partially matches
	• It’s common to use OpenSearch as a complement to another database
	• OpenSearch also has some usage for Big Data applications
	• You can provision a cluster of instances
	• Built-in integrations: Amazon Kinesis Data Firehose, AWS IoT, and Amazon
	• In DynamoDB, queries only exist by primary key or indexes…
	• OpenSearch requires a cluster of instances (not serverless)
	• Does not support SQL (it has its own query language)

CloudWatch Logs for data ingestion


OpenSearch for Solutions Architect:
	• Operations: similar to RDS
	• Security: Cognito, IAM, VPC, KMS, SSL
	• Reliability: Multi-AZ, clustering
	• Performance: based on ElasticSearch project (open source), petabyte scale
	• Cost: pay per node provisioned (similar to RDS)
	• Remember: OpenSearch = Search / Indexing


CloudSearch:
	ElasticSearch and CloudSearch are Amazon’s cloud-based solutions for search. 
	ElasticSearch is an open source solution, whereas CloudSearch is a fully managed search service. 
	It is quite simple to set up, easy to, and a cost-effective search solution. 
	
	Amazon CloudSearch is an AWS Cloud managed service that helps users to create fast, scalable, 
	profitable, easy to setup search solutions for their applications. Amazon CloudSearch uses 
	Apache Solr as the underlying text search engine, which supports full-text search, 
	faceted search, real-time indexing, dynamic clustering, database integration, 
	NoSQL features, and productive document handling.

OpenSearch and CloudSearch can be categorized as "Search as a Service" tools.

ElasticSearch Data and Index backup manual.
Cloudsearch automatically takes backup of the data. Unlike Elasticsearch.

Amazon Elasticsearch offers a licensed plugin called Shield. It handles the authorization, 
user authentication and permission allocation.
Cloudsearch offers an IAM-based access control mechanism.

There is no up-front licensing cost of Elasticsearch. Therefore, users do not need to 
pay any additional costs for licensing. 
Cloudsearch charges on an hourly basis, which depends on the size of user searches. 

Area:
=====
Regional.
Elasticsearch is not a multi-region service .


UseCase:
========
Monitor and debug applications and infrastructure
Manage security and event information (SIEM)



BillCost:
=========
Amazon OpenSearch Service with AWS Free Tier.

You are charged based on three dimensions: instance hours, which are the number of hours an instance is available to you 
for use;  the amount of storage you need; and data transferred in and out of Amazon OpenSearch Service. 
Storage pricing depends on the  storge tier and type of instance you choose.


Security:
=========
Security through Cognito & IAM, KMS encryption, SSL & VPC
Comes with OpenSearch Dashboards (visualization)



Kinesis Data Analytics (SQL application)
-------------------------------------------------
• Real-time analytics on Kinesis Data Streams & Firehose using SQL
• Add reference data from Amazon S3 to enrich streaming data
• Fully managed, no servers to provision

Output:
• Kinesis Data Streams: create streams out of the real-time analytics queries
• Kinesis Data Firehose: send analytics query results to destinations

• Use Flink (Java, Scala or SQL) to process and analyze streaming data



79
##EMR Elastic MapReduce
-------------------------------------------------
 Amazon EMR is the industry-leading cloud big data platform for processing vast amounts of data 
 using open source tools such as Apache Spark, Apache Hive, Apache HBase, Apache Flink, 
 Apache Hudi, and Presto. 
 Amazon EMR uses Hadoop, an open-source framework, to distribute 
 your data and processing across a resizable cluster of Amazon EC2 instances. 
 
• EMR stands for “Elastic MapReduce”
• EMR helps creating Hadoop clusters (Big Data) to analyze and process vast amount of data
• The clusters can be made of hundreds of EC2 instances
• EMR comes bundled with Apache Spark, HBase, Presto, Flink…
• EMR takes care of all the provisioning and configuration
• Auto-scaling and integrated with Spot instances
• Use cases: Data processing, machine learning, web indexing, Big data.



Amazon EMR – Node types & purchasing:
• Master Node: Manage the cluster, coordinate, manage health – long running
• Core Node: Run tasks and store data – long running
• Task Node (optional): Just to run tasks – usually Spot
• Purchasing options:
	• On-demand: reliable, predictable, won’t be terminated
	• Reserved (min 1 year): cost savings (EMR will automatically use if available)
	• Spot Instances: cheaper, can be terminated, less reliable
• Can have long-running cluster, or transient (temporary) cluster


EMR does not offer the same storage and processing speed as FSx for Lustre.


Amazon EMR is the industry-leading cloud big data platform for processing vast amounts 
of data using open source tools such as Apache Spark, Apache Hive, Apache HBase, 
Apache Flink, Apache Hudi, and Presto.
 
Amazon EMR uses Hadoop, an open-source framework, 
to distribute your data and processing across a resizable cluster of Amazon EC2 instances. 

Using an EMR cluster would imply managing the underlying infrastructure so it’s ruled out 
because the correct solution for the given use-case should require the least amount of 
infrastructure maintenance.

Athena		 ->Serverless sql analytic services.
ReadShift 	 ->Redshift is a managed petabyte-scale data warehouse that is accessed via SQL. 
EMR          ->Open Serach service with full text search services, No SQL.


If you want to use SQL and you have structured data (eg CSV files), then Redshift is the simplest solution.
If you want to process unstructured data (eg in strange formats rather than structured CSV files), 
Amazon EMR can provide a Hadoop system that is very capable.




80
++QuickSight (ML dashboards)
-------------------------------------------------
• Serverless machine learning-powered business intelligence service to create interactive dashboards
• Fast, automatically scalable, embeddable, with per-session pricing
• Use cases:
	• Business analytics
	• Building visualizations
	• Perform ad-hoc analysis
	• Get business insights using data
• Integrated with RDS, Aurora, Athena, Redshift, S3…
• In-memory computation using SPICE engine if data is imported into QuickSight
• Enterprise edition: Possibility to setup Column-Level security (CLS)



QuickSight – Dashboard & Analysis:
• Define Users (standard versions) and Groups (enterprise version)
• These users & groups only exist within QuickSight, not IAM !!
• A dashboard…
	• is a read-only snapshot of an analysis that you can share
	• preserves the configuration of the analysis (filtering, parameters, controls, sort)
• You can share the analysis or the dashboard with Users or Groups
• To share a dashboard, you must first publish it
• Users who see the dashboard can also see the underlying data 





81
##Glue
-------------------------------------------------
A fully managed extract, transform, and load 
	(ETL = one or more source databases into a target database or information warehouse) 
service that makes it easy for customers to prepare and load their data for analytics.

Amazon Athena and AWS Glue can be categorized as "Big Data" tools.

AWS Glue provides both visual and code-based interfaces to make data integration easier. 
Users can easily find and access data using the AWS Glue Data Catalog.

The purpose of Glue is to facilitate the construction of an enterprise-class data warehouse. 
It can move information into the warehouse from a variety of sources, including 
transactional databases and the Amazon cloud.


AWS Glue Data Catalog:
	Glue Data Catalog provides centralized uniform metadata storing for tracking, querying and transforming data using saved metadata.




Glue – things to know at a high-level:
• Glue Job Bookmarks: prevent re-processing old data
• Glue Elastic Views:
	• Combine and replicate data across multiple data stores using SQL
	• No custom code, Glue monitors for changes in the source data, serverless
	• Leverages a “virtual table” (materialized view)
• Glue DataBrew: clean and normalize data using pre-built transformation
• Glue Studio: new GUI to create, run and monitor ETL jobs in Glue
• Glue Streaming ETL (built on Apache Spark Structured Streaming):
  compatible with Kinesis Data Streaming, Kafka, MSK (managed Kafka)


What is difference between AWS Glue and lambda?
Glue can only execute jobs using Scala or Python code. 
Lambda can execute code from triggers by other services (SQS, Kafka, DynamoDB, Kinesis, CloudWatch, etc.) vs. 
Glue which can be triggered by lambda events, another Glue jobs, manually or from a schedule.

Using AWS Glue involves significant development efforts to write custom migration scripts to 
copy the database data into Redshift.


AWS Glue job is meant to be used for batch ETL data processing. 
AWS Glue does not offer the same storage and processing speed as FSx for Lustre.


Area:
=====
Regional.
AWS Glue is available in several AWS Regions


UseCase:
========
AWS Glue is a serverless data integration service that makes it easy to discover, prepare, and combine data for 
analytics, machine learning, and application development.


BillCost:
=========
With AWS Glue, you pay an hourly rate, billed by the second, for crawlers (discovering data) and ETL jobs 
processing and loading data). 
For the AWS Glue Data Catalog, you pay a simple monthly fee for storing and accessing the metadata.



Security:
=========
IAM, VPC, KMS, SSL (like RDS).
We provide server-side encryption for data at rest and SSL for data in motion.





82
++AWS Lake Formation | AWS Lake Formation 
-------------------------------------------------
Use AWS Lake Formation to consolidate data from multiple accounts into a single account.

AWS Lake Formation is a service that makes it easy to set up a secure data lake in days. 
A data lake is a centralized, curated, and secured repository that stores all your data, 
both in its original form and prepared for analysis. 

Amazon S3 forms the storage layer for Lake Formation. 



• Data lake = central place to have all your data for analytics purposes
• Fully managed service that makes it easy to setup a data lake in days
• Discover, cleanse, transform, and ingest data into your Data Lake
• It automates many complex manual steps (collecting, cleansing, moving, cataloging data, …) 
  and de-duplicate (using ML Transforms)
• Combine structured and unstructured data in the data lake
• Out-of-the-box source blueprints: S3, RDS, Relational & NoSQL DB…
• Fing-grained Access Control for your applications (row and column-level).
• Built on top of AWS Glue.



AWS Lake Formation is integrated with AWS Glue which you can use to create a data catalog 
that describes available datasets and their appropriate business applications. 
 
You can assign permissions to IAM users, roles, groups, and Active Directory users using federation. 
You specify permissions on catalog objects (like tables and columns) rather than on buckets and objects.


Setting up a Kinesis Firehose in each and every account to move data into a single location 
is costly and impractical. A better approach is to set up cross-account sharing which is 
free with AWS Lake Formation.




83
##Amazon Managed Streaming for Apache Kafka (Amazon MSK)
-------------------------------------------------

What is Kafka?
Apache Kafka is a distributed pub-sub messaging system and a robust queue that can handle a high volume of 
data and enables you to pass messages from one end-point to another.


• Alternative to Amazon Kinesis
• Fully managed Apache Kafka on AWS
• Allow you to create, update, delete clusters
• MSK creates & manages Kafka brokers nodes & Zookeeper nodes for you
• Deploy the MSK cluster in your VPC, multi-AZ (up to 3 for HA)
• Automatic recovery from common Apache Kafka failures
• Data is stored on EBS volumes for as long as you want
• MSK Serverless
• Run Apache Kafka on MSK without managing the capacity
• MSK automatically provisions resources and scales compute & storage



Kinesis Data Streams vs Amazon MSK:
	Kinesis Data Streams:
		• 1 MB message size limit
		• Data Streams with Shards
		• Shard Splitting & Merging
		• TLS In-flight encryption
		• KMS at-rest encryption

	Amazon MSK
		• 1MB default, configure for higher (ex: 10MB)
		• Kafka Topics with Partitions
		• Can only add partitions to a topic
		• PLAINTEXT or TLS In-flight Encryption
		• KMS at-rest encryption

ExamTips:Need to know both different.



Big Data Ingestion Pipeline?
	• We want the ingestion pipeline to be fully serverless
	• We want to collect data in real time
	• We want to transform the data
	• We want to query the transformed data using SQL
	• The reports created using the queries should be in S3
	• We want to load that data into a warehouse and create dashboards


Big Data Ingestion Pipeline discussion
	• IoT Core allows you to harvest data from IoT devices
	• Kinesis is great for real-time data collection
	• Firehose helps with data delivery to S3 in near real-time (1 minute)
	• Lambda can help Firehose with data transformations
	• Amazon S3 can trigger notifications to SQS
	• Lambda can subscribe to SQS (we could have connecter S3 to Lambda)
	• Athena is a serverless SQL service and results are stored in S3
	• The reporting bucket contains analyzed data and can be used by
	  reporting tool such as AWS QuickSight, Redshift, etc.




AWS Analytics Services:
	Athena
	EMR
	CloudSearch
	Elasticsearch Service
	Kinesis
	Redshift
	Data Pipeline
	QuickSight
	Glue
	Lake Formation




==| Machine Learning
84
##Rekognition
-------------------------------------------------
• Find objects, people, text, scenes in images and videos using ML
• Facial analysis and facial search to do user verification, people counting
• Create a database of “familiar faces” or compare against celebrities


Amazon Rekognition – Content Moderation:
• Detect content that is inappropriate, unwanted, or offensive (image and videos)
• Used in social media, broadcast media, advertising, and e-commerce situations to create
  a safer user experience
• Set a Minimum Confidence Threshold for items that will be flagged
• Flag sensitive content for manual review in Amazon Augmented AI (A2I)
• Help comply with regulations



Area:
=====
MayRegional


UseCase:
========
• Labeling
• Content Moderation
• Text Detection
• Face Detection and Analysis (gender, age range, emotions…)
• Face Search and Verification
• Celebrity Recognition
• Pathing (ex: for sports game analysis)



BillCost:
=========
As part of the AWS Free Tier, 
Rekognition there are 3 different types of usage
Amazon Rekognition Image pricing 
Amazon Rekognition Video pricing
Amazon Rekognition Custom Labels pricing



Security:
=========
It is a aws managed service, security is a shared responsibility.





85
##Transcribe
-------------------------------------------------
Automatically convert speech to text
Uses a deep learning process called automatic speech recognition (ASR) to convert speech to text quickly and accurately
Automatically remove Personally Identifiable Information (PII) using Redaction


Area:
=====
MayRegional



UseCase:
========
• transcribe customer service calls
• automate closed captioning and subtitling
• generate metadata for media assets to create a fully searchable archive


Get insights from customer conversations
Search and analyze media content
Create subtitles and meeting notes
Improve clinical documentation


BillCost:
=========
AWS Free Tier
Amazon Transcribe API for both streaming and batch transcriptions is 
billed monthly based on the tiered pricing.



Security:
=========
It is a aws managed service, security is a shared responsibility.






86
##Polly
-------------------------------------------------
Amazon Polly is a service that turns text into lifelike speech, allowing you to create applications that talk, 
and build entirely new categories of speech-enabled products. 
• Turn text into lifelike speech using deep learning 
• Allowing you to create applications that talk



Amazon Polly – Lexicon & SSML:
• Customize the pronunciation of words with Pronunciation lexicons
	• Stylized words: St3ph4ne => “Imran”
	• Acronyms: AWS => “Amazon Web Services”
• Upload the lexicons and use them in the SynthesizeSpeech operation
• Generate speech from plain text or from documents marked up with Speech 
  Synthesis Markup Language (SSML) – enables more customization.
	• emphasizing specific words or phrases
	• using phonetic pronunciation
	• including breathing sounds, whispering
	• using the Newscaster speaking style


Area:
=====
MayRegional



UseCase:
========
Content creation
Audio can be used as a complementary media to written and/or visual communication.



BillCost:
=========
AWS Free Tier
PAY-AS-YOU-GO MODEL
You are billed monthly for the number of characters of text that you processed.


Security:
=========
It is a aws managed service, security is a shared responsibility.







87
##Translate
-------------------------------------------------
Amazon Translate is a text translation service, to translate text between languages.

• Natural and accurate language translation
• Amazon Translate allows you to localize content - such as websites and applications - 
  for international users, and to easily translate large volumes of text efficiently.


Area:
=====
MayRegional


UseCase:
========
Language Localization
It’s very difficult for human translation teams to keep up with dynamic or real-time content.



BillCost:
=========
AWS Free Tiering
Amazon Translate, you pay-as-you-go based on the number of characters of text that you processed.



Security:
=========
It is a aws managed service, security is a shared responsibility.





88
##Amazon Lex & Connect
-------------------------------------------------
With Amazon Lex, you can build conversational interactions (bots) that feel natural to your customers. 


Amazon Lex: (same technology that powers Alexa)
• Automatic Speech Recognition (ASR) to convert speech to text
• Natural Language Understanding to recognize the intent of text, callers
• Helps build chatbots, call center bots

Amazon Connect:
• Receive calls, create contact flows, cloud-based virtual contact center
• Can integrate with other CRM systems or AWS
• No upfront payments, 80% cheaper than traditional contact center solutions



Area:
=====
MayRegional


UseCase:
========

BillCost:
=========
Security:
=========
It is a aws managed service, security is a shared responsibility.




89
##Comprehend
-------------------------------------------------
Amazon Comprehend provides natural language processing, Personal Identifiable Information (PII) detection and redaction, 
Custom Classification and Entity detection, and topic modeling, enabling a broad range of 
applications that can analyze raw text, and with some APIs, document formats like PDF and Word.

For Natural Language Processing – NLP:
	• Fully managed and serverless service
	• Uses machine learning to find insights and relationships in text
	• Language of the text
	• Extracts key phrases, places, people, brands, or events
	• Understands how positive or negative the text is
	• Analyzes text using tokenization and parts of speech
	• Automatically organizes a collection of text files by topic



Area:
=====


UseCase:
========
• analyze customer interactions (emails) to find what leads to a positive or negative experience
• Create and groups articles by topics that Comprehend will uncover


Mine business and call center analytics
Index and search product reviews
Legal briefs management
Process financial documents


BillCost:
=========
Amazon Comprehend offers a free tier covering 50K units



Security:
=========
It is a aws managed service, security is a shared responsibility.




90
++Comprehend Medical Overview
-------------------------------------------------
Amazon Comprehend Medical detects and returns useful information in unstructured clinical text:
	• Physician’s notes
	• Discharge summaries
	• Test results
	• Case notes
• Uses NLP to detect Protected Health Information (PHI) – DetectPHI API
• Store your documents in Amazon S3, analyze real-time data with Kinesis
  Data Firehose, or use Amazon Transcribe to transcribe patient narratives
  into text that can be analyzed by Amazon Comprehend Medical.






91
##SageMaker
-------------------------------------------------
Amazon SageMaker is a fully managed machine learning service. 
With SageMaker, data scientists and developers can quickly and easily build and train machine learning models, 
and then directly deploy them into a production-ready hosted environment. 



Fully managed service for developers / data scientists to build ML models
• Typically, difficult to do all the processes in one place + provision servers
• Machine learning process (simplified): predicting your exam score


Area:
=====
MayRegional

UseCase:
========


BillCost:
=========


Security:
=========
It is a aws managed service, security is a shared responsibility.





92
##Forecast
-------------------------------------------------
Fully managed service that uses ML to deliver highly accurate forecasts
• Example: predict the future sales of a raincoat
• 50% more accurate than looking at the data itself
• Reduce forecasting time from months to hours
• Use cases: Product Demand Planning, Financial Planning, Resource Planning


Area:
=====


UseCase:
========
Add ML forecasting to your SaaS solutions
Enhance software as a service (SaaS) product capabilities with integrated ML-based forecasts to identify complex 
demand relationships.
Optimize product demand planning

Predict inventory needs at individual stores by combining historical sales and demand data with associated web traffic, 
pricing, product category, weather, and holiday information.

Improve utilization and customer satisfaction with accurate resource requirement forecasting in near-real time.



BillCost:
=========
AWS Free Tier

There are four different types of costs to consider when using Amazon Forecast:
 
	Imported data: Cost for each GB of data imported into Amazon Forecast for training and forecasting.

	Training a predictor: Cost for each hour of infrastructure use required for building a custom predictor 
	based on your input data or for monitoring predictor performance.

	Generated forecast data points:  Cost for number of unique forecast values generated across all time 
	series (items and dimensions) combinations.

	Forecast explanations: Cost for explaining the impact of attributes or related data on your forecasts for each 
	item and time point. 




Security:
=========
It is a aws managed service, security is a shared responsibility.





93
##Kendra
-------------------------------------------------
Fully managed document search service powered by Machine Learning
• Extract answers from within a document (text, pdf, HTML, PowerPoint, MS Word, FAQs…)
• Natural language search capabilities
• Learn from user interactions/feedback to promote preferred results (Incremental Learning)
• Ability to manually fine-tune search results (importance of data, freshness, custom, …)


Area:
=====
MayRegional


UseCase:
========
Accelerate research and development
Minimize regulatory and compliance risks
Improve customer interactions
Increase employee productivity


BillCost:
=========
AWS Free Tier



Security:
=========
It is a aws managed service, security is a shared responsibility.






94
##Personalize
-------------------------------------------------
Fully managed ML-service to build apps with real-time personalized recommendations
• Example: personalized product recommendations/re-ranking, customized direct marketing
	• Example: User bought gardening tools, provide recommendations on the next one to buy
• Same technology used by Amazon.com
• Integrates into existing websites, applications, SMS, email marketing systems, …
• Implement in days, not months (you don’t need to build, train, and deploy ML solutions)



Area:
=====
MayRegional

UseCase:
========
• Use cases: retail stores, media and entertainment…


BillCost:
=========

Security:
=========
It is a aws managed service, security is a shared responsibility.




95
##Textract
-------------------------------------------------
• Automatically extracts text, handwriting, and data from any scanned documents using AI and ML
• Extract data from forms and tables
• Read and process any type of document for extracts text (PDFs, images, …)


Area:
=====
MayRegional


UseCase:
========
• Financial Services (e.g., invoices, financial reports)
• Healthcare (e.g., medical records, insurance claims)
• Public Sector (e.g., tax forms, ID documents, passports)

Personalized recommendations
Similar items
Personalized reranking (i.e. rerank a list of items for a user)
Personalized promotions/notifications


BillCost:
=========
2 months free


Security:
=========
It is a aws managed service, security is a shared responsibility.




AWS Machine Learning- Summary:
• Rekognition: face detection, labeling, celebrity recognition
• Transcribe: audio to text (ex: subtitles) 
• Polly: text to audio 
• Translate: translations 
• Lex: build conversational bots – chatbots 
• Connect: cloud contact center 
• Comprehend: natural language processing 
• SageMaker: machine learning for every developer and data scientist 
• Forecast: build highly accurate forecasts 
• Kendra: ML-powered search engine 
• Personalize: real-time personalized recommendations 
• Textract: detect text and data in documents








==|AWS Monitoring, Audit and Performance

96
##AWS Monitoring 
-------------------------------------------------
AWS monitoring is a set of practices you can use to verify the security and performance of your AWS resources and data.

Use a single platform for observability, Collect metrics on AWS and on premises.






97
##CloudWatch 
-------------------------------------------------
AWS CloudWatch is made up of multiple monitoring tools such as:
	Events - You can trigger an action based on an event.
	Alarms - With alarms, you need to define a threshold, a condition, and what to trigger.
	Logs - CloudWatch logs allow you to store the log files for various sources such as 
		EC2 instances, CloudTrail, and many more. You can then use these logs to detect issues, 
		find leaks, patterns, and so on.
		
		
CloudWatch enables you to monitor your complete stack (applications, infrastructure, and services) 
and use alarms, logs, and events data to take automated actions and reduce mean time to resolution (MTTR). 


CloudWatch is basically a metrics repository. EC2—puts metrics into the repository, and you retrieve statistics based on those metrics. 
If you put your own custom metrics into the repository, you can retrieve statistics on these metrics as well.

Metrics are data about the performance of your systems. 
CloudWatch alarms perform an action when a CloudWatch metric exceeds a specified value for some amount of time.

Amazon CloudWatch can load all the metrics in your account 
(both AWS resource metrics and application metrics that you provide) for search, graphing, and alarms.




Amazon CloudWatch cannot track the API calls to your AWS resources.
CloudWatch is all about performance. CloudTrail is all about auditing also API call.


• EC2 instance metrics have metrics “every 5 minutes”
• With detailed monitoring (for a cost), you get data “every 1 minute”

Install the CloudWatch agent to all the EC2 instances that gather the memory and disk utilization data. 
View the custom metrics in the Amazon CloudWatch console.

By default, CloudWatch does not automatically provide memory and disk utilization metrics of your instances. 
You have to set up custom CloudWatch metrics to monitor the memory, 
disk swap, disk space, and page file utilization of your instances.

Enhanced Monitoring is a feature of Amazon RDS. 
By default, Enhanced Monitoring metrics are stored for 30 days in the CloudWatch Logs.


CloudWatch Alarms only monitor service metrics, not changes in DynamoDB table data.


Custom metrics that you can set up:
– Memory utilization
– Disk swap utilization
– Disk space utilization
– Page file utilization
– Log collection


Area:
=====
Regional
All dashboards are global, not region-specific.



UseCase:
========
 - Use a single platform for observability
 - Collect metrics on AWS and on premises
 - Improve operational performance and resource optimization
 - Get operational visibility and insight
 - Derive actionable insights from logs
 
-Monitor Amazon EC2
-Monitor Other Amazon Web Services Resources
-Monitor Custom Metrics
-Monitor and Store Logs
-Set Alarms
-Monitor and React to Resource Changes
 
 
 
BillCost:
=========
You simply pay for what you use and will be charged at the end of the month for your usage.




AWS X-Ray:
-------------------------------------------------
It helps you debug and analyze your microservices applications with request tracing 
so you can find the root cause of issues and performance.

AWS X-Ray is NOT a suitable service to use to track each API call to your AWS resources. 








98
##AWS CloudWatch Metrics
-------------------------------------------------
Metrics are data about the performance of your systems. 

By default, many services provide free metrics for resources 
such as Amazon EC2 instances, Amazon EBS volumes, and Amazon RDS DB instances. 

You can also enable detailed monitoring for some resources, such as your Amazon EC2 instances, 
or publish your own application metrics. 

Amazon CloudWatch can load all the metrics in your account 
(both AWS resource metrics and application metrics that you provide) for search, graphing, and alarms.


• CloudWatch provides metrics for every services in AWS
• Metric is a variable to monitor (CPUUtilization, NetworkIn…)
• Metrics belong to namespaces
• Dimension is an attribute of a metric (instance id, environment, etc…).
• Up to 10 dimensions per metric
• Metrics have timestamps
• Can create CloudWatch dashboards of metrics
• Can create CloudWatch Custom Metrics (for the RAM for example)

Option to filter metrics to only stream a subset of them.


Dimensions are name/value pairs that categorize metric characteristics. 
Each metric you create can have up to 10 dimensions defined. 
You can use these dimensions to distinguish between multiple instances of the same service and to 
filter metrics by service use. For example, you can assign InstanceId dimensions to your 
EC2 instances to distinguish between them for monitoring.





$$
99
##CloudWatch Custom Metrics
-------------------------------------------------
A custom metric enables you to monitor a specific application binary or runtime. 
CloudWatch helps you monitor the infrastructure portion of an EC2 instance, such as CPU, hard disk and network.

When CloudWatch doesn't have a metric for your specific use case, you'll want to implement a custom metric.
You can define custom metrics for your own use, and Elastic Beanstalk will push those metrics to Amazon CloudWatch.

Possibility to define and send your own custom metrics to CloudWatch
• Example: memory (RAM) usage, disk space, number of logged in users …
• Use API call PutMetricData


Important: Accepts metric data points is two weeks in the past and two hours in the
           future (make sure to configure your EC2 instance time correctly)



CloudWatch Metric Streams:
Continually stream CloudWatch metrics to a destination of your choice, with near-real-time delivery and low latency.
	• Amazon Kinesis Data Firehose (and then its destinations)
	• 3rd party service provider: Datadog, Dynatrace, New Relic, Splunk, Sumo Logic…
	• Option to filter metrics to only stream a subset of them.




$$
100
##CloudWatch Dashboards
-------------------------------------------------
Amazon CloudWatch dashboards are customizable home pages in the CloudWatch console that you can use to 
monitor your resources in a single view, even those resources that are spread across different Regions. 

You can use CloudWatch dashboards to create customized views of the metrics and alarms for your AWS resources.

• Dashboards can include graphs from different AWS accounts and regions
• Dashboards can be shared with people who don’t have an AWS account (public,
email address, 3rd party SSO provider through Amazon Cognito)



Area:
=====
Dashboards are global



UseCase:
========
Monitor your resources in a single view.



BillCost:
=========
• 3 dashboards (up to 50 metrics) for free
• $3/dashboard/month afterwards






101
##CloudWatch Logs
-------------------------------------------------
You can use Amazon CloudWatch Logs to monitor, store, and access your log files from  EC2 instances, AWS CloudTrail, Route 53, 
and other sources.

CloudWatch Logs enables you to centralize the logs from all of your systems, applications, and 
AWS services that you use, in a single, highly scalable service.


CloudWatch Logs - Sources:
• EC2
• SDK, CloudWatch Logs Agent, CloudWatch Unified Agent 
• ElasticBeanstalk: collection of logs from application 
• ECS: collection from containers 
• AWS Lambda: collection from function logs 
• VPC Flow Logs: VPC specific logs 
• API Gateway 
• CloudTrail based on filter 
• Route53: Log DNS queries


CloudWatch Logs Metric Filter & Insights:
• CloudWatch Logs can use filter expressions
	• For example, find a specific IP inside of a log
	• Or count occurrences of “ERROR” in your logs
• Metric filters can be used to trigger CloudWatch alarms
• CloudWatch Logs Insights can be used to query logs and add queries to CloudWatch Dashboards



CloudWatch Logs – S3 Export:
• Log data can take up to 12 hours to become available for export
• The API call is CreateExportTask
• Not near-real time or real-time… use Logs Subscriptions instead




• Log groups: arbitrary name, usually representing an application
• Log stream: instances within application / log files / containers
• Can define log expiration policies (never expire, 30 days, etc..)


CloudWatch Logs can send logs to:
	• Amazon S3 (exports)
	• Kinesis Data Streams
	• Kinesis Data Firehose
	• AWS Lambda
	• ElasticSearch


CloudWatch Logs for EC2:
	• By default, no logs from your EC2 machine will go to CloudWatch
	• You need to run a CloudWatch agent on EC2 to push the log files you want
	• Make sure IAM permissions are correct
	• The CloudWatch log agent can be setup on-premises too


CloudWatch Logs enables you to centralize the logs from all of your systems, applications, and AWS services 
that you use, in a single, highly scalable service.

CloudWatch Logs Insights enables you to interactively search and analyze your log data in 
Amazon CloudWatch Logs. 
You can perform queries to help you more efficiently 
and effectively respond to operational issues like: Top 10 IP to make request.
 
If an issue occurs, you can use 
CloudWatch Logs Insights to identify potential causes and validate deployed fixes.

Like: Top ten IP address for hit the api.

Area:
=====
MayRegional


UseCase:
========
Query your log data
Monitor logs from Amazon EC2 instances 
Monitor AWS CloudTrail logged events 
Log retention


BillCost:
=========
There are 2 components to the price you pay:
	1) ingestion costs: you pay when you send/upload the logs
	2) storage costs: you pay to keep the logs around.


Security:
=========
It is a aws managed service, security is a shared responsibility.






102
##CloudWatch Agent | Logs Agent
-------------------------------------------------
The CloudWatch Logs agent provides an automated way to send log data to CloudWatch Logs from Amazon EC2 instances. 

The agent includes the following components:
A plug-in to the AWS CLI that pushes log data to CloudWatch Logs.
A script (daemon) that initiates the process to push data to CloudWatch Logs.
A cron job that ensures that the daemon is always running.



CloudWatch Logs Agent & Unified Agent:
• For virtual servers (EC2 instances, on-premises servers…)
• CloudWatch Logs Agent
	• Old version of the agent
	• Can only send to CloudWatch Logs
• CloudWatch Unified Agent
	• Collect additional system-level metrics such as RAM, processes, etc…
	• Collect logs to send to CloudWatch Logs
	• Centralized configuration using SSM Parameter Store


CloudWatch Unified Agent – Metrics:
	• Collected directly on your Linux server / EC2 instance
	• CPU (active, guest, idle, system, user, steal)
	• Disk metrics (free, used, total), Disk IO (writes, reads, bytes, iops)
	• RAM (free, inactive, used, total, cached)
	• Netstat (number of TCP and UDP connections, net packets, bytes)
	• Processes (total, dead, bloqued, idle, running, sleep)
	• Swap Space (free, used, used %)
Reminder: out-of-the box metrics for EC2 – disk, CPU, network (high level)






103
##CloudWatch Alarms
-------------------------------------------------
An alarm watches a single metric over a specified time period, and performs one or more specified actions,
based on the value of the metric relative to a threshold over time. 

You can attach multiple Alarms to each metric and each one can have multiple actions.
A CloudWatch Alarm is always in one of three states: OK, ALARM, or INSUFFICIENT_DATA
 
CloudWatch Alarm Targets:
	• Stop, Terminate, Reboot, or Recover an EC2 Instance
	• Trigger Auto Scaling Action
	• Send notification to SNS (from which you can do pretty much anything)


• Recovery: Same Private, Public, Elastic IP, metadata, placement group


To test alarms and notifications, set the alarm state to Alarm using CLI
=>aws cloudwatch set-alarm-state --alarm-name "myalarm" --state-value ALARM --state-reason "testing purposes"





104
##CloudWatch Events
-------------------------------------------------
Event:
An event indicates a change in an environment such as an AWS environment, a SaaS partner service or application, 
or one of your applications or services. 

The following are examples of events:
Amazon EC2 generates an event when the state of an instance changes from pending to running.
Amazon EC2 Auto Scaling generates events when it launches or terminates instances.
AWS CloudTrail publishes events when you make API calls.

You can also set up scheduled events that are generated on a periodic basis.
Events are represented as JSON objects and they all have a similar structure, and the same top-level fields.


Amazon CloudWatch Events delivers a near real-time stream of system events that describe changes in 
Amazon Web Services (AWS) resources.

• Event Pattern: Intercept events from AWS services (Sources)
	• Example sources: EC2 Instance Start, CodeBuild Failure, S3, Trusted Advisor
	• Can intercept any API call with CloudTrail integration
• Schedule or Cron (example: create an event every 4 hours)

• A JSON payload is created from the event and passed to a target…
	• Compute: Lambda, Batch, ECS task
	• Integration: SQS, SNS, Kinesis Data Streams, Kinesis Data Firehose
	• Orchestration: Step Functions, CodePipeline, CodeBuild
	• Maintenance: SSM, EC2 Actions




105
##EventBridge
-------------------------------------------------
Amazon EventBridge is a serverless event bus that makes it easier to build event-driven applications at scale 
by leveraging events generated by your applications, integrated SaaS applications, and AWS services. 

EventBridge streams real-time data from event sources like Zendesk or Shopify to destinations like AWS 
Lambda and other SaaS applications.

Those events can come from state changes of AWS services, other AWS accounts, or external 
applications like Auth0, Shopify, and others. You can, of course, also send your custom messages.


Customers can integrate their own AWS applications with microservices, SaaS applications, and custom 
applications as event sources that publish events to an event bus. 

You can define a filtering rule to filter events and route them to AWS service 
targets and API destinations (via HTTP endpoints).

EventBridge sends metrics to Amazon CloudWatch every minute for everything from the number of 
matched events to the number of times a target is invoked by a rule.


In EventBridge, there are three types of event bus:
	Default event bus: Every account has a default bus, which accepts events emitted by AWS services.
	Partner event bus: This can receive events from applications hosted at AWS SaaS partners. 
		For each SaaS partner, a separate event bus needs to be created.
	Custom event bus: This type can receive events from custom applications and services.


• Event buses can be accessed by other AWS accounts



Amazon EventBridge vs CloudWatch Events:
• Amazon EventBridge builds upon and extends CloudWatch Events.
• It uses the same service API and endpoint, and the same underlying service infrastructure.
• EventBridge allows extension to add event buses for your custom applications and your third-party SaaS apps.
• Event Bridge has the Schema Registry capability
• EventBridge has a different name to mark the new capabilities
• Over time, the CloudWatch Events name will be replaced with EventBridge.

You can use Amazon EventBridge to run Amazon ECS tasks when certain AWS events occur. 
You can set up an EventBridge rule that runs an Amazon ECS task whenever a file is uploaded to a certain Amazon S3 bucket using 
the Amazon S3 PUT operation.


• Event buses can be accessed by other AWS accounts using Resource-based Policies
• You can archive events (all/filter) sent to an event bus (indefinitely or set period)
• Ability to replay archived events



What is a schema?
A schema represents the structure of an event, and commonly includes information such as the title and type 
of each piece of data. Schema can be versioned.

The Schema Registry:
	The registry also enables you to generate code bindings for programming languages like Java, Python, 
	or TypeScript directly in your IDE, allowing you to use the event as an object in your code.



Whatever the event type you're acting upon, your application will need to know the basic structure of the event. 


Amazon EventBridge – Resource-based Policy:
• Manage permissions for a specific Event Bus
• Example: allow/deny events from another AWS account or AWS region
• Use case: aggregate all events from your AWS Organization in a single AWS account or AWS region



CloudWatch Container Insights:
• Collect, aggregate, summarize metrics and logs from containers
• Available for containers on…
	• Amazon Elastic Container Service (Amazon ECS)
	• Amazon Elastic Kubernetes Services (Amazon EKS)
	• Kubernetes platforms on EC2
	• Fargate (both for ECS and EKS)
In Amazon EKS and Kubernetes, CloudWatch Insights is using a containerized version 
of the CloudWatch Agent to discover containers.



CloudWatch Lambda Insights:
	• Monitoring and troubleshooting solution for serverless applications running on AWS Lambda
	• Collects, aggregates, and summarizes system-level metrics including CPU time, memory, disk, and network
	• Collects, aggregates, and summarizes diagnostic information such as cold starts and Lambda worker shutdowns
	• Lambda Insights is provided as a Lambda Layer




CloudWatch Contributor Insights:
Contributor Insights helps you understand who or what is impacting your system and application performance 
by pinpointing outliers, finding the heaviest traffic patterns, and ranking the top system processes. 

	• Analyze log data and create time series that display contributor data.
	• See metrics about the top-N contributors
	• The total number of unique contributors, and their usage.
	• This helps you find top talkers and understand who or what is impacting system performance.
	• Works for any AWS-generated logs (VPC, DNS, etc..)
	• For example, you can find bad hosts, identify the heaviest network users, or find the 
	  URLs that generate the most errors.
	• You can build your rules from scratch, or you can also use sample rules that AWS has 
	  created – leverages your CloudWatch Logs
	• CloudWatch also provides built-in rules that you can use to analyze metrics from other AWS services.



CloudWatch Application Insights:
	• Provides automated dashboards that show potential problems with monitored applications, 
	  to help isolate ongoing issues.
	• Findings and alerts are sent to Amazon EventBridge and SSM OpsCenter




CloudWatch Insights and Operational Visibility | Summary:
	• CloudWatch Container Insights
		• ECS, EKS, Kubernetes on EC2, Fargate, needs agent for Kubernetes
		• Metrics and logs
	• CloudWatch Lambda Insights
		• Detailed metrics to troubleshoot serverless applications
	• CloudWatch Contributors Insights
		• Find “Top-N” Contributors through CloudWatch Logs
	• CloudWatch Application Insights
		• Automatic dashboard to troubleshoot your application and related AWS services






Area:
=====
MayRegional



UseCase:
========
Build event-driven architectures
Write less custom code



BillCost:
=========
Free tier: All state change events published by AWS services by default are free.
Amazon EventBridge, you pay for events published to your event bus, events ingested for 
Schema Discovery, and Event Replay. 
There are no additional charges for rules or event delivery. 
Glue



Security:
=========
It is a aws managed service, security is a shared responsibility.






105
##CloudTrail
-------------------------------------------------
The two services, CloudWatch and CloudTrail, can be used together. 

CloudTrail is primarily used to monitor and record the account activity across your 
AWS resources and not your web applications. 
 
You cannot use CloudTrail to capture the detailed 
information of all HTTP requests that go through your public-facing Application Load Balancer (ALB). 
 
CloudTrail can only track the resource changes made to your ALB, but not the actual IP traffic that goes through it.


• Provides governance, compliance and audit for your AWS Account
• CloudTrail is enabled by default!
• Can put logs from CloudTrail into CloudWatch Logs or S3
• A trail can be applied to All Regions (default) or a single Region.
• If a resource is deleted in AWS, investigate CloudTrail first!


Configure AWS CloudTrail to record all account activity - AWS recommends to turn on 
CloudTrail to log all IAM actions for monitoring and audit purposes.


Actions taken by a user, role, or an AWS service are recorded as events in CloudTrail. 
Events include actions taken in the AWS Management Console, AWS Command Line Interface, and AWS SDKs and APIs.


There are two types of events that can be logged in CloudTrail: 
Management events and Data events.

Management events provide information about management operations that are performed on resources in your AWS account.
Think of Management events as things normally done by people when they are in AWS. 

Examples:
a user sign in
a policy changed
a newly created security configuration
a logging rule deletion


Data events provide information about the resource operations performed on or in a resource.
Think of Data events as things normally done by software when hitting various AWS endpoints. 

Examples:
S3 object-level API activity
Lambda function execution activity
By default, CloudTrail logs management events, but not data events.


CloudTrail Events
• Management Events:
	• Operations that are performed on resources in your AWS account
  Examples:
	• Configuring security (IAM AttachRolePolicy)
	• Configuring rules for routing data (Amazon EC2 CreateSubnet)
	• Setting up logging (AWS CloudTrail CreateTrail)
	• By default, trails are configured to log management events.
	• Can separate Read Events (that don’t modify resources) from Write Events (that may modify resources)
• Data Events:
	• By default, data events are not logged (because high volume operations)
	• Amazon S3 object-level activity (ex: GetObject, DeleteObject, PutObject): can separate Read and Write Events
	• AWS Lambda function execution activity (the Invoke API)


CloudTrail Insights:
CloudTrail Insights automatically analyzes write management events from 
CloudTrail trails and alerts you to unusual activity.

CloudTrail Insights is designed to automatically( Amazon applying Machine Learning to your CloudTrail Events) 
analyze management events from your CloudTrail trails to 
establish a baseline for normal behavior, and then raise issues by generating Insights events when it 
detects unusual patterns.

  Enable CloudTrail Insights to detect unusual activity in your account:
	• inaccurate resource provisioning
	• hitting service limits
	• Bursts of AWS IAM actions
	• Gaps in periodic maintenance activity
• CloudTrail Insights analyzes normal management events to create a baseline
	• And then continuously analyzes write events to detect unusual patterns
	• Anomalies appear in the CloudTrail console
	• Event is sent to Amazon S3
	• An EventBridge event is generated (for automation needs)

CloudTrail Events Retention:
• Events are stored for 90 days in CloudTrail
• To keep events beyond this period, log them to S3 and use Athena


The difference between CloudTrail Insights and CloudTrail Events? 
CloudTrail Events are a log of every event that takes place inside your AWS environment 
whereas CloudTrail Insights applies 
Machine Learning to report on “insights” into groups of events that deviate from expected behavior.

CloudWatch mainly monitors performance, whereas CloudTrail mainly monitors actions in your AWS environment.



UseCase:
========
AWS CloudTrail is a service that automatically records events such as AWS API calls. 
Audit activity: Monitor, store, and validate activity events for authenticity. 



BillCost:
=========
For CloudTrail Insights, choose which event to analyze and pay as you go.
For AWS CloudTrail Lake, you pay for ingestion and storage together. 
You can store your data for up to seven years. 
Choose which data to analyze, and pay as you go.



Security:
=========
Track user activity and API usage AWS CloudTrail.
	
By default, CloudTrail log files are encrypted using Amazon S3 Server Side Encryption (SSE) 
and placed into your S3 bucket. 
You can control access to log files by applying IAM or S3 bucket policies. 
You can add an additional layer of security by 
enabling S3 Multi Factor Authentication (MFA) Delete on your S3 bucket.






106
##AWS Config
-------------------------------------------------
AWS Config provides a detailed view of the configuration of AWS resources in your AWS account. 

AWS Config is a service that enables you to assess, audit, and evaluate the configurations of 
your AWS resources. 

Config continuously monitors and records your AWS resource configurations and allows you to 
automate the evaluation of recorded configurations against desired configurations.

• Helps with auditing and recording compliance of your AWS resources
• Helps record configurations and changes over time

• Questions that can be solved by AWS Config:
	• Is there unrestricted SSH access to my security groups?
	• Do my buckets have any public access?
	• How has my ALB configuration changed over time?
	• You can receive alerts (SNS notifications) for any changes
	• AWS Config is a per-region service
	• Can be aggregated across regions and accounts
	• Possibility of storing the configuration data into S3 (analyzed by Athena)


ConfigRule:
	AWS Config rules evaluate the configuration settings of your AWS resources. 
	A rule can run when AWS Config detects a configuration change to an AWS resource or at 
	a periodic frequency that you choose (for example, every 24 hours). 
	There are two types of rules: 
		AWS Config Managed Rules and 
		AWS Config Custom Rules. 
	Managed rules are predefined, customizable rules created by AWS Config.

After you activate a rule, AWS Config compares your resources to the conditions of the rule. 
After this initial evaluation, AWS Config continues to run evaluations each time one is triggered.

AWS Config Rules does not prevent actions from happening (no deny)
You can set Remediation Retries if the resource is still non-compliant after auto- remediation


Config Rules – Remediations:
Automate remediation of non-compliant resources using SSM Automation Documents
You can set Remediation Retries if the resource is still non-compliant after auto- remediation


Config Rules – Notifications:
• Use EventBridge to trigger notifications when AWS resources are noncompliant
• Ability to send configuration changes and compliance state notifications 
  to SNS (all events – use SNS Filtering or filter at client-side)




CloudWatch vs CloudTrail vs Config:
CloudWatch 
	• Performance monitoring (metrics, CPU, network, etc…) & dashboards 
	• Events & Alerting 
	• Log Aggregation & Analysis 
CloudTrail 
	• Record API calls made within your Account by everyone 
	• Can define trails for specific resources 
	• Global Service 
Config 
	• Record configuration changes 
	• Evaluate resources against compliance rules 
	• Get timeline of changes and compliance


For an Elastic Load Balancer:
CloudWatch:
	• Monitoring Incoming connections metric
	• Visualize error codes as % over time
	• Make a dashboard to get an idea of your load balancer performance
Config:
	• Track security group rules for the Load Balancer
	• Track configuration changes for the Load Balancer
	• Ensure an SSL certificate is always assigned to the Load Balancer (compliance)
CloudTrail:
	• Track who made any changes to the Load Balancer with API calls


Use the AWS Config managed rule to check if the IAM user access keys are not rotated within 90 days. 
Create an Amazon EventBridge (Amazon CloudWatch Events) rule for the non-compliant keys, 
and define a target to invoke a custom Lambda function to deactivate and delete the keys. 

AWS EventBridge can check for AWS Config non-compliant events and then trigger a Lambda for remediation.


AWS Config can only use Systems Manager Automation documents as remediation actions. 
Moreover, the AWS Batch service cannot be used for remediating non-compliant resources like IAM access keys.

Amazon EventBridge cannot directly check for IAM events that contain the age of IAM access keys. 
You have to use the AWS Config service to detect the non-compliant IAM keys.


Area:
=====
Regional


UseCase:
========
Helps with auditing and recording compliance of your AWS resources
• Helps record configurations and changes over time
• Questions that can be solved by AWS Config:
	• Is there unrestricted SSH access to my security groups?
	• Do my buckets have any public access?
	• How has my ALB configuration changed over time?
• Possibility of storing the configuration data into S3 (analyzed by Athena)


BillCost:
=========
Pricing: no free tier, $0.003 per configuration item recorded per region,
$0.001 per config rule evaluation per region



Security:
=========
Record and evaluate configurations of your AWS resources AWS Config
It is a aws managed service, security is a shared responsibility.






==|Advanced Identity in AWS
106
##Organizations
-------------------------------------------------
How do you centralize your management (billing, access control, compliance and security) across multiple AWS accounts?
Welcome AWS Organizations! administer them as a single unit.

Organize accounts into Organizational Units (OU) Provides API to automate creation of new accounts.

Global service
• Allows to manage multiple AWS accounts
• The main account is the management account
• Other accounts are member accounts
• Member accounts can only be part of one organization
• Consolidated Billing across all accounts - single payment method
• Pricing benefits from aggregated usage (volume discount for EC2, S3…)
• Shared reserved instances and Savings Plans discounts across accounts
• API is available to automate AWS account creation
• Allows to manage multiple AWS accounts
• The main account is the master account – you can’t change it
• Other accounts are member accounts
• Member accounts can only be part of one organization


Advantages:
	• Multi Account vs One Account Multi VPC
	• Use tagging standards for billing purposes
	• Enable CloudTrail on all accounts, send logs to central S3 account
	• Send CloudWatch Logs to central logging account
	• Establish Cross Account Roles for Admin purposes
Security: Service Control Policies (SCP):
	• IAM policies applied to OU or Accounts to restrict Users and Roles
	• They do not apply to the management account (full admin power)
	• Must have an explicit allow (does not allow anything by default – like IAM)



An organizational unit (OU) provides a means to group accounts within a root. An OU can also contain other OUs. 
When you attach a policy to one of the nodes in the hierarchy, it flows down and affects 
all the branches (OUs) and leaves (accounts) beneath it. An OU can have exactly one parent, and each account can be a 
member of exactly one OU.

Organizations typically have multiple AWS accounts
Different business units
Different environments



Features:
-One consolidated bill for all AWS accounts
-Centralized compliance management for AWS Config Rules
-Send AWS CloudTrail data to one S3 bucket (across accounts)
-AWS Firewall Manager to manage firewall rules (across accounts)
-AWS WAF, AWS Shield Advanced protections and Security Groups
-Use Service control policies (SCPs) to define restrictions for actions (across accounts).
-Prevent users from disabling AWS Config or changing its rules
-Require MFA to stop an Amazon EC2 instance
-Require a tag upon resource creation



Service Control Policies (SCP):
	SCPs are a type of organization policy that you can use to manage permissions in your organization. 

SCPs offer central control over the maximum available permissions for all accounts in your organization.

SCPs don't affect users or roles in the management account. 
They affect only the member accounts in your organization.

• Whitelist or blacklist IAM actions
• Applied at the OU or Account level
• Does not apply to the Master Account
• SCP is applied to all the Users and Roles of the Account, including Root user
• The SCP does not affect service-linked roles
	• Service-linked roles enable other AWS services to integrate with AWS Organizations and can't be restricted by SCPs.
• SCP must have an explicit Allow (does not allow anything by default)
Use cases:
	• Restrict access to certain services (for example: can’t use EMR)
	• Enforce PCI compliance by explicitly disabling services
Master Account Can do anything (no SCP apply)




AWS Organization – Moving Accounts:
To migrate accounts from one organization to another
	1. Remove the member account from the old organization
	2. Send an invite to the new organization
	3. Accept the invite to the new organization from the member account

If you want the master account of the old organization to also join the new organization, do the following:
	1. Remove the member accounts from the organizations using procedure above
	2. Delete the old organization
	3. Repeat the process above to invite the old master account to the new org


IAM Permission Boundaries( with User and Role):
• IAM Permission Boundaries are supported for users and roles (not groups)
• Advanced feature to use a managed policy to set the maximum permissions an IAM entity can get.

Use cases:
• Delegate responsibilities to non administrators within their permission boundaries, for example create new IAM users.
• Allow developers to self-assign policies and manage their own permissions, while
  making sure they can’t “escalate” their privileges (= make themselves admin).




Area:
=====
Global service.



UseCase:
========
Automate the creation of AWS accounts and categorize workloads using groups
Implement and enforce audit and compliance policies

• Enable CloudTrail on all accounts, send logs to central S3 account
• Send CloudWatch Logs to central logging account
• Establish Cross Account Roles for Admin purposes



BillCost:
=========
AWS Organizations is offered at no additional charge.



Security:
=========
It is a aws managed service, security is a shared responsibility.







107
++IAM Roles vs Resource Based Policies
-------------------------------------------------

• When you assume a role (user, application or service), you give up your original permissions and take the 
   permissions assigned to the role.
• When using a resource based policy, the principal doesn’t have to give up his permissions.
• Example: User in account A needs to scan a DynamoDB table in Account A and dump it in an S3 bucket in Account B.
• Supported by: Amazon S3 buckets, SNS topics, SQS queues.



How to access from AccA   to AccB S3 ?
	Way1
	From AccA asumeRole using STS (AWS Security Token Service),the Role of AccB to Access S3 of AccB.

	Way2
	Create a backet with the policy that allow the AccA in AccB.


Example:Supose  From AccA, scan a DaynamoDB table and dump it to AccB.

If you asumeRole from AccB, it not work. 
Because you asume a role of AccB then you lost the access of you self account access.
In this case you have to use resouece base policy.






++Amazon EventBridge – Security
-------------------------------------------------
When a rule runs, it needs permissions from the target.
	• Resource-based policy: Lambda, SNS, SQS, CloudWatch Logs, API Gateway…
	• IAM role: Kinesis stream, Systems Manager Run Command, ECS task…





108
##AWS Cognito
-------------------------------------------------
Cognito only helps with authentication, not authorization.
Amazon Cognito lets you add user sign-up, sign-in, and access control to your web and mobile apps easyly.

Amazon Cognito scales to millions of users and supports sign-in with social identity providers, 
such as Apple, Facebook, Google, and Amazon, and enterprise identity providers via SAML 2.0 and OpenID Connect.  



The two main components of Amazon Cognito are User pools and Identity pools. 
	User pools are user directories that provide sign-up and sign-in options for your app users. 
	With an Identity pool, your users can obtain temporary Amazon credentials to access Amazon services, 
	such as Amazon S3 and DynamoDB. 
	
You can use Identity pools and  User pools separately or together.


AWS Cognito:
• We want to give our users an identity so that they can interact with our application.
Cognito UserPools: appUser->CUP->ALBOrAPIGateway->LambdaOrOtherBackend
	• Sign in functionality for app users
	• Integrate with API Gateway
Cognito Identity Pools (Federated Identity):appUser->CUP/ThardPartyUser->CIP->S3OrDaynamoDB.
      For direct access AWS resource, without ALB or API Gateway.
	• Provide AWS credentials to users so they can access AWS resources directly
	• Integrate with Cognito User Pools as an identity provider.
	
CognitoSync:
	• Synchronize data from device to Cognito.
	• May be deprecated and replaced by AppSync

With Amazon Cognito, you can save user data in datasets that contain key-value pairs. 
Amazon Cognito associates this data with an identity in your identity pool so that your 
app can access it across logins and devices. To sync this data between the 
Amazon Cognito service and an end user’s devices, invoke the synchronize method. 
Each dataset can have a maximum size of 1 MB. You can associate up to 20 datasets with an identity.
https://blog.knoldus.com/starting-with-aws-cognito-sync/



Cognito User Pools (CUP):
	• Create a serverless database of user for your mobile apps
	• Simple login: Username (or email) / password combination
	• Possibility to verify emails / phone numbers and add MFA
	• Can enable Federated Identities (Facebook, Google, SAML…)
	• Sends back a JSON Web Tokens (JWT)
	• Can be integrated with API Gateway for authentication
Users can then access AWS services directly or through API Gateway.


Cognito Identity Pools (Federated Identities):
	• Get identities for “users” so they obtain temporary AWS credentials
	• Users source can be Cognito User Pools, 3rd party logins, etc…
	• Users can then access AWS services directly or through API Gateway
	• The IAM policies applied to the credentials are defined in Cognito
	• They can be customized based on the user_id for fine grained control
	• Default IAM roles for authenticated and guest users



Cognito= “hundreds of users”, ”mobile users”, “authenticate with SAML”

Area:
=====
Regional.
Amazon Cognito user pools are each created in one AWS Region, and they store the user profile data only in that region. 
User pools can send user data to a different AWS Region, depending on how optional features are configured.


UseCase:
========
Secure and scalable identity store
Social and enterprise identity federation
Standards-based authentication
Access control for AWS resources
Amazon Cognito provides solutions to control access to AWS resources from your app.
Easy integration with your app

For MobileUser, ThaousandOf User etc.



BillCost:
=========
Free Tier
If you are using Cognito Identity to create a User Pool, you pay based on your monthly active users (MAUs) only. 



Security:
=========
Amazon Cognito supports multi-factor authentication and encryption of data-at-rest and in-transit. 
Amazon Cognito is HIPAA eligible and PCI DSS, SOC, ISO/IEC 27001, ISO/IEC 27017, ISO/IEC 27018, and ISO 9001 compliant.





109
##IAM Identity Center | sso| Active Directory
-------------------------------------------------
AWS Single Sign-On (AWS SSO) is now AWS IAM Identity Center. It is where you create, or connect, 
your workforce users once and centrally manage their access to multiple AWS accounts and applications.

By default, AWS SSO now provides a directory that  you can use to create users, 
organize them in groups, and set permissions across those groups.

AWS IAM Identity Center (successor to AWS Single Sign-On) expands the capabilities of AWS Identity and 
Access Management (IAM) to provide a central place that brings together administration of users and their access to
AWS accounts and cloud applications.

AWS Single Sign-On (SSO):
• Centrally manage Single Sign-On to access multiple accounts and 3rd-party business applications.
• Integrated with AWS Organizations
• Supports SAML 2.0 markup
• Integration with on-premises Active Directory
• Centralized permission management
• Centralized auditing with CloudTrail


AWS IAM Identity Center (successor to AWS Single Sign-On) provides single sign-on access for all of your 
AWS accounts and cloud applications. It connects with Microsoft Active Directory through 
AWS Directory Service to allow users in that directory to sign in to a personalized 
AWS access portal using their existing Active Directory user names and passwords. 
From the AWS access portal, users have access to all the AWS accounts and cloud applications 
that they have permission for.


Amazon Cognito is used for single sign-on in mobile and web applications. 
You don’t have to use it if you already have an existing Directory Service to be used for authentication.

You can use SCP if you want to restrict or implement a policy across several accounts in the organization.


AWS by creating policies and attaching them to IAM identities (users, group, roles) or AWS resources.
A policy is an object in AWS that, when associated with an identity or resource, defines their permissions.

AWS supports six types of policies: 
	identity-based policies, 
	resource-based policies, 
	permissions boundaries, 
	AWS Organizations SCPs, 
	ACLs, and 
	session policies.



The AWS IAM authenticator for Kubernetes is integrated into the EKS cluster for role-based access control (RBAC) and 
cluster authentication.

Area:
=====
Global


UseCase:
========
Enable multi-account access to your AWS accounts: 
Your users can use their directory credentials for single sign-on access to multiple AWS accounts.
Enable single sign-on access to your AWS applications
Enable single sign-on access to Amazon EC2 Windows instances
Enable single sign-on access to cloud-based applications

BillCost:
=========
AWS SSO and its directory are available at no additional cost to you.



Security:
=========
Cloud single-sign-on (SSO) service
AWS IAM Identity Center (successor to AWS SSO)






==|AWS Security & Encryption
110
##AWS Security & Encryption
-------------------------------------------------
AWS provides services that help you protect your data, accounts, and workloads from unauthorized access.
	-Identity & access management
	-Network & application protection
	-Threat detection & continuous monitoring
	-Compliance & data privacy






111
##Encryption 101
-------------------------------------------------
Encryption in flight (SSL):
	• Data is encrypted before sending and decrypted after receiving
	• SSL certificates help with encryption (HTTPS)
	• Encryption in flight ensures no MITM (man in the middle attack) can happen



Server side encryption at rest:
	• Data is encrypted after being received by the server
	• Data is decrypted before being sent
	• It is stored in an encrypted form thanks to a key (usually a data key)
	• The encryption/decryption keys must be managed somewhere(KMS) and the server must have access to it.
It is for if the server in case hijack or stolen then data will be encryption from !


Client side encryption:
	• Data is encrypted by the client and never decrypted by the server
	• Data will be decrypted by a receiving client
	• The server should not be able to decrypt the data
	• Could leverage Envelope Encryption




114
##KMS (Key Management Service)
------------------------------------------------- 
KMS is a managed service that makes it easy for you to create and control the encryption 
keys used to encrypt your data. 

AWS also KMS uses Hardware Security Modules (HSMs) to protect the security of your keys. 
You can use AWS KMS to protect your data in AWS services and in your applications.

AWS KMS is integrated with AWS CloudTrail to provide you with logs of all key usage to help meet your 
regulatory and compliance needs.
AWS KMS keys must be in the same Region as the bucket.


• Anytime you hear “encryption” for an AWS service, it’s most likely KMS
• AWS manages encryption keys for us
• Fully integrated with IAM for authorization
• Easy way to control access to your data
• Able to audit KMS Key usage using CloudTrail 
  (Every single api with KMS Key call can track using cloudTrail ExamTip)
• Seamlessly integrated into most AWS services (EBS, S3, RDS, SSM…)
• Never ever store your secrets in plaintext, especially in your code!
	• KMS Key Encryption also available through API calls (SDK, CLI)
	• Encrypted secrets can be stored in the code / environment variables

Amazon S3 uses AWS KMS keys to encrypt your Amazon S3 objects. AWS KMS encrypts only the object data.



KMS Keys Types:
KMS Keys is the new name of KMS Customer Master Key
	• Symmetric (AES-256 keys)
		• Single encryption key that is used to Encrypt and Decrypt
		• AWS services that are integrated with KMS use Symmetric CMKs
		• You never get access to the KMS Key unencrypted (must call KMS API to use)
	• Asymmetric (RSA & ECC key pairs)
		• Public (Encrypt) and Private Key (Decrypt) pair
		• Used for Encrypt/Decrypt, or Sign/Verify operations
		• The public key is downloadable, but you can’t access the Private Key unencrypted
		• Use case: encryption outside of AWS by users who can’t call the KMS API

Use case: encryption outside of AWS by users who can’t call the KMS API



AWS KMS (Key Management Service):

Three types of KMS Keys:
	• AWS Managed Key: free (aws/service-name, example: aws/rds or aws/ebs)
	• Customer Managed Keys (CMK) created in KMS: $1 / month
	• Customer Managed Keys imported (must be 256-bit symmetric key): $1 / month
• + pay for API call to KMS ($0.03 / 10000 calls)

Automatic Key rotation:
	• AWS-managed KMS Key: automatic every 1 year
	• Customer-managed KMS Key: (must be enabled) automatic every 1 year
	• Imported KMS Key: only manual rotation possible using alias

Region1->encryptEBSSnapshort->region2->copySnapshortWithDifferentEncryptKey->backToEBSWithNewkey.


• Anytime you need to share sensitive information… use KMS
	• Database passwords
	• Credentials to external service
	• Private Key of SSL certificates
• Never ever store your secrets in plaintext, especially in your code!
• Encrypted secrets can be stored in the code / environment variables
• KMS can only help in encrypting up to 4KB of data per call
• If data > 4 KB, use envelope encryption

To give access to KMS to someone:
	• Make sure the Key Policy allows the user
	• Make sure the IAM Policy allows the API calls
	
	
KMS Key Policies:
• Control access to KMS keys, “similar” to S3 bucket policies
• Difference: you cannot control access without them
	• Default KMS Key Policy:
		• Created if you don’t provide a specific KMS Key Policy
		• Complete access to the key to the root user = entire AWS account
	• Custom KMS Key Policy:
		• Define users, roles that can access the KMS key
		• Define who can administer the key
		• Useful for cross-account access of your KMS key



Copying Snapshots across accounts:
	1. Create a Snapshot, encrypted with your own KMS Key (Customer Managed Key)
	2. Attach a KMS Key Policy to authorize cross-account access
	3. Share the encrypted snapshot
	4. (in target) Create a copy of the Snapshot, encrypt it with a CMK in your account
	5. Create a volume from the snapshot


AWS KMS enforces a waiting period. To delete a CMK in AWS KMS you schedule key deletion. 
You can set the waiting period from a minimum of 7 days up to a maximum of 30 days. 
The default waiting period is 30 days.

After the waiting period ends, AWS KMS deletes the KMS key, its aliases, and all related AWS KMS metadata.



Use AWS Key Management Service to create a CMK in a custom key store and store the non-extractable key material in AWS CloudHSM.



Area:
=====
AWS KMS has been strictly isolated to a single AWS Region for each implementation, with no sharing of keys, 
policies, or audit information across Regions. 
Region isolation can help you comply with security standards and data residency requirements.


UseCase:
========
Encrypt data in your applications
Digitally sign data
Built-in auditing


BillCost:
=========
Has Free tier.
Each AWS KMS key that you create in KMS costs $1/month (prorated hourly). 
Cost for request.
The standard AWS CloudHSM charges apply



Security:
=========	
AWS KMS is designed so that no one, including AWS employees, can retrieve your plaintext keys from the service. 
The service uses hardware security modules (HSMs) that have been validated under FIPS 140-2, or are in the process 
of being validated, to protect the confidentiality and integrity of your keys.




116
++KMS Multi-Region Keys
-------------------------------------------------
• Identical KMS keys in different AWS Regions that can be used interchangeably
• Multi-Region keys have the same key ID, key material, automatic rotation…

• Encrypt in one Region and decrypt in other Regions
• No need to re-encrypt or making cross-Region API calls

• KMS Multi-Region are NOT global (Primary + Replicas)
• Each Multi-Region key is managed independently

• Use cases: global client-side encryption, encryption on Global DynamoDB, Global Aurora



117
++DynamoDB Global Tables and KMS MultiRegion Keys Client-Side encryption:
-------------------------------------------------
• We can encrypt specific attributes client-side in our DynamoDB table using the Amazon DynamoDB Encryption Client
• Combined with Global Tables, the client-side encrypted data is replicated to other regions
• If we use a multi-region key, replicated in the same region as the DynamoDB Global table,
  then clients in these regions can use lowlatency API calls to KMS in their region to decrypt the data client-side
• Using client-side encryption we can protect specific fields and guarantee only decryption if the client has access to an API key





118
++Global Aurora and KMS Multi-Region Keyspace
-------------------------------------------------
Client-Side encryption
• We can encrypt specific attributes client-side in our Aurora table using the AWS Encryption SDK
• Combined with Aurora Global Tables, the client-side encrypted data is replicated to other regions
• If we use a multi-region key, replicated in the same region as the Global Aurora DB, then clients in these 
  regions can use low-latency API calls to KMS in their region to decrypt the data client-side.
• Using client-side encryption we can protect specific fields and guarantee only decryption
  if the client has access to an API key, we can protect specific fields even from database admins




119
++S3 Replication Encryption Considerations
-------------------------------------------------
• Unencrypted objects and objects encrypted with SSE-S3 are replicated by default
• Objects encrypted with SSE-C (customer provided key) are never replicated
• For objects encrypted with SSE-KMS, you need to enable the option
• Specify which KMS Key to encrypt the objects within the target bucket
• Adapt the KMS Key Policy for the target key
• An IAM Role with kms:Decrypt for the source KMS Key and kms:Encrypt for the target KMS Key
• You might get KMS throttling errors, in which case you can ask for a Service Quotas increase
• You can use multi-region AWS KMS Keys, but they are currently treated as
	independent keys by Amazon S3 (the object will still be decrypted and then
	encrypted)




120
++AMI Sharing Process Encrypted via KMS
-------------------------------------------------
1. AMI in Source Account is encrypted with KMS Key from Source Account
2. Must modify the image attribute to add a Launch
   Permission which corresponds to the specified target AWS account.
3. Must share the KMS Keys used to encrypted the snapshot the AMI references with the target account / IAM Role
4. The IAM Role/User in the target account must have the permissions to DescribeKey, ReEncrypted, CreateGrant, Decrypt
5. When launching an EC2 instance from the AMI, optionally the target account can specify a new KMS
   key in its own account to re-encrypt the volumes






121
##SSM Parameter Store | AWS Systems Manager
-------------------------------------------------
Parameter Store, a capability of AWS Systems Manager,  configuration data  management and secrets management. 
You can store data such as passwords, database strings, 
Amazon Machine Image (AMI) IDs, and license codes as parameter values. 
You can store values as plain text or encrypted data. 

You can reference Systems Manager parameters in your scripts, commands, SSM documents, and configuration and automation workflows 
by using the unique name that you specified when you created the parameter.


• Secure storage for configuration and secrets
• Optional Seamless Encryption using KMS
• Serverless, scalable, durable, easy SDK
• Version tracking of configurations / secrets
• Configuration management using path & IAM
• Notifications with CloudWatch Events
• Integration with CloudFormation


Store parameters reliably because Parameter Store is hosted in multiple Availability Zones in an AWS Region.



Parameters Policies (for advanced parameters):
• Allow to assign a TTL to a parameter (expiration date) to force updating or deleting sensitive data such as passwords
• Can assign multiple policies at a time


Area:
=====
MayRegional.


UseCase:
========
You can store data such as passwords, database strings, 
Amazon Machine Image (AMI) IDs, and license codes as parameter values.

BillCost:
=========
Standard parameters are available at no additional charge. When you create advanced parameters, 
you are charged based on the number of advanced parameters stored each month and per API interaction. 


Security:
=========
AWS Manage service.








122
##AWS Secrets Manager
-------------------------------------------------
This service enables you to easily rotate, manage, and retrieve database credentials, 
API keys, and other secrets throughout their lifecycle. 

Using Secrets Manager - you can secure and manage secrets used to access resources in the AWS Cloud, on 
third-party services, and on-premises.

Newer service, meant for storing secrets
• Capability to force rotation of secrets every X days
• Automate generation of secrets on rotation (uses Lambda)
• Integration with Amazon RDS (MySQL, PostgreSQL, Aurora)
• Secrets are encrypted using KMS
• Mostly meant for RDS integration


Provision, manage, and deploy public and private SSL/TLS certificates
AWS Certificate Manager

Rotate, manage, and retrieve secrets
AWS Secrets Manager


Need to pay for it $$$.
	



123
++AWS Certificate Manager (ACM)
-------------------------------------------------
Any SSL/TLS certificates created via ACM do not need any monitoring/intervention for expiration. 
ACM automatically renews such certificates. 

Certificates provided by ACM are automatically renewed. 
ACM does not automatically renew the certificates that you import. 

AWS Config to stream configuration changes and notifications to an Amazon SNS topic.

Easily provision, manage, and deploy TLS Certificates
• Provide in-flight encryption for websites (HTTPS)
• Supports both public and private TLS certificates
• Free of charge for public TLS certificates
• Automatic TLS certificate renewal
• Integrations with (load TLS certificates on)
	• Elastic Load Balancers (CLB, ALB, NLB)
	• CloudFront Distributions
	• APIs on API Gateway
• Cannot use ACM with EC2 (can’t be extracted)




124
++ACM – Requesting Public Certificates
-------------------------------------------------
1. List domain names to be included in the certificate 
	• Fully Qualified Domain Name (FQDN): corp.example.com 
	• Wildcard Domain: *.example.com
2. Select Validation Method: DNS Validation or Email validation 
	• DNS Validation is preferred for automation purposes 
	• Email validation will send emails to contact addresses in the WHOIS database 
	• DNS Validation will leverage a CNAME record to DNS config (ex: Route 53)
3. It will take a few hours to get verified
4. The Public Certificate will be enrolled for automatic renewal 
	• ACM automatically renews ACM-generated certificates 60 days before expiry



125
++ACM – Importing Public Certificates
-------------------------------------------------
• Option to generate the certificate outside of ACM and then import it
• No automatic renewal, must import a new certificate before expiry
• ACM sends daily expiration events starting 45 days prior to expiration
• The # of days can be configured
• Events are appearing in EventBridge
• AWS Config has a managed rule named acm-certificate-expiration-check
  to check for expiring certificates (configurable number of days)



126
++API Gateway - Endpoint Types
-------------------------------------------------
• Edge-Optimized (default): For global clients
	• Requests are routed through the CloudFront Edge locations (improves latency)
	• The API Gateway still lives in only one region
• Regional:
	• For clients within the same region
	• Could manually combine with CloudFront (more control over the caching strategies and the distribution)
• Private:
	• Can only be accessed from your VPC using an interface VPC endpoint (ENI)
	• Use a resource policy to define access



127
##WAF (Web Application Firewall)
-------------------------------------------------
AWS WAF is a web application firewall service that lets you monitor web requests and protect 
your web applications from malicious requests. 

Use AWS WAF to block or allow requests based on conditions that you specify, such as the IP addresses. 
You can also use AWS WAF preconfigured protections to block common attacks like 
SQL injection or cross-site scripting.

Deploy on
• Application Load Balancer
• API Gateway
• CloudFront
• AppSync GraphQL API
• Cognito User Pool

Define your Web ACL rules in WAF.

Define Web ACL (Web Access Control List):
• Rules can include: IP addresses, HTTP headers, HTTP body, or URI strings
• Protects from common attack - SQL injection and Cross-Site Scripting (XSS)
• Size constraints, geo-match (block countries)
• Rate-based rules (to count occurrences of events) – for DDoS protection




WAF – Fixed IP while using WAF with a Load Balancer
• WAF does not support the Network Load Balancer (Layer 4)
• We can use Global Accelerator for fixed IP and WAF on the ALB





You can use AWS WAF with your Application Load Balancer to allow or block requests based on 
the rules in a web access control list (web ACL). With geo match conditions you can choose the countries 
from which AWS WAF should allow access.


Geo Restriction feature of CloudFront helps in restricting traffic based on the user's geographic location. 
But, CloudFront works from edge locations and doesn't belong to a VPC.


AWS WAF is tightly integrated with (monitor the HTTP(S) requests) Amazon CloudFront, the 
Application Load Balancer (ALB),  Amazon API Gateway, and AWS AppSync.




Area:
=====
Global.



UseCase:
========
These conditions include 
IP addresses, HTTP headers, HTTP body, URI strings, SQL injection and cross-site scripting.



BillCost:
=========
AWS Firewall Manager protection policy - Monthly fee per Region.



Security:
=========
Filter malicious web traffic
WAF icon AWS Web Application Firewall (WAF)

Central management of firewall rules
AWS Firewall Manager







128
++Firwall Manager
-------------------------------------------------
 AWS Network Firewall is account-specific by default and needs to be integrated with the 
 AWS Firewall Manager to easily share the firewall across your other AWS accounts. 
 In addition, refactoring the web application will require an immense amount of time.
 
• Manage rules in all accounts of an AWS Organization
• Security policy: common set of security rules
	• WAF rules (Application Load Balancer, API Gateways, CloudFront)
	• AWS Shield Advanced (ALB, CLB, NLB, Elastic IP, CloudFront)
	• Security Groups for EC2, Application Load BAlancer and ENI resources in VPC
	• AWS Network Firewall (VPC Level)
	• Amazon Route 53 Resolver DNS Firewall
	• Policies are created at the region level
• Rules are applied to new resources as they are created (good for compliance) across all and future accounts in your Organization.




129
++WAF vs. Firewall Manager vs. Shield
-------------------------------------------------
• WAF, Shield and Firewall Manager are used together for comprehensive/complate protection.
• Define your Web ACL rules in WAF
• For granular protection of your resources, WAF alone is the correct choice
• If you want to use AWS WAF across accounts, accelerate WAF configuration,
  automate the protection of new resources, use Firewall Manager with AWS WAF
• Shield Advanced adds additional features on top of AWS WAF, such as dedicated
  support from the Shield Response Team (SRT) and advanced reporting.
• If you’re prone to frequent DDoS attacks, consider purchasing Shield Advanced




DDoS Protection from AWS:
-------------------------------------------------
https://stackarmor.com/protecting-against-a-distributed-denial-of-service-ddos-attack-using-aws/
https://www.imperva.com/blog/protecting-aws-ec2-ddos-attacks/\
http://salerno-rafael.blogspot.com/2019/07/what-should-we-know-about-aws-security_27.html

There are 7 best practices identified by AWS that can be leveraged to mitigate specific attack vectors.

Based on its DDoS-resilient reference architecture, AWS separates its network components into two categories:

1)AWS Edge Location	
	BP1 = CloudFront	
	BP2 = WAF	
	BP3 = Route53	
	BP4 = API Gateway
2)AWS Regions
	BP5 = VPCs
	BP6 = ELB
	BP7 = EC2 with Auto Scaling


130
++AWS Best Practices for DDoS Resiliency Edge Location Mitigation (BP1, BP3)
-------------------------------------------------
• BP1 – CloudFront
	• Web Application delivery at the edge
	• Protect from DDoS Common Attacks (SYN floods, UDP reflection…)
• BP1 – Global Accelerator
	• Access your application from the edge
	• Integration with Shield for DDoS protection
	• Helpful if your backend is not compatible with CloudFront
• BP3 – Route 53
	• Domain Name Resolution at the edge
	• DDoS Protection mechanism



131
++AWS Best Practices for DDoS Resiliency Best pratices for DDoS mitigation
-------------------------------------------------
• Infrastructure layer defense (BP1, BP3, BP6)
	• Protect Amazon EC2 against high traffic
	• That includes using Global Accelerator, Route 53, CloudFront, Elastic Load Balancing
• Amazon EC2 with Auto Scaling (BP7)
	• Helps scale in case of sudden traffic surges including a flash crowd or a DDoS attack
• Elastic Load Balancing (BP6)
	• Elastic Load Balancing scales with the traffic increases and will distribute the traffic to many EC2 instances



132
++AWS Best Practices for DDoS Resiliency Application Layer Defense
-------------------------------------------------
• Detect and filter malicious web requests (BP1, BP2)
	• CloudFront cache static content and serve it from edge locations, protecting your backend
	• AWS WAF is used on top of CloudFront and Application Load Balancer to filter and block requests based on request signatures
	• WAF rate-based rules can automatically block the IPs of bad actors
	• Use managed rules on WAF to block attacks based on IP reputation, or block anonymous Ips
	• CloudFront can block specific geographies
• Shield Advanced (BP1, BP2, BP6)
	• Shield Advanced automatic application layer DDoS mitigation automatically creates, evaluates and deploys AWS
  WAF rules to mitigate layer 7 attacks




133
++AWS Best Practices for DDoS Resiliency Attack surface reduction
-------------------------------------------------
• Obfuscating AWS resources (BP1, BP4, BP6)
	• Using CloudFront, API Gateway, Elastic Load Balancing to hide your backend resources (Lambda functions, EC2 instances)
• Security groups and Network ACLs (BP5)
	• Use security groups and NACLs to filter traffic based on specific IP at the subnet or ENI-level
	• Elastic IP are protected by AWS Shield Advanced
• Protecting API endpoints (BP4)
	• Hide EC2, Lambda, elsewhere
	• Edge-optimized mode, or CloudFront + regional mode (more control for DDoS)
	• WAF + API Gateway: burst limits, headers filtering, use API keys




134
##GuardDuty
-------------------------------------------------
Amazon GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized 
behavior to protect your  Amazon Web Services accounts, workloads, and data stored in Amazon S3. 

Intelligent Threat discovery to Protect AWS Account
• Uses Machine Learning algorithms, anomaly detection, 3rd party data
• One click to enable (30 days trial), no need to install software
•Input data includes:
	• CloudTrail Events Logs – unusual API calls, unauthorized deployments
		• CloudTrail Management Events – create VPC subnet, create trail, …
		• CloudTrail S3 Data Events – get object, list objects, delete object, …
	• VPC Flow Logs – unusual internal traffic, unusual IP address
	• DNS Logs – compromised EC2 instances sending encoded data within DNS queries
	• Kubernetes Audit Logs – suspicious activities and potential EKS cluster compromises
	
• Can setup CloudWatch Event rules to be notified in case of findings
• CloudWatch Events rules can target AWS Lambda or SNS
• Can protect against CryptoCurrency attacks (has a dedicated “finding” for it)


Disable the service in the general settings:
	Disabling the service will delete all remaining data, including your findings and configurations 
	before relinquishing the service permissions and resetting the service.

Suspend the service in the general settings :
	This will immediately stop the service from analyzing data, but does not delete your 
	existing findings or configurations.

Area:
=====
Regional


UseCase:
========
Improve security operations visibility
Assist security analysts in investigations
Protect AWS accounts with intelligent threat detection


BillCost:
=========
Start your 30-day free trial
GuardDuty prices are based on the volume of both analyzed service logs and data scanned for malware. 
 

Security:
=========
Protect AWS accounts with intelligent threat detection
Amazon GuardDuty
It is a aws managed service, security is a shared responsibility.





135
##Amazon Inspector
-------------------------------------------------
• Remember: only for EC2 instances and container infrastructure
• Continuous scanning of the infrastructure, only when needed
• Package vulnerabilities (EC2 & ECR) – database of CVE
• Network reachability (EC2)
• A risk score is associated with all vulnerabilities for prioritization



Area:
=====
Amazon Inspector is a Regional service



UseCase:
========
Automated vulnerability management



BillCost:
=========
Monthly costs are determined by a combination of two dimensions: Amazon EC2 instances being scanned, and the total number of 
container images initially scanned when pushed to Amazon Elastic Container Registry (ECR) and rescanned during a month
Free 15-day trial for accounts new to Amazon Inspector



Security:
=========
Automate vulnerability management
Amazon Inspector

It is a aws managed service, security is a shared responsibility.





136
##Macie
-------------------------------------------------
• Amazon Macie is a fully managed data security and data privacy service
  that uses machine learning and pattern matching to discover and protect your sensitive data in AWS.
• Macie helps identify and alert you to sensitive data, such as personally identifiable information (PII)



Area:
=====
Global.


UseCase:
========
Discover your sensitive data at scale
Assessing your data privacy and security
Maintaining regulatory compliance
Identifying sensitive data in data migrations


BillCost:
=========
Yes, there is a 30-day free trial. 
Charged based on two dimensions, the number of Amazon S3 buckets in your account per month and the amount of data processed
 for sensitive data discovery in a given month.



Security:
=========
Discover and protect your sensitive data at scale
Amazon Macie
It is a aws managed service, security is a shared responsibility.





137
##AWS Shared Responsibility Model
-------------------------------------------------
AWS responsibility - Security of the Cloud
	• Protecting infrastructure (hardware, software, facilities, and networking) that runs all the AWS services
	• Managed services like S3, DynamoDB, RDS, etc.

Customer responsibility - Security in the Cloud
	• For EC2 instance, customer is responsible for management of the guest OS
	  (including security patches and updates), firewall & network configuration, IAM
	• Encrypting application data
	• Shared controls:
	• Patch Management, Configuration Management, Awareness & Training


Example, for S3:
AWS responsibility: 
	• Guarantee you get unlimited storage 
	• Guarantee you get encryption 
	• Ensure separation of the data between different customers 
	• Ensure AWS employees can’t access your data 

Your responsibility: 
	• Bucket configuration 
	• Bucket policy / public setting 
	• IAM user and roles 
	• Enabling encryption





AWS Control Tower:
	AWS Control Tower provides a single location to easily set up your new well-architected multi-account environment 
	and govern your AWS workloads with rules for security, operations, and internal compliance. 

You can automate the setup of your AWS environment with best-practices 
blueprints for multi-account structure, identity, access management, and account provisioning workflow. 
For ongoing governance, you can select and apply pre-packaged policies enterprise-wide or to specific groups of accounts.


AWS Control Tower provides three methods for creating member accounts:
	– Through the Account Factory console that is part of AWS Service Catalog.
	– Through the Enroll account feature within AWS Control Tower.
	– From your AWS Control Tower landing zone’s management account, using Lambda code and appropriate IAM roles.


AWS Control Tower automatically implements guardrails using multiple building blocks such as 
AWS CloudFormation to establish a baseline, SCPs to prevent configuration changes, 
and AWS Config rules to continuously detect non-conformance.


AWS Service Catalog:
	AWS Service Catalog is a service that allows Organizations and Teams to create and manage catalogs of IT services 
	that can be used on AWS.
	
Although Service Catalog is built on top of CloudFormation, unlike CloudFormation it's designed for sharing commonly deployed 
applications to promote collaboration. 
This means you can share the service catalog across multiple accounts and have the same constraints copy over.


 

==| Virtual Private Cloud (VPC)
138
##VPC | Internet Protocol (IP)   
-------------------------------------------------
The Internet Protocol (IP) uses Three types of addressing schemes: 
Unicast, Multicast, and Anycast.

Unicast:
A Unicast address is used to identify a single unique host. 


Multicast:
	A Multicast address is used to deliver data to a group of destinations (a one-to-many transmission). 
	IP multicast group addresses are represented by class-D IP addresses reserved specifically for multicast 
	communications, ranging from 224.0.0.0 through 239.255.255.255. 
IPv6 multicast replaces broadcast addresses that were supported in IPv4. 


Anycast:
	Anycast,  is an IP network addressing scheme that allows multiple servers to share the same 
	IP address, allowing for multiple physical destination servers to be logically identified 
	by a single IP address. 
Based on the location of the user request, the anycast routers send it to the server in the 
network based on a least-cost analysis  that includes assessing the number of hops, 
shortest distance, lowest transit cost, and minimum latency measurements to optimize the 
selection of a destination server.





139
##VPC Amazon Virtual Private Cloud 
-------------------------------------------------
VPC enables you to launch AWS resources into a virtual network that you've defined.

When you create a VPC endpoint, you can attach an endpoint policy that controls access to the service to which you are connecting. 

VPN Connections are a good solution if you have an immediate need, have low to 
modest bandwidth requirements, and can tolerate the inherent variability in Internet-based connectivity. 

However, Site-to-site VPN cannot provide low latency and high throughput connection as DirectConnect.

The CIDR block for the default VPC is it's 172.31.0.0/16. It means this VPC can provide up to 65,536 IP addresses.

- It is logical Isolated from Other virtual Network in the AWS.
- Max 5 VPC can be created and 200 subnet in 1 VPC.
- We Can allocate Max 5 Elastic IP.
- Once a VPC created DHCP, NACL and Sucurity Group will be created automatically.
- A VPC is confied to on AWS Region and dos not extend between Region.

- Onece the VPC is created, Its CIDR block range can NOT change.
  (You can create another CIDR make it primary and older one will be Secondary then you can delete fitst one)
- If you need a diffenent CIDR size, create a new VPC.
- The different subnets wihtin a VPC cannot overlap.
- You can expend yor vpc CIDR by adding new/extra IP address.

VPC Two Type
- Default Vpc
- Custom Vpc

Primary diffent of this tow: default vpc has Internet Gatway custom vpc has not, you can added.


Monitor VPC by using:
CloudWatch and CloudWatch logs
VPC Flow Logs

Component of VPC:
 - CIDR and IP address subnet
 - Implied router and Routing Table
 - Internet Gatway
 - Security Group
 - Network ACL
 - Virtual Private gatway 
 - Peering connections
 - Elastic IP


-Each VPC has a main route table, by default
-Main route table has a default route enabling communication between resources in all subnets in a VPC
-Default route rule CANNOT be deleted/edited
-HOWEVER you can add/edit/delete other routing rules to the main route table



VPC Sharing:
Using VPC sharing, an account that owns the VPC (owner) shares one or more subnets with other 
accounts (participants) that belong to the same organization from AWS Organizations. 
The owner account cannot share the VPC itself. 

You must enable resource sharing from the management account for your organization.

You can share Amazon VPCs to leverage the implicit routing within a VPC for applications that 
require a high degree of interconnectivity and are within the same trust boundaries.

A VPC peering connection is a networking connection between two VPCs that enables you to 
route traffic between them using private IPv4 addresses or IPv6 addresses. Instances in either 
VPC can communicate with each other as if they are within the same network. 
VPC peering does not facilitate centrally managed VPCs.


Things to know:
VPC sharing allows customers to share subnets with other AWS accounts within the same AWS Organization. 
Turn on resource sharing within AWS Organizations using the management account.

	VPC sharing is only available within the same AWS Organization.
	Sharing of default VPCs/subnets is not possible.
	Participants can’t launch resources using security groups that are owned by other participants or the owner.
	Participants can’t launch resources using the default security group for the VPC because it belongs to the owner.

AWS owner account cannot share the VPC itself with another AWS account.


VPC console wizard:
	The Amazon VPC console wizard provides the following four configurations:
		1)VPC with a single public subnet.
		4)VPC with a private subnet only and AWS Site-to-Site VPN access
				
		2)VPC with public and private subnets (NAT) 
		3)VPC with public and private subnets and AWS Site-to-Site VPN access


https://docs.aws.amazon.com/vpc/latest/userguide/vpc-scenarios-intro.html



To enable the connection to a service running on an instance, the associated network 
ACL must allow both inbound traffic on the port that the service is listening on as well as 
allow outbound traffic from ephemeral ports. 

When a client connects to a server, 
a random port from the ephemeral port range (1024-65535) becomes the client’s source port.


Area:
=====
VPC – Regional
VPC create on Region not available zone. all property of VPC are Regional becaus its exists on Region. 
One vpc cannot extend more then one Region.



UseCase:
========
Private Network.



BillCost:
=========
There are no additional charges for creating and using the VPC itself. 

Amazon VPC costs:
If you opt to create a hardware VPN connection associated with your VPC using Virtual Private Gateway, you will have to pay for each 
VPN connection hour that your VPN connection is provisioned and available. 
Each partial VPN connection hour consumed is billed as a full hour. 
You'll also incur standard AWS data transfer charges for all data transferred via the VPN connection. 

If you create a NAT gateway in your VPC, Charges are levied for each NAT gateway hour that your NAT gateway is 
provisioned and available for. Data processing charges apply for each gigabyte processed through a NAT gateway. 
Each partial NAT gateway hour consumed is billed as a full hour.




Security:
=========
Security groups in a VPC specify which traffic is allowed to or from an Amazon EC2 instance. 
Network ACLs operate at the subnet level and evaluate traffic entering and exiting a subnet. 
Network ACLs can be used to set both Allow and Deny rules. 
Network ACLs do not filter traffic  between instances in the same subnet. 
In addition, network ACLs perform stateless filtering while 
security groups perform stateful filteri

Default VPC has default CIDR, Security Group, NACL and Route table setting.

Security products and features:
-Security groups - This acts as a firewall for the EC2 instances, controlling inbound and outbound traffic at the instance level.
-Network access control lists (ACLs)- It acts as a firewall for the subnets, controlling inbound/outbound traffic at the subnet level.
-Flow logs - These capture the inbound and outbound traffic from the network interfaces in your VPC.








140
##CIDR
-------------------------------------------------
The /32 denotes one IP address and the /0 refers to the entire network.

The allowed block size is between  /16 netmask to /28 netmask.
The CIDR block must not overlap with any existing CIDR block that's associated with the VPC.
The CIDR block must not be the same or larger than a destination CIDR range in a route in any of the VPC route tables. 


CIDR blocks from the RFC 1918* range:
10.0.0.0/8
172.16.0.0/12
192.168.0.0/16
198.19.0.0/16

192.168.0.0/16


Private IP can only allow certain values:
	• 10.0.0.0 – 10.255.255.255 (10.0.0.0/8)  big networks
	• 172.16.0.0 – 172.31.255.255 (172.16.0.0/12)  AWS default VPC in that range
	• 192.168.0.0 – 192.168.255.255 (192.168.0.0/16) home networks
All the rest of the IP addresses on the Internet are Public




141
##Default VPC Walkthrough
-------------------------------------------------
• All AWS accounts have a default VPC
• New EC2 instances are launched into the default VPC if no subnet is specified
• Default VPC has Internet connectivity and all EC2 instances inside it have public IPv4 addresses
• We also get a public and a private IPv4 DNS names


172.31.0.0/16	local
0.0.0.0/0	internet_gateway_id

Because VPC is private, only the Private IPv4 ranges are allowed:
• 10.0.0.0 – 10.255.255.255 (10.0.0.0/8)
• 172.16.0.0 – 172.31.255.255 (172.16.0.0/12)
• 192.168.0.0 – 192.168.255.255 (192.168.0.0/16)






142
##Subnet
-------------------------------------------------
A subnet is a range of IP addresses in your VPC. A subnet must reside in a single Availability Zone. 
After you add subnets, you can deploy AWS resources in your VPC.

Each subnet in your VPC must be associated with a route table, which controls the routing for 
the subnet (subnet route table). You can explicitly associate a subnet with a particular route table. 
Otherwise, the subnet is implicitly associated with the main route table.

A subnet can only be associated with one route table at a time, but you can associate 
multiple subnets with the same subnet route table.

Public and private subnets:
If a subnet is associated with a route table that has a route to an internet gateway, 
it's known as a public subnet. other wise it's known as a private subnet.

The first four and last IP address of Subnet cannot be assigned.
Suppose a IP: 10.0.0.0/16
	- 10.0.0.0 Network address
	- 10.0.0.1 Reserved by AWS for VPC Route
	- 10.0.0.2 Reserved By AWS for DNS server
	- 10.0.0.3 Reserved By AWS for future use
	- 10.0.0.255 Brodycast Address
AWS do not support brodcust in a VPC but reserve the address.
	
	

Exam Tip, if you need 29 IP addresses for EC2 instances:
• You can’t choose a subnet of size /27 (32 IP addresses, 32 – 5 = 27 < 29)
• You need to choose a subnet of size /26 (64 IP addresses, 64 – 5 = 59 > 29)


Area:
=====
Subnet – Availability Zone
Subnet can span only a single Availability Zone
		
		
		
UseCase:
========
You can attach AWS resources, such as EC2 instances and RDS DB instances, to subnets. 
You can create subnets to group instances together according to your security and operational needs.



BillCost:
=========
There are no additional charges



Security:
=========
AWS provides two features that you can use to increase security in your VPC: security groups and network ACLs. 






143
##Internet Gateway (IGW)
-------------------------------------------------
An internet gateway is a horizontally scaled, redundant, and highly available VPC component that allows 
communication between your  VPC and the internet. It supports IPv4 and IPv6 traffic. 

• Must be created separately from a VPC
• One VPC can only be attached to one IGW and vice versa
• Internet Gateways on their own do not allow Internet access…
• Route tables must also be edited!


UseCase:
========
An internet gateway enables resources in your public subnets (such as EC2 instances) to connect to the internet 
if the resource has a public IPv4 address or an IPv6 address.

BillCost:
=========
There's no additional charge for creating an internet gateway.




144
##Route Table 
-------------------------------------------------
A route table contains a set of rules, called routes, that determine where network traffic from your subnet or gateway is directed.

Main route table: 
	The route table that automatically comes with your VPC. It controls the routing for all 
	subnets that are not explicitly associated with any other route table.

Custom route table: A route table that you create for your VPC.


Destination: The range of IP addresses(subnet)  from where your  traffic to go (destination CIDR). 
For example, an external corporate network with the CIDR 172.16.0.0/12.

Target: The gateway, network interface, or connection through which to send the destination traffic; 
for example, an internet gateway.

There are no additional charges.




145
##Bastion Host
-------------------------------------------------
A bastion host is a server (EC2 instance in a public subnet) whose purpose is to provide access to 
a private network from an external network, such as the Internet.


• We can use a Bastion Host to SSH into our private EC2 instances
• The bastion is in the public subnet which is then connected to all other private subnets
• Bastion Host security group must allow inbound from the internet on port 22 from restricted CIDR, for example the public
  CIDR of your corporation
• Security Group of the EC2 Instances must allow the Security Group of the Bastion Host, or the private IP of the Bastion host.

• Bastion Host security group must allow inbound from the internet on port 22 from
restricted CIDR, for example the public CIDR of your corporation.

• Security Group of the EC2 Instances must allow the Security Group of the Bastion
Host, or the private IP of the Bastion host.

Exam Tip: Make sure the bastion host only has port 22 traffic from the IP address you need,
         not from the security groups of your other EC2 instances.


Area:
=====
az



UseCase:
========
SSH into our private EC2



BillCost:
=========
Instance cost.



Security:
=========
Security group.








146
##NAT Instances
-------------------------------------------------
AWS provides two kinds of NAT devices:
	NAT gateway 
	NAT instance


NAT instance = Network Address Translation
	• Allows EC2 instances in private subnets to connect to the Internet
	• Must be launched in a public subnet
	• Must disable EC2 setting: Source / destination Check
	• Must have Elastic IP attached to it
	• Route Tables must be configured to route traffic from private subnets to the NAT Instance


• Pre-configured Amazon Linux AMI is availablez
• Reached the end of standard support on December 31, 2020
• Not highly available / resilient setup out of the box
• You need to create an ASG in multi-AZ + resilient user-data script
• Internet traffic bandwidth depends on EC2 instance type
• You must manage Security Groups & rules:
	Inbound:
	• Allow HTTP / HTTPS traffic coming from Private Subnets
	• Allow SSH from your home network (access is provided through Internet Gateway)
	Outbound:
	• Allow HTTP / HTTPS traffic to the Internet


Area:
=====
MayRegional



UseCase:
========
Private subnets to connect to the Internet



BillCost:
=========
NAT Gateway Hourly Charge: NAT Gateway is charged on an hourly basis.



Security:
=========
Security is a shared responsibility







147
##NAT Gateways
-------------------------------------------------
NAT device is added to the public subnet to get internet connectivity.
 

• AWS-managed NAT, higher bandwidth, high availability, no administration
• Pay per hour for usage and bandwidth
• NATGW is created in a specific Availability Zone, uses an Elastic IP
• Can’t be used by EC2 instance in the same subnet (only from other subnets)
• Requires an IGW (Private Subnet => NATGW => IGW)
• 5 Gbps of bandwidth with automatic scaling up to 45 Gbps
• No Security Groups to manage / required

Once created, you need to update the route table associated with your private subnet to point 
internet-bound traffic to the NAT gateway.
This way, the instances in your private subnet can communicate with the internet.




NAT Gateway with High Availability:
• NAT Gateway is resilient within a single Availability Zone
• Must create multiple NAT Gateways in multiple AZs for fault-tolerance


For single point of failure:
Create a NAT Gateway in each availability zone. Configure the route table in each private subnet 
to ensure that instances use the NAT Gateway in the same availability zone.


You should configure the route table in the private subnet and not the public subnet to associate 
the right instances in the private subnet.

A single NAT Gateway in each availability zone is enough. 
NAT Gateway is already redundant in nature, meaning, AWS already handles any failures that occur 
in your NAT Gateway in an availability zone.

But Use a script to manage failover between instances.

Area:
=====
Each NAT gateway is created in a specific Availability Zone



UseCase:
========
A NAT gateway supports the following protocols: TCP, UDP, and ICMP.
NAT gateways are supported for IPv4 or IPv6 traffic. 


BillCost:
=========
Pay per hour for usage and bandwidth
When you provision a NAT gateway, you are charged for each hour that your 
NAT gateway is available and each Gigabyte of data that it processes. 



Security:
=========
No Security Groups to manage / required
It is a aws managed service, security is a shared responsibility.







148
##NACL | Security Groups
-------------------------------------------------
AWS Network ACLs allow controlling/regulating traffic flow to and from subnets.

NACL:
	NACL also adds an additional layer of security associated with subnets that control both inbound and outbound 
	traffic at the subnet level.

• NACL are like a firewall which control traffic from and to subnets
• One NACL per Subnet, new subnets are assigned the Default NACL
• You define NACL Rules:
	• Rules have a number (1-32766), higher precedence with a lower number
	• First rule match will drive the decision
	• Example: if you define #100 ALLOW 10.0.0.10/32 and #200 DENY 10.0.0.10/32, the IP address will be allowed because 
	  100 has a higher precedence over 200
	• The last rule is an asterisk (*) and denies a request in case of no rule match
	• AWS recommends adding rules by increment of 100
• Newly created NACLs will deny everything
• NACL are a great way of blocking a specific IP address at the subnet leve


Default NACL:
• Accepts everything inbound/outbound with the subnets it’s associated with
• Do NOT modify the Default NACL, instead create custom NACLs



Security Groups:
	Security Groups are used to control access (SSH, HTTP, RDP, etc.) with EC2. 
	They act as a virtual firewall for your instances to control inbound and outbound traffic. 
	When you launch an instance in a VPC, you can assign up to five security groups to 
	the instance and security groups act at the instance level, not the subnet level.

Security Groups supports only allow rules, and by default, all the rules are denied. 
You cannot deny the rule for establishing a connection.

Security Groups are stateful:
-If an outgoing request is allowed, the incoming response for it is automatically allowed.
-If an incoming request is allowed, an outgoing response for it is automatically allowed



Ephemeral Ports:
When a client initiates a request, it choose a random port from ephemeral port range and 
it expects the response from the server at that port only. 

Ports with numbers 0-1023 are called system or well-known ports; 
ports with numbers 1024-49151 are called user or registered ports, 
and ports with numbers 49152-65535 are called dynamic, private or ephemeral ports.


• For any two endpoints to establish a connection, they must use ports
• Clients connect to a defined port, and expect a response on an ephemeral port
• Different Operating Systems use different port ranges, examples:
• IANA & MS Windows 10 è 49152 – 65535
• Many Linux Kernels è 32768 – 60999




Area:
=====
Security Groups are regional and can span AZs, but can't be cross-regional.

Security groups are specific to a single VPC, so you can't share a Security Group 
between multiple VPCs. However, you can copy a Security Group to create a new Security Group with the same 
rules in another VPC for the same AWS Account.



UseCase:
========
You cannot block specific IP addresses with Security Groups (use NACLs instead)



BillCost:
=========
There are no additional charges



Security:
=========
It is a aws managed service, security is a shared responsibility.







149
##VPC Peering
-------------------------------------------------
A VPC peering connection is a networking connection between two VPCs that 
enables you to route traffic between them using private IPv4 addresses or IPv6 addresses.

VPCs, but transitive peering is not supported. In other words, VPC A can connect to B and C in the above diagram, 
but C cannot communicate with B unless directly paired.

Privately connect two VPCs using AWS’network:
• Make them behave as if they were in thesame network
• Must not have overlapping CIDRs
• You can create VPC Peering connection between VPCs in different AWSaccounts/regions
• You can reference a security group in a peered VPC (works crossaccounts – same region)
• You must update route tables in each VPC’s subnets to ensure EC2 instances can communicate with each other



Since Software VPN just handles connectivity between the remote network and Amazon VPC, not able to send any data its self.
therefore it cannot be used to send and receive data between the remote branch offices of the company.


Area:
=====
VPC Peering – Regional



UseCase:
========
Privately connect two VPCs.



BillCost:
=========
There is no charge to create a VPC peering connection. 
There is a charge for data transfer across peering connections.



Security:
=========
It is a aws managed service, security is a shared responsibility.







##PrivateLink
-------------------------------------------------
AWS PrivateLink connects your AWS services with other AWS services through a non-public tunnel.

AWS PrivateLink provides private connectivity between VPCs, AWS services, 
and your on-premises networks, without exposing your traffic to the public internet. 

AWS PrivateLink makes it easy to connect services across different accounts and VPCs to significantly 
simplify your network architecture.




150
##VPC Endpoint
-------------------------------------------------
A VPC endpoint enables you to privately connect your VPC to supported AWS services and VPC endpoint services 
powered by PrivateLink without requiring an internet gateway, NAT device, VPN connection, or AWS Direct Connect connection.

When you use VPC endpoint, the traffic between your VPC and the other AWS service does not leave the Amazon network.

VPC Endpoints (AWS PrivateLink)
• Every AWS service is publicly exposed (public URL)
• VPC Endpoints (powered by AWS PrivateLink) allows you to connect to AWS services using 
  from a private network instead of using the public Internet
• They’re redundant and scale horizontally
• They remove the need of IGW, NATGW, … to access AWS Services
• In case of issues:
	• Check DNS Setting Resolution in your VPC
	• Check Route Tables


Types of Endpoints:
Interface Endpoints:
	• Provisions an ENI (private IP address) as an entry point (must attach a Security Group)
	• Supports most AWS services
	• $ per hour + $ per GB of data processed
	• Supports most AWS services
An interface endpoint is an elastic network interface that allows a private IP address in a subnet to connect 
VPC resources to  AWS services, such as CloudFormation,Elastic Load Balancers (ELBs), SNS, and more.
	
Gateway Endpoints:
	• Provisions a gateway and must be used as a target in a route table (does not use security groups)
	• Supports both S3 and DynamoDB(Only)
	• Free 
	
In contrast, a gateway endpoint is a target for a route in a route table to connect VPC resources to S3 or DynamoDB. 
Traffic is then routed from instances in a subnet to one of these two services.



Gateway or Interface Endpoint for S3?
	• Gateway is most likely going to be preferred all the time at the exam
	• Cost: free for Gateway, $ for interface endpoint
	• Interface Endpoint is preferred access is required from onpremises (Site to Site VPN or Direct Connect), 
		a different VPC or a different region



• Use Gateway Endpoint if the AWS service is either DynamoDB or S3, 
   VPC level and specify a route in the route table in subnet.
   Must be inside the VPC to use.
   Can use an IAM policies or resource policies to restrict access.
  
• Use Interface Endpoint for everything else, Subnet level required ip and SecurirtyGroup.
   Available to be used outside of the VPC with VPN, Direct Connect, or VPC peering
   Can use an IAM policies or resource policies to restrict access


A Gateway Load Balancer endpoint is a VPC endpoint that provides private connectivity 
between virtual appliances in the service provider VPC and application servers in the service consumer VPC. 
You deploy the Gateway Load Balancer in the same VPC as the virtual appliances.

	Enables you to intercept traffic and route .
	Security groups and endpoint policies are not supported.
	Endpoints support IPv4 traffic only.


Lambda in VPC accessing DynamoDB:
• DynamoDB is a public service from AWS
• Option 1: Access from the public internet
	• Because Lambda is in a VPC, it needs a NAT Gateway in a public subnet and an internet gateway
• Option 2 (better & free): Access from the private VPC network
	• Deploy a VPC Gateway endpoint for DynamoDB
	• Change the Route Tables




VPC Classic Link 
-------------------------------------------------
ClassicLink allows you to link EC2-Classic instances to a VPC in your account, within the same Region. 
If you associate the VPC security groups with a EC2-Classic instance, this enables communication between your 
EC2-Classic instance and instances in your VPC using private IPv4 addresses.


Linking your EC2-Classic instance to a VPC does not affect your EC2-Classic security groups. 
They continue to control all traffic to and from the instance. 


ClassicLink allows you to link EC2-Classic instances to a VPC in your account
	• Must associate a security group
	• Enables communication using private IPv4 addresses
	• Removes the need to make use of public IPv4 addresses or Elastic IP addresses







Area:
=====
VPC Endpoints – Regional



UseCase:
========
Enables you to privately connect your VPC


BillCost:
=========
Interface endpoints:
AWS charges usage and data processing rates for PrivateLink, so there are additional costs 
involved with creating and using an interface endpoint. 

Gateway endpoints: do not use PrivateLink. 
Therefore, apart from the standard costs of data transfer and resource use, 
AWS doesn't charge extra for using gateway endpoints, unlike interface endpoints.

Security:
=========
It is a aws managed service, security is a shared responsibility.






151
##VPC Flow Logs
-------------------------------------------------
VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to and from network 
interfaces in your VPC. 

Flow log data can be published to CloudWatch Logs ors S3. 

After you create a flow log, you can retrieve and view its data in the chosen destination.


• Capture information about IP traffic going into your interfaces:
	• VPC Flow Logs
	• Subnet Flow Logs
	• Elastic Network Interface (ENI) Flow Logs
• Helps to monitor & troubleshoot connectivity issues
• Flow logs data can go to S3 / CloudWatch Logs
• Captures network information from AWS managed interfaces too: ELB,
  RDS, ElastiCache, Redshift, WorkSpaces, NATGW, Transit Gateway


VPC Flow Logs Syntax:
• srcaddr & dstaddr – help identify problematic IP
• srcport & dstport – help identity problematic ports
• Action – success or failure of the request due to Security Group / NACL
• Can be used for analytics on usage patterns, or malicious behavior
• Query VPC flow logs using Athena on S3 or CloudWatch Logs Insights






152
##Site to Site VPN
-------------------------------------------------
You can enable access to your remote network from your VPC by creating an AWS Site-to-Site VPN 
(Site-to-Site VPN) connection, and configuring routing to pass traffic through the connection.


Virtual Private Gateway (VGW):
	• VPN concentrator on the AWS side of the VPN connection
	• VGW is created and attached to the VPC from which you want to create the Site-to-Site VPN connection
	• Possibility to customize the ASN (Autonomous System Number)

Customer Gateway (CGW):
	• Software application or physical device on customer side of the VPN connection
	
	
Customer Gateway Device (On-premises):
• What IP address to use?
	• Public Internet-routable IP address for your Customer Gateway device
	• If it’s behind a NAT device that’s enabled for NAT traversal (NAT-T), use the public IP address of the NAT device

• Important step: enable Route Propagation for the Virtual Private Gateway in the route table
  that is associated with your subnets.
• If you need to ping your EC2 instances from on-premises, make sure you add the 
  ICMP protocol on the inbound of your security groups.




AWS VPN CloudHub:
• Provide secure communication between multiple sites, if you have multiple VPN connections
• Low-cost hub-and-spoke model for primary or secondary network connectivity between different locations (VPN only)
• It’s a VPN connection so it goes over the public Internet
• To set it up, connect multiple VPN connections on the same VGW, setup
  dynamic routing and configure route tables



AWS Site-to-Site VPN enables you to securely connect your on-premises network or branch office 
site to your Amazon Virtual Private Cloud (Amazon VPC). 

You can securely extend your data center 
or branch office network to the cloud with an AWS Site-to-Site VPN (Site-to-Site VPN) connection. 
It uses internet protocol security (IPSec) communications to create encrypted VPN tunnels 
between two locations. 

You cannot use AWS Site-to-Site VPN to integrate data files via the NFS interface.

Area:
=====
MayRegional 



UseCase:
========
Application Migration: Moving applications to the cloud is easier with a Site-to-site 
VPN connection between your network and the AWS cloud. 

You can host Amazon VPCs behind your corporate firewall and seamlessly move your IT resources, without 
changing the way your users access these applications.
You can use AWS Site-to-Site VPN connections to securely communicate between remote sites.


BillCost:
=========
You are charged for each VPN connection hour that your VPN connection is provisioned and available.
 

Security:
=========
Site-to-Site VPN supports Internet Protocol security (IPsec) VPN connections.
Security is a shared responsibility





153
##Direct Connect
-------------------------------------------------
DirectConnect connects your on-prem with your VPC through a non-public tunnel.
AWS Direct Connect Private dedicated network connection from on-premises to AWS.

Data transfer pricing over Direct Connect is lower than data transfer pricing over the internet.

AWS Direct Connect lets you establish a dedicated network connection between your network 
and one of the AWS Direct Connect locations.



• Provides a dedicated private connection from a remote network to your VPC
• Dedicated connection must be setup between your DC and AWS Direct Connect locations
• You need to setup a Virtual Private Gateway on your VPC
• Access public resources (S3) and private (EC2) on same connection Supports both IPv4 and IPv6.


You can create this private connectivity to reduce network costs, increase bandwidth, and provide more consistent network experience 
compared to regular internet-based connections.



Direct Connect Gateway:
---------------------------------------------
DirectConnecte->PrivateVertualInterface->VGW->VPC

Virtual Private Gateway or VGW is used to associated to VPC and it can work with VPN or Direct Connect.

If you want to setup a Direct Connect to one or more VPC in many
different regions (same account) for your on-parmiss, you must use a Direct Connect Gateway.

AWS Direct Connect gateway is aimed at making it easier to connect from a single 
Direct Connect location to multiple AWS regions or VPCs.

You can set up Direct Connect to bypass a parent Availability 
Zone and connect directly to the internet or through Direct Connect points-of-presence.

Use AWS Direct Connect gateway to connect your VPCs. You associate an AWS Direct Connect 
gateway with either of the following gateways:
	A transit gateway when you have multiple VPCs in the same Region
	A virtual private gateway

A Direct Connect gateway does not allow gateway associations that are on the same Direct Connect 
gateway to send traffic to each other (for example, a virtual private gateway to another virtual private gateway).

A virtual interface (VIF) is necessary to access AWS services, and is either public or private. 

A public virtual interface enables access to public services, such as Amazon S3. 
A private virtual interface enables access to your VPC.



Direct Connect – Connection Types:
• Dedicated Connections: 1Gbps and 10 Gbps capacity
	• Physical ethernet port dedicated to a customer
	• Request made to AWS first, then completed by AWS Direct Connect Partners
• Hosted Connections: 50Mbps, 500 Mbps, to 10 Gbps
	• Connection requests are made via AWS Direct Connect Partners
	• Capacity can be added or removed on demand
	• 1, 2, 5, 10 Gbps available at select AWS Direct Connect Partners
	
Lead times are often longer than 1 month to establish a new connection



Direct Connect – Encryption:
• Data in transit is not encrypted but is private
• AWS Direct Connect + VPN provides an IPsec-encrypted private connection
• Good for an extra level of security, but slightly more complex to put in place


Direct Connect - Resiliency:
-One connection at multiple locations
-Maximum resilience is achieved by separate connections terminating on separate devices in more than one location.


Site-to-Site VPN connection as a backup
• In case Direct Connect fails, you can set up a backup Direct Connect connection (expensive), or a Site-to-Site VPN connection


What is the difference between Direct Connect gateway and virtual private gateway?
	The Direct Connect Gateway is connected to multiple AWS VPCs in different AWS regions via Virtual private Gateways. 
	The Direct Connect Gateway is in turn connected to the Direct Connect via a virtual private interface. 
	This allows multiple VPCs to be connected to the customer network via one virtual private interface.



Area:
=====
MayRegional


UseCase:
========
• Increase bandwidth throughput - working with large data sets – lower cost
• More consistent network experience - applications using real-time data feeds
• Hybrid Environments (on prem + cloud)

The use case for Direct Connect is high throughput workloads or if you need a stable or reliable connection




BillCost:
=========
AWS Direct Connect has two separate charges: port hours and data transfer. 
Pricing is per port-hour consumed for each port type. Partial port hours consumed are billed as full hours. 
The account that owns the port will be charged the port hour charges.



Security:
=========
• Data in transit is not encrypted but is private
• AWS Direct Connect + VPN provides an IPsec-encrypted private connection
AWS Direct Connect does not involve the Internet; instead, it uses dedicated, private network connections between 
your intranet and Amazon VPC. ,
Customers can also implement additional security controls by encrypting the traffic that rides the direct connections 
using similar protocols like SSL, HTTPs and SSH.






154
##Transit  Gateway 
-------------------------------------------------
It acts as a cloud router that allows you to integrate multiple networks.
 
Transit Gateway is a Regional resource and can connect thousands of VPCs within the same AWS Region. 
You can create multiple Transit Gateway instances per Region, and you can 
connect to a maximum of three Transit Gateway instances over a single Direct Connect connection for hybrid connectivity.

Traffic between VPC and Transit Gateway remains on the AWS global private network and is not exposed to the public internet. 

Transit Gateway Peering:
AWS Transit Gateway supports the ability to establish peering connections between Transit Gateways in the same and different  Regions. 
Inter-region peering enables customers to extend this connectivity and build global networks spanning multiple AWS Regions.

This means that all the Amazon VPCs, Site-to-Site VPN connections, and Direct Connect gateways attached to an 
Transit Gateway hosted in one AWS Region can exchange traffic with resources deployed in other AWS Regions.

• For having transitive peering between thousands of VPC and on-premises, hub-and-spoke (star) connection
• Regional resource, can work cross-region
• Share cross-account using Resource Access Manager (RAM)
• You can peer Transit Gateways across regions
• Route Tables: limit which VPC can talk with other VPC
• Works with Direct Connect Gateway, VPN connections
• Supports IP Multicast (not supported by any other AWS service)


Transit Gateway: Site-to-Site VPN ECMP":
• ECMP = Equal-cost multi-path routing
• Routing strategy to allow to forward a packet over multiple best path
• Use case: create multiple Siteto-Site VPN connections to increase the bandwidth of your connection to AWS


Here are the steps to follow to set up a sharing Transit gateway across AWS Accounts using Resource Access Manager (RAM).
	Create a Transit gateway in Account-1
	Share the Transit gateway with Account-2 using RAM
	Attach VPC’s to the Transit gateway
	Update the Routes with TGW
	Test the connection



Transit Gateway – Share Direct Connect between multiple accounts.
You can use AWS Resource Access Manager to share Transit Gateway with other accounts.



Area:
=====
Regional resource



UseCase:
========
Deliver applications around the world
Rapidly move to global scale
Smoothly respond to spikes in demanding
Host multicast applications in the cloud


Transit Gateways in different regions can peer with each other to enable VPC communications across regions. 
Transit Gateway inter-Region peering encrypts all traffic, with no single point of 
failure or bandwidth bottleneck which helps you to get improved security.



BillCost:
=========
Create multiple Siteto-Site VPN connections to increase the bandwidth of your connection to AWS
In AWS Transit Gateway you are charged for the number of connections that you make to the Transit 
Gateway per hour and the amount of traffic that flows through AWS Transit Gateway.



Security:
=========
AWS Transit Gateway inherits compliance from Amazon VPC and meets the standards for PCI DSS Level 1, 
ISO 9001, ISO 27001, ISO 27017, ISO 27018, SOC 1, SOC 2, SOC 3, FedRAMP Moderate, 
FedRAMP High and HIPAA eligibility.





155
##VPC Traffic Mirroring
-------------------------------------------------
Traffic Mirroring is an Amazon VPC feature that you can use to copy network traffic from an elastic network interface 
of type interface. You can then send the traffic to out-of-band security and monitoring appliances for:

• Allows you to capture and inspect network traffic in your VPC
• Route the traffic to security appliances that you manage
• Capture the traffic
	• From (Source) – ENIs
	• To (Targets) – an ENI or a Network Load Balancer
• Capture all packets or capture the packets of your interest (optionally, truncate packets)
• Source and Target can be in the same VPC or different VPCs (VPC Peering)
• Use cases: content inspection, threat monitoring, troubleshooting



Area:
=====
MayRegional


UseCase:
========
Content inspection
Threat monitoring
Troubleshooting

BillCost:
=========


Security:
=========
It is a aws managed service, security is a shared responsibility.




156
##IPv6 VPC
-------------------------------------------------
Your VPC can operate in dual-stack mode — your resources can communicate over IPv4, or IPv6, or both. 
IPv4 and IPv6 communication are independent of each other. 

• IPv4 cannot be disabled for your VPC and subnets,this is the default IP addressing system for Amazon VPC.
• You can enable IPv6 (they’re public IP addresses) to operate in dual-stack mode
• Your EC2 instances will get at least a private internal IPv4 and a public IPv6
• They can communicate using either IPv4 or IPv6 to the internet through an Internet Gateway.




157
##Egress Only Internet Gateway
-------------------------------------------------
An egress-only internet gateway is used to enable outbound communication over IPv6 from instances in your VPC to the internet, 
and prevents hosts outside of your VPC from initiating an IPv6 connection with your instance.

Used for IPv6 only (similar to a NAT Gateway but for IPv6) You must update the Route Tables.

Minimizing egress traffic network cost
• Try to keep as much internet traffic within AWS to minimize costs
• Direct Connect location that are co-located in the same AWS Region result in lower cost for egress network





158
##VPC Section Summary
-------------------------------------------------
• CIDR – IP Range
• VPC – Virtual Private Cloud (list of IPv4 & IPv6 CIDR)
• Subnets – tied to an AZ, we define a CIDR
• Internet Gateway – at the VPC level, provide IPv4 & IPv6 Internet Access
• Route Tables – must be edited to add routes from subnets to the IGW, VPC Peering Connections, VPC Endpoints, …
• Bastion Host – public EC2 instance to SSH into, that has SSH connectivity to EC2 instances in private subnets
• NAT Instances – gives Internet access to EC2 instances in private subnets. Old, must be setup in a public subnet, disable Source / Destination check flag
• NAT Gateway – managed by AWS, provides scalable Internet access to private EC2 instances, IPv4 only
• Private DNS + Route 53 – enable DNS Resolution + DNS Hostnames (VPC).

• NACL – stateless, subnet rules for inbound and outbound, don’t forget Ephemeral Ports
• Security Groups – stateful, operate at the EC2 instance level
• Reachability Analyzer – perform network connectivity testing between AWS resources
• VPC Peering – connect two VPCs with non overlapping CIDR, non-transitive
• VPC Endpoints – provide private access to AWS Services (S3, DynamoDB, CloudFormation, SSM) within a VPC
• VPC Flow Logs – can be setup at the VPC / Subnet / ENI Level, for ACCEPT and REJECT traffic, helps identifying attacks, 
  analyze using Athena or CloudWatch Logs Insights
• Site-to-Site VPN – setup a Customer Gateway on DC, a Virtual Private Gateway on VPC, and site-to-site VPN over public Internet
• VPN CloudHub – hub-and-spoke VPN model to connect your sites.
• Direct Connect – setup a Virtual Private Gateway on VPC, and establish a direct private connection to an AWS Direct Connect Location
• Direct Connect Gateway – setup a Direct Connect to many VPCs in different AWS regions.

AWS PrivateLink / VPC Endpoint Services:
	• Connect services privately from your service VPC to customers VPC
	• Doesn’t need VPC Peering, public Internet, NAT Gateway, Route Tables
	• Must be used with Network Load Balancer & ENI
• ClassicLink – connect EC2-Classic EC2 instances privately to your VPC
• Transit Gateway – transitive peering connections for VPC, VPN & DX
• Traffic Mirroring – copy network traffic from ENIs for further analysis
• Egress-only Internet Gateway – like a NAT Gateway, but for IPv6





159
##Networking Cost
-------------------------------------------------
• Use Private IP instead of Public IP for good savings and better network performance
• Use same AZ for maximum savings (at the cost of high availability)



Minimizing egress traffic network cost:
• Egress traffic: outbound traffic (from AWS to outside)
• Ingress traffic: inbound traffic - from outside to AWS (typically free)
• Try to keep as much internet traffic within AWS to minimize costs
• Direct Connect location that are co-located in the same AWS Region result in lower cost for egress network





160
##S3 Data Transfer Pricing – Analysis for USA
-------------------------------------------------
• S3 ingress: free
• S3 to Internet: $0.09 per GB
• S3 Transfer Acceleration:
	• Faster transfer times (50 to 500% better)
	• Additional cost on top of Data Transfer Pricing: +$0.04 to $0.08 per GB
	• S3 to CloudFront: $0.00 per GB
• CloudFront to Internet: $0.085 per GB (slightly cheaper than S3)
	• Caching capability (lower latency)
	• Reduce costs associated with S3 Requests Pricing (7x cheaper with CloudFront)
• S3 Cross Region Replication: $0.02 per GB




161
##Network Protection on AWS
-------------------------------------------------
• To protect network on AWS, we’ve seen
	• Network Access Control Lists (NACLs)
	• Amazon VPC security groups
	• AWS WAF (protect against malicious requests)
	• AWS Shield & AWS Shield Advanced
	• AWS Firewall Manager (to manage them across accounts)
• But what if we want to protect in a sophisticated way our entire VPC?

Its AWS Network Firewall !!!

• Protect your entire Amazon VPC
• From Layer 3 to Layer 7 protection
• Any direction, you can inspect
	• VPC to VPC traffic
	• Outbound to internet
	• Inbound from internet
	• To / from Direct Connect & Site-to-Site VPN
• Internally, the AWS Network Firewall uses the AWS Gateway Load Balancer
• Rules can be centrally managed cross- account by AWS Firewall Manager to apply to many VPCs


Network Firewall – Fine Grained Controls:
• Supports 1000s of rules
	• IP & port - example: 10,000s of IPs filtering
	• Protocol – example: block the SMB protocol for outbound communications
	• Stateful domain list rule groups: only allow outbound traffic to *.mycorp.com or third-party software repo
	• General pattern matching using regex
• Traffic filtering: Allow, drop, or alert for the traffic that matches the rules
• Active flow inspection to protect against network threats with intrusion-prevention
  capabilities (like Gateway Load Balancer, but all managed by AWS)
• Send logs of rule matches to Amazon S3, CloudWatch Logs, Kinesis Data Firehose

AWS Network Firewall has a highly flexible rules engine, so you can build custom firewall rules to protect your unique workloads.

AWS Network Firewall’s stateful firewall can incorporate context from traffic flows, 
like tracking connections and protocol identification, to enforce policies such as preventing 
your VPCs from accessing domains using an unauthorized protocol.





==|Disaster Recovery & Migrations
162
##Disaster Recovery Overview
-------------------------------------------------
• Any event that has a negative impact on a company’s business continuity or finances is a disaster
• Disaster recovery (DR) is about preparing for and recovering from a disaster
• What kind of disaster recovery?
	• On-premise => On-premise: traditional DR, and very expensive
	• On-premise => AWS Cloud: hybrid recovery
	• AWS Cloud Region A => AWS Cloud Region B
• Need to define two terms:
	• RPO: Recovery Point Objective
	• RTO: Recovery Time Objective



RTO and RPO
-------------------------------------------------
Among the components of a DR plan are two key parameters that define:
	1)how long your business can afford to be offline and 
	2)how much data loss it can tolerate. 
These are the Recovery Time Objective (RTO) and Recovery Point Objective (RPO).


RTO:
RTO refers to how much time an application can be down without causing significant damage to the business. 
Some applications can be down for days without significant consequences. 


RPO:
RPO is your goal for the maximum amount of data the organization can tolerate losing. 
This parameter is measured in time: from the moment a failure occurs to your last valid data backup. 
For example, if you experience a failure now and your last full data backup was 24 hours ago, the RPO is 24 hours. 

If you back up all or most of your data in regularly scheduled 24-hour increments, then in the worst-case scenario you 
will lose 24 hours’ worth of data.


Disaster Recovery Strategies (Architecture of the DR strategies):
• Backup and Restore $
• Pilot Light $$
• Warm Standby $$$
• Hot Site / Multi Site Approach  $$$$
https://aws.amazon.com/blogs/architecture/disaster-recovery-dr-architecture-on-aws-part-i-strategies-for-recovery-in-the-cloud/




Backup and restore:
Backups are created in the same Region as their source and are also copied to another Region.
Backup and restore is a suitable approach for mitigating against data loss or corruption.

For Region failover, in addition to data recovery from backup, you must also be able to restore your infrastructure 
in the recovery Region. 

Infrastructure as Code such as AWS CloudFormation or AWS Cloud Development Kit 
(AWS CDK) enables you to deploy consistent infrastructure across Regions.

The backup and recovery strategy is considered the least efficient for RTO.



Pilot Light:
With the pilot light strategy, the data is live, but the services are idle. 
Live data means the data stores and databases are up-to-date (or nearly up-to-date) with the active 
Region and ready to service read operations.

In the pilot light strategy, basic infrastructure elements are in place like Elastic Load Balancing 
and Amazon EC2 Auto Scaling in Figure 6. But functional elements (like compute) are “shut off.” 


• A small version of the app is always running in the cloud
• Useful for the critical core (pilot light)
• Very similar to Backup and Restore
• Faster than Backup and Restore as critical systems are already up



Warm Standby :
Like the pilot light strategy, the warm standby strategy maintains live data in addition to periodic backups. 

• Full system is up and running, but at minimum size 
• Upon disaster, we can scale to production load



Multi Site / Hot Site Approach 
With multi-site active/active, two or more Regions are actively accepting requests. 
Failover consists of re-routing requests away from a Region that cannot serve them.

Here, data is replicated across Regions and is actively used to serve read requests in those Regions.
 
• Very low RTO (minutes or seconds) – very expensive 
• Full Production Scale is running AWS and On Premise




Disaster Recovery Tips:
• Backup 
	• EBS Snapshots, RDS automated backups / Snapshots, etc… 
	• Regular pushes to S3 / S3 IA / Glacier, Lifecycle Policy, Cross Region Replication 
	• From On-Premise: Snowball or Storage Gateway 
• High Availability 
	• Use Route53 to migrate DNS over from Region to Region 
	• RDS Multi-AZ, ElastiCache Multi-AZ, EFS, S3 
	• Site to Site VPN as a recovery from Direct Connect 
• Replication 
	• RDS Replication (Cross Region), AWS Aurora + Global Databases 
	• Database replication from on-premises to RDS 
	• Storage Gateway 
• Automation 
	• CloudFormation / Elastic Beanstalk to re-create a whole new environment 
	• Recover / Reboot EC2 instances with CloudWatch if alarms fail 
	• AWS Lambda functions for customized automations 
• Chaos 
	• Netflix has a “simian-army” randomly terminating EC2



Creating a highly available EC2 instance
Creating a highly available EC2 instance With an Auto Scaling Group
Creating a highly available EC2 instance With ASG + EBS


High Availability for a Bastion Host
HA options for the bastion host:
	• Run 2 across 2 AZ
	• Run 1 across 2 AZ with 1 ASG 1:1:1
Routing to the bastion host:
	• If 1 bastion host, use an elastic IP withc ec2 user-data script to access it
	• If 2 bastion hosts, use an NetworkcLoad Balancer (layer 4) deployed in multiple AZ
	• If NLB, the bastion hosts can live in the private subnet directly
Note: Can’t use ALB as the ALB is layer 7 (HTTP protocol)








163
##DMS (Database Migration Service)
-------------------------------------------------
The source database remains fully operational during the migration, minimizing downtime to applications 
that rely on the database. 


AWS Database Migration Service supports homogeneous migrations such as Oracle to Oracle, as well as 
heterogeneous migrations between different database platforms, 
such as Oracle or Microsoft SQL Server to Amazon Aurora.


You must create an EC2 instance to perform the replication tasks.


DMS Sources and Targets:
SOURCES:
	• On-Premise and EC2 instances databases: Oracle, MS SQL Server, MySQL, MariaDB, PostgreSQL, MongoDB, SAP, DB2
	• Azure: Azure SQL Database
	• Amazon RDS: all including Aurora
	• Amazon S3

TARGETS:
	• On-Premise and EC2 instances databases: Oracle, MS SQL Server, MySQL, MariaDB, PostgreSQL, SAP
	• Amazon RDS
	• Amazon Redshift
	• Amazon DynamoDB
	• Amazon S3
	• ElasticSearch Service
	• Kinesis Data Streams
	• DocumentDB


AWS Schema Conversion Tool (SCT):
• Convert your Database’s Schema from one engine to another
• Example OLTP: (SQL Server or Oracle) to MySQL, PostgreSQL, Aurora
• Example OLAP: (Teradata or Oracle) to Amazon Redshift
• Prefer compute-intensive instances to optimize data conversions

You do not need to use SCT if you are migrating the same DB engine
• Ex: On-Premise PostgreSQL => AWS RDS PostgreSQL
• The DB engine is still PostgreSQL (RDS is the platform)




RDS & Aurora MySQL Migrations
• RDS MySQL to Aurora MySQL
	• Option 1: DB Snapshots from RDS MySQL restored asvMySQL Aurora DB
	• Option 2: Create an Aurora Read Replica from your RDS MySQL, and when the replication lag is 0, promote it as its
      own DB cluster (can take time and cost $)
• External MySQL to Aurora MySQL
	• Option 1:
		• Use Percona XtraBackup to create a file backup in Amazon S3
		• Create an Aurora MySQL DB from Amazon S3
	• Option 2:
		• Create an Aurora MySQL DB
		• Use the mysqldump utility to migrate MySQL into Aurora (slower than S3 method)
• Use DMS if both databases are up and running




RDS & Aurora PostgreSQL Migrations:
• RDS PostgreSQL to Aurora PostgreSQL
	• Option 1: DB Snapshots from RDS PostgreSQL restored as PostgreSQL Aurora DB
	• Option 2: Create an Aurora Read Replica from your RDS PostgreSQL, and when the replication lag is 0, promote it
	- as its own DB cluster (can take time and cost $)
• External PostgreSQL to Aurora PostgreSQL
	• Create a backup and put it in Amazon S3
	• Import it using the aws_s3 Aurora extension
• Use DMS if both databases are up and running


AWS Application Discovery Service :
	Application Discovery Service helps enterprises obtain a snapshot of the current state of their 
	data center servers by collecting server specification information, hardware configuration, 
	performance data, and details of running processes and network connections. 
	Once the data is collected, you can use it to perform a Total Cost of Ownership 
	(TCO) analysis and then create a cost optimized migration plan based on your unique business requirements.


On-Premise strategy with AWS:
• Ability to download Amazon Linux 2 AMI as a VM (.iso format) 
	• VMWare, KVM, VirtualBox (Oracle VM), Microsoft Hyper-V 
• VM Import / Export 
	• Migrate existing applications into EC2 
	• Create a DR repository strategy for your on-premises VMs 
	• Can export back the VMs from EC2 to on-premises 
• AWS Application Discovery Service 
	• Gather information about your on-premises servers to plan a migration 
	• Server utilization and dependency mappings 
	• Track with AWS Migration Hub 
• AWS Database Migration Service (DMS) 
	• replicate On-premise => AWS , AWS => AWS, AWS => On-premise 
	• Works with various database technologies (Oracle, MySQL, DynamoDB, etc..) 
• AWS Server Migration Service (SMS) 
	• Incremental replication of on-premises live servers to AWS



With AWS Database Migration Service, you can continuously replicate your data with high availability 
and consolidate databases into a petabyte-scale data warehouse by streaming data to 
Amazon Redshift and Amazon S3.



AWS DMS enables you to seamlessly migrate data from supported sources to relational databases, 
data warehouses, streaming platforms, and other data stores in AWS cloud.

AWS DMS performs this task out of box without any complex configuration or code development. 
You can also configure an AWS DMS replication instance to scale up or down depending on the workload.


https://aws.amazon.com/blogs/big-data/streaming-data-from-amazon-s3-to-amazon-kinesis-data-streams-using-aws-dms/



BillCost:
=========
You only pay for your replication instances and any additional log storage. 
Each database migration instance includes storage sufficient for swap space, replication logs, 
and data cache for most replications and inbound data transfer is free.






164
##AWS Backup
-------------------------------------------------
AWS Backup is a fully managed service that enables you to centralize and automate data protection 
across on-premises and AWS services. 

Can I use AWS Backup to back up on-premises data?
Yes, you can use AWS Backup to back up your on-premises Storage Gateway (volumes) and VMware virtual machines, 
providing a common way to manage the backups of your application data both on premises and on AWS.

AWS Backup supports both of the scenarios above—it lets you backup data from Amazon EFS, 
DynamoDB, RDS or EBS, and also from on-premise resources via the AWS Storage Gateway.


AWS Backup allows you to define a central data protection policy (called a backup plan) that works across 
AWS services for compute, storage, and databases. 
The backup plan defines parameters such as backup frequency and backup retention period. 

• Fully managed service 
• Centrally manage and automate backups across AWS services 
• No need to create custom scripts and manual processes 
• Supported services: 
	• Amazon EC2 / Amazon EBS 
	• Amazon S3 
	• Amazon RDS (all DBs engines) / Amazon Aurora / Amazon DynamoDB 
	• Amazon DocumentDB / Amazon Neptune 
	• Amazon EFS / Amazon FSx (Lustre & Windows File Server) 
	• AWS Storage Gateway (Volume Gateway) 
• Supports cross-region backups 
• Supports cross-account backups

Supports PITR for supported services
• On-Demand and Scheduled backups
• Tag-based backup policies
	• You create backup policies known as Backup Plans
	• Backup frequency (every 12 hours, daily, weekly, monthly, cron expression)
	• Backup window
	• Transition to Cold Storage (Never, Days, Weeks, Months, Years)
	• Retention Period (Always, Days, Weeks, Months, Years)



AWS Backup Vault Lock:
• Enforce a WORM (Write Once Read Many) state for all the backups that you store in your AWS Backup Vault
• Additional layer of defense to protect your backups against: 
	• Inadvertent or malicious delete operations 
	• Updates that shorten or alter retention periods
• Even the root user cannot delete backups when enabled



AWS Application Discovery Service:
• Plan migration projects by gathering information about on-premises data centers
• Server utilization data and dependency mapping are important for migrations
	• Agentless Discovery (AWS Agentless Discovery Connector)
	    • VM inventory, configuration, and performance history such as CPU, memory, and disk usage
	• Agent-based Discovery (AWS Application Discovery Agent)
		• System configuration, system performance, running processes, and details of the network connections between systems
	• Resulting data can be viewed within AWS Migration Hub


AWS Application Discovery Service helps enterprise customers plan migration projects by 
gathering information about their on-premises data centers.

Not permorming of Migration.



AWS Application Migration Service (MGN)
-------------------------------------------------
You can migrate any workload – applications, websites, databases, storage, physical 
or virtual servers – and even entire data centers from an on-premises environment, 
hosting facility, or other public cloud to AWS.

With Application Migration Service, you can migrate your applications from physical infrastructure, 
VMware vSphere, Microsoft Hyper-V,  EC2, VPC, and other clouds to AWS.


• The “AWS evolution” of CloudEndure Migration, replacing AWS Server Migration Service (SMS)
• Lift-and-shift (rehost) solution which simplify migrating applications to AWS
• Converts your physical, virtual, and cloud-based servers to run natively on AWS
• Supports wide range of platforms, Operating Systems, and databases
• Minimal downtime, reduced costs


Differences between AWS Application Migration Service(MGN) and AWS Server Migration Service(SMS) ?
	Application Migration Service utilizes continuous, block-level replication and enables 
	short cutover  windows measured in minutes. 

	Server Migration Service utilizes incremental, snapshot-based replication 
	and enables cutover windows measured in hours.



Application Migration Service is the primary service(New) recommended to migrate your applications to AWS. 
If Application Migration Service is not available in a specific AWS Region, you can use 
SMS APIs until March 31, 2023.


CloudEndure:
	Application Migration Service is the next generation of CloudEndure Migration, and offers 
	key features and operational benefits that are not available with CloudEndure Migration. 

Application Migration Service is not yet supported in AWS GovCloud and China Regions, and 
does not currently support some legacy operating systems that are supported by CloudEndure Migration.
 
Consider using CloudEndure Migration if your preferred AWS Region or operating system is not currently 
supported by AWS Application Migration Service.

Note: While CloudEndure Migration will continue to be available for use in AWS GovCloud and 
China Regions, it will no longer be available in other AWS Regions as of December 30, 2022. 
After January 1, 2023, CloudEndure Migration will be available only for migrations to 
AWS Outposts and AWS GovCloud and China Regions.






VMware Cloud on Aws
-------------------------------------------------
• Some customers use VMware Cloud to manage their on-premises Data Center
• They want to extend the Data Center capacity to AWS, but keep using the VMware Cloud software
• …Enter VMware Cloud on AWS
• Use cases
	• Migrate your VMware vSphere-based workloads to AWS
	• Run your production workloads across VMware vSphere-based private, public, and hybrid cloud environments
	• Have a disaster recover strategy
	
	
	
	
Transferring large amount of data into AWS:
• Example: transfer 200 TB of data in the cloud. We have a 100 Mbps internet connection.

• Over the internet / Site-to-Site VPN:
	• Immediate to setup
	• Will take 200(TB)*1000(GB)*1000(MB)*8(Mb)/100 Mbps = 16,000,000s = 185d
• Over direct connect 1Gbps:
	• Long for the one-time setup (over a month)
	• Will take 200(TB)*1000(GB)*8(Gb)/1 Gbps = 1,600,000s = 18.5d
• Over Snowball:
	• Will take 2 to 3 snowballs in parallel
	• Takes about 1 week for the end-to-end transfer
	• Can be combined with DMS
• For on-going replication / transfers: Site-to-Site VPN or DX with DMS or DataSync





BillCost:
=========
With AWS Backup, you pay only for the amount of backup storage you use,



Security:
=========
It is a aws managed service, security is a shared responsibility.







==|Extra Solution Architecture discussions

165
S3 Event Notifications
-------------------------------------------------
Amazon S3 events Lambda Function SQS SNS
• S3:ObjectCreated, S3:ObjectRemoved, S3:ObjectRestore, S3:Replication…
• Object name filtering possible (*.jpg)
• Use case: generate thumbnails of images uploaded to S3
• Can create as many “S3 events” as desired
• S3 event notifications typically deliver events in seconds but can sometimes take a minute or longer




S3 Event Notifications with Amazon EventBridge
-------------------------------------------------
S3->EventBridge -> Over 18 AWS services as destinations

• Advanced filtering options with JSON rules (metadata, object size, name...)
• Multiple Destinations – ex Step Functions, Kinesis Streams / Firehose…
• EventBridge Capabilities – Archive, Replay Events, Reliable delivery

Amazon EventBridge – Intercept API Calls
API Gateway – AWS Service Integration Kinesis Data Streams example



There are two primary methods to trigger Lambda when an object is added to an S3 bucket — 
S3 Notifications and EventBridge.

CloudTrail/EventBridge:
	CloudTrail Data events are charged at $0.10 per 100,000 events.
	EventBridge supports a larger number of target services.
	EventBridge allows you to use to extract and transform part of the event to be sent to the target.


S3 Notifications:
	There is no pricing associated with S3 Notifications. 
	S3 Notifications can target Lambda, SNS and SQS.
	Object events can filter based on a prefix or suffix or both.


166
##Caching Strategies
-------------------------------------------------
Caching helps applications perform dramatically faster and cost significantly less at scale
In computing, a cache is a high-speed data storage layer which stores a subset of data, typically transient in nature,
 so that future requests for that data are served up faster than is possible by accessing the data’s 
 primary storage location. 
 
 Caching allows you to efficiently reuse previously retrieved or computed data.


UseCase:
========
Database Caching
Domain Name System (DNS) Caching
Session Management
Application Programming Interfaces (APIs)
Web Caching



167
##Blocking and IP address
-------------------------------------------------
Network ACL if it allows communication between the two subnets.

To allow or block specific IP addresses for your EC2 instances, use a network Access Control List (ACL) 
or security group rules in your VPC. 

Network ACLs and security group rules act as firewalls allowing or blocking 
IP addresses from accessing your resources.



Blocking an IP address
Blocking an IP address–with an ALB
Blocking an IP address–with an NLB
Blocking an IP address – ALB + WAF
Blocking an IP address – ALB, CloudFront WAF






168
##HPC (High performance Computing)
-------------------------------------------------
HPC has been key to solving the most complex problems in every industry, and changing the way we work and live. 
From weather modeling to genome mapping to the search for extraterrestrial intelligence,
HPC is helping to push the boundaries of what’s possible with advanced computing technologies.


The cloud is the perfect place to perform HPC
• You can create a very high number of resources in no time
• You can speed up time to results by adding more resources
• You can pay only for the systems you have used
• Perform genomics, computational chemistry, financial risk modeling,
  weather prediction, machine learning, deep learning, autonomous driving
• Which services help perform HPC?



Data Management & Transfer:
• AWS Direct Connect:
	• Move GB/s of data to the cloud, over a private secure network.
• Snowball & Snowmobile
	• Move PB of data to the cloud
• AWS DataSync
	• Move large amount of data between on-premises and S3, EFS, FSx for Windows




Compute and Networking:
• EC2 Instances:
• CPU optimized, GPU optimized
• Spot Instances / Spot Fleets for cost savings + Auto Scaling
• EC2 Placement Groups: Cluster for good network performance



EC2 Enhanced Networking (SR-IOV):
	• Higher bandwidth, higher PPS (packet per second), lower latency
	• Option 1: Elastic Network Adapter (ENA) up to 100 Gbps
	• Option 2: Intel 82599 VF up to 10 Gbps – LEGACY


ParallelCluster:
	AWS ParallelCluster is just an AWS-supported open-source cluster management tool that makes 
	it easy for you to deploy and manage High-Performance Computing (HPC) clusters on AWS. 

ParallelCluster uses a simple text file to model and provision all the resources needed 
for your HPC applications in an automated and secure manner.



169
##Elastic Fabric Adapter (EFA)
-------------------------------------------------
Great for inter-node communications.

• Improved ENA for HPC, only works for Linux
• Great for inter-node communications, tightly coupled workloads
• Leverages Message Passing Interface (MPI) standard
• Bypasses the underlying Linux OS to provide low-latency, reliable transport

	
An Elastic Fabric Adapter (EFA) is a network device that you can attach to your Amazon EC2 instance to accelerate 
High Performance Computing (HPC) and machine learning applications. 
EFA enables you to achieve the application performance of 
an on-premises HPC cluster, with the scalability, flexibility, and elasticity provided by the AWS Cloud.

EFAs cannot be attached to or detached from an instance in a running state.


 
 
170
##Storage
-------------------------------------------------
Instance-attached storage:
	• EBS: scale up to 256,000 IOPS with io2 Block Express
	• Instance Store: scale to millions of IOPS, linked to EC2 instance, low latency
Network storage:
	• Amazon S3: large blob, not a file system
	• Amazon EFS: scale IOPS based on total size, or use provisioned IOPS
	• Amazon FSx for Lustre:
		• HPC optimized distributed file system, millions of IOPS
		• Backed by S3 




171
##Automation and Orchestration
-------------------------------------------------
AWS Batch
	• AWS Batch supports multi-node parallel jobs, which enables you to run single jobs that span multiple EC2 instances.
	• Easily schedule jobs and launch EC2 instances accordingly

AWS ParallelCluster
	• Open-source cluster management tool to deploy HPC on AWS
	• Configure with text files
	• Automate creation of VPC, Subnet, cluster type and instance types
	• Ability to enable EFA on the cluster (improves network performance)









==Other Services
172
##CloudFormation
-------------------------------------------------
An AWS CloudFormation template is a formatted text file in JSON or YAML language 
that describes your AWS infrastructure. 


Format version: Format version defines the capability of a template.
Description: Any comments about your template.
Metadata: Metadata can be used in the template to provide further information. 

CloudFormation Building Blocks:
Templates components
	1. Resources: your AWS resources declared in the template (MANDATORY)
	2. Parameters: the dynamic inputs for your template
	3. Mappings: the static variables for your template
	4. Outputs: References to what has been created
	5. Conditionals: List of conditions to perform resource creation
	6. Metadata
Templates helpers:
	1. References
	2. Functions

HowToWork:
-Create or use an existing CloudFormation template using JSON or YAML format.
-Save the code in an S3 bucket, which serves as a repository for the code.
-Use AWS CloudFormation to call the bucket and create a stack on your template. 
-CloudFormation reads the file and understands the services that are called, their order, 
 the relationship between the services, and provisions the services one after the other.


For example,you say:
	• I want a security group
	• I want two EC2 machines using this security group
	• I want two Elastic IPs for these EC2 machines
	• I want an S3 bucket
	• I want a load balancer (ELB) in front of these machines


Benefits of AWS CloudFormation (1/2):
Infrastructure as code:
	• No resources are manually created, which is excellent for control
	• The code can be version controlled for example using git
	• Changes to the infrastructure are reviewed through code

Cost:
	• Each resources within the stack is tagged with an identifier so you can easily see how much a stack costs you
	• You can estimate the costs of your resources using the CloudFormation template
	• Savings strategy: In Dev, you could automation deletion of templates at 5 PM and recreated at 8 AM, safely


• Productivity
	• Ability to destroy and re-create an infrastructure on the cloud on the fly
	• Automated generation of Diagram for your templates!
	• Declarative programming (no need to figure out ordering and orchestration)
• Don’t re-invent the wheel
	• Leverage existing templates on the web!
	• Leverage the documentation
• Supports (almost) all AWS resources:
	• Everything we’ll see in this course is supported
	• You can use “custom resources” for resources that are not supported


CloudFormation Stack Designer:
	• We can see all the resources 
	• We can see the relations between the components




How CloudFormation Works:
• Templates have to be uploaded in S3 and then referenced in CloudFormation
• To update a template, we can’t edit previous ones. 
  We have to reupload a new version of the template to AWS.
• Stacks are identified by a name
• Deleting a stack deletes every single artifact that was created by CloudFormation.


Deploying CloudFormation templates:
Manual way:
	• Editing templates in the CloudFormation Designer
	• Using the console to input parameters, etc
Automated way:
	• Editing templates in a YAML file
	• Using the AWS CLI (Command Line Interface) to deploy the templates
	• Recommended way when you fully want to automate your flow

CloudFormation is a declarative way of outlining your AWS Infrastructure, for any resources (most of them are supported).

CloudFormation Stack Designer:
	• We can see all the resources .
	• We can see the relations between the components.


UseCase:
========
Manage infrastructure with DevOps
Automate, test, and deploy infrastructure templates with continuous integration and delivery (CI/CD) automations.
Scale production stacks
Run anything from a single Amazon Elastic Compute Cloud (EC2) instance to a complex multi-region application.

BillCost:
=========
Free Tier
Handler operation
Handler operation duration


Security:
=========
It is a aws managed service, security is a shared responsibility.





173
##SES Amazon Simple Email Service
-------------------------------------------------
• Fully managed service to send emails securely, globally and at scale
• Allows inbound/outbound emails
• Reputation dashboard, performance insights, anti-spam feedback
• Provides statistics such as email deliveries, bounces, feedback loop results, email open
• Supports DomainKeys Identified Mail (DKIM) and Sender Policy Framework (SPF)
• Flexible IP deployment: shared, dedicated, and customer-owned IPs
• Send emails using your application using AWS Console, APIs, or SMTP
• Use cases: transactional, marketing and bulk email communications




174
##Pinpoint
-------------------------------------------------
• Scalable 2-way (outbound/inbound) marketing communications service
• Supports email, SMS, push, voice, and in-app messaging
• Ability to segment and personalize messages with the right content to customers
• Possibility to receive replies
• Scales to billions of messages per day
• Use cases: run campaigns by sending marketing, bulk, transactional SMS messages
• Versus Amazon SNS or Amazon SES (Simple Email Service - AWS)
	• In SNS & SES you managed each message's audience, content, and delivery schedule
	• In Amazon Pinpoint, you create message templates, delivery schedules, highly-targeted segments, and full campaigns




175
##SSM Systems Manager – SSM Session Manager
-------------------------------------------------
Parameter Store allows you to create key-value parameters to save your application configurations, custom environment variables, 
product keys, and credentials on a single interface. 
Parameter Store allows you to secure your data by encryption which is integrated with AWS KMS.

AWS Systems Manager Session Manager is a new interactive shell and CLI that helps to provide secure, 
access-controlled, and audited Windows and Linux EC2 instance management. 

Session Manager removes the need to open inbound ports, manage SSH keys, or use bastion hosts.

Allows you to start a secure shell on your EC2 and on-premises servers
• No SSH access, bastion hosts, or SSH keys needed
• No port 22 needed (better security)
• Supports Linux, macOS, and Windows
• Send session log data to S3 or CloudWatch Logs
Install the SSM Agent IF not already installed.



From (on-premises servers, EC2, ECS, Lambda, etc.) sends a parameter request to SSM Parameter Store:
Then: 
	For plaintext check IAM if the user/role is allowed to retrieve the parameter.
    For encrypted check IAM if the user/role is allowed to retrieve and decrypt the parameter.

SecureString :
Fortunately, there’s no real effort to decrypting the variables. No keys to store or additional processing that needs to 
be done manually. To fetch the SecureString type parameters and have them decrypted along the way, 
simply include the WithDecryption property:




 
AWS Secrets Manager:
	AWS Secrets Manager enables you to rotate, manage, and retrieve database credentials, 
	API keys and other secrets throughout their lifecycle. 
	It also makes it really easy for you to follow security best practices such as 
	encrypting secrets and rotating these regularly. 

If you are a security administrator responsible for storing and managing secrets, 
and ensuring that your organization follows regulatory and compliance requirements, 
you can use Secrets Manager.


 
 
176
##Elastic Transcoder
-------------------------------------------------
• Elastic Transcoder is used to convert media files stored in S3 into media
  files in the formats required by consumer playback devices (phones etc..)
• Benefits:
	• Easy to use
	• Highly scalable – can handle large volumes of media files and large file sizes
	• Cost effective – duration-based pricing model
	• Fully managed & secure, pay for what you use



177
##AWS Batch
-------------------------------------------------
AWS Batch is a set of batch management capabilities that enables developers, scientists, 
and engineers to easily and  efficiently run hundreds of thousands of batch computing jobs on AWS. 

• Fully managed batch processing at any scale
• Efficiently run 100,000s of computing batch jobs on AWS
• A “batch” job is a job with a start and an end (opposed to continuous)
• Batch will dynamically launch EC2 instances or Spot Instances
• AWS Batch provision	s the right amount of compute / memory
• You submit or schedule batch jobs and AWS Batch does the rest!
• Batch jobs are defined as Docker images and run on ECS
• Helpful for cost optimizations and focusing less on the infrastructure



Batch vs Lambda:
• Lambda: 
	• Time limit 
	• Limited runtimes 
	• Limited temporary disk space 
	• Serverless 
• Batch: 
	• No time limit 
	• Any runtime as long as it’s packaged as a Docker image 
	• Rely on EBS / instance store for disk space 
	• Relies on EC2 (can be managed by AWS)





178

##AppFlow
-------------------------------------------------
As a managed serverless platform, 
• Fully managed integration service that enables you to securely transfer
  data between Software-as-a-Service (SaaS) applications and AWS.
• Sources: Salesforce, SAP, Zendesk, Slack, and ServiceNow
• Destinations: AWS services like Amazon S3, Amazon Redshift or nonAWS such as SnowFlake and Salesforce
• Frequency: on a schedule, in response to events, or on demand
• Data transformation capabilities like filtering and validation
• Encrypted over the public internet or privately over AWS PrivateLink
• Don’t spend time writing the integrations and leverage APIs immediately






==White Papers and Architectures
179
##AWS Well-Architected Framework And Tool
-------------------------------------------------
Well Architected Framework General Guiding Principles -architected
• Stop guessing your capacity needs 
• Test systems at production scale 
• Automate to make architectural experimentation easier 
• Allow for evolutionary architectures 
• Design based on changing requirements 
• Drive architectures using data 
• Improve through game days 
• Simulate applications for flash sale days



Well Architected Framework 6 Pillars
• 1) Operational Excellence
• 2) Security
• 3) Reliability
• 4) Performance Efficiency
• 5) Cost Optimization
• 6) Sustainability

They are not something to balance, or trade-offs, they’re a synergy



AWS Well-Architected Tool:
• Free tool to review your architectures against the 6 pillars Well-Architected
  Framework and adopt architectural best practices
• How does it work?
	• Select your workload and answer questions
	• Review your answers against the 6 pillars
	• Obtain advice: get videos and documentations, generate a report, see the results in a dashboard



180
##AWS Trusted Advisor
-------------------------------------------------
AWS Trusted Advisor provides recommendations that help you follow AWS best practices. 

Trusted Advisor evaluates your account by using checks. 
These checks identify ways to optimize your AWS infrastructure, 
improve security and performance, reduce costs, and monitor service quotas.

No need to install anything – high level AWS account assessment
Analyze your AWS accounts and provides recommendation:
Core Checks and recommendations – all customers
• Can enable weekly email notification from the console
• Full Trusted Advisor – Available for Business & Enterprise support plans
	• Ability to set CloudWatch alarms when reaching limits
	• Programmatic Access using AWS Support API



Trusted Advisor Checks Examples:
• Cost Optimization: 
	• low utilization EC2 instances, idle load balancers, under-utilized EBS volumes… 
	• Reserved instances & savings plans optimizations, 
• Performance: 
	• High utilization EC2 instances, CloudFront CDN optimizations 
	• EC2 to EBS throughput optimizations, Alias records recommendations 
• Security: 
	• MFA enabled on Root Account, IAM key rotation, exposed Access Keys 
	• S3 Bucket Permissions for public access, security groups with unrestricted ports 
• Fault Tolerance: 
	• EBS snapshots age, Availability Zone Balance 
	• ASG Multi-AZ, RDS Multi-AZ, ELB configuration… 
• Service Limits


More Architecture Examples:
• We’ve explored the most important architectural patterns:
	• Classic: EC2, ELB, RDS, ElastiCache, etc…
	• Serverless: S3, Lambda, DynamoDB, CloudFront, API Gateway, etc…
• If you want to see more AWS architectures:
	• https://aws.amazon.com/architecture/
	• https://aws.amazon.com/solutions/




UseCase:
========
Recommendations for cost optimization, performance, security and fault tolerance
-Red - Action recommended Yellow - investigate and Green - Good to go
-All AWS customers get 4 checks for free:
-Service limits (usage > 80%)
-Security groups having unrestricted access (0.0.0.0/0)
-Proper use of IAM
-MFA on Root Account
-Business or Enterprise AWS support plan provides over 50 checks
-Disable those you are not interested in
-How much will you save by using Reserved Instances?
-How does your resource utilization look like? Are you right sized?



AWS Amplify:
-------------------------------------------------
AWS Amplify is a complete solution that lets frontend web and mobile developers easily build, ship, 
and host full-stack applications on AWS, with the flexibility to leverage the breadth of 
AWS services as use cases evolve. No cloud expertise needed.

AWS is a full-suite platform developed to aid web and mobile developers in building full-stack 
and scalable applications operated by AWS.




##END
=================================================
@@@Proceed by elimination
=================================================
=================================================
• Most questions are going to be scenario based
• For all the questions, rule out answers that you know for sure are wrong
• For the remaining answers, understand which one makes the most sense
• There are very few trick questions
• Don’t over-think it
• If a solution seems feasible but highly complicated, it’s probably wrong














181
##VPC Reachability Analyzer
-------------------------------------------------
VPC Reachability Analyzer to determine whether a destination resource in your 
virtual private cloud (VPC) is reachable from a source resource. 

It builds a model of the network configuration, then checks the reachability based on 
these configurations (it doesn’t send packets).


UseCase:
========
Troubleshoot connectivity issues,ensure network configuration is as intended.


BillCost:
=========
You are charged per analysis run between a source and destination.




182
##DNS Resolution
-------------------------------------------------
DNS Resolution (enableDnsSupport)
• Decides if DNS resolution from Route53 Resolver server is supported for the VPC
• True (default): it queries the Amazon Provider DNS Server at 169.254.169.253 or the
reserved IP address at the base of the VPC IPv4 network range plus two (.2)

DNS Hostnames (enableDnsHostnames)
• By default,
• True => default VPC
• False => newly created VPCs
• Won’t do anything unless enableDnsSupport=true
• If True, assigns public hostname to EC2 instance if it has a public IPv4

If you use custom DNS domain names in a Private Hosted Zone in
Route53, you must set both these attributes (enableDnsSupport & enableDnsHostname) to true





183
##Route53 Private Zones
-------------------------------------------------
A private hosted zone is a container that holds information about how you want  Route53 to respond to
DNS queries for a domain and its subdomains within one or more VPCs that you create with the  VPC service.

You create a private hosted zone, such as example.com, and specify the VPCs that you want to associate with the hosted zone.
Managing hosted zones: You pay a monthly charge for each hosted zone managed with Route53.





184
#Data Warehouse
-------------------------------------------------
A data warehouse is a specialized type of relational database, which is optimized for
analysis and reporting of large amounts of data. It can be used to combine
transactional data from disparate sources.

Traditionally, setting up, running, and scaling a data warehouse has been complicated
and expensive. 
On AWS, you can leverage Amazon Redshift, a managed data
warehouse service that is designed to operate at less than a tenth the cost of
traditional solutions.





185
##EC2 Nitro
-------------------------------------------------
Underlying Platform for the next generation of EC2 instances
• New virtualization technology
• Allows for better performance:
	• Better networking options (enhanced networking, HPC, IPv6)
	• Higher Speed EBS (Nitro is necessary for 64,000 EBS IOPS – max 32,000 on non-Nitro)
• Better underlying security




186
#Storage | DB | db | s3 | efs | ebs           
-------------------------------------------------
There are several categories of databases:
Relational (OLTP and OLAP), Document, KeyValue, Graph, In Memory among others


SDK:
• Good to know: if you don’t specify or configure a default region, then
us-east-1 will be chosen by default





187
##SAM (Serverless Appliction Model)
-------------------------------------------------
The AWS Serverless Application Model (AWS SAM) is an open-source framework that you can use to build serverless 
applications on AWS.

A serverless application is a combination of Lambda functions, event sources, and other resources that work 
together to perform tasks.

All the configuration is YAML code:
• Lambda Functions
• DynamoDB tables
• API Gateway
• Cognito User Pools



Area:
=====
Regional.


UseCase:
========
Single-deployment configuration.
Extension of AWS CloudFormation.


BillCost:
=========
There is no additional charge to use AWS SAM. 


Security:
=========
It is a aws managed service, security is a shared responsibility.




189
##AWS Distributing Paid Content
-------------------------------------------------
You can configure CloudFront to require that users access your files using either 
signed URLs or signed cookies. 

Premium User Video service:
We have implemented a fully serverless solution:
• Cognito for authentication
• DynamoDB for storing users that are premium
• 2 serverless applications
	• Premium User registration
	• CloudFront Signed URL generator
• Content is stored in S3 (serverless and scalable)
• Integrated with CloudFront with OAI for security (users can’t bypass)
• CloudFront can only be used using Signed URLs to prevent unauthorized users
• What about S3 Signed URL? They’re not efficient for global access-enabled


An ending date and time, after which the URL is no longer valid.
(Optional) The date and time that the URL becomes valid.
(Optional) The IP address or range of addresses of the computers that can be used to access your content.






188
##Big Data Ingestion Pipeline
-------------------------------------------------
AWS Data Pipeline is a web service that helps you reliably process and move data between different 
AWS compute and storage services, as well as on-premises data sources, at specified intervals. 

Efficiently transfer the results to  AWS services such as S3,  RDS,  DynamoDB, and  EMR.

 
Big Data Ingestion Pipeline discussion:
• IoT Core allows you to harvest data from IoT devices
• Kinesis is great for real-time data collection
• Firehose helps with data delivery to S3 in near real-time (1 minute)
• Lambda can help Firehose with data transformations
• Amazon S3 can trigger notifications to SQS
• Lambda can subscribe to SQS (we could have connecter S3 to Lambda)
• Athena is a serverless SQL service and results are stored in S3
• The reporting bucket contains analyzed data and can be used by reporting tool such as AWS QuickSight, Redshift, etc.






190
##Neptune
-------------------------------------------------
Fully managed graph database.

Neptune for Solutions Architect:
• Operations: similar to RDS
• Security: IAM, VPC, KMS, SSL (similar to RDS) + IAM Authentication
• Reliability: Multi-AZ, clustering
• Performance: best suited for graphs, clustering to improve performance
• Remember: Neptune = Graphs




UseCase:
========
• High relationship data 
• Social Networking: Users friends with Users, replied to comment on post of user and likes other comments.
• Knowledge graphs (Wikipedia) 


BillCost:
=========
Pay per node provisioned (similar to RDS)



Security:
=========
Amazon Neptune supports HTTPS encrypted client connections and also allows you to encrypt your databases using keys you 
manage through AWS Key Management Service (KMS). 






191
##STS (Security Token Services)
-------------------------------------------------
AWS STS is  to request temporary security credentials for your AWS resources, 
for IAM authenticated users and users that are authenticated in AWS such as federated users via OpenID or SAML2.0.


AWS STS – Security Token Service:
• Allows to grant limited and temporary access to AWS resources.
• AssumeRole
• Cross Account Access: assume role in target account to perform actions there
• AssumeRoleWithSAML
• AssumeRoleWithWebIdentity
• return creds for users logged with an IdP (Facebook Login, Google Login, OIDC compatible…)
• GetSessionToken
• Temporary credentials can be valid between 15 minutes to 1 hour




Identity Federation in AWS:
• Federation lets users outside of AWS to assume temporary role for accessing AWS resources.
• These users assume identity provided access role.
Federations can have many flavors:
	• SAML 2.0
	• Custom Identity Broker
	• Web Identity Federation with Amazon Cognito
	• Web Identity Federation without Amazon Cognito
	• Single Sign On
	• Non-SAML with AWS Microsoft AD
Using federation, you don’t need to create IAM users (user management is outside of AWS)


ExamTips: Any time you see AssumeRole or Cross account access, its STS.


AWS STS supports open standards like Security Assertion Markup Language (SAML) 2.0, 
with which you can use Microsoft AD FS to leverage your Microsoft Active Directory. 
You can also use SAML 2.0 to manage your own solution for federating user identities.

Area:
=====
STS is a global service


UseCase:
========
-Identity Federation Use-Case
-Cross-Account Access using AWS STS
-EC2 Instance STS Credentials



BillCost:
=========
No additional charge.







194
##RAM (Resource Access Manager)
-------------------------------------------------
You can use AWS RAM to share resources with other AWS accounts.
When you share a resource with another account, that account is granted access to the resource and any 
policies and permissions in that account apply to the shared resource.


Avoid resource duplication!

AWS Resource Access Manager (RAM) is a service that enables you to easily and securely share AWS resources with any 
AWS account or within your AWS Organization. 
You can share AWS Transit Gateways, Subnets, AWS License Manager configurations, and Amazon Route 53 Resolver rules resources with RAM.


The AWS Resource Access Manager (RAM) service simply helps you to securely share your resources across AWS accounts 
or within your organization or organizational units (OUs) in AWS Organizations. 
It is not capable of launching new AWS accounts with preapproved configurations.




195
##CloudHSM
-------------------------------------------------
AWS CloudHSM provides hardware security modules in the AWS Cloud. 
A hardware security module (HSM) is a computing device that 
processes cryptographic operations and provides secure storage for cryptographic keys.


Q: What can I do with CloudHSM?
You can use the CloudHSM service to support a variety of use cases and applications, 
such as database encryption, Digital Rights Management (DRM), Public Key Infrastructure (PKI), 
authentication and authorization, document signing, and transaction processing.

There are no upfront costs to use AWS CloudHSM. 
With CloudHSM, you pay an hourly fee for each HSM you launch until you terminate the HSM.


• You manage your own encryption keys entirely (not AWS)
• Must use the CloudHSM Client Software
• Redshift supports CloudHSM for database encryption and key management
• Good option to use with SSE-C encryption



UseCase:
========
Offload the SSL/TLS processing for web servers
Protect the private keys for an issuing certificate authority (CA)
Enable transparent data encryption (TDE) for Oracle databases

Generate, store, import, export, and manage cryptographic keys, including symmetric keys and asymmetric key pairs.
Use symmetric and asymmetric algorithms to encrypt and decrypt data.
Use cryptographic hash functions to compute message digests and hash-based message authentication codes (HMACs).

Cryptographically sign data (including code signing) and verify signatures.
Generate cryptographically secure random data.


BillCost:
=========
No free tier available
With AWS CloudHSM, you pay by the hour with no long-term commitments or upfront payments


Security:
=========
Hardware based key storage for regulatory compliance
AWS CloudHSM
	
Security is a shared responsibility.






197
##CICD 
-------------------------------------------------
CI/CD can be pictured as a pipeline, where new code is submitted on one end, tested over a series of stages 
(source, build, test, staging, and production), and then published as production-ready code.

Tools:
-CodeCommit - Private source control (Git)
-CodePipeline - Orchestrate CI/CD pipelines
-CodeBuild - Build and Test Code (application packages and containers)
-CodeDeploy - Automate Deployment (EC2, ECS, Elastic Beanstalk, EKS, Lambda etc)

AWS CodeDeploy is a service that automates code deployments to any instance, including 
Amazon EC2 instances and instances running on-premises. 

AWS CodeDeploy makes it easier for you to rapidly release new features, helps you avoid downtime during deployment, 
and handles the complexity of updating your applications.



BillCost:
=========
AWS Free Tier, AWS CodePipeline offers new and existing customers one free active pipeline each month.



Security:
=========
It is a aws managed service, security is a shared responsibility.





200
##SWF - Simple Workflow Service
-------------------------------------------------
The Amazon Simple Workflow Service (Amazon SWF) makes it easy to build applications that 
coordinate work across distributed components. 

Amazon SWF acts as the coordination hub for all of the different components of your application:

Coordinate work amongst applications:
• Code runs on EC2 (not serverless)
• 1 year max runtime
• Concept of “activity step” and “decision step”
• Has built-in “human intervention” step
• Example: order fulfilment from web to warehouse to delivery
• Step Functions is recommended to be used for new applications, except:
	• If you need external signals to intervene in the processes
	• If you need child processes that return values to parent processes



UseCase:
========
Maintaining application state
Tracking workflow executions and logging their progress
Holding and dispatching tasks
Controlling which tasks each of your application hosts will be assigned to execute

BillCost:
=========
Free Tier *
All charges are metered daily and billed monthly.



Security:
=========
It is a aws managed service, security is a shared responsibility.





201
##EMR - Elastic MapReduce (open source BI tools)
-------------------------------------------------
EMR does not offer the same storage and processing speed as FSx for Lustre. 

Amazon Elastic MapReduce (Amazon EMR) is a web service that makes it easy to quickly and 
cost-effectively process vast amounts of data.

Amazon EMR is the industry-leading cloud big data platform for processing vast amounts of data using 
open source tools such as Apache Spark, Apache Hive, Apache HBase, Apache Flink, Apache Hudi, and Presto. 

• EMR stands for “Elastic MapReduce”
• EMR helps creating Hadoop clusters (Big Data) to analyze and process vast amount of data
• The clusters can be made of hundreds of EC2 instances
• Also supports Apache Spark, HBase, Presto, Flink…
• EMR takes care of all the provisioning and configuration
• Auto-scaling and integrated with Spot instances





202
##OpsWorks
-------------------------------------------------
AWS OpsWorks is a configuration management service that provides managed instances of Chef and Puppet. 
Chef and Puppet are automation platforms that allow you to use code to automate the configurations 
of your servers. 

Chef & Puppet help you perform server configuration automatically, or repetitive actions
• They work great with EC2 & On Premise VM
• AWS Opsworks = Managed Chef & Puppet
• It’s an alternative to AWS SSM (AWS Systems Manager)




203
##AWS WorkSpaces
-------------------------------------------------
-Desktop-as-a-Service (DaaS)
--Provision Windows or Linux desktops in minutes
-Eliminate traditional desktop management - Virtual Desktop Infrastructure (VDI)

Managed, Secure Cloud Desktop
• Great to eliminate management of on-premises VDI (Virtual Desktop Infrastructure)
• On Demand, pay per by usage
• Secure, Encrypted, Network Isolation
• Integrated with Microsoft Active Directory





##GraphQL:
------------------------------------------------
GraphQL is a query language for APIs and a runtime for fulfilling those queries with your existing data. 

Send a GraphQL query to your API and get exactly what you need, nothing more and nothing less. 
GraphQL queries always return predictable results. Apps using GraphQL are fast and stable because they 
control the data they get, not the server.






207
##AppSync (GraphQL API)
-------------------------------------------------
What is AWS AppSync?
The fundamental idea of a GraphQL API is that all API functionality is available via a 
unified query language (the Graph Query Language) under a single endpoint. 


AWS AppSync is a serverless GraphQL and Pub/Sub API service that simplifies building modern web and mobile applications.
AWS AppSync GraphQL APIs simplify application development by providing a single endpoint to securely 
query or update data from multiple databases, microservices, and APIs.


GraphQL APIs built with AWS AppSync give front-end developers the ability 
        to query multiple databases, microservices, and APIs from a single GraphQL endpoint. 
Pub/Sub APIs built with AWS AppSync give front-end developers the ability 
        to publish real-time data updates to subscribed API clients via serverless WebSockets connections.

AWS AppSync SDKs support iOS, Android, and JavaScript. 

Store and sync data across mobile and web apps in real-time
• Makes use of GraphQL (mobile technology from Facebook)
• Client Code can be generated automatically
• Integrations with DynamoDB / Lambda
• Real-time subscriptions
• Offline data synchronization (replaces Cognito Sync)
• Fine Grained Security




Area:
=====
MayRegional

UseCase:
========
Unified data access
Retrieve or modify data from multiple data sources (SQL, NoSQL, search data, REST endpoints, and serverless backends) with a single call.
Unified microservices access

Real-time chat application
Build conversational mobile or web applications that support multiple private chat rooms, offer access to conversation history, 
and queue outbound messages, even when a device is offline.

Offline application sync
Automatically synchronize data between mobile/web applications and the cloud with AWS AppSync and Amplify DataStore, 
an on-device persistent storage engine with built-in support for data versioning and conflict detection and resolution.

BillCost:
=========
You are billed separately for query and data modification operations, and for performing real-time updates on your data.

Security:
=========
It is a aws managed service, security is a shared responsibility.






209
##BGP |   Border Gateway Protocol          
-------------------------------------------------
BGP is a means by which all junction points on the internet (routers) communicate with each other to dynamically establish the 
correct (and correctly weighted) paths that network packets should follow to traverse the global networking



210
##AWS Policy Simulator | IAM
-------------------------------------------------
The AWS IAM Simulator is a tool that helps you to test the effects of IAM access control policies. 
This tool helps when you find yourself manually performing actions to test a policy. 
The IAM simulator can simulate actions for any IAM principal, resource, and policy conditions.

The simulator does not make an actual AWS service request, so you can safely test 
requests that might make unwanted changes to your live AWS environment.




212
##ECMP
-------------------------------------------------
Equal-cost multi-path routing (ECMP) is a routing strategy where packet forwarding to a single destination can occur over 
multiple best paths with equal routing priority. 

AWS Transit Gateway VPN supports ECMP protocol that can load balance traffic across multiple VPN tunnels. 
The question is, can Transit Gateway ECMP be used to deploy a transit DMZ as shown in the diagram below?




213
##ENA (Elastic Network Adapter)
-------------------------------------------------
The Elastic Network Adapter (ENA) is designed to improve operating system health and reduce 
the chances of long-term disruption because of unexpected hardware behavior and or failures. 


#With EC2
The Elastic Network Adapter (ENA) driver publishes network performance metrics from the instances where they are enabled. 




215
##Managed Services | IAAS| PAAS       
-------------------------------------------------
AWS Managed Service Offerings:

-Elastic Load Balancing - Distribute incoming traffic across multiple targets
-AWS ElasticBeanstalk - Run and Manage Web Apps
-Amazon Elastic Container Service (ECS) - Containers orchestration on AWS
-AWS Fargate - Serverless compute for containers
-Amazon Elastic Kubernetes Service (EKS) - Run Kubernetes on AWS
-Amazon RDS - Relational Databases - MySQL, Oracle, SQL Server etc



IAAS:
IAAS (Infrastructure as a Service) is all about using only infrastructure from cloud provider. 
It is also called “Lift and Shift”. Example: Using EC2 to deploy your applications or databases

With IAAS, you are responsible for:
	Application Code and Runtime
	Configuring load balancing
	Auto scaling
	OS upgrades and patches
	Availability


PAAS:
PAAS (Platform as a Service) is all about using a platform provided by cloud

Cloud provider is responsible for:
	OS (incl. upgrades and patches)
	Application Runtime
	Auto scaling, Availability & Load balancing etc..
	You are responsible for:
	Application code
	Configuration

Examples of PAAS
CAAS (Container as a Service): Containers instead of Applications
FAAS (Function as a Service) or Serverless: Functions instead of Applications




##CloudMap
================================================
AWS Cloud Map is a cloud resource discovery service. Cloud Map enables you to name your 
application resources with custom names, and it automatically updates the locations of these 
dynamically changing resources. 
This increases your application availability because your 
applications always discover the most up-to-date locations of its resources.


CloudMap->Implementing lightweight on-premises API connectivity using inverting traffic proxy.





##END
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@







=================================================
##Section 4: IAM & AWS CLI        
================================================= 

#IAM | User | Group | Policy
-------------------------------------------------
IAM is a global AWS services that is not limited by regions. 
Any user, group, role or policy is accessible globally.

AWS IAM is also called AWS Identity and Access Management.
It helps you securely manage AWS resources and services.
IAM user represents an entity (person or an application) that interacts with AWS resources and services.


IAM Entities:
-------------
-Users - any individual end user such as an employee, system architect, CTO, etc.
-Groups - any collection of similar people with shared permissions such as system administrators, HR employees, finance teams, etc. 
 Each user within their specified group will inherit the permissions set for the group.
-Roles - any software service that needs to be granted permissions to do its job, 
 e.g- AWS Lambda needing write permissions to S3 or a fleet of EC2 instances needing read permissions from a RDS MySQL database.
-Policies - the documented rule sets that are applied to grant or limit access. 
 In order for users, groups, or roles to properly set permissions, they use policies. 
 Policies are written in JSON and you can either use custom policies for your specific needs or use the default policies set by AWS.


IAM features are:
    AWS account root user
	IAM Users
	IAM policy
	IAM groups
	IAM roles
	Multi-factor authentication
	

User can access AWS Three way:
-AwS Management Console
-AWS Command Line Interface(CLI)
-AWS Softeare Developer Kit (SDK)
Also CloudShell like CLI	


ConfigCLI:
==========
Download/Install CLI Software you Operstion system
=>aws --version
=>aws configure
Then Enter access ID and Key
=>aws iam list-users


CloudShell:
AWS CloudShell is a browser-based shell that gives you command-line access to your AWS resources in the selected AWS region. 
AWS CloudShell comes pre-installed with popular tools for resource management and creation.


IAM Role:
Some aws service will need to perform action on your behalf, to do so, we will\
assign permissions to AWS services with IAM Roles.



IAM Security Tools:
==================
• IAM Credentials Report (account-level)
-A report that lists all your account's users and the status of their various credentials.

• IAM Access Advisor (user-level)
-Access advisor shows the service permissions granted to a user and when those services were last accessed.
-You can use this information to revise your policies.



IAM Guidelines & Best Practices:
• Don’t use the root account except for AWS account setup
• One physical user = One AWS user
• Assign users to groups and assign permissions to groups
• Create a strong password policy
• Use and enforce the use of Multi Factor Authentication (MFA)
• Create and use Roles for giving permissions to AWS services
• Use Access Keys for Programmatic Access (CLI / SDK)
• Audit permissions of your account with the IAM Credentials Report



IAM Section – Summary:
• Users: mapped to a physical user, has a password for AWS Console
• Groups: contains users only
• Policies: JSON document that outlines permissions for users or groups
• Roles: for EC2 instances or AWS services
• Security: MFA + Password Policy
• Access Keys: access AWS using the CLI or SDK
• Audit: IAM Credential Reports & IAM Access Advisor


Golden AMI is an image that contains all your software, dependencies, and configurations, so that future 
EC2 instances can boot up quickly from that AMI.



=================================================
##Section 5: EC2 Fundamentals
=================================================
Ec2=> Elastic Compute Cloud (Infrastructure as a Service on AWS)
EC2 provides virtual computing environments called “instances.”
Copy of EC2 instances: Old instance -> Snapshot -> Image (AMI) -> New instance


The following EC2-related resources don't generate charges when used or provisioned to an account:
-Virtual private clouds (VPCs)
-Security groups
-Key pairs
-Elastic network interfaces
-Auto Scaling groups


IAM Roles are the right way to provide credentials and permissions to an EC2 instance.


#EC2    
-------------------------------------------------
t2.micro:
	t - Instance Family
	2 - generation. Improvements with each generation.
	micro - size. (nano < micro < small < medium < large < xlarge < …..)


EC2 provides two important services to get details:
-Instance Metadata Service
-Dynamic Data Service

MetadataURL: 
http://169.254.169.254/latest/meta-data/
http://169.254.169.254/latest/dynamic/
http://169.254.169.254/latest/dynamic/instance-identity/document


#AWS EC2 Instance Metadata
-------------------------------------------------
AWS EC2 Instance Metadata is powerful but one of the least known features to developers
• It allows AWS EC2 instances to ”learn about themselves” without using an IAM Role for that purpose.
• The URL is http://169.254.169.254/latest/meta-data
• You can retrieve the IAM Role name from the metadata, but you CANNOT retrieve the IAM Policy.
• Metadata = Info about the EC2 instance
• Userdata = launch script of the EC2 instance
• Let’s practice and see what we can do with it!


#Bootstrapping: 
-------------------------------------------------
Install OS patches or software when an EC2 instance is launched.
def-sub-south-1a

Example:
#!/bin/bash
yum update -y
yum install -y httpd
systemctl start httpd
systemctl enable httpd
curl -s http://169.254.169.254/latest/dynamic/instance-identity/document > /var/www/html/index.html
You can call http://169.254.169.254/latest/user-data/ from inside the EC2 instance to lookup user data configured for that instance.

http://3.110.83.133/home.html

#Security Groups:
-------------------------------------------------
Security groups are acting as a “firewall” on EC2 instances 
They regulate: 
• Access to Ports 
• Authorised IP ranges – IPv4 and IPv6 
• Control of inbound network (from other to the instance) 
• Control of outbound network (from the instance to other)

Security Groups can be attached to multiple EC2 instances within the same AWS Region/VPC.


-Security Groups are the fundamental of network security in AWS
-They control how traffic is allowed into or out of our EC2 Instances
-Security groups only contain allow rules
-Security groups rules can reference by IP or by security group



General Ports:
22    = SSH (Secure Shell) - log into a Linux instance
21    = FTP (File Transfer Protocol) – upload files into a file share
22    = SFTP (Secure File Transfer Protocol) – upload files using SSH
80    = HTTP – access unsecured websites
443   = HTTPS – access secured websites
3389  = RDP (Remote Desktop Protocol) – log into a Windows instance


Key Pairs:
-Public key cryptography (Key Pairs) used to protect your EC2 instances
-You need private key with right permissions (chmod 400) to connect to your EC2 instance. 
(Windows EC2 instances only) You need admin password also.
-Security group should allow SSH(22) or RDP(3389)


SSH:
-------------------------------------------------

Windows PowerShell:
Go to .pem file directory then
=>ssh -i .\MyKeFile.pem ec2-user@ec2PublicIP


=>whoami
=>ping google.com


#EC2 Tenancy - Shared vs Dedicated
--------------------------------------------------

There are two Dedicated EC2 options:
-EC2 Dedicated Instances
-EC2 Dedicated Hosts


EC2 Dedicated Instances are Virtualized instances on hardware dedicated to one customer:
You do NOT have visibility into the hardware of underlying host

EC2 Dedicated Hosts are Physical servers dedicated to one customer:
You have visibility into the hardware of underlying host (sockets and physical cores)
(Use cases) Regulatory needs or server-bound software licenses like Windows Server, SQL Server


#EC2 Pricing Models:
--------------------------------------------------
-On Demand: Request when you want it	Flexible and Most Expensive
-Spot: Quote the maximum price	Cheapest (upto 90% off) BUT NO Guarantees
-Reserved: Reserve ahead of time	Upto 75% off. 1 or 3 years reservation.
-Savings Plans: Commit spending $X per hour on (EC2 or AWS Fargate or Lambda)	Upto 66% off. No restrictions. 1 or 3 years reservation.


Compute Optimized:
Compute Optimized EC2 instances are great for compute-intensive workloads requiring high-performance 
processors (e.g., batch processing, media transcoding, high-performance computing, scientific modeling & 
machine learning, and dedicated gaming servers).


Memory Optimized:
Memory Optimized EC2 instances are great for workloads requiring large data sets in memory.


Storage Optimized:
Storage Optimized EC2 instances are great for workloads requiring high, sequential read/write access to large data sets on local storage.


Dedicated Hosts:
Dedicated Hosts are good for companies with strong compliance needs or for software that have complicated licensing models. 
This is the most expensive EC2 Purchasing Option available.


#Monitoring EC2 instances
-------------------------------------------------
Amazon CloudWatch is used to monitor EC2 instances.

There are two types of monitoring:
(FREE) Basic monitoring (“Every 5 minutes”) provided for all EC2 instance types
(\(\)) EC2 Detailed Monitoring can be enabled for detailed metrics every 1 minute


VM Import/Export:
VM Import/Export enables you to easily import virtual machine images from your existing environment to Amazon EC2 instances. 
This service allows you to leverage your existing investments in the virtual machines that you have built to meet your IT security, 
configuration management, and compliance requirements by bringing those virtual machines into Amazon EC2 as ready-to-use instances. 
If you are planning to use your own Microsoft licenses, use the ImportImage tool made available by the 
VM Import/Export service to import your own Microsoft media.
The VM Import/Export service is available at no additional charge beyond standard usage charges for Amazon EC2 and Amazon S3.


Termination Protection:
Remember:EC2 Termination Protection is not effective for terminations from a) Auto Scaling Groups (ASG) b) Spot Instances c) OS Shutdown
Launch Templates - Pre-configured templates (AMI ID, instance type, and network settings) simplifying the creation of EC2 instances.


INP:
====
-You want to update the EC2 instance to a new AMI updated with latest patches
Relaunch a new instance with an updated AMI

-Create EC2 instances based on on-premise Virtual Machine (VM) images	
Use VM Import/Export. You are responsible for licenses.


-You are installing a lot of software using user data slowing down instance launch. How to make it faster?	
Create an AMI from the EC2 instance and use it for launching new instances

-I’ve stopped my EC2 instance. Will I be billed for it?	
ZERO charge for a stopped instance (If you have storage attached, you have to pay for storage)




=================================================
##Section 6: EC2 - Solutions Architect Associate Level
=================================================
Spot Fleet is a set of Spot Instances and optionally On-demand Instances. 
It allows you to automatically request Spot Instances with the lowest price.


#Elastic IP Addresses:
-------------------------------------------------
How do you get a constant public IP address for a EC2 instance? Quick and dirty way is to use an Elastic IP!
An Elastic IP can be switched to another EC2 instance within the same region. Elastic IP remains attached even if you stop the instance. 
You have to manually detach it.



#Placement Group        
-------------------------------------------------

Placement group is a way to impact the placement of interdependent EC2 instance groups in order 
to suit your workload requirements. 
AWS provides three placement strategies which you can use based on the type of your workload.


General rules and limitations:
	You can create a maximum of 500 placement groups per account in each Region.
	You cannot launch Dedicated Hosts in placement groups.




Cluster Placement Groups: 
	A logical grouping of instances within a single AZ.
	A cluster placement group can't span multiple Availability Zones.
	You can launch multiple instance types into a cluster placement group. 
	We recommend using the same instance type for all instances in a cluster placement group.
	Network traffic to the internet and over an AWS Direct Connect connection to on-premises 
	resources is limited to 5 Gbps.


Partition Placement Groups: 
Logical partition of instance groups such that no two partitions within a 
placement group share the same underlying hardware.

A partition placement group supports a maximum of seven partitions per Availability Zone. 
The number of instances that you can launch in a partition placement group is limited 
only by your account limits.
	Amazon EC2 doesn’t guarantee an even distribution of instances across all partitions.
	A partition placement group with Dedicated Instances can have a maximum of two partitions.
	You can't use Capacity Reservations to reserve capacity in a partition placement group.


Spread Placement Groups: 
each instance within a spread placement group will be placed in a different rack.
	A rack spread placement group supports a maximum of seven running instances per Availability Zone.
For example, in a Region with three Availability Zones, you can run a total of 21 instances in 
the group, with seven instances in each Availability Zone.
	Spread placement groups are not supported for Dedicated Instances.
	You can't use Capacity Reservations to reserve capacity in a spread placement group.

Host level spread placement groups are only supported for placement groups on AWS Outposts. 
There are no restrictions for the number of running instances with host level spread placement groups.


Cluster Placement Group:
	Recommended for low network latency, and/or high network throughput applications.
	Only specific to a single AZ
	Can span peered VPCs in the same Region
It usually supports up to 10Gbps network.
problem with cluster placement groups is if the rack fails, all the instances will fail at the same time 
compromising the high availability of the application.
  
  


Partition Placement Group:
 Each logical partition is composed of multiple instances. The partition placement group option allows 
 you to place those partitions within a single AZ or in a multi-AZ setup within the same region.

for large distributed and replicated workloads such as kafka, hadoop and cassandra etc.
	Reduces the impact of correlated hardware failures for your application
	Mainly used to deploy large distributed and replicated workloads, such as 
	HDFS, HBase, and Cassandra, across distinct racks.
	Can have partitions in multiple Availability Zones in the same Region.



Spread Placement Group:
If you deploy five instances and put them into this type of placement group, each one of 
those five instances will reside on a different rack with its own network access and power, 
either within a single AZ or in multi-AZ architecture.

You can create up to 7 EC2 instances per availability zone per spread placement group. 
Spread placement groups are designed for applications that require maximum high availability and where 
each instance must be isolated from failure from each other.

Recommended for applications that have a small number of critical instances that should be 
kept separate from each other.
Can span multiple Availability Zones in the same Region.




#ENI |  Elastic network interfaces             
-------------------------------------------------
An elastic network interface is a logical networking component in a VPC that represents a virtual network card. 
Elastic Network Interfaces (ENIs) are bounded to a specific AZ. You can not attach an ENI to an EC2 instance in a different AZ.
ENIs have security groups, just like EC2 instances, which act as a built in firewall.


It can include the following attributes:
A primary private IPv4 address from the IPv4 address range of your VPC
One or more secondary private IPv4 addresses from the IPv4 address range of your VPC
One Elastic IP address (IPv4) per private IPv4 address
One public IPv4 address
One or more IPv6 addresses
One or more security groups
A MAC address
A source/destination check flag
A description

ENIs are virtual network cards you can attach to your EC2 instances. They are used to enable network connectivity for your instances, 
and having more than one of them connected to your instance allows it to communicate on two different subnets.


A common use case for ENIs is the creation of management networks. 
This allows you to have public-facing applications like web servers in a public subnet but lock down SSH access 
down to a private subnet on a secondary network interface. In this scenario, you would connect using a 
VPN to the private management subnet, then administrate your servers as usual.



Each EC2 instance is connected to primary network interface (eth0). 
You can create and attach a secondary network interface - eth1.

This allows an instance to be dual homed - present in two subnets in a VPC. 
It can be used to create a management network or a low budget high availability solution.


Important terminology with ENI:
Hot attach: Attaching ENI when EC2 instance is running
Warm attach: Attaching ENI when EC2 instance is stopped
Cold attach: Attaching ENI at launch time of EC2 instance


ENI make you more control over the provate network, and IP.


EC2 Hibernate:
For enable EC2 Hibernation root ebs volume must be encrypted and must enought size to fit RAM in Root EBS.
To enable EC2 Hibernate, the EC2 Instance Root Volume type must be an EBS volume and must be encrypted to ensure the 
protection of sensitive content.


EC2 Nitro:
• Underlying Platform for the next generation of EC2 instances
• New virtualization technology
• Allows for better performance:
• Better networking options (enhanced networking, HPC, IPv6)
• Higher Speed EBS (Nitro is necessary for 64,000 EBS IOPS – max 32,000 on non-Nitro)
• Better underlying security
• Instance types example:
• Virtualized:A1, C5,C6gn, D3, G4, I3en, Inf1, M5,M5a,M5n.
• Bare metal: a1.metal, c5.metal, c5d.metal, c5n.metal, c6g.metal, c6gd.metal.



#EC2 capacity reservation:
-------------------------------------------------
Capacity Reservations enable you to reserve capacity for your Amazon EC2 instances in a specific Availability Zone for any duration. 
This gives you the flexibility to selectively add capacity reservations and still get the Regional RI discounts for that usage. 
By creating Capacity Reservations, you ensure that you always have access to Amazon EC2 capacity when you need it, 
for as long as you need it. 

You must reserve the instance only for 1 or 3 years, not any time in between.



=================================================
##Section 7: EC2 Instance Storage
=================================================

The OS provides access to block-level storage via open, write, and read system calls.

The simplified flow of a read request goes like this:
-An application wants to read the file /path/to/file.txt and makes a read system call.
-The OS forwards the read request to the file system.
-The file system translates /path/to/file.txt to the block on the disk where the data is stored.


#Elastic Block Storage (EBS)
-------------------------------------------------
AWS provides two kinds of block-level storage: network-attached storage (NAS) and instance storage. 
NAS is (like iSCSI) attached to your EC2 instance via a network connection, whereas instance storage is a normal hard 
disk that the host system provides to your EC2 instance. 

You can use block-level storage only in combination with an EC2 instance where the OS runs.
The OS provides access to block-level storage via open, write, and read system calls.

Aren’t part of your EC2 instances; they’re attached to your EC2 instance via a network connection. 
If you terminate your EC2 instance, the EBS volumes remain.

EBS Volumes are created for a specific AZ. It is possible to migrate them between different AZs using EBS Snapshots.

What’s an EBS Volume?
• An EBS (Elastic Block Store) Volume is a network drive you can attach
to your instances while they run
• It allows your instances to persist data, even after their termination
• They can only be mounted to one instance at a time (at the CCP level)
• They are bound to a specific availability zone
• Analogy: Think of them as a “network USB stick”
• Free tier: 30 GB of free EBS storage of type General Purpose (SSD) or
Magnetic per month


 - One EBS for one EC2
 - EBS Volume locked at the AZ level, EC2 and EBS have to be same zone to attach.
 - Make shapshots to sent data volum to AZ or region.
 - Using EBS Multi-Attach, you can attach the same EBS volume to multiple EC2 instances in the same AZ (with Full rw).
 - EBS Multi-Attach volume can be attached to multiple EC2 instances

When creating EC2 instances, you can only use the following EBS volume types as boot volumes: gp2, gp3, io1, io2, and Magnetic (Standard).
Using EBS Multi-Attach, you can attach the same EBS volume to multiple EC2 instances in the same AZ. 
Each EC2 instance has full read/write permissions.


#Instance stores:
-------------------------------------------------
An instance store provides block-level storage like a normal hard disk. Instance store is part of an EC2 instance and available only 
if your instance is running; it won’t persist your data if you stop or terminate the instance. Therefore you don’t pay 
separately for an instance store; instance store charges are included in the EC2 instance price.

64,000 is the maximum IOPS you can achieve when you're using EBS io1 or io2 volume types.


You are running a high-performance database that requires an IOPS of 310,000 for its underlying storage. What do you recommend?
You can run a database on an EC2 instance that uses an Instance Store, but you'll have a problem that the data will be lost if the 
EC2 instance is stopped (it can be restarted without problems). 
One solution is that you can set up a replication mechanism on another EC2 instance with an Instance Store to have a standby copy. 
Another solution is to set up backup mechanisms for your data. 
It's all up to you how you want to set up your architecture to validate your requirements. 
In this use case, it's around IOPS, so we have to choose an EC2 Instance Store.

 
Spot Instances:
===============
Spot instances are cost-effective when you can be flexible with your application’s availability and when your 
applications can be interrupted after a two-minute warning notification. 

Spot instances are ideal for stateless, error-tolerant, or flexible applications like data analysis, batch jobs, 
background processing, and optional tasks.

A Spot Price is the hourly rate for a Spot instance. 


There are two types of spot requests:
-A one-time spot request stays active until Amazon EC2 runs the spot instance, you cancel the request, or it expires. 
If the spot price goes above your bid or capacity is unavailable, Amazon terminates the spot instance and closes the spot request.

-A persistent spot instance request stays active until you cancel it or until it expires—even if Amazon carried out the request. 
If the spot price goes above your bid or capacity becomes unavailable, the spot instance is interrupted. 
Amazon keeps the request open, and if conditions change and a spot instance does become available, it starts or 
resumes the spot instance



#AMI  | Amazon Machine Image
-------------------------------------------------
You need to choose the Amazon Machine Image or AMI based on what operating system and what software do you want on the EC2 instance.
AMIs are stored in Amazon S3 (region specific).

AMIs are built for a specific AWS Region, they're unique for each AWS Region. You can't launch an EC2 instance using an 
AMI in another AWS Region, but you can copy the AMI to the target AWS Region and then use it to create your EC2 instances.


AMIs contain:
-Root volume block storage (OS and applications)
-Block device mappings for non-root volumes
-You can configure launch permissions on an AMI

Who can use the AMI?
You can share your AMIs with other AWS accounts

Best Practice: Backup upto date AMIs in multiple regions
Critical for Disaster Recovery


Three AMI sources:
-Provided by AWS
-AWS Market Place: Online store for customized AMIs. Per hour billing
-Customized AMIs: Created by you.


AMI Process (from an EC2 instance):
• Start an EC2 instance and customize it
• Stop the instance (for data integrity)
• Build an AMI – this will also create EBS snapshots
• Launch instances from other AMIs


EC2 Instance Store:
Instance store volumes, unlike EBS volumes, cannot be detached or attached to another instance


• EBS volumes are network drives with good but “limited” performance
• If you need a high-performance hardware disk, use EC2 Instance Store
• Better I/O performance
• EC2 Instance Store lose their storage if they’re stopped (ephemeral)
• Good for buffer / cache / scratch data / temporary content
• Risk of data loss if hardware fails
• Backups and Replication are your responsibility


EBS Multi-Attach – io1/io2 family:
Only io1/io2 family can have multi-attach.
Attach the same EBS volume to multiple EC2 instances in the same AZ.
Each instance has full read & write permissions to the volume.



#EFS – Elastic File System
-------------------------------------------------
AWS Elastic File System (EFS) is one of three main storage services offered by Amazon. 
It is a scalable, cloud-based file system for Linux-based applications and workloads that can be used in combination with 
AWS cloud services and on-premise resources. 
EFS offers a choice between two storage classes, Infrequent Access and Standard access.

EFS is a network file system (NFS) that allows you to mount the same file system on EC2 instances that are in different AZs.
• Managed NFS (network file system) that can be mounted on many EC2
• EFS works with EC2 instances in multi-AZ



EFS – Performance & Storage Classes:
EFS Scale
• 1000s of concurrent NFS clients, 10 GB+ /s throughput
• Grow to Petabyte-scale network file system, automatically

Performance mode (set at EFS creation time)
• General purpose (default): latency-sensitive use cases (web server, CMS, etc…)
• Max I/O – higher latency, throughput, highly parallel (big data, media processing)

Throughput mode
• Bursting (1 TB = 50MiB/s + burst of up to 100MiB/s)
• Provisioned: set your throughput regardless of storage size, ex: 1 GiB/s for 1 TB storage

Storage Tiers (lifecycle management feature – move file after N days) 
• Standard: for frequently accessed files 
• Infrequent access (EFS-IA): cost to retrieve files, lower price to store. 
Enable EFS -IA with a Lifecycle Policy.

Availability and durability 
• Standard: Multi-AZ, great for prod 
• One Zone: One AZ, great for dev, backup enabled by default, compatible with IA (EFS One Zone -IA)


EBS vs EFS:
-EBS have to pay for provisioning the Size and have to pay full capicity not the uses.
-EFS you do bill only what you used.


EFS multi AZ network file system for multiple EC2
EBS Single AZ single EC2
Instance Storage for single EC2 High permormance and Maximam I/O.


AWS S3:
Simple Storage Service (S3) provides object storage, without the use of EC2 instances, that is accessible directly through the internet.



=================================================
Section 8: High Availability and Scalability: ELB & ASG
=================================================
High Availability: 
Run instances for the same application across multi AZ
• Auto Scaling Group multi AZ
• Load Balancer multi AZ


The following cookie names are reserved by the ELB (AWSALB, AWSALBAPP, AWSALBTG).


Load Balances:
Load Balances are servers that forward traffic to multiple
servers (e.g., EC2 instances) downstream

It is integrated with many AWS offerings/services:
• EC2, EC2 Auto Scaling Groups, Amazon ECS
• AWS Certificate Manager (ACM), CloudWatch
• Route 53, AWS WAF, AWS Global Accelerator



AWS has 4 kinds of managed Load Balancers:

• Classic Load Balancer (v1 - old generation) – 2009 – CLB
HTTP, HTTPS, TCP, SSL (secure TCP)

• Application Load Balancer (v2 - new generation) – 2016 – ALB
HTTP, HTTPS, WebSocket

• Network Load Balancer (v2 - new generation) – 2017 – NLB
TCP, TLS (secure TCP), UDP

• Gateway Load Balancer – 2020 – GWLB
Operates at layer 3 (Network layer) – IP Protocol

Overall, it is recommended to use the newer generation load balancers as they
provide more features
Some load balancers can be setup as internal (private) or external (public) ELBs



Application Load Balancer (v2):
-------------------------------------------------

ALBs can route traffic to different Target Groups based on URL Path, Hostname, HTTP Headers, and Query Strings.
You can't attach an Elastic IP address to Application Load Balancers.

• Application load balancers is Layer 7 (HTTP)
• Load balancing to multiple HTTP applications across machines
(target groups)
• Load balancing to multiple applications on the same machine
(ex: containers)
• Support for HTTP/2 and WebSocket
• Support redirects (from HTTP to HTTPS for example)
 Routing tables to different target groups:
• Routing based on path in URL (example.com/users & example.com/posts)
• Routing based on hostname in URL (one.example.com & other.example.com)
• Routing based on Query String, Headers
(example.com/users?id=123&order=false)
• ALB are a great fit for micro services & container-based application
(example: Docker & Amazon ECS)
• Has a port mapping feature to redirect to a dynamic port in ECS
• In comparison, we’d need multiple Classic Load Balancer per application

• ALB can route to multiple target groups
• Health checks are at the target group level

Fixed hostname (XXX.region.elb.amazonaws.com)
The application servers don’t see the IP of the client directly
• The true IP of the client is inserted in the header X-Forwarded-For
• We can also get Port (X-Forwarded-Port) and proto (X-Forwarded-Proto)



#Network Load Balancer (v2):
-------------------------------------------------
Network Load Balancer has one static IP address per AZ and you can attach an Elastic IP address to it. 
Application Load Balancers and Classic Load Balancers have a static DNS name.
NLB has fixed IP, one fixep for per AZ.

• Network load balancers (Layer 4) allow to:
• Forward TCP & UDP traffic to your instances
• Handle millions of request per seconds
• Less latency ~100 ms (vs 400 ms for ALB)
• NLB has one static IP per AZ, and supports assigning Elastic IP
  (helpful for whitelisting specific IP)
• NLB are used for extreme performance, TCP or UDP traffic
• Not included in the AWS free tier



Network Load Balancer –Target Groups:
• EC2 instances
• IP Addresses – must be private IPs
• Application Load Balancer




#Gateway Load Balancer
-------------------------------------------------
• Deploy, scale, and manage a fleet of 3rd party network virtual appliances in AWS
• Example: Firewalls, Intrusion Detection and Prevention Systems, Deep Packet Inspection
Systems, payload manipulation


Operates at Layer 3 (Network Layer) – IPPackets, Combines the following functions:
• Transparent Network Gateway – single entry/exit for all traffic
• Load Balancer – distributes traffic to your virtual appliances
• Uses the GENEVE protocol on port 6081



Sticky Sessions (Session Affinity):
-------------------------------------------------
• It is possible to implement stickiness so that the same client is always redirected to the same instance behind a load balancer
• This works for Classic Load Balancers & Application Load Balancers
• The “cookie” used for stickiness has an expiration date you control
• Use case: make sure the user doesn’t lose his session data




Cross-Zone Load Balancing:
-------------------------------------------------
When Cross-Zone Load Balancing is enabled, ELB distributes traffic evenly across all registered EC2 instances in all AZs.

Application Load Balancer 
• Always on (can’t be disabled) 
• No charges for inter AZ data 

Network Load Balancer 
• Disabled by default 
• You pay charges ($) for inter AZ data if enabled 

Classic Load Balancer 
• Disabled by default 
• No charges for inter AZ data if enabled


Sticky Sessions – Cookie Names
• Application-based Cookies
• Custom cookie
• Generated by the target
• Can include any custom attributes required by the application
• Cookie name must be specified individually for each target group
• Don’t use AWSALB, AWSALBAPP, or AWSALBTG (reserved for use by the ELB)
• Application cookie
• Generated by the load balancer
• Cookie name is AWSALBAPP
• Duration-based Cookies
• Cookie generated by the load balancer
• Cookie name is AWSALB for ALB, AWSELB for CLB



Elastic Load Balancers – SSL Certificates:
-------------------------------------------------
Classic Load Balancer (v1)
• Support only one SSL certificate
• Must use multiple CLB for multiple hostname with multiple SSL certificates

Application Load Balancer (v2)
• Supports multiple listeners with multiple SSL certificates
• Uses Server Name Indication (SNI) to make it work

Network Load Balancer (v2)
• Supports multiple listeners with multiple SSL certificates
• Uses Server Name Indication (SNI) to make it work


Connection Draining
-------------------------------------------------
Feature naming
• Connection Draining – for CLB
• Deregistration Delay – for ALB & NLB

• Time to complete “in-flight requests” while the instance is de-registering or unhealthy
• Stops sending new requests to the EC2 instance which is de-registering
• Between 1 to 3600 seconds (default: 300 seconds)
• Can be disabled (set value to 0)
• Set to a low value if your requests are short


Auto Scaling Group Attributes
-------------------------------------------------

A Launch Template (older “Launch Configurations” are deprecated)
• AMI + Instance Type
• EC2 User Data
• EBS Volumes
• Security Groups
• SSH Key Pair
• IAM Roles for your EC2 Instances
• Network + Subnets Information
• Load Balancer Information
• Min Size / Max Size / Initial Capacity
• Scaling Policies

It is possible to scale an ASG based on CloudWatch alarms
An alarm monitors a metric (such as Average CPU, or a custom metric)



Auto Scaling Groups – Dynamic Scaling Policies
• Target Tracking Scaling
• Most simple and easy to set-up
• Example: I want the average ASG CPU to stay at around 40%
• Simple / Step Scaling
• When a CloudWatch alarm is triggered (example CPU > 70%), then add 2 units
• When a CloudWatch alarm is triggered (example CPU < 30%), then remove 1
• Scheduled Actions
• Anticipate a scaling based on known usage patterns
• Example: increase the min capacity to 10 at 5 pm on Fridays


VIP:
===
Only Network Load Balancer provides both static DNS name and static IP. 
While, Application Load Balancer provides a static DNS name but it does NOT provide a static IP. 
The reason being that AWS wants your Elastic Load Balancer to be accessible using a static endpoint, 
even if the underlying infrastructure that AWS manages changes.


Which feature in both Application Load Balancers and Network Load Balancers allows you to load multiple 
SSL certificates on one listener?
Server Name Indication (SNI)

Server Name Indication (SNI) allows you to expose multiple HTTPS applications each with its own 
SSL certificate on the same listener. 

Make sure you remember the Default Termination Policy for Auto Scaling Group. 
It tries to balance across AZs first, then terminates based on the age of the Launch Configuration.



=================================================
#Elastic Load Blancer |  | elb | ELB          
=================================================
Elastic Load Balancer are used to distribute traffic across EC2 instances in one or more AZs in a single region. 

The AWS ELB is an AWS service for automatic distribution of incoming application traffic across its components 
like Amazon EC2 instances, AWS Lambda, and containers..

A load balancer accepts incoming traffic from clients and routes requests to its registered targets (such as EC2) in one or more AZ. 
The load balancer also monitors the health of its registered targets and ensures that it routes traffic only to healthy targets.
When the load balancer detects an unhealthy target, it stops routing traffic to that target. 
It then resumes routing traffic to that target when it detects that the target is healthy again.

TypeOfLoadBalancer:
 - Application Load Balancers
 - Network Load Balancers
 - Gateway Load Balancers
 - Classic Load Balancers
 
Load balancers are a regional service. They do not balance load across different regions. 
You must provision a new ELB in each region that you operate out of.


With Application Load Balancers, Network Load Balancers, and Gateway Load Balancers, 
you register targets in target groups, and route traffic to the target groups. 
With Classic Load Balancers,you register instances with the load balancer.


Cross-zone load balancing is enable then every target equal load, if disable the  every AZ are equal. 
Application Load Balancers, cross-zone load balancing is always enabled.
With Network Load Balancers and Gateway Load Balancers, cross-zone load balancing is disabled by default. 


Elastic Load Balancing works with the following services:

 - Amazon EC2 — Virtual servers that run your applications in the cloud. 
 - Amazon EC2 Auto Scaling — Ensures that you are running your desired number of instances, even if an instance fails. 
 - AWS Certificate Manager — When you create an HTTPS listener, you can specify certificates provided by ACM. The load balancer uses certificates to terminate connections and decrypt requests from clients.
 - Amazon CloudWatch — Enables you to monitor your load balancer and to take action as needed. 
 - Amazon ECS — Enables you to run, stop, and manage Docker containers on a cluster of EC2 instances. 
 - AWS Global Accelerator — Improves the availability and performance of your application. Use an accelerator to distribute traffic across multiple load balancers in one or more AWS Regions.
 - Route 53 — Provides a reliable and cost-effective way to route visitors to websites by translating domain names into the numeric IP addresses that computers use to connect to each other.
 - AWS WAF — You can use AWS WAF (Web Application Firewall) with your Application Load Balancer to allow or block requests based on the rules in a web access control list (web ACL).

 
#AWS Classic Load Balancer:
------------------------------------------------- 
This simple load balancer operates both at the request level and the connection level and was originally used for classic EC2 instances. 
Classic Load balancer in AWS is used on EC2-classic instances. This is the previous generation’s load balancer and also it doesn’t allow host-based or path based routing.
Mostly it is used to route traffic to one single URL.


#NLB
------------------------------------------------- 
AWS recommends AWS Network Load Balancer (NLB) if the application needs to achieve static IP and extreme performance.
AWS network load balancers also avoid DNS caching problems and work with existing firewall security policies of users thanks to its static and resilient IP addresses. 
And AWS load balancer TLS termination is only possible with NLB.


#Create a ELB
------------------------------------------------- 
Select which type of AWS load balancer to use
Complete basic configuration
Configure a security group
Configure a target group
Register targets
Create a load balancer and test it
Get more details on how to configure AWS load balancers


#GateWay Load Balancer
-------------------------------------------------
Gateway Load Balancer helps you easily deploy, scale, and manage your third-party virtual appliances. 
It gives you one gateway for distributing traffic across multiple virtual appliances while scaling them up or down, 
based on demand. This decreases potential points of failure in your network and increases availability.

You can find, test, and buy virtual appliances from third-party vendors directly in AWS Marketplace.  

Use cases:
Centralize your third-party virtual appliances
Consolidating your third-party virtual appliances with Gateway Load Balancer can reduce operational overhead and costs.



=================================================
#ASG Auto Scaling | Auto Scaling groups        
================================================= 
An Auto Scaling group contains a collection of EC2 instances that are treated as a 
logical grouping for the purposes of automatic scaling and management. 
An Auto Scaling group also enables you to use Amazon EC2 Auto Scaling features such as health check replacements and scaling policies.

When you create an EC2 Auto Scaling group, you must specify a launch configuration. You can specify your launch configuration with multiple EC2 Auto Scaling groups. 

AutoScaling happend in a available zone.


Component of Authscaling
 - Launch configuation
 - AutoScaling Group
 - Scaling Policy
 
 
AutoScalingPolicies
 - Manual
 - Daynamic
 
 
You cannot modify a launch configuration after you've created it. If you want to change the launch 
configuration for an Auto Scaling group, you must create a new launch configuration and update your Auto Scaling group to 
inherit this new launch configuration.

The default termination policy for an Auto Scaling Group is to automatically terminate a stopped instance, 
so unless you've configured it to do otherwise, stopping an instance will result in termination regardless 
if you wanted that to happen or not. 
A new instance will be spun up in its place.
 
 

#Amazon EC2 Auto Scaling vs. AWS Auto Scaling           
--------------------------------------------------
AWS Auto Scaling:
You should use AWS Auto Scaling to manage scaling for multiple resources across multiple services. 
AWS Auto Scaling lets you define dynamic scaling policies for multiple EC2 Auto Scaling groups or other resources using predefined scaling strategies. Using AWS Auto Scaling to configure scaling policies for all of the scalable resources 
in your application is faster than managing scaling policies for each resource via its individual service console

Amazon EC2 Auto Scaling :
You should use EC2 Auto Scaling if you only need to scale Amazon EC2 Auto Scaling groups, or if you are only interested 
in maintaining the health of your EC2 fleet. You should also use EC2 Auto Scaling if you need 
to create or configure Amazon EC2 Auto Scaling groups, or if you need to set up scheduled or step 
scaling policies (as AWS Auto Scaling supports only target tracking scaling policies)


Predictive Scaling of AWS Auto Scaling pla:
Predictive Scaling Policy brings the similar prediction algorithm offered through AWS Auto Scaling 
plan as a native scaling policy in EC2 Auto Scaling. 
If you have predictable load changes, you can use Predictive Scaling policy to proactively increase capacity ahead of upcoming demand. 
Amazon EC2 Auto Scaling enables you to run your Amazon EC2 fleet at optimal utilization.


What is fleet management and how is it different from dynamic scaling:

Fleet management refers to the functionality that automatically replaces unhealthy instances and maintains your fleet at the 
desired capacity. Amazon EC2 Auto Scaling fleet management ensures that your application is able to receive traffic 
and that the instances themselves are working properly. 
When Auto Scaling detects a failed health check, it can replace the instance automatically.


The dynamic scaling capabilities of Amazon EC2 Auto Scaling refers to the functionality that automatically increases 
or decreases capacity based on load or other metrics. For example, if your CPU spikes above 80% (and you have an alarm setup)
 Amazon EC2 Auto Scaling can add a new instance dynamically.



================================================= 
##Section 9: AWS Fundamentals: RDS + Aurora + ElastiCache
================================================= 
RDS is a managed service that makes it easy to set up, operate, and scale a relational database in AWS. 
It provides cost-efficient and resizable capacity while automating or outsourcing time-consuming administration tasks such as 
hardware provisioning, database setup, patching and backups.

RDS comes in six different flavors:
SQL Server
Oracle
MySQL Server
PostgreSQL
MariaDB
Aurora


RDS has two key features when scaling out:
-Read replication for improved performance
-Multi-AZ for high availability


You cannot SSH into an RDS instance so therefore you cannot patch the OS. 
This means that AWS is responsible for the security and maintenance of RDS. 

You can provision an EC2 instance as a database if you need or want to manage the underlying server yourself, 
but not with an RDS engine.

Multi-AZ is supported for all DB flavors except aurora. This is because Aurora is completely fault-tolerant on its own.
Multi-AZ feature allows for high availability across availability zones and not regions.

During a failover, the recovered former primary becomes the new secondary and the promoted secondary becomes primary. 
Once the original DB is recovered, there will be a sync process kicked off where the two 
DBs mirror each other once to sync up on the new data that the failed former primary might have missed out on.



Read Replication is exclusively used for performance enhancement.

You can promote read replicas to be their very own production database if needed.
Read replicas are supported for all six flavors of DB on top of RDS.
Each Read Replica will have its own DNS endpoint.
Automated backups must be enabled in order to use read replicas.



RDS supports MySQL, PostgreSQL, MariaDB, Oracle, MS SQL Server, and Amazon Aurora.

IAM Auth is not supported for ElastiCache Redis. It works with both RDS MySQL and RDS PostgreSQL.
For your RDS database, you can have up to 5 Read Replicas.
You can not create encrypted Read Replicas from an unencrypted RDS DB instance.


RDS is a managed service:
• Automated provisioning, OS patching
• Continuous backups and restore to specific timestamp (Point in Time Restore)!
• Monitoring dashboards
• Read replicas for improved read performance
• Multi AZ setup for DR (Disaster Recovery)
• Maintenance windows for upgrades
• Scaling capability (vertical and horizontal)
• Storage backed by EBS (gp2 or io1)
• BUT you can’t SSH into your instances


Backups are automatically enabled in RDS,Automated backups:

• Daily full backup of the database (during the maintenance window)
• Transaction logs are backed-up by RDS every 5 minutes
• => ability to restore to any point in time (from oldest backup to 5 minutes ago)
• 7 days retention (can be increased to 35 days)



ReadReplica:
----------------------------------------------

Up to 5 Read Replicas
• Within AZ, Cross AZ or Cross Region
• Replication is ASYNC, so reads are eventually consistent (My Old data found some time)

Read replicas are used for SELECT (=read) only kind of statements(not INSERT, UPDATE, DELETE)

Multi-AZ keeps the same connection string regardless of which database is up.
Read Replicas add new endpoints with their own DNS name. We need to change our application to reference them individually 
to balance the read load.

Storing Session Data in ElastiCache is a common pattern to ensuring different EC2 instances can retrieve 
your user's state if needed.


Multi AZ:
SYNC replication and One DNS name – automatic app failover to standby
• Increase availability
• Failover in case of loss of AZ, loss of network, instance or storage failure
• No manual intervention in apps
• Not used for scaling
• Multi-AZ replication is free
• Note:The Read Replicas be setup as Multi AZ for Disaster Recovery (DR)
 

RDS – From Single-AZ to Multi-AZ:
• Zero downtime operation (no need to stop the DB)
• Just click on “modify” for the database
• The following happens internally:
• A snapshot is taken
• A new DB is restored from the snapshot in a new AZ
• Synchronization is established between the two databases



RDS Security – Network & IAM
--------------------------------------------
Network Security
• RDS databases are usually deployed within a private subnet, not in a public one
• RDS security works by leveraging security groups (the same concept as for EC2
  instances) – it controls which IP / security group can communicate with RDS
Access Management
• IAM policies help control who can manage AWS RDS (through the RDS API)
• Traditional Username and Password can be used to login into the database
• IAM-based authentication can be used to login into RDS MySQL & PostgreSQL-compatible


Aurora and RDS Security – Summary
--------------------------------------------
Encryption at rest:
• Is done only when you first create the DB instance
• or: unencrypted DB => snapshot => copy snapshot as encrypted => create DB from snapshot
• Your responsibility:
• Check the ports / IP / security group inbound rules in DB’s SG
• In-database user creation and permissions or manage through IAM
• Creating a database with or without public access
• Ensure parameter groups or DB is configured to only allow SSL connections

AWS responsibility:
• No SSH access
• No manual DB patching
• No manual OS patching
• No way to audit the underlying instance

ElastiCache – Redis vs Memcached
----------------------------------------
REDIS:
• Multi AZ with Auto-Failover
• Read Replicas to scale reads and have high availability
• Data Durability using AOF persistence
• Backup and restore features

MEMCACHED:
• Multi-node for partitioning of data (sharding)
• No high availability (replication)
• Non persistent
• No backup and restore
• Multi-threaded architecture


ElastiCache – Cache Security
----------------------------------------
All caches in ElastiCache:
• Do not support IAM authentication
• IAM policies on ElastiCache are only used for AWS API-level security

Redis AUTH
• You can set a “password/token” when you create a Redis cluster
• This is an extra level of security for your cach (on top of security groups)
• Support SSL in flight encryption

Memcached
• Supports SASL-based authentication (advanced)


RDS Backups:
When it comes to RDS, there are two kinds of backups:
-automated backups
-database snapshots

Automated backups allow you to recover your database to any point in time within a retention period (between one and 35 days). 
Automated backups will take a full daily snapshot and will also store transaction logs throughout the day. 
When you perform a DB recovery, RDS will first choose the most recent daily backup and apply the relevant transaction logs from that day. 


DB snapshots are done manually by the administrator. A key different from automated backups is that they are retained even 
after the original RDS instance is terminated. With automated backups, the backed up data in 
S3 is wiped clean along with the RDS engine. 


A relational database system does not scale well for the following reasons:
It normalizes data and stores it on multiple tables that require multiple queries to write to disk.
It generally incurs the performance costs of an ACID-compliant transaction system.
It uses expensive joins to reassemble required views of query results.


Scenario:
=========
-You want full control of OS or need elevated permissions	Consider going for a custom installation (EC2 + EBS)
-You want to migrate data from an on-premise database to cloud database of the same type	Consider using AWS Database Migration Service
-You want to migrate data from one database engine to another (Example : Microsoft SQL Server to Amazon Aurora)	Consider using AWS Schema Conversion Tool
-What are retained when you delete a RDS database instance?	All automatic backups are deleted
-All manual snapshots are retained (until explicit deletion)
(Optional) Take a final snapshot
-How do you reduce global latency and improve disaster recovery?	Use multi region read replicas
-How do you select the subnets a RDS instance is launched into?	Create DB Subnet groups
-How can you add encryption to an unencrypted database instance?	Create a DB snapshot
-Encrypt the database snapshot using keys from KMS
-Create a database from the encrypted snapshot
-Are you billed if you stop your DB instance?	You are billed for storage, IOPS, backups and snapshots. You are NOT billed for DB instance hours
-I will need RDS for at least one year. How can I reduce costs?	Use Amazon RDS reserved instances.
-Efficiently manage database connections	Use Amazon RDS Proxy
-Sits between client applications (including lambdas) and RDS



Let’s add a standby database in the second data center with replication.

Let’s consider some challenges:

Challenge 1 (SOLVED): Your database will go down if the data center crashes
You can switch to the standby database
Challenge 2 (SOLVED): You will lose data if the database crashes
Challenge 3 (SOLVED): Database will be slow when you take snapshots
Take snapshots from standby.
Applications connecting to master will get good performance always




=================================================
##Section 10: Route 53
=================================================
DNS Terminologies:
• Domain Registrar: Amazon Route 53, GoDaddy, …
• DNS Records: A, AAAA, CNAME, NS, …
• Zone File: contains DNS records
• Name Server: resolves DNS queries (Authoritative or Non-Authoritative)
• Top Level Domain (TLD): .com, .us, .in, .gov, .org, …
• Second Level Domain (SLD): amazon.com, google.com


Route 53 – Records:
• How you want to route traffic for a domain
Each record contains:
• Domain/subdomain Name – e.g., example.com
• Record Type – e.g., A or AAAA
• Value – e.g., 12.34.56.78
• Routing Policy – how Route 53 responds to queries
• TTL – amount of time the record cached at DNS Resolvers

Route 53 supports the following DNS record types:
• (must know) A / AAAA / CNAME / NS
• (advanced) CAA / DS / MX / NAPTR / PTR / SOA / TXT / SPF / SRV


Route 53 – Record Types:
• A – maps a hostname to IPv4
• AAAA – maps a hostname to IPv6

CNAME – maps a hostname to another hostname
• The target is a domain name which must have an A or AAAA record
• Can’t create a CNAME record for the top node of a DNS namespace (Zone Apex)
• Example: you can’t create for example.com, but you can create for www.example.com

NS – Name Servers for the Hosted Zone
• Control how traffic is routed for a domain



Route 53 – Alias Records Targets:
--------------------------------------------
• Elastic Load Balancers
• CloudFront Distributions
• API Gateway
• Elastic Beanstalk environments
• S3 Websites
• VPC Interface Endpoints
• Global Accelerator accelerator
• Route 53 record in the same hosted zone
• You cannot set an ALIAS record for an EC2 DNS name


CNAME vs Alias
--------------------------------------------
• AWS Resources (Load Balancer, CloudFront...) expose an AWS hostname:
• lb1-1234.us-east-2.elb.amazonaws.com and you want myapp.mydomain.com

CNAME:
• Points a hostname to any other hostname. (app.mydomain.com => blabla.anything.com)
• ONLY FOR NON ROOT DOMAIN (aka. something.mydomain.com)

Alias:
• Points a hostname to an AWS Resource (app.mydomain.com => blabla.amazonaws.com)
• Works for ROOT DOMAIN and NON ROOT DOMAIN (aka mydomain.com)
• Free of charge
• Native health check 



Route 53 Routing Policies:
--------------------------------------------
• Define how Route 53 responds to DNS queries 
• Don’t get confused by the word “Routing” 
• It’s not the same as Load balancer routing which routes the traffic 
• DNS does not route any traffic, it only responds to the DNS queries 

Route 53 Supports the following Routing Policies 
• Simple 
• Weighted 
• Failover 
• Latency based 
• Geolocation 
• Multi-Value Answer 
• Geoproximity (using Route 53 Traffic Flow feature)



VIP:
====
Weighted Routing Policy allows you to redirect part of the traffic based on weight (e.g., percentage). 
It's a common use case to send part of traffic to a new version of your application.

Each DNS record has a TTL (Time To Live) which orders clients for how long to cache these values and not overload the 
DNS Resolver with DNS requests. The TTL value should be set to strike a balance between how long the value should be cached vs. 
how many requests should go to the DNS Resolver.

Latency Routing Policy will evaluate the latency between your users and AWS Regions, and help them get a 
DNS response that will minimize their latency (e.g. response time)



You have a legal requirement that people in any country but France should NOT be able to access your website. 
Which Route 53 Routing Policy helps you in achieving this?
Geolocation.


You have purchased a domain on GoDaddy and would like to use Route 53 as the DNS Service Provider. 
What should you do to make this work?
Public Hosted Zones are meant to be used for people requesting your website through the Internet. Finally, NS records must be updated on the 3rd party Registrar.




=================================================
##Section 12: Amazon S3 Introduction
=================================================
S3 not a global service, need to region select for it.

Amazon S3 Overview – Objects (continued)
• Object values are the content of the body:
• Max Object Size is 5TB (5000GB)
• If uploading more than 5GB, must use “multi-part upload”
• Metadata (list of text key / value pairs – system or user metadata)
• Tags (Unicode key / value pair – up to 10) – useful for security / lifecycle
• Version ID (if versioning is enabled)

S3 Encryption for Objects, There are 4 methods of encrypting objects in S3:
• SSE-S3: encrypts S3 objects using keys handled & managed by AWS
• SSE-KMS: leverage AWS Key Management Service to manage encryption keys
• SSE-C: when you want to manage your own encryption keys
• Client Side Encryption 

Multi-Part Upload is recommended as soon as the file is over 100 MB.
5GB is Max file size to upload.

With SSE-C, the encryption happens in AWS and you have full control over the encryption keys.

With SSE-KMS, the encryption happens in AWS, and the encryption keys are managed by AWS but you 
have full control over the rotation policy of the encryption key. Encryption keys stored in AWS.

With Client-Side Encryption, you have to do the encryption yourself and you have full control over the encryption keys. 
You perform the encryption yourself and send the encrypted data to AWS. 
AWS does not know your encryption keys and cannot decrypt your data.

Explicit DENY in an IAM Policy will take precedence over an S3 bucket policy.


S3 Select & Glacier Select:
• Retrieve less data using SQL by performing server side filtering
• Can filter by rows & columns (simple SQL statements)
• Less network transfer, less CPU cost client-side


S3 Event Notifications with Amazon EventBridge:
• Advanced filtering options with JSON rules (metadata, object size, name...)
• Multiple Destinations – ex Step Functions, Kinesis Streams / Firehose…
• EventBridge Capabilities – Archive, Replay Events, Reliable delivery


S3 – Requester Pays
-------------------------------------------------
• In general, bucket owners pay for all
Amazon S3 storage and data transfer costs associated with their bucket

• With Requester Pays buckets, the
requester instead of the bucket owner pays the cost of the request and the data download from the bucket

• Helpful when you want to share large datasets with other accounts
• The requester must be authenticated in AWS (cannot be anonymous)



MFA Delete forces users to use MFA codes before deleting S3 objects. 
It's an extra level of security to prevent accidental deletions.

S3 Access Logs log all the requests made to S3 buckets and Amazon Athena can then be used to run 
cserverless analytics on top of the log files.

S3 Replication allows you to replicate data from an S3 bucket to another in the same/different AWS Region.

S3 Pre-Signed URLs are temporary URLs that you generate to grant time-limited access to some actions in your S3 bucket.


S3 MFA-Delete:
• MFA (multi factor authentication) forces user to generate a code on a device (usually a
mobile phone or hardware) before doing important operations on S3
• Only the bucket owner (root account) can enable/disable MFA-Delete
• MFA-Delete currently can only be enabled using the CLI


S3 Default Encryption vs Bucket Policies:
• One way to “force encryption” is to use a bucket policy and refuse any
API call to PUT an S3 object without encryption headers

Another way is to use the “default encryption” option in S3
• Note: Bucket Policies are evaluated before “default encryption”

S3 Access Logs: Warning
• Do not set your logging bucket to be the monitored bucket
• It will create a logging loop, and your bucket will grow in size exponentially


S3 Replication – Notes
• After activating, only new objects are replicated
• Optionally, you can replicate existing objects using S3 Batch Replication
• Replicates existing objects and objects that failed replication
• For DELETE operations:
• Can replicate delete markers from source to target (optional setting)
• Deletions with a version ID are not replicated (to avoid malicious deletes)
• There is no “chaining” of replication
• If bucket 1 has replication into bucket 2, which has replication into bucket 3
• Then objects created in bucket 1 are not replicated to bucket 3


S3 Pre-Signed URLs:
• Can generate pre-signed URLs using SDK or CLI
• For downloads (easy, can use the CLI)
• For uploads (harder, must use the SDK)
• Valid for a default of 3600 seconds, can change timeout with --expires-in [TIME_BY_SECONDS] argument
• Users given a pre-signed URL inherit the permissions of the person who generated the URL for GET / PUT
• Examples :
• Allow only logged-in users to download a premium video on your S3 bucket
• Allow an ever changing list of users to download files by generating URLs dynamically
• Allow temporarily a user to upload a file to a precise location in our bucket

Transition actions:
These actions define when objects transition to another storage class. 
For example, you might choose to transition objects to the S3 Standard-IA storage class 
30 days after creating them, or archive objects to the S3 Glacier Flexible Retrieval storage class one year after creating them.




=================================================
##Section 15: CloudFront & AWS Global Accelerator
=================================================
CloudFront is a global service.

CloudFront– Origins:
-------------------------------------------------
• S3 bucket 
• For distributing files and caching them at the edge 
• Enhanced security with CloudFront Origin Access Identity (OAI) 
• CloudFront can be used as an ingress (to upload files to S3) 
Custom Origin (HTTP) 
• Application Load Balancer 
• EC2 instance 
• S3 website (must first enable the bucket as a static S3 website) 
• Any HTTP backend you want


CloudFront Geo Restriction:
-------------------------------------------------
• You can restrict who can access your distribution
• Whitelist: Allow your users to access your content only if they're in one of the
countries on a list of approved countries.
• Blacklist: Prevent your users from accessing your content if they're in one of the

countries on a blacklist of banned countries.
• The “country” is determined using a 3rd party Geo-IP database
• Use case: Copyright Laws to control access to content


CloudFront vs S3 Cross Region Replication
-------------------------------------------------
CloudFront:
• Global Edge network
• Files are cached for a TTL (maybe a day)
• Great for static content that must be available everywhere

S3 Cross Region Replication:
• Must be setup for each region you want replication to happen
• Files are updated in near real-time
• Read only
• Great for dynamic content that needs to be available at low-latency in few regions


CloudFront Signed URL / Signed Cookies
-------------------------------------------------
• You want to distribute paid shared content to premium users over the world
• We can use CloudFront Signed URL / Cookie. We attach a policy with:
• Includes URL expiration
• Includes IP ranges to access the data from
• Trusted signers (which AWS accounts can create signed URLs)
• How long should the URL be valid for?
• Shared content (movie, music): make it short (a few minutes)
• Private content (private to the user): you can make it last for years
• Signed URL = access to individual files (one signed URL per file)
• Signed Cookies = access to multiple files (one signed cookie for many files)


CloudFront Signed URL vs S3 Pre-Signed URL
-------------------------------------------------
CloudFront Signed URL:
• Allow access to a path, no matter
the origin
• Account wide key-pair, only the root
can manage it
• Can filter by IP, path, date, expiration
• Can leverage caching features

S3 Pre-Signed URL:
• Issue a request as the person who
pre-signed the URL
• Uses the IAM key of the signing
IAM principal
• Limited lifetime

CloudFront – Origin Groups
• To increase high-availability and do failover
• Origin Group: one primary and one secondary origin
• If the primary origin fails, the second one is used


CloudFront Signed URLs are commonly used to distribute paid content through dynamically generated signed URLs.

Signed Cookies are useful when you want to access multiple files.

AWS Global Accelerator will provide us with the two static IP addresses and the ALB will provide us with the HTTP routing rules.



=================================================
##Section 16: AWS Storage Extras
=================================================
Highly-secure, portable devices to collect and process data at the edge,
and migrate data into and out of AWS

What is Edge Computing?
-------------------------------------------------
• Process data while it’s being created on an edge location
• A truck on the road, a ship on the sea, a mining station underground

These locations may have
• Limited / no internet access
• Limited / no easy access to computing power

We setup a Snowball Edge / Snowcone device to do edge computing
Use cases of Edge Computing:
• Preprocess data
• Machine learning at the edge
• Transcoding media streams
Eventually (if need be) we can ship back the device to AWS (for transferring data for example)


Amazon FSx for Windows (File Server) 
-------------------------------------------------
Amazon FSx For Windows File Server does not have a parallel file system, unlike Lustre.

• EFS is a shared POSIX system for Linux systems. 
• FSx for Windows is a fully managed Windows file system share drive 
• Supports SMB protocol & Windows NTFS 
• Microsoft Active Directory integration, ACLs, user quotas 
• Can be mounted on Linux EC2 instances 
• Scale up to 10s of GB/s, millions of IOPS, 100s PB of data 

Storage Options: 
• SSD – latency sensitive workloads (databases, media processing, data analytics, …) 
• HDD – broad spectrum of workloads (home directory, CMS, …) 

• Can be accessed from your on-premises infrastructure (VPN or Direct Connect) 
• Can be configured to be Multi-AZ (high availability) •
• Data is backed-up daily to S3



Amazon FSx for Lustre
-------------------------------------------------
FSx for Lustre is compatible with the most popular Linux-based AMIs, including Amazon Linux, 
Amazon Linux 2, Red Hat Enterprise Linux (RHEL), CentOS, SUSE Linux and Ubuntu.

Since the Lustre file system is designed for high-performance computing workloads that typically run on compute clusters, 
choose EFS for normal Linux file system if your requirements don't match this use case.

FSx Lustre has the ability to store and retrieve data directly on S3 on its own.

• Lustre is a type of parallel distributed file system, for large-scale computing
• The name Lustre is derived from “Linux” and “cluster
• Machine Learning, High Performance Computing (HPC)
• Video Processing, Financial Modeling, Electronic Design Automation
• Scales up to 100s GB/s, millions of IOPS, sub-ms latencies

Storage Options:
• SSD – low-latency, IOPS intensive workloads, small & random file operations
• HDD – throughput-intensive workloads, large & sequential file operations

• Seamless integration with S3
• Can “read S3” as a file system (through FSx)
• Can write the output of the computations back to S3 (through FSx)
• Can be used from on-premises servers (VPN or Direct Connect)


FSx File System Deployment Options
-------------------------------------------------
Scratch File System
• Temporary storage
• Data is not replicated (doesn’t persist if file server fails)
• High burst (6x faster, 200MBps per TiB)
• Usage: short-term processing, optimize costs

Persistent File System
• Long-term storage
• Data is replicated within same AZ
• Replace failed files within minutes
• Usage: long-term processing, sensitive data



AWS Storage Gateway
-------------------------------------------------
Storage Gateway is a way of Hybrid cloud make possible
Gateway mean Interface, FileGateway->FileInterface.(AWS Storage Gateway's file interface, or file gateway)


3 types of Storage Gateway:
• File Gateway
• Volume Gateway
• Tape Gateway


File Gateway:
• Configured S3 buckets are accessible using the NFS and SMB protocol
• Supports S3 standard, S3 IA, S3 One Zone IA
• Bucket access using IAM roles for each File Gateway
• Most recently used data is cached in the file gateway
• Can be mounted on many servers
• Integrated with Active Directory (AD) for user authentication

Volume Gateway:

• Block storage using iSCSI protocol backed by S3
• Backed by EBS snapshots which can help restore on-premises volumes!
• Cached volumes: low latency access to most recent data
• Stored volumes: entire dataset is on premise, scheduled backups to S3

Tape Gateway:
.
• Some companies have backup processes using physical tapes (!)
• With Tape Gateway, companies use the same processes but, in the cloud
• Virtual Tape Library (VTL) backed by Amazon S3 and Glacier
• Back up data using existing tape-based processes (and iSCSI interface)
• Works with leading backup software vendors 


Amazon FSx File Gateway:
• Native access to Amazon FSx for Windows File Server
• Local cache for frequently accessed data
• Windows native compatibility (SMB, NTFS, Active Directory...)
• Useful for group file shares and home directories


AWS Storage Gateway Summary:
• Exam tip: Read the question well, it will hint at which gateway to use
• On-premises data to the cloud => Storage Gateway
• File access / NFS – user auth with Active Directory => File Gateway(backed by S3)
• Volumes / Block Storage / iSCSI => Volume gateway (backed by S3 with EBS snapshots)
• VTL Tape solution / Backup with iSCSI = > Tape Gateway (backed by S3 and Glacier)
• No on-premises virtualization => Hardware Appliance


AWS Transfer Family:
-------------------------------------------------
• A fully-managed service for file transfers into and out of Amazon S3 or Amazon EFS using the FTP protocol

Supported Protocols
• AWS Transfer for FTP (File Transfer Protocol (FTP))
• AWS Transfer for FTPS (File Transfer Protocol over SSL (FTPS))
• AWS Transfer for SFTP (Secure File Transfer Protocol (SFTP))

• Managed infrastructure, Scalable, Reliable, Highly Available (multi-AZ)
• Pay per provisioned endpoint per hour + data transfers in GB
• Store and manage users’ credentials within the service
• Integrate with existing authentication systems (Microsoft Active Directory,LDAP, Okta, Amazon Cognito, custom)
• Usage: sharing files, public datasets, CRM, ERP.



Storage Comparison:
• S3: Object Storage
• Glacier: Object Archival
• EFS: Network File System for Linux instances, POSIX filesystem
• FSx for Windows: Network File System for Windows servers
• FSx for Lustre: High Performance Computing Linux file system
• EBS volumes: Network storage for one EC2 instance at a time
• Instance Storage: Physical storage for your EC2 instance (high IOPS)
• Storage Gateway: File Gateway, Volume Gateway (cache & stored), Tape Gateway
• Snowball / Snowmobile: to move large amount of data to the cloud, physically
• Database: for specific workloads, usually with indexing and querying

Snowball Edge is the right answer as it comes with computing capabilities and allows you to pre-process the data while it's being moved into Snowball.




=================================================
##Section 17: Decoupling applications: SQS, SNS, Kinesis, Active MQ
=================================================
Decouple your applications using:
• using SQS: queue model
• using SNS: pub/sub model
• using Kinesis: real-time streaming model
• These services can scale independently from our application!



Amazon SQS:
-------------------------------------------------

Standard Queue:
	• Oldest offering (over 10 years old)
	• Fully managed service, used to decouple applications
	• Attributes:
	• Unlimited throughput, unlimited number of messages in queue
	• Default retention of messages: 4 days, maximum of 14 days
	• Low latency (<10 ms on publish and receive)
	• Limitation of 256KB per message sent
	• Can have duplicate messages (at least once delivery, occasionally)
	• Can have out of order messages (best effort ordering)

SQS FIFO:
(First-In-First-Out) Queues have all the capabilities of the SQS Standard Queue, plus the following two features. 
First, The order in which messages are sent and received are strictly preserved and a message is 
delivered once and remains available until a consumer process and deletes it. 
Second, duplicated messages are not introduced into the queue.


SQS Delay Queues is a period of time during which Amazon SQS keeps new SQS messages invisible to consumers. 
In SQS Delay Queues, a message is hidden when it is first added to the queue. (default: 0 mins, max.: 15 mins)


When SQS Long Polling is enabled, Amazon SQS reducing the number of empty responses when there are no messages 
available to return and eliminating false empty responses (when SQS messages are available but aren't included in a response).

SQS Visibility Timeout is a period of time during which Amazon SQS prevents other consumers from receiving and processing 
the message again. In Visibility Timeout, a message is hidden only after it is consumed from the queue. 
Increasing the Visibility Timeout gives more time to the consumer to process the message and prevent 
duplicate reading of the message. (default: 30 sec., min.: 0 sec., max.: 12 hours)

SQS Dead Letter Queue is where other SQS queues (source queues) can send messages that can't be processed (consumed) 
successfully. It's useful for debugging as it allows you to isolate problematic messages so 
you can debug why their processing doesn't succeed.


This is a common pattern where only one message is sent to the SNS topic and then "fan-out" to multiple SQS queues. 
This approach has the following features: it's fully decoupled, no data loss, and you have the ability to add more
SQS queues (more applications) over time.


The capacity limits of a Kinesis data stream are defined by the number of shards within the data stream. 
The limits can be exceeded by either data throughput or the number of reading data calls. Each shard allows for 
1 MB/s incoming data and 2 MB/s outgoing data. You should increase the number of shards within 
your data stream to provide enough capacity.


Kinesis Data Stream uses the partition key associated with each data record to determine which shard a 
given data record belongs to. When you use the identity of each user as the partition key, this ensures the 
data for each user is ordered hence 
sent to the same shard.


Which AWS service is most appropriate when you want to perform real-time analytics on streams of data?
Use Kinesis Data Analytics with Kinesis Data Streams as the underlying source of data.


This is a perfect combo of technology for loading data near real-time data into S3 and Redshift. 
Kinesis Data Firehose supports custom data transformations using AWS Lambda.


Which of the following is NOT a supported subscriber for AWS SNS?
Kinesis Data Firehose is now supported, but not Kinesis Data Streams.

Amazon MQ supports industry-standard APIs such as JMS and NMS, and protocols for messaging, 
including AMQP, STOMP, MQTT, and WebSocket.




Kinesis:
-------------------------------------------------
What is a source in Kinesis Data Firehose?
A source is where your streaming data is continuously generated and captured. For example, a source can be a logging server 
running on Amazon EC2 instances, an application running on mobile devices, or a sensor on an IoT device. 
You can connect your sources to Kinesis Data Firehose using 
1) Amazon Kinesis Data Firehose API, which uses the AWS SDK for Java, .NET, Node.js, Python, or Ruby. 
2) Kinesis Data Stream, where Kinesis Data Firehose reads data easily from an existing Kinesis data 
stream and load it into Kinesis Data Firehose destinations. 
3) AWS natively supported Service like AWS Cloudwatch, AWS EventBridge, AWS IOT, or AWS Pinpoint. 
For complete list, see the Amazon Kinesis Data Firehose developer guide. 
4) Kinesis Agents, which is a stand-alone Java software application that continuously monitors a set of files and 
sends new data to your stream. 
5) Fluentbit, which an open source Log Processor and Forwarder. 
6) AWS Lambda, which is a serverless compute service that lets you run code without provisioning or managing servers. 
You can use write your Lambda function to send traffic from S3 or DynamoDB to 
Kinesis Data Firehose based on a triggered event.

What is a destination in Kinesis Data Firehose?
A destination is the data store where your data will be delivered. 
Kinesis Data Firehose currently supports Amazon S3, Amazon Redshift, Amazon OpenSearch Service, Splunk, Datadog, 
NewRelic, Dynatrace, Sumologic, LogicMonitor, MongoDB, and HTTP End Point as destinations.


• Makes it easy to collect, process, and analyze streaming data in real-time
• Ingest real-time data such as: Application logs, Metrics, Website clickstreams, IoT telemetry data
• Kinesis Data Streams: capture, process, and store data streams
• Kinesis Data Firehose: load data streams into AWS data stores
• Kinesis Data Analytics: analyze data streams with SQL or Apache Flink
• Kinesis Video Streams: capture, process, and store video streams


Kinesis Data Streams vs Firehose:

Kinesis Data Streams:
• Streaming service for ingest at scale
• Write custom code (producer / consumer)
• Real-time (~200 ms)
• Manage scaling (shard splitting / merging)
• Data storage for 1 to 365 days
• Supports replay capability

Kinesis Data Firehose:
• Load streaming data into S3 / Redshift / ES / 3rd party / custom HTTP
• Fully managed
• Near real-time (buffer time min. 60 sec)
• Automatic scaling
• No data storage
• Doesn’t support replay capability

Kinesis Data Analytics (SQL application):
• Perform real-time analytics on Kinesis Streams using SQL
• Fully managed, no servers to provision
• Automatic scaling
• Real-time analytics
• Pay for actual consumption rate
• Can create streams out of the real-time queries
• Use cases:
• Time-series analytics
• Real-time dashboards
• Real-time metrics


Kinesis vs SQS ordering:
• Let’s assume 100 trucks, 5 kinesis shards, 1 SQS FIFO

Kinesis Data Streams:
• On average you’ll have 20 trucks per shard
• Trucks will have their data ordered within each shard
• The maximum amount of consumers in parallel we can have is 5
• Can receive up to 5 MB/s of data

SQS FIFO
• You only have one SQS FIFO queue
• You will have 100 Group ID
• You can have up to 100 Consumers (due to the 100 Group ID)
• You have up to 300 messages per second (or 3000 if using batching)



Amazon MQ:
-------------------------------------------------
• SQS, SNS are “cloud-native” services, and they’re using proprietary protocols from AWS.
• Traditional applications running from on-premises may use open protocols such as: MQTT, AMQP, STOMP, Openwire, WSS
• When migrating to the cloud, instead of re-engineering the application to use SQS and SNS, we can use Amazon MQ
• Amazon MQ = managed Apache ActiveMQ
• Amazon MQ doesn’t “scale” as much as SQS / SNS
• Amazon MQ runs on a dedicated machine, can run in HA with failover
• Amazon MQ has both queue feature (~SQS) and topic features (~SNS)




=================================================
##Section 18: Containers on AWS: ECS, Fargate, ECR & EKS
=================================================
Docker Containers Management on AWS:

Amazon Elastic Container Service (Amazon ECS)
• Amazon’s own container platform

Amazon Elastic Kubernetes Service (Amazon EKS)
• Amazon’s managed Kubernetes (open source)

AWS Fargate
• Amazon’s own Serverless container platform
• Works with ECS and with EKS

Amazon ECR:
• Store container images



Note:
• Amazon S3 cannot be mounted as a file system



Amazon ECS - EC2 Launch Type
• ECS = Elastic Container Service
• Launch Docker containers on AWS =
Launch ECS Tasks on ECS Clusters
• EC2 Launch Type: you must provision
& maintain the infrastructure (the EC2
instances)
• Each EC2 Instance must run the ECS
Agent to register in the ECS Cluster
• AWS takes care of starting / stopping
containers



Amazon ECS – Fargate Launch Type
• Launch Docker containers on AWS
• You do not provision the infrastructure
(no EC2 instances to manage)
• It’s all Serverless!
• You just create task definitions
• AWS just runs ECS Tasks for you based
on the CPU / RAM you need
• To scale, just increase the number of
tasks. Simple - no more EC2 instances


ECS Task Role:
• Allows each task to have a specific role
• Use different roles for the different ECS Services you run
• Task Role is defined in the task definition



Amazon EKS Overview:
---------------------------------------------------
• Amazon EKS = Amazon Elastic Kubernetes Service
• It is a way to launch managed Kubernetes clusters on AWS
• Kubernetes is an open-source system for automatic deployment, scaling and
management of containerized (usually Docker) application
• It’s an alternative to ECS, similar goal but different API
• EKS supports EC2 if you want to deploy worker nodes or Fargate to deploy
serverless containers
• Use case: if your company is already using Kubernetes on-premises or in
another cloud, and wants to migrate to AWS using Kubernetes
• Kubernetes is cloud-agnostic (can be used in any cloud – Azure, GCP…)
• For multiple regions, deploy one EKS cluster per region



Amazon ECS – Load Balancer Integrations:

• Application Load Balancer supported
and works for most use cases

• Network Load Balancer recommended
only for high throughput / high
performance use cases, or to pair it with
AWS Private Link

• Elastic Load Balancer supported but not
recommended (no advanced features –
no Fargate)



EC2 Launch Type – Auto Scaling EC2 Instances:
• Accommodate ECS Service Scaling by adding underlying EC2 Instances

Auto Scaling Group Scaling
• Scale your ASG based on CPU Utilization
• Add EC2 instances over time

ECS Cluster Capacity Provider
• Used to automatically provision and scale the infrastructure for your ECS Tasks
• Capacity Provider paired with an Auto Scaling Group
• Add EC2 Instances when you’re missing capacity (CPU, RAM…)



EC2 Instance Profile is the IAM Role used by the ECS Agent on the EC2 instance to execute ECS-specific 
actions such as pulling Docker images from ECR and storing the container logs into CloudWatch Logs.

ECS Task Role is the IAM Role used by the ECS task itself. Use when your container wants to call 
other AWS services like S3, SQS, etc.

EFS volume can be shared between different EC2 instances and different ECS Tasks. 
It can be used as a persistent multi-AZ shared storage for your containers.

Amazon ECR is a fully managed container registry that makes it easy to store, manage, share, and deploy your container images. 
It won't help in running your Docker-based applications.




=================================================
##Section 19: Serverless Overviews from a Solution Architect Perspective
=================================================

 Serverless does not mean there are no servers…
it means you just don’t manage / provision / see them

Serverless in AWS:

• AWS Lambda 
• DynamoDB 
• AWS Cognito 
• AWS API Gateway 
• Amazon S3 
• AWS SNS & SQS 
• AWS Kinesis Data Firehose 
• Aurora Serverless 
• Step Functions 
• Fargate




AWS Lambda Limits to Know - per region
-------------------------------------------------
Execution:
• Memory allocation: 128 MB – 10GB (1 MB increments)
• Maximum execution time: 900 seconds (15 minutes)
• Environment variables (4 KB)
• Disk capacity in the “function container” (in /tmp): 512 MB
• Concurrency executions: 1000 (can be increased)

Deployment:
• Lambda function deployment size (compressed .zip): 50 MB
• Size of uncompressed deployment (code + dependencies): 250 MB
• Can use the /tmp directory to load other files at startup
• Size of environment variables: 4 KB



Lambda@Edge Use Cases:
• Website Security and Privacy 
• Dynamic Web Application at the Edge 
• Search Engine Optimization (SEO) 
• Intelligently Route Across Origins and Data Centers 
• Bot Mitigation at the Edge 
• Real-time Image Transformation 
• A/B Testing 
• User Authentication and Authorization 
• User Prioritization 
• User Tracking and Analytics


DynamoDB - Basics:
• DynamoDB is made of Tables
• Each table has a Primary Key (must be decided at creation time)
• Each table can have an infinite number of items (= rows)
• Each item has attributes (can be added over time – can be null)
• Maximum size of an item is 400KB
Data types supported are:
• Scalar Types – String, Number, Binary, Boolean, Null
• Document Types – List, Map
• Set Types – String Set, Number Set, Binary Set


DynamoDB – Read/Write Capacity Modes:
Control how you manage your table’s capacity (read/write throughput)

Provisioned Mode (default)
• You specify the number of reads/writes per second
• You need to plan capacity beforehand
• Pay for provisioned Read Capacity Units (RCU) & Write Capacity Units (WCU)
• Possibility to add auto-scaling mode for RCU & WCU

On-Demand Mode
• Read/writes automatically scale up/down with your workloads
• No capacity planning needed
• Pay for what you use, more expensive ($$$)
• Great for unpredictable workloads


DynamoDB Streams:
Ordered stream of item-level modifications (create/update/delete) in a table

Stream records can be:
• Sent to Kinesis Data Streams
• Read by AWS Lambda
• Read by Kinesis Client Library applications
• Data Retention for up to 24 hours

Use cases:
• react to changes in real-time (welcome email to users)
• Analytics
• Insert into derivative tables
• Insert into ElasticSearch
• Implement cross-region replication


AWS API Gateway :
• AWS Lambda + API Gateway: No infrastructure to manage 
• Support for the WebSocket Protocol 
• Handle API versioning (v1, v2…) 
• Handle different environments (dev, test, prod…) 
• Handle security (Authentication and Authorization) 
• Create API keys, handle request throttling 
• Swagger / Open API import to quickly define APIs 
• Transform and validate requests and responses 
• Generate SDK and API specifications 
• Cache API responses


API Gateway – Integrations High Level:
Lambda Function
• Invoke Lambda function
• Easy way to expose REST API backed by AWS Lambda

HTTP
• Expose HTTP endpoints in the backend
• Example: internal HTTP API on premise, Application Load Balancer…
• Why? Add rate limiting, caching, user authentications, API keys, etc…

AWS Service
• Expose any AWS API through the API Gateway?
• Example: start an AWS Step Function workflow, post a message to SQS
• Why? Add authentication, deploy publicly, rate control…


API Gateway – Security – Summary
IAM:
• Great for users / roles already within your AWS account
• Handle authentication + authorization
• Leverages Sig v4

Custom Authorizer:
• Great for 3rd party tokens
• Very flexible in terms of what IAM policy is returned
• Handle Authentication + Authorization
• Pay per Lambda invocation

Cognito User Pool:
• You manage your own user pool (can be backed by Facebook, Google login etc…)
• No need to write any custom code
• Must implement authorization in the backend 



AWS Cognito:
We want to give our users an identity so that they can interact with our
application.

Cognito User Pools:
• Sign in functionality for app users
• Integrate with API Gateway

Cognito Identity Pools (Federated Identity):
• Provide AWS credentials to users so they can access AWS resources directly
• Integrate with Cognito User Pools as an identity provider

Cognito Sync:
• Synchronize data from device to Cognito.
• May be deprecated and replaced by AppSync 


SAM = Serverless Application Model
• Framework for developing and deploying serverless applications

All the configuration is YAML code
• Lambda Functions
• DynamoDB tables
• API Gateway
• Cognito User Pools
SAM can help you to run Lambda, API Gateway, DynamoDB locally
SAM can use CodeDeploy to deploy Lambda functions


AWS Cognito:
• We want to give our users an identity so that they can interact with our application.
• Cognito User Pools:
• Sign in functionality for app users
• Integrate with API Gateway
• Cognito Identity Pools (Federated Identity):
• Provide AWS credentials to users so they can access AWS resources directly
• Integrate with Cognito User Pools as an identity provider
• Cognito Sync:
• Synchronize data from device to Cognito.
• May be deprecated and replaced by AppSync 

Premium User Video service:
• We have implemented a fully serverless solution:
• Cognito for authentication
• DynamoDB for storing users that are premium
• 2 serverless applications
	• Premium User registration
	• CloudFront Signed URL generator
• Content is stored in S3 (serverless and scalable)
• Integrated with CloudFront with OAI for security (users can’t bypass)
• CloudFront can only be used using Signed URLs to prevent unauthorized users
• What about S3 Signed URL? They’re not efficient for global access


Why CloudFront?
• No changes to architecture
• Will cache software update files at the edge
• Software update files are not dynamic, they’re static (never changing)
• Our EC2 instances aren’t serverless
• But CloudFront is, and will scale for us
• Our ASG will not scale as much, and we’ll save tremendously in EC2
• We’ll also save in availability, network bandwidth cost, etc
• Easy way to make an existing application more scalable and cheaper!


Big Data Ingestion Pipeline discussion:
• IoT Core allows you to harvest data from IoT devices
• Kinesis is great for real-time data collection
• Firehose helps with data delivery to S3 in near real-time (1 minute)
• Lambda can help Firehose with data transformations
• Amazon S3 can trigger notifications to SQS
• Lambda can subscribe to SQS (we could have connecter S3 to Lambda)
• Athena is a serverless SQL service and results are stored in S3
• The reporting bucket contains analyzed data and can be used by
reporting tool such as AWS QuickSight, Redshift, etc



DynamoDB is serverless with no servers to provision, patch, or manage and no software to install, maintain or operate. 
It automatically scales tables up and down to adjust for capacity and maintain performance. 
It provides both provisioned (specify RCU & WCU) and on-demand (pay for what you use) capacity modes.

RCU and WCU are decoupled, so you can increase/decrease each value separately.

DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB that delivers up to 
10x performance improvement. It caches the most frequently used data, thus offloading the heavy reads on 
hot keys off your DynamoDB table,


DynamoDB Streams allows you to capture a time-ordered sequence of item-level modifications in a DynamoDB table. 
It's integrated with AWS Lambda so that you create triggers that automatically respond to events in real-time.


An Edge-Optimized API Gateway is best for geographically distributed clients. 
API requests are routed to the nearest CloudFront Edge Location which improves latency. 
The API Gateway still lives in one AWS Region.

Lambda@Edge is a feature of CloudFront that lets you run code closer to your users, which 
improves performance and reduces latency.


Mibile client to direct S3, for Authentication use AWSCognito and STS
Mobile client to CloudFront to S3 then use OAI to access S3 from of CloudFront


Amazon Cognito lets you add user sign-up, sign-in, and access control to your web and mobile apps quickly and easily. 
Amazon Cognito scales to millions of users and supports sign-in with social identity providers, 
such as Apple, Facebook, Google, and Amazon, and enterprise identity providers via SAML 2.0 and OpenID Connect

DynamoDB Streams enable DynamoDB to get a changelog and use that changelog to replicate data across replica 
tables in other AWS Regions.

Amazon CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, 
and APIs to customers globally with low latency, high transfer speeds. Amazon CloudFront can be used in front of 
an Application Load Balancer.


Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. 
It can continuously capture gigabytes of data per second from hundreds of sources such as website clickstreams, 
database event streams, financial transactions, social media feeds, IT logs, and location-tracking events.




=================================================
##Section 21: Databases in AWS
=================================================
Database Types:
• RDBMS (= SQL / OLTP): RDS, Aurora – great for joins
• NoSQL database: DynamoDB (~JSON), ElastiCache (key / value pairs),
Neptune (graphs) – no joins, no SQL
• Object Store: S3 (for big objects) / Glacier (for backups / archives)
• Data Warehouse (= SQL Analytics / BI): Redshift (OLAP), Athena
• Search: ElasticSearch (JSON) – free text, unstructured searches
• Graphs: Neptune – displays relationships between data


RDS Overview:
• Managed PostgreSQL / MySQL / Oracle / SQL Server
• Must provision an EC2 instance & EBS Volume type and size
• Support for Read Replicas and Multi AZ
• Security through IAM, Security Groups, KMS , SSL in transit
• Backup / Snapshot / Point in time restore feature
• Managed and Scheduled maintenance
• Monitoring through CloudWatch
• Use case: Store relational datasets (RDBMS / OLTP), perform SQL queries,
transactional inserts / update / delete is available 

RDS for Solutions Architect:
• Operations: small downtime when failover happens, when maintenance
happens, scaling in read replicas / ec2 instance / restore EBS implies
manual intervention, application changes
• Security: AWS responsible for OS security, we are responsible for setting
up KMS, security groups, IAM policies, authorizing users in DB, using SSL
• Reliability: Multi AZ feature, failover in case of failures
• Performance: depends on EC2 instance type, EBS volume type, ability to
add Read Replicas. Storage auto-scaling & manual scaling of instances
• Cost: Pay per hour based on provisioned EC2 and EBS


Aurora for Solutions Architect:
• Operations: less operations, auto scaling storage
• Security: AWS responsible for OS security, we are responsible for setting
up KMS, security groups, IAM policies, authorizing users in DB, using SSL
• Reliability: Multi AZ, highly available, possibly more than RDS, Aurora
Serverless option, Aurora Multi-Master option
• Performance: 5x performance (according to AWS) due to architectural
optimizations. Up to 15 Read Replicas (only 5 for RDS)
• Cost: Pay per hour based on EC2 and storage usage. Possibly lower
costs compared to Enterprise grade databases such as Oracle

ElastiCache Overview:
• Managed Redis / Memcached (similar offering as RDS, but for caches)
• In-memory data store, sub-millisecond latency
• Must provision an EC2 instance type
• Support for Clustering (Redis) and Multi AZ, Read Replicas (sharding)
• Security through IAM, Security Groups, KMS, Redis Auth
• Backup / Snapshot / Point in time restore feature
• Managed and Scheduled maintenance
• Monitoring through CloudWatch
• Use Case: Key/Value store, Frequent reads, less writes, cache results for DB queries, store session data for websites, cannot use SQL. 


ElastiCache for Solutions Architect
• Operations: same as RDS
• Security: AWS responsible for OS security, we are responsible for setting
up KMS, security groups, IAM policies, users (Redis Auth), using SSL
• Reliability: Clustering, Multi AZ
• Performance: Sub-millisecond performance, in memory, read replicas for
sharding, very popular cache option
• Cost: Pay per hour based on EC2 and storage usage

DynamoDB for Solutions Architect:
• Operations: no operations needed, auto scaling capability, serverless
• Security: full security through IAM policies, KMS encryption, SSL in flight
• Reliability: Multi AZ, Backups
• Performance: single digit millisecond performance, DAX for caching
reads, performance doesn’t degrade if your application scales
• Cost: Pay per provisioned capacity and storage usage (no need to guess
in advance any capacity – can use auto scaling)


S3 for Solutions Architect:
• Operations: no operations needed
• Security: IAM, Bucket Policies, ACL, Encryption (Server/Client), SSL
• Reliability: 99.999999999% durability / 99.99% availability, Multi AZ, CRR
• Performance: scales to thousands of read / writes per second, transfer
acceleration / multi-part for big files
• Cost: pay per storage usage, network cost, requests number

Athena for Solutions Architect:
• Operations: no operations needed, serverless
• Security: IAM + S3 security
• Reliability: managed service, uses Presto engine, highly available
• Performance: queries scale based on data size
• Cost: pay per query / per TB of data scanned, serverless



Redshift for Solutions Architecto:
• Operations: like RDS
• Security: IAM, VPC, KMS, SSL (like RDS)
• Reliability: auto healing features, cross-region snapshot copy
• Performance: 10x performance vs other data warehousing, compression
• Cost: pay per node provisioned, 1/10th of the cost vs other warehouses
• vs Athena: faster queries / joins / aggregations thanks to indexes
• Remember: Redshift = Analytics / BI / Data Warehouse

AWS Glue:
• Managed extract, transform, and load (ETL) service
• Useful to prepare and transform data for analytics
• Fully serverless service


Neptune for Solutions Architect:
• Operations: similar to RDS
• Security: IAM, VPC, KMS, SSL (similar to RDS) + IAM Authentication
• Reliability: Multi-AZ, clustering
• Performance: best suited for graphs, clustering to improve performance
• Cost: pay per node provisioned (similar to RDS)
• Remember: Neptune = Graphs


OpenSearch for Solutions Architect:
• Operations: similar to RDS
• Security: Cognito, IAM, VPC, KMS, SSL
• Reliability: Multi-AZ, clustering
• Performance: based on ElasticSearch project (open source), petabyte scale
• Cost: pay per node provisioned (similar to RDS)
• Remember: OpenSearch = Search / Indexing


Amazon ElastiCache is a fully managed in-memory data store, compatible with Redis or Memcached.

Amazon DynamoDB is a key-value, document, NoSQL database.

Amazon Aurora is a MySQL and PostgreSQL-compatible relational database. It features a distributed, fault-tolerant, 
self-healing storage system that auto-scales up to 128TB per database instance. It delivers high performance and a
vailability with up to 15 low-latency read replicas, point-in-time recovery, continuous backup to Amazon S3, and r
eplication across 3 AZs.


Amazon Neptune is a fast, reliable, fully-managed graph database service that makes it easy to build and run 
applications that work with highly connected datasets.

Amazon Athena is an interactive serverless query service that makes it easy to analyze data in S3 buckets using Standard SQL.



================================================
##Section 22: AWS Monitoring & Audit: CloudWatch, CloudTrail & Config
=================================================

Amazon CloudWatch is a monitoring service that allows you to monitor your applications, respond to system-wide 
performance changes, optimize resource utilization, and get a unified view of operational health. 
It is used to monitor your applications' performance and metrics.


AWS CloudTrail allows you to log, continuously monitor, and retain account activity related to actions across your 
AWS infrastructure. It provides the event history of your AWS account activity, audit API calls made through the 
AWS Management Console, AWS SDKs, AWS CLI. So, the EC2 instance termination API call will appear here. 
You can use CloudTrail to detect unusual activity in your AWS accounts.


You can use the CloudTrail Console to view the last 90 days of recorded API activity. 
For events older than 90 days, use Athena to analyze CloudTrail logs stored in S3.



AWS CloudWatch Metrics
• CloudWatch provides metrics for every services in AWS
• Metric is a variable to monitor (CPUUtilization, NetworkIn…)
• Metrics belong to namespaces
• Dimension is an attribute of a metric (instance id, environment, etc…).
• Up to 10 dimensions per metric
• Metrics have timestamps
• Can create CloudWatch dashboards of metrics

Dimensions are name/value pairs that categorize metric characteristics. 
Each metric you create can have up to 10 dimensions defined. 
You can use these dimensions to distinguish between multiple instances of the same service and to 
filter metrics by service use. For example, you can assign InstanceId dimensions to your 
EC2 instances to distinguish between them for monitoring.


EC2 Detailed monitoring
• EC2 instance metrics have metrics “every 5 minutes”
• With detailed monitoring (for a cost), you get data “every 1 minute”
• Use detailed monitoring if you want to scale faster for your ASG!
• The AWS Free Tier allows us to have 10 detailed monitoring metrics
• Note: EC2 Memory usage is by default not pushed (must be pushed
from inside the instance as a custom metric)


CloudWatch Custom Metrics
• Possibility to define and send your own custom metrics to CloudWatch
• Example: memory (RAM) usage, disk space, number of logged in users …
• Use API call PutMetricData
• Ability to use dimensions (attributes) to segment metrics
• Instance.id
• Environment.name
• Metric resolution (StorageResolution API parameter – two possible value):
• Standard: 1 minute (60 seconds)
• High Resolution: 1/5/10/30 second(s) – Higher cost
• Important: Accepts metric data points two weeks in the past and two hours in the
future (make sure to configure your EC2 instance time correctly)

CloudWatch Logs
• Log groups: arbitrary name, usually representing an application
• Log stream: instances within application / log files / containers
• Can define log expiration policies (never expire, 30 days, etc..)
• CloudWatch Logs can send logs to:
• Amazon S3 (exports)
• Kinesis Data Streams
• Kinesis Data Firehose
• AWS Lambda
• ElasticSearch

CloudWatch Logs- Sources
• SDK, CloudWatch Logs Agent, CloudWatch Unified Agent 
• Elastic Beanstalk: collection of logs from application 
• ECS: collection from containers 
• AWS Lambda: collection from function logs 
• VPC Flow Logs: VPC specific logs 
• API Gateway 
• CloudTrail based on filter 
• Route53: Log DNS queries


CloudWatch Logs Agent & Unified Agent
• For virtual servers (EC2 instances, on-premises servers…)
• CloudWatch Logs Agent
• Old version of the agent
• Can only send to CloudWatch Logs

CloudWatch Unified Agent
• Collect additional system-level metrics such as RAM, processes, etc…
• Collect logs to send to CloudWatch Logs
• Centralized configuration using SSM Parameter Store


CloudWatch Unified Agent – Metrics
• Collected directly on your Linux server / EC2 instance
• CPU (active, guest, idle, system, user, steal)
• Disk metrics (free, used, total), Disk IO (writes, reads, bytes, iops)
• RAM (free, inactive, used, total, cached)
• Netstat (number of TCP and UDP connections, net packets, bytes)
• Processes (total, dead, bloqued, idle, running, sleep)
• Swap Space (free, used, used %)
• Reminder: out-of-the box metrics for EC2 – disk, CPU, network (high level)


Amazon EventBridge
• EventBridge is the next evolution of CloudWatch Events
• Default Event Bus – generated by AWS services (CloudWatch Events)
• Partner Event Bus – receive events from SaaS service or applications (Zendesk,
DataDog, Segment, Auth0…)
• Custom Event Buses – for your own applications
• Event buses can be accessed by other AWS accounts
• You can archive events (all/filter) sent to an event bus (indefinitely or set period)
• Ability to replay archived events
• Rules: how to process the events (like CloudWatch Events)


Amazon EventBridge vs CloudWatch Events:
• Amazon EventBridge builds upon and extends CloudWatch Events.
• It uses the same service API and endpoint, and the same underlying service
infrastructure.
• EventBridge allows extension to add event buses for your custom
applications and your third-party SaaS apps.
• Event Bridge has the Schema Registry capability
• EventBridge has a different name to mark the new capabilities
• Over time, the CloudWatch Events name will be replaced with EventBridge.


AWS CloudTrail:
• Provides governance, compliance and audit for your AWS Account
• CloudTrail is enabled by default!
• Get an history of events / API calls made within your AWS Account by:
	• Console
	• SDK
	• CLI
	• AWS Services
• Can put logs from CloudTrail into CloudWatch Logs or S3
• A trail can be applied to All Regions (default) or a single Region.
• If a resource is deleted in AWS, investigate CloudTrail first!



CloudTrail Events:
• Management Events:
	• Operations that are performed on resources in your AWS account
	• Examples:
		• Configuring security (IAM AttachRolePolicy)
		• Configuring rules for routing data (Amazon EC2 CreateSubnet)
		• Setting up logging (AWS CloudTrail CreateTrail)
	• By default, trails are configured to log management events.
	• Can separate Read Events (that don’t modify resources) from Write Events (that may modify resources)
• Data Events:
	• By default, data events are not logged (because high volume operations)
	• Amazon S3 object-level activity (ex: GetObject, DeleteObject, PutObject): can separate Read and Write Events
	• AWS Lambda function execution activity (the Invoke API)
• CloudTrail Insights Events:
 • See next slide J


AWS Config
• Helps with auditing and recording compliance of your AWS resources
• Helps record configurations and changes over time
• Questions that can be solved by AWS Config:
	• Is there unrestricted SSH access to my security groups?
	• Do my buckets have any public access?
	• How has my ALB configuration changed over time?
• You can receive alerts (SNS notifications) for any changes
• AWS Config is a per-region service
• Can be aggregated across regions and accounts
• Possibility of storing the configuration data into S3 (analyzed by Athena)


Config Rules:
• Can use AWS managed config rules (over 75)
• Can make custom config rules (must be defined in AWS Lambda)
	• Ex: evaluate if each EBS disk is of type gp2
	• Ex: evaluate if each EC2 instance is t2.micro
• Rules can be evaluated / triggered:
	• For each config change
	• And / or: at regular time intervals
• AWS Config Rules does not prevent actions from happening (no deny)

• Pricing: no free tier, $0.003 per configuration item recorded per region,
$0.001 per config rule evaluation per region



CloudWatch vs CloudTrail vs Config:
• CloudWatch 
	• Performance monitoring (metrics, CPU, network, etc…) & dashboards 
	• Events & Alerting 
	• Log Aggregation & Analysis 
• CloudTrail 
	• Record API calls made within your Account by everyone 
	• Can define trails for specific resources 
	• Global Service 
• Config 
	• Record configuration changes 
	• Evaluate resources against compliance rules 
	• Get timeline of changes and compliance



For an Elastic Load Balancer
• CloudWatch:
	• Monitoring Incoming connections metric
	• Visualize error codes as % over time
	• Make a dashboard to get an idea of your load balancer performance
• Config:
	• Track security group rules for the Load Balancer
	• Track configuration changes for the Load Balancer
	• Ensure an SSL certificate is always assigned to the Load Balancer (compliance)
• CloudTrail:
	• Track who made any changes to the Load Balancer with API calls





=================================================
##Section 23: Identity and Access Management (IAM) - Advanced
=================================================
AWS STS – Security Token Service
• Allows to grant limited and temporary access to AWS resources.
• Token is valid for up to one hour (must be refreshed)
• AssumeRole
	• Within your own account: for enhanced security
	• Cross Account Access: assume role in target account to perform actions there
• AssumeRoleWithSAML
	• return credentials for users logged with SAML
	• AssumeRoleWithWebIdentity
	• return creds for users logged with an IdP (Facebook Login, Google Login, OIDC compatible…)
• AWS recommends against using this, and using Cognito instead
	• GetSessionToken
	• for MFA, from a user or AWS account root user


Using STS to Assume a Role:
• Define an IAM Role within your account or cross-account
• Define which principals can access this IAM Role
• Use AWS STS (Security Token Service) to retrieve credentials and impersonate the IAM Role you
have access to (AssumeRole API)
• Temporary credentials can be valid between 15 minutes to 1 hour



Identity Federation in AWS:
• Federation lets users outside of AWS to assume temporary role for accessing AWS resources.
• These users assume identity provided access role.
• Federations can have many flavors:
	• SAML 2.0
	• Custom Identity Broker
	• Web Identity Federation with Amazon Cognito
	• Web Identity Federation without Amazon Cognito
	• Single Sign On
	• Non-SAML with AWS Microsoft AD
• Using federation, you don’t need to create IAM users
(user management is outside of AWS)



SAML 2.0 Federation:
• To integrate Active Directory / ADFS with AWS (or any SAML 2.0)
• Provides access to AWS Console or CLI (through temporary creds)
• No need to create an IAM user for each of your employees


Custom Identity Broker Application:
• Use only if identity provider is not compatible with SAML 2.0
• The identity broker must determine the appropriate IAM policy
• Uses the STS API: AssumeRole or GetFederationToken


AWS Cognito:
• Goal:
	• Provide direct access to AWS Resources from
	the Client Side (mobile, web app)
• Example:
	• provide (temporary) access to write to S3
	bucket using Facebook Login
• Problem:
	• We don’t want to create IAM users for our app
	users
• How:
	• Log in to federated identity provider – or
	remain anonymous
	• Get temporary AWS credentials back from the
	Federated Identity Pool
	• These credentials come with a pre-defined IAM
	policy stating their permissions




AWS Directory Services:
AWS Managed Microsoft AD
	• Create your own AD in AWS, manage users
	locally, supports MFA
	• Establish “trust” connections with your on- premises AD
AD Connector
	• Directory Gateway (proxy) to redirect to on- premises AD, supports MFA
	• Users are managed on the on-premises AD
Simple AD
	• AD-compatible managed directory on AWS
	• Cannot be joined with on-premises AD


AWS Organizations:
• Global service
• Allows to manage multiple AWS accounts
• The main account is the master account – you can’t change it
• Other accounts are member accounts
• Member accounts can only be part of one organization
• Consolidated Billing across all accounts - single payment method
• Pricing benefits from aggregated usage (volume discount for EC2, S3…)
• API is available to automate AWS account creation


Service Control Policies (SCP):
• Whitelist or blacklist IAM actions
• Applied at the OU or Account level
• Does not apply to the Master Account
• SCP is applied to all the Users and Roles of the Account, including Root user
• The SCP does not affect service-linked roles
• Service-linked roles enable other AWS services to integrate with AWS Organizations
and can't be restricted by SCPs.
• SCP must have an explicit Allow (does not allow anything by default)
• Use cases:
• Restrict access to certain services (for example: can’t use EMR)
• Enforce PCI compliance by explicitly disabling services



AWS Resource Access Manager (AWS RAM) helps you securely share your AWS resources within your organization or 
organizational units (OUs) in AWS Organizations and with AWS Accounts. You can also share resources with IAM Roles and IAM Users.



Amazon Cognito can be used to federate mobile user accounts and provide them with their own IAM permissions, 
so they can be able to acces`s their own personal 
space in the S3 bucket.





=================================================
##Section 24: AWS Security & Encryption: KMS, SSM Parameter Store, CloudHSM, Shield, WAF
=================================================
KMS Key bound in a specificed region.


Encryption in flight (SSL):
• Data is encrypted before sending and decrypted after receiving
• SSL certificates help with encryption (HTTPS)
• Encryption in flight ensures no MITM (man in the middle attack) can happen


KMS – Customer Master Key (CMK) Types:

Symmetric (AES-256 keys)
• First offering of KMS, single encryption key that is used to Encrypt and Decrypt
• AWS services that are integrated with KMS use Symmetric CMKs
• Necessary for envelope encryption
• You never get access to the Key unencrypted (must call KMS API to use)

Asymmetric (RSA & ECC key pairs)
• Public (Encrypt) and Private Key (Decrypt) pair
• Used for Encrypt/Decrypt, or Sign/Verify operations
• The public key is downloadable, but you can’t access the Private Key unencrypted
• Use case: encryption outside of AWS by users who can’t call the KMS API



AWS KMS (Key Management Service) :
• Able to fully manage the keys & policies: 
	• Create 
	• Rotation policies 
	• Disable 
	• Enable 
• Able to audit key usage (using CloudTrail) 
• Three types of Customer Master Keys (CMK): 
	• AWS Managed Service Default CMK: free 
	• User Keys created in KMS: $1 / month 
	• User Keys imported (must be 256-bit symmetric key): $1 / month 
+ pay for API call to KMS ($0.03 / 10000 calls)



KMS Key Policies:
• Control access to KMS keys, “similar” to S3 bucket policies
• Difference: you cannot control access without them
Default KMS Key Policy:
	• Created if you don’t provide a specific KMS Key Policy
	• Complete access to the key to the root user = entire AWS account
	• Gives access to the IAM policies to the KMS key
Custom KMS Key Policy:
	• Define users, roles that can access the KMS key
	• Define who can administer the key
	• Useful for cross-account access of your KMS key



KMS Automatic Key Rotation
• For Customer-managed CMK (not AWS managed CMK)
• If enabled: automatic key rotation happens every 1 year
• Previous key is kept active so you can decrypt old data
• New Key has the same CMK ID (only the backing key is changed)


KMS Alias Updating
• Better to use aliases in this case (to hide the change of key for the
application)



Parameters Policies (for advanced parameters)
• Allow to assign a TTL to a parameter (expiration date) to force
updating or deleting sensitive data such as passwords
• Can assign multiple policies at a time



AWS Secrets Manager:L
• Newer service, meant for storing secrets
• Capability to force rotation of secrets every X days
• Automate generation of secrets on rotation (uses Lambda)
• Integration with Amazon RDS (MySQL, PostgreSQL, Aurora)
• Secrets are encrypted using KMS
• Mostly meant for RDS integration



AWS Shield:
• AWS Shield Standard:
• Free service that is activated for every AWS customer
• Provides protection from attacks such as SYN/UDP Floods, Reflection attacks
and other layer 3/layer 4 attacks

AWS Shield Advanced:
• Optional DDoS mitigation service ($3,000 per month per organization)
• Protect against more sophisticated attack on Amazon EC2, Elastic Load
Balancing (ELB), Amazon CloudFront, AWS Global Accelerator, and Route 53
• 24/7 access to AWS DDoS response team (DRP)
• Protect against higher fees during usage spikes due to DDoS


AWS WAF – Web Application Firewall:
• Protects your web applications from common web exploits (Layer 7)
• Layer 7 is HTTP (vs Layer 4 is TCP)
• Deploy on Application Load Balancer, API Gateway, CloudFront
• Define Web ACL (Web Access Control List):
	• Rules can include: IP addresses, HTTP headers, HTTP body, or URI strings
	• Protects from common attack - SQL injection and Cross-Site Scripting (XSS)
	• Size constraints, geo-match (block countries)
	• Rate-based rules (to count occurrences of events) – for DDoS protection



Amazon GuardDuty
• Intelligent Threat discovery to Protect AWS Account
• Uses Machine Learning algorithms, anomaly detection, 3rd party data
• One click to enable (30 days trial), no need to install software
• Input data includes:
	• CloudTrail Events Logs – unusual API calls, unauthorized deployments
	• CloudTrail Management Events – create VPC subnet, create trail, …
	• CloudTrail S3 Data Events – get object, list objects, delete object, …
	• VPC Flow Logs – unusual internal traffic, unusual IP address
	• DNS Logs – compromised EC2 instances sending encoded data within DNS queries
	• Kubernetes Audit Logs – suspicious activities and potential EKS cluster compromises
• Can setup CloudWatch Event rules to be notified in case of findings
• CloudWatch Events rules can target AWS Lambda or SNS
• Can protect against CryptoCurrency attacks (has a dedicated “finding” for it)

What does AWS Inspector evaluate?
• Remember: only for EC2 instances and container infrastructure
• Continuous scanning of the infrastructure, only when needed
• Package vulnerabilities (EC2 & ECR) – database of CVE
• Network reachability (EC2)
• A risk score is associated with all vulnerabilities for prioritization


Amazon Macie:
• Amazon Macie is a fully managed data security and data privacy service
that uses machine learning and pattern matching to discover and protect your sensitive data in AWS.
• Macie helps identify and alert you to sensitive data, such as personally identifiable information (PII)


AWS Shared Responsibility Model
-------------------------------------------
• AWS responsibility - Security of the Cloud
	• Protecting infrastructure (hardware, software, facilities, and networking) that runs
	all the AWS services
	• Managed services like S3, DynamoDB, RDS, etc.
• Customer responsibility - Security in the Cloud
	• For EC2 instance, customer is responsible for management of the guest OS
	(including security patches and updates), firewall & network configuration, IAM
	• Encrypting application data
• Shared controls:
• Patch Management, Configuration Management, Awareness & Training




Example, for RDS:
AWS responsibility:
	• Manage the underlying EC2 instance, disable SSH access
	• Automated DB patching
	• Automated OS patching
	• Audit the underlying instance and disks & guarantee it functions
Your responsibility:
	• Check the ports / IP / security group inbound rules in DB’s SG
	• In-database user creation and permissions
	• Creating a database with or without public access
	• Ensure parameter groups or DB is configured to only allow SSL connections
	• Database encryption setting

Example, for S3 • 

AWS responsibility: 
• Guarantee you get unlimited storage 
• Guarantee you get encryption 
• Ensure separation of the data between different customers 
• Ensure AWS employees can’t access your data 

Your responsibility: 
• Bucket configuration 
• Bucket policy / public setting 
• IAM user and roles 
• Enabling encryption


Amazon Macie is a fully managed data security service that uses Machine Learning to discover and protect your sensitive data 
stored in S3 buckets. It automatically provides an inventory of S3 buckets including a list of unencrypted buckets, 
publicly accessible buckets, and buckets shared with other AWS accounts. It allows you to identify and 
alert you to sensitive data, such as Personally Identifiable Information (PII).



AWS Firewall Manager is a security management service that allows you to centrally configure and manage firewall 
rules across your accounts and applications in AWS Organizations. It is integrated with AWS Organizations so you can enable 
AWS WAF rules, AWS Shield Advanced protection, security groups, AWS Network Firewall rules, and Amazon Route 53 Resolver 
DNS Firewall rules.




SSM Parameters Store can be used to store secrets and has built-in version tracking capability. 
Each time you edit the value of a parameter, SSM Parameter Store creates a new version of the parameter and retains the previous 
versions. You can view the details, including the values, of all versions in a parameter's history.



KMS keys can be symmetric or asymmetric. A symmetric KMS key represents a 256-bit key used for encryption and decryption. 
An asymmetric KMS key represents an RSA key pair used for encryption and decryption or signing and verification, but not both. 
Or it represents an elliptic curve (ECC) key pair used for signing and verification.




In-flight Encryption = HTTPS, and HTTPS can not be enabled without an SSL certificate.


=================================================
##Section 25: Networking - VPC
=================================================
VPC in AWS – IPv4:
VPC = Virtual Private Cloud
You can have multiple VPCs in an AWS region (max. 5 per region – soft limit)
	• Max. CIDR per VPC is 5, for each CIDR:
	• Min. size is /28 (16 IP addresses)
	• Max. size is /16 (65536 IP addresses)
Because VPC is private, only the Private IPv4 ranges are allowed:
	• 10.0.0.0 – 10.255.255.255 (10.0.0.0/8)
	• 172.16.0.0 – 172.31.255.255 (172.16.0.0/12)
	• 192.168.0.0 – 192.168.255.255 (192.168.0.0/16)
Your VPC CIDR should NOT overlap with your other networks (e.g., corporate)



VPC – Subnet (IPv4):
• AWS reserves 5 IP addresses (first 4 & last 1) in each subnet
• These 5 IP addresses are not available for use and can’t be assigned to an
EC2 instance
Example: if CIDR block 10.0.0.0/24, then reserved IP addresses are:
	• 10.0.0.0 – Network Address
	• 10.0.0.1 – reserved by AWS for the VPC router
	• 10.0.0.2 – reserved by AWS for mapping to Amazon-provided DNS
	• 10.0.0.3 – reserved by AWS for future use
	• 10.0.0.255 – Network Broadcast Address. AWS does not support broadcast in a VPC, 
	  therefore the address is reserved
Exam Tip, if you need 29 IP addresses for EC2 instances:
• You can’t choose a subnet of size /27 (32 IP addresses, 32 – 5 = 27 < 29)
• You need to choose a subnet of size /26 (64 IP addresses, 64 – 5 = 59 > 29)




Internet Gateway (IGW)
• Allows resources (e.g., EC2 instances) in a VPC connect to the Internet
• It scales horizontally and is highly available and redundant
• Must be created separately from a VPC
• One VPC can only be attached to one IGW and vice versa
• Internet Gateways on their own do not allow Internet access…
• Route tables must also be edited!



Bastion Hosts
• We can use a Bastion Host to SSH into our
private EC2 instances
• The bastion is in the public subnet which is
then connected to all other private subnets
• Bastion Host security group must be tightened
• Exam Tip: Make sure the bastion host only has
port 22 traffic from the IP address you need,
not from the security groups of your other
EC2 instances



NAT Instance – Comments:
Pre-configured Amazon Linux AMI is available
• Reached the end of standard support on December 31, 2020
Not highly available / resilient setup out of the box
• You need to create an ASG in multi-AZ + resilient user-data script
Internet traffic bandwidth depends on EC2 instance type
You must manage Security Groups & rules:
• Inbound:
	• Allow HTTP / HTTPS traffic coming from Private Subnets
	• Allow SSH from your home network (access is provided through Internet Gateway)
• Outbound:
	• Allow HTTP / HTTPS traffic to the Internet



NAT Gateway:
• AWS-managed NAT, higher bandwidth, high availability, no administration
• Pay per hour for usage and bandwidth
• NATGW is created in a specific Availability Zone, uses an Elastic IP
• Can’t be used by EC2 instance in the same subnet (only from other
subnets)
• Requires an IGW (Private Subnet => NATGW => IGW)
• 5 Gbps of bandwidth with automatic scaling up to 45 Gbps
• No Security Groups to manage / required


• NAT Gateway is resilient within a single Availability Zone
• Must create multiple NAT Gateways in multiple AZs for fault-tolerance
• There is no cross-AZ failover needed because if an AZ goes down it doesn't need NAT


DNS Resolution in VPC
• DNS Resolution (enableDnsSupport)
• Decides if DNS resolution from Route 53 Resolver server is supported for the VPC
• True (default): it queries the Amazon Provider DNS Server at 169.254.169.253 or the
reserved IP address at the base of the VPC IPv4 network range plus two (.2)

DNS Resolution in VPC
• DNS Hostnames (enableDnsHostnames)
• By default,
• True => default VPC
• False => newly created VPCs
• Won’t do anything unless enableDnsSupport=true
• If True, assigns public hostname to EC2 instance if it has a public IPv4

DNS Resolution in VPC
• If you use custom DNS domain names in a Private Hosted Zone in
Route 53, you must set both these attributes (enableDnsSupport &
enableDnsHostname) to true



Network Access Control List (NACL):
• NACL are like a firewall which control traffic from and to subnets
• One NACL per subnet, new subnets are assigned the Default NACL
• You define NACL Rules:
	• Rules have a number (1-32766), higher precedence with a lower number
	• First rule match will drive the decision
	• Example: if you define #100 ALLOW 10.0.0.10/32 and #200 DENY 10.0.0.10/32, the IP
	address will be allowed because 100 has a higher precedence over 200
	• The last rule is an asterisk (*) and denies a request in case of no rule match
• AWS recommends adding rules by increment of 100
• Newly created NACLs will deny everything
• NACL are a great way of blocking a specific IP address at the subnet level


Accepts everything inbound/outbound with the subnets it’s associated with
• Do NOT modify the Default NACL, instead create custom NACLs

Ephemeral Ports
• For any two endpoints to establish a connection, they must use ports
• Clients connect to a defined port, and expect a response on an ephemeral port
• Different Operating Systems use different port ranges, examples:
	• IANA & MS Windows 10 è 49152 – 65535
	• Many Linux Kernels è 32768 – 60999


VPC – Reachability Analyzer
• A network diagnostics tool that troubleshoots
network connectivity between two endpoints
in your VPC(s)
• It builds a model of the network configuration,
then checks the reachability based on these
configurations (it doesn’t send packets)
• When the destination is
	• Reachable – it produces hop-by-hop details of the virtual network path
	• Not reachable – it identifies the blocking component(s) (e.g., configuration issues in SGs,
NACLs, Route Tables, …)
• Use cases: troubleshoot connectivity issues,
ensure network configuration is as intended,



VPC Peering:
• Privately connect two VPCs using AWS’network
• Make them behave as if they were in the
same network
• Must not have overlapping CIDRs • VPC Peering connection is NOT transitive
(must be established for each VPC that
need to communicate with one another)
• You must update route tables in each VPC’s
subnets to ensure EC2 instances can
communicate with each other


You can create VPC Peering connection between VPCs in different AWS
accounts/regions
• You can reference a security group in a peered VPC (works cross
accounts – same region)


VPC Endpoints (AWS PrivateLink)
• Every AWS service is publicly exposed(public URL)
• VPC Endpoints (powered by AWS PrivateLink) allows you to connect to AWS
services using a private network instead of using the public Internet
• They’re redundant and scale horizontally
• They remove the need of IGW, NATGW, …to access AWS Services
• In case of issues:
• Check DNS Setting Resolution in your VPC
• Check Route Tables



Types of Endpoints
--------------------------------------------------

There are three types of VPC endpoints: 
	gateway load balancer endpoints, 
	gateway endpoints,
	interface endpoints.
	
Gateway Load Balancer endpoint:
	The first type of endpoint, a Gateway Load Balancer endpoint, allows you to intercept traffic and route 
	it to a network or security service that you’ve configured using a Gateway Load Balancer. 
	Gateway load balancers enable you to deploy, scale, and manage virtual appliances, 
	such as firewalls, intrusion detection and prevention systems, and deep packet inspection systems. 
UserReq->GWLB->GWLBE->VPC->EC2

Gateway endpoint:
	The second type of endpoint, a Gateway endpoint, allows you to provide access to 
	Amazon Simple Storage Service (S3) and Amazon DynamoDB. You can configure resource policies 
	on both the gateway endpoint and the AWS resource that the endpoint provides access to.


Interface endpoint:
	The third type of endpoint, an Interface endpoint, allows you to connect to services powered by AWS PrivateLink. 
	This includes a large number of AWS services. It also can also include services hosted by other 
	AWS customers, and AWS Partner Network (APN) partners in their own VPCs. 
	By using AWS partner services through AWS PrivateLink, you no longer have to rely on access to the public internet. 


 A VPC endpoint policy is an AWS Identity and Access Management (AWS IAM) resource policy that you can attach to an endpoint. 
 It is a separate policy for controlling access from the endpoint to the specified service. 
 
 
Interface Endpoints:
• Provisions an ENI (private IP address) as an entry point (must attach a Security Group)
• Supports most AWS services

Gateway Endpoints:
• Provisions a gateway and must be used as a target in a route table
• Supports both S3 and DynamoDB


A gateway connects dissimilar networks. You can think of a gateway as a protocol converter. 
AWS VPC uses an internet gateway to connect an AWS private network to the world wide web.


VPC Flow Logs:
• Capture information about IP traffic going into your interfaces:
	• VPC Flow Logs
	• Subnet Flow Logs
	• Elastic Network Interface (ENI) Flow Logs
• Helps to monitor & troubleshoot connectivity issues
• Flow logs data can go to S3 / CloudWatch Logs
• Captures network information from AWS managed interfaces too: ELB,
RDS, ElastiCache, Redshift, WorkSpaces, NATGW, Transit Gateway


WS Site-to-Site VPN
• Virtual Private Gateway (VGW)
• VPN concentrator on the AWS side of the VPN connection
• VGW is created and attached to the VPC from which you want to create the Site-to-Site VPN connection
• Possibility to customize the ASN (Autonomous System Number)

Customer Gateway (CGW)
• Software application or physical device on customer side of the VPN connection


Site-to-Site VPN Connections:
----------------------------------------------
To create a VPN connection, you must create a customer gateway resource in AWS, which provides information to AWS about your customer gateway device. Next, you have to set up an Internet-routable IP address (static) of the customer gateway’s external interface.
don’t need a NAT instance for you to be able to create a VPN connection.


Customer Gateway Device (On-premises)
• What IP address to use?
	• Public Internet-routable IP address for your Customer Gateway device
	• If it’s behind a NAT device that’s enabled for NAT
	  traversal (NAT-T), use the public IP address of the NAT device
• Important step: enable Route Propagation for
the Virtual Private Gateway in the route table
that is associated with your subnets
• If you need to ping your EC2 instances from
on-premises, make sure you add the ICMP
protocol on the inbound of your security
groups


AWS VPN CloudHub 
• Provide secure communication between
multiple sites, if you have multiple VPN
connections
• Low -cost hub -and -spoke model for primary or secondary network connectivity
between different locations (VPN only)
• It’s a VPN connection so it goes over the
public Internet
• To set it up, connect multiple VPN
connections on the same VGW, setup
dynamic routing and configure route tables




Direct Connect (DX)
• Provides a dedicated private connection from a remote network to your VPC
• Dedicated connection must be setup between your DC and AWS Direct
Connect locations
• You need to setup a Virtual Private Gateway on your VPC
• Access public resources (S3) and private (EC2) on same connection
• Use Cases:
	• Increase bandwidth throughput - working with large data sets – lower cost
	• More consistent network experience - applications using real-time data feeds
	• Hybrid Environments (on prem + cloud)
• Supports both IPv4 and IPv6



Direct Connect – Connection Types
Dedicated Connections: 1Gbps and 10 Gbps capacity
• Physical ethernet port dedicated to a customer
• Request made to AWS first, then completed by AWS Direct Connect Partners

Hosted Connections: 50Mbps, 500 Mbps, to 10 Gbps
• Connection requests are made via AWS Direct Connect Partners
• Capacity can be added or removed on demand
• 1, 2, 5, 10 Gbps available at select AWS Direct Connect Partners
• Lead times are often longer than 1 month to establish a new connection



Direct Connect - Resiliency:
High Resiliency for Critical Workloads
One connection at multiple locations

Maximum Resiliency for Critical Workloads
Maximum resilience is achieved by separate connections
terminating on separate devices in more than one location.




AWS PrivateLink (VPC Endpoint Services):
• Most secure & scalable way to expose a service to 1000s of VPC (own or other accounts)
• Does not require VPC peering, internet gateway, NAT, route tables…
• Requires a network load balancer (Service VPC) and ENI (Customer VPC) or GWLB
• If the NLB is in multiple AZ, and the ENIs in multiple AZ, the solution is fault tolerant! 



EC2-Classic & AWS ClassicLink (deprecated)
• EC2-Classic: instances run in a single network shared with other customers
• Amazon VPC: your instances run logically isolated to your AWS account
ClassicLink allows you to link EC2-Classic instances to a VPC in your account
• Must associate a security group
• Enables communication using private IPv4 addresses
• Removes the need to make use of public IPv4 addresses or Elastic IP addresses
Likely to be distractors at the exam 


Transit Gateway
• For having transitive peering between thousands of VPC and
on-premises, hub-and-spoke (star) connection
• Regional resource, can work cross-region
• Share cross-account using Resource Access Manager (RAM)
• You can peer Transit Gateways across regions
• Route Tables: limit which VPC can talk with other VPC
• Works with Direct Connect Gateway, VPN connections
• Supports IP Multicast (not supported by any other AWS
service)



Transit Gateway: Site-to-Site VPN ECMP
• ECMP = Equal-cost multi-path
routing
• Routing strategy to allow to
forward a packet over multiple
best path
• Use case: create multiple Siteto-Site VPN connections to
increase the bandwidth of your
connection to AWS

IPv6 Troubleshooting
• IPv4 cannot be disabled for your VPC
and subnets
• So, if you cannot launch an EC2 instance
in your subnet
	• It’s not because it cannot acquire an IPv6
	(the space is very large)
	• It’s because there are no available IPv4 in
	your subnet
• Solution: create a new IPv4 CIDR in
your subnet



Egress-only Internet Gateway
• Used for IPv6 only
• (similar to a NAT Gateway but for IPv6)
• Allows instances in your VPC outbound
connections over IPv6 while preventing
the internet to initiate an IPv6 connection
to your instances
• You must update the Route Tables



VPC Section Summary:
------------------------------------------------
• CIDR – IP Range
• VPC – Virtual Private Cloud => we define a list of IPv4 & IPv6 CIDR
• Subnets – tied to an AZ, we define a CIDR
• Internet Gateway – at the VPC level, provide IPv4 & IPv6 Internet Access
• Route Tables – must be edited to add routes from subnets to the IGW, VPC Peering Connections, VPC Endpoints, …
• Bastion Host – public EC2 instance to SSH into, that has SSH connectivity to EC2 instances in private subnets
• NAT Instances – gives Internet access to EC2 instances in private subnets. Old, must be setup in a public subnet, disable Source / Destination check flag
• NAT Gateway – managed by AWS, provides scalable Internet access to private EC2 instances, IPv4 only
• Private DNS + Route 53 – enable DNS Resolution + DNS Hostnames (VPC)
NACL – stateless, subnet rules for inbound and outbound, don’t forget Ephemeral Ports
• Security Groups – stateful, operate at the EC2 instance level
• Reachability Analyzer – perform network connectivity testing between AWS
resources
• VPC Peering – connect two VPCs with non overlapping CIDR, non-transitive
• VPC Endpoints – provide private access to AWS Services (S3, DynamoDB, CloudFormation, SSM) within a VPC
• VPC Flow Logs – can be setup at the VPC / Subnet / ENI Level, for ACCEPT and REJECT traffic, helps identifying attacks, analyze using Athena or CloudWatch Logs
Insights
• Site-to-Site VPN – setup a Customer Gateway on DC, a Virtual Private Gateway on VPC, and site-to-site VPN over public Internet
• AWS VPN CloudHub – hub-and-spoke VPN model to connect your sites
Direct Connect – setup a Virtual Private Gateway on VPC, and establish a direct private connection to an AWS Direct Connect Location
• Direct Connect Gateway – setup a Direct Connect to many VPCs in different AWS regions
• AWS PrivateLink / VPC Endpoint Services:
• Connect services privately from your service VPC to customers VPC
• Doesn’t need VPC Peering, public Internet, NAT Gateway, Route Tables
• Must be used with Network Load Balancer & ENI
• ClassicLink – connect EC2-Classic EC2 instances privately to your VPC
• Transit Gateway – transitive peering connections for VPC, VPN & DX
• Traffic Mirroring – copy network traffic from ENIs for further analysis
• Egress-only Internet Gateway – like a NAT Gateway, but for IPv6



Networking Costs in AWS per GB - Simplified
Use Private IP
instead of Public
IP for good
savings and
better network
performance

Use same AZ for
maximum savings
(at the cost of
high availability)


Minimizing egress traffic network cost
• Egress traffic: outbound
traffic (from AWS to
outside)
• Ingress traffic: inbound
traffic - from outside to
AWS (typically free)
• Try to keep as much
internet traffic within
AWS to minimize costs
• Direct Connect location
that are co-located in
the same AWS Region
result in lower cost for
egress network


S3 Data Transfer Pricing – Analysis for USA
• S3 ingress: free
• S3 to Internet: $0.09 per GB
• S3 Transfer Acceleration:
	• Faster transfer times (50 to 500% better)
	• Additional cost on top of Data Transfer Pricing: +$0.04 to $0.08 per GB
• S3 to CloudFront: $0.00 per GB
• CloudFront to Internet: $0.085 per GB (slightly cheaper than S3)
	• Caching capability (lower latency)
	• Reduce costs associated with S3 Requests
	Pricing (7x cheaper with CloudFront)
• S3 Cross Region Replication: $0.02 per GB\





VP:
==
/28 means 16 IPs (=2^(32-28) = 2^4), means only the last digit can change.

CIDR not should overlap, and the max CIDR size in AWS is /16.

Route tables must be updated in both VPCs that are peered.

You can't create a VPC Peering connection between VPC and on-premises corporate data center. Also, the VPC Peering connection is not transitive.

You have set up a Direct Connect connection between your corporate data center and your VPC A in your AWS account. 
You need to access VPC B in another AWS region from your corporate datacenter as well.
 What should you do?
Ans:This is the main use case of Direct Connect Gateways.


When using VPC Endpoints, what are the only two AWS services that have a Gateway Endpoint available?
S3/DymanoDB
These two services have a VPC Gateway Endpoint (remember it), all the other ones have an 
Interface endpoint (powered by Private Link - means a private IP).

VPC Flow Logs is a VPC feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC.

Hosted Direct Connect connection supports 50Mbps, 500Mbps, up to 10Gbps.


Allows you to expose a private application to other AWS customers without making the application public to the Internet and without making a VPC Peering connection.
Use VPC Endpoint Services (AWS PrivateLink)

AWS VPN CloudHub allows you to securely communicate with multiple sites using AWS VPN. 
It operates on a simple hub-and-spoke model that you can use with or without a VPC.





=================================================
##Section 26: Disaster Recovery & Migrations
=================================================
Disaster Recovery Strategies:
• Backup and Restore
• Pilot Light 
• Warm Standby 
• Hot Site / Multi Site Approach



Disaster Recovery Tips:
Backup 
	• EBS Snapshots, RDS automated backups / Snapshots, etc… 
	• Regular pushes to S3 / S3 IA / Glacier, Lifecycle Policy, Cross Region Replication 
	• From On-Premise: Snowball or Storage Gateway 
High Availability 
	• Use Route53 to migrate DNS over from Region to Region 
	• RDS Multi-AZ, ElastiCache Multi-AZ, EFS, S3 
	• Site to Site VPN as a recovery from Direct Connect 
Replication 
	• RDS Replication (Cross Region), AWS Aurora + Global Databases 
	• Database replication from on-premises to RDS 
	• Storage Gateway 
Automation 
	• CloudFormation / Elastic Beanstalk to re-create a whole new environment 
	• Recover / Reboot EC2 instances with CloudWatch if alarms fail 
	• AWS Lambda functions for customized automations 
Chaos 
	• Netflix has a “simian-army” randomly terminating EC2




AWS Schema Conversion Tool (SCT):
• Convert your Database’s Schema from one engine to another
• Example OLTP: (SQL Server or Oracle) to MySQL, PostgreSQL, Aurora
• Example OLAP: (Teradata or Oracle) to Amazon Redshift
• Prefer compute-intensive instances to optimize data conversions
• You do not need to use SCT if you are migrating the same DB engine
	• Ex: On-Premise PostgreSQL => RDS PostgreSQL
	• The DB engine is still PostgreSQL (RDS is the platform)



On-Premise strategy with AWS:
• Ability to download Amazon Linux 2 AMI as a VM (.iso format) 
	• VMWare, KVM, VirtualBox (Oracle VM), Microsoft Hyper-V 
• VM Import / Export 
	• Migrate existing applications into EC2 
	• Create a DR repository strategy for your on-premises VMs 
	• Can export back the VMs from EC2 to on-premises 
• AWS Application Discovery Service 
	• Gather information about your on-premises servers to plan a migration 
	• Server utilization and dependency mappings 
	• Track with AWS Migration Hub 
• AWS Database Migration Service (DMS) 
	• replicate On-premise => AWS , AWS => AWS, AWS => On-premise 
	• Works with various database technologies (Oracle, MySQL, DynamoDB, etc..) 
• AWS Server Migration Service (SMS) 
	• Incremental replication of on-premises live servers to AWS


AWS DataSync
• Move large amount of data from on-premises to AWS
• Can synchronize to: Amazon S3 (any storage classes – including
Glacier), Amazon EFS, Amazon FSx (Windows, Lustre...)
• Move data from your NAS or file system via NFS or SMB
• Replication tasks can be scheduled hourly, daily, weekly
• Leverage the DataSync agent to connect to your systems
• Can setup a bandwidth limit




Transferring large amount of data into AWS:
• Example: transfer 200 TB of data in the cloud. We have a 100 Mbps internet connection.
• Over the internet / Site-to-Site VPN:
	• Immediate to setup
	• Will take 200(TB)*1000(GB)*1000(MB)*8(Mb)/100 Mbps = 16,000,000s = 185d
• Over direct connect 1Gbps:
	• Long for the one-time setup (over a month)
	• Will take 200(TB)*1000(GB)*8(Gb)/1 Gbps = 1,600,000s = 18.5d
• Over Snowball:
	• Will take 2 to 3 snowballs in parallel
	• Takes about 1 week for the end-to-end transfer
	• Can be combined with DMS
• For on-going replication / transfers: Site-to-Site VPN or DX with DMS or DataSync





AWS DataSync is an online data transfer service that simplifies, automates, and accelerates moving data between 
on-premises storage systems and AWS Storage services, as well as between AWS Storage services.


AWS Backup enables you to centralize and automate data protection across AWS services. 
It helps you support your regulatory compliance or business policies for data protection.




=================================================
##Section 27: Machine Learning
=================================================
Amazon Rekognition:
• Find objects, people, text, scenes in images and videos using ML
• Facial analysis and facial search to do user verification, people counting
• Create a database of “familiar faces” or compare against celebrities
• Use cases:
	• Labeling
	• Content Moderation
	• Text Detection
	• Face Detection and Analysis (gender, age range, emotions…)
	• Face Search and Verification
	• Celebrity Recognition
	• Pathing (ex: for sports game analysis)



Amazon Transcribe:
• Automatically convert speech to text
• Uses a deep learning process called automatic speech recognition
(ASR) to convert speech to text quickly and accurately
• Use cases:
	• transcribe customer service calls
	• automate closed captioning and subtitling
	• generate metadata for media assets to create a fully searchable archive



Amazon Polly:
• Turn text into lifelike speech using deep learning 
• Allowing you to create applications that talk


Amazon Translate:
• Natural and accurate language translation
• Amazon Translate allows you to localize content - such as websites and
applications - for international users, and to easily translate large
volumes of text efficiently


Amazon Lex & Connect:
• Amazon Lex: (same technology that powers Alexa)
	• Automatic Speech Recognition (ASR) to convert speech to text
	• Natural Language Understanding to recognize the intent of text, callers
	• Helps build chatbots, call center bots
• Amazon Connect:
	• Receive calls, create contact flows, cloud-based virtual contact center
	• Can integrate with other CRM systems or AWS
	• No upfront payments, 80% cheaper than traditional contact center solutions




Amazon Comprehend:
• For Natural Language Processing – NLP
• Fully managed and serverless service
• Uses machine learning to find insights and relationships in text
	• Language of the text
	• Extracts key phrases, places, people, brands, or events
	• Understands how positive or negative the text is
	• Analyzes text using tokenization and parts of speech
	• Automatically organizes a collection of text files by topic
• Sample use cases:
	• analyze customer interactions (emails) to find what leads to a positive or negative experience
	• Create and groups articles by topics that Comprehend will uncover



Amazon SageMaker:
• Fully managed service for developers / data scientists to build ML models
• Typically, difficult to do all the processes in one place + provision servers
• Machine learning process (simplified): predicting your exam score


Amazon Forecast:
• Fully managed service that uses ML to deliver highly accurate forecasts
• Example: predict the future sales of a raincoat
• 50% more accurate than looking at the data itself
• Reduce forecasting time from months to hours
• Use cases: Product Demand Planning, Financial Planning, Resource Planning


Amazon Kendra:
• Fully managed document search service powered by Machine Learning
• Extract answers from within a document (text, pdf, HTML, PowerPoint, MS Word, FAQs…)
• Natural language search capabilities
• Learn from user interactions/feedback to promote preferred results (Incremental Learning)
• Ability to manually fine-tune search results (importance of data, freshness, custom, …)


Amazon Personalize:
• Fully managed ML-service to build apps with real-time personalized recommendations
• Example: personalized product recommendations/re-ranking, customized direct marketing
• Example: User bought gardening tools, provide recommendations on the next one to buy
• Integrates into existing websites, applications, SMS, email marketing systems, …
• Implement in days, not months (you don’t need to build, train, and deploy ML solutions)
• Use cases: retail stores, media and entertainment


Amazon Textract:
• Automatically extracts text, handwriting, and data from any scanned
documents using AI and ML
Extract data from forms and tables
• Read and process any type of document (PDFs, images, …)
• Use cases:
	• Financial Services (e.g., invoices, financial reports)
	• Healthcare (e.g., medical records, insurance claims)
	• Public Sector (e.g., tax forms, ID documents, passports)


AWS Machine Learning- Summary:
• Rekognition
: face detection, labeling, celebrity recognition
• Transcribe: audio to text (ex: subtitles) 
• Polly: text to audio 
• Translate: translations 
• Lex: build conversational bots – chatbots 
• Connect: cloud contact center 
• Comprehend: natural language processing 
• SageMaker: machine learning for every developer and data scientist 
• Forecast: build highly accurate forecasts 
• Kendra: ML-powered search engine 
• Personalize: real-time personalized recommendations 
• Textract: detect text and data in documents


Amazon Transcribe is an AWS service that makes it easy for customers to convert speech-to-text. 
Amazon Polly is a service that turns text into lifelike speech.

Amazon Transcribe is an AWS service that makes it easy for customers to convert speech-to-text. ]
However, it does not have natural language understanding.

Amazon Connect is a self-service, cloud-based contact center service that makes it easy for any business to 
deliver better customer service at lower cost. It does not provide speech-to-text conversion or natural language understanding.


Amazon Lex is a service for building conversational interfaces into any application using voice and text. 
Lex provides the advanced deep learning functionalities of automatic speech recognition (ASR) 
for converting speech to text, and natural language understanding (NLU) to recognize the intent of the text, 
to enable you to build applications with highly engaging user experiences and lifelike conversational interactions.


Amazon Personalize is a machine learning service that makes it easy for developers to create individualized recommendations 
for customers using their applications.


Amazon Comprehend is a natural language processing (NLP) service that uses machine learning to find meaning and insights in text.

Amazon Translate is a neural machine translation service that delivers fast, high-quality, and affordable language translation.
Amazon Transcribe is an AWS service that makes it easy for customers to convert speech-to-text.

Amazon Kendra is a highly accurate and easy to use enterprise search service that’s powered by machine learning.




=================================================
##Section 28: More Solution Architectures
=================================================

S3 Events:
-------------------------------------------------
S3:ObjectCreated, S3:ObjectRemoved,
S3:ObjectRestore, S3:Replication…
• Object name filtering possible (*.jpg)
• Use case: generate thumbnails of images uploaded to S3
• Can create as many “S3 events” as desired
• S3 event notifications typically deliver events in
seconds but can sometimes take a minute or
longer
• If two writes are made to a single non- versioned object at the same time, it is
possible that only a single event notification
will be sent
• If you want to ensure that an event
notification is sent for every successful write,
you can enable versioning on your bucket.


High Performance Computing (HPC)
• The cloud is the perfect place to perform HPC
• You can create a very high number of resources in no time
• You can speed up time to results by adding more resources
• You can pay only for the systems you have used
• Perform genomics, computational chemistry, financial risk modeling,
weather prediction, machine learning, deep learning, autonomous driving
• Which services help perform HPC?



Data Management & Transfer:
• AWS Direct Connect:
	• Move GB/s of data to the cloud, over a private secure network
• Snowball & Snowmobile
	• Move PB of data to the cloud
• AWS DataSync
	• Move large amount of data between on-premises and S3, EFS, FSx for Windows


Compute and Networking:
• EC2 Instances:
• CPU optimized, GPU optimized
• Spot Instances / Spot Fleets for cost savings + Auto Scaling
• EC2 Placement Groups: Cluster for good network performance


Compute and Networking:
EC2 Enhanced Networking (SR-IOV)
• Higher bandwidth, higher PPS (packet per second), lower latency
• Option 1: Elastic Network Adapter (ENA) up to 100 Gbps
• Option 2: Intel 82599 VF up to 10 Gbps – LEGACY

Elastic Fabric Adapter (EFA)
• Improved ENA for HPC, only works for Linux
• Great for inter-node communications, tightly coupled workloads
• Leverages Message Passing Interface (MPI) standard
• Bypasses the underlying Linux OS to provide low-latency, reliable transport


Storage:
Instance-attached storage:
• EBS: scale up to 256,000 IOPS with io2 Block Express
• Instance Store: scale to millions of IOPS, linked to EC2 instance, low latency

Network storage:
• Amazon S3: large blob, not a file system
• Amazon EFS: scale IOPS based on total size, or use provisioned IOPS
• Amazon FSx for Lustre:
	• HPC optimized distributed file system, millions of IOPS
	• Backed by S3


Automation and Orchestration:
AWS Batch
• AWS Batch supports multi-node parallel jobs, which enables you to run single
jobs that span multiple EC2 instances.
• Easily schedule jobs and launch EC2 instances accordingly

AWS ParallelCluster
• Open-source cluster management tool to deploy HPC on AWS
• Configure with text files
• Automate creation of VPC, Subnet, cluster type and instance types
• Ability to enable EFA on the cluster (improves network performance)


High Availability for a Bastion Host:

HA options for the bastion host
• Run 2 across 2 AZ
• Run 1 across 2 AZ with 1 ASG 1:1:1

Routing to the bastion host
• If 1 bastion host, use an elastic IP with ec2 user-data script to access it
• If 2 bastion hosts, use an Network Load Balancer (layer 4) deployed in multiple AZ
• If NLB, the bastion hosts can live in the private subnet directly
Note: Can’t use ALB as the ALB is layer 7 (HTTP protocol) ssH LAYER 4




=================================================
##Section 29: Other Services
=================================================

Technology Stack for CICD:
-------------------------------------------------
AWS CodeCommit->AWS CodeBuild->AWS Elastic Beanstalk
GitHub
Or 3rd party
code repository

Jenkins CI
Or 3rd party CI servers

AWS CodeDeploy
User Managed
EC2 Instances
Fleet
(CloudFormation)


What is CloudFormation:
• CloudFormation is a declarative way of outlining your AWS
Infrastructure, for any resources (most of them are supported).
• For example, within a CloudFormation template, you say:
	• I want a security group
	• I want two EC2 machines using this security group
	• I want two Elastic IPs for these EC2 machines
	• I want an S3 bucket
	• I want a load balancer (ELB) in front of these machines
• Then CloudFormation creates those for you, in the right order, with the
exact configuration that you specify


Benefits of AWS CloudFormation (1/2):

nfrastructure as code
• No resources are manually created, which is excellent for control
• The code can be version controlled for example using git
• Changes to the infrastructure are reviewed through code

Cost
• Each resources within the stack is tagged with an identifier so you can easily see how
much a stack costs you
• You can estimate the costs of your resources using the CloudFormation template
• Savings strategy: In Dev, you could automation deletion of templates at 5 PM and
recreated at 8 AM, safely

Productivity
• Ability to destroy and re-create an infrastructure on the cloud on the fly
• Automated generation of Diagram for your templates!
• Declarative programming (no need to figure out ordering and orchestration)

Separation of concern: create many stacks for many apps, and many layers. Ex:
• VPC stacks
• Network stacks
• App stacks

Don’t re-invent the wheel
• Leverage existing templates on the web!
• Leverage the documentation


How CloudFormation Works:
• Templates have to be uploaded in S3 and then referenced in
CloudFormation
• To update a template, we can’t edit previous ones. We have to reupload a new version of the template to AWS
• Stacks are identified by a name
• Deleting a stack deletes every single artifact that was created by
CloudFormation. 


How CloudFormation Works:
• Templates have to be uploaded in S3 and then referenced in
CloudFormation
• To update a template, we can’t edit previous ones. We have to reupload a new version of the template to AWS
• Stacks are identified by a name
• Deleting a stack deletes every single artifact that was created by
CloudFormation.


CloudFormation Building Blocks:
Templates components
1. Resources: your AWS resources declared in the template (MANDATORY)
2. Parameters: the dynamic inputs for your template
3. Mappings: the static variables for your template
4. Outputs: References to what has been created
5. Conditionals: List of conditions to perform resource creation
6. Metadata

Templates helpers:
1. References
2. Functions



CloudFormation - StackSets:
• Create, update, or delete stacks
across multiple accounts and regions
with a single operation
• Administrator account to create
StackSets
• Trusted accounts to create, update,
delete stack instances from StackSets
• When you update a stack
set, all associated stack instances are
updated throughout all accounts and
regions.


AWS Step Functions:
• Build serverless visual workflow to orchestrate your Lambda functions
• Represent flow as a JSON state machine
• Features: sequence, parallel, conditions, timeouts, error handling…
• Can also integrate with EC2, ECS, On premise servers, API Gateway
• Maximum execution time of 1 year
• Possibility to implement human approval feature

Use cases:
• Order fulfillment
• Data processing
• Web applications
• Any workflow


AWS SWF – Simple Workflow Service:
Step Functions is recommended to be used for new applications, except:
• If you need external signals to intervene in the processes
• If you need child processes that return values to parent processes


Amazon EMR:
• EMR stands for “Elastic MapReduce”
• EMR helps creating Hadoop clusters (Big Data) to analyze and process
vast amount of data
• The clusters can be made of hundreds of EC2 instances
• Also supports Apache Spark, HBase, Presto, Flink…
• EMR takes care of all the provisioning and configuration
• Auto-scaling and integrated with Spot instances
• Use cases: data processing, machine learning, web indexing, big data



Quick word on Chef / Puppet:
• They help with managing configuration as code
• Helps in having consistent deployments
• Works with Linux / Windows
• Can automate: user accounts, cron, ntp, packages, services…
• They leverage “Recipes” or ”Manifests”
• Chef / Puppet have similarities with SSM / Beanstalk / CloudFormation
but they’re open-source tools that work cross-cloud



AWS WorkSpaces:
• Managed, Secure Cloud Desktop
• Great to eliminate management of on-premises VDI (Virtual Desktop Infrastructure)
• On Demand, pay per by usage
• Secure, Encrypted, Network Isolation
• Integrated with Microsoft Active Directory



AWS AppSync:
• Store and sync data across mobile and web apps in real-time
• Makes use of GraphQL (mobile technology from Facebook)
• Client Code can be generated automatically
• Integrations with DynamoDB / Lambda
• Real-time subscriptions
• Offline data synchronization (replaces Cognito Sync)
• Fine Grained Security


Cost Explorer:
• Visualize, understand, and manage your AWS costs and usage over time
• Create custom reports that analyze cost and usage data.
• Analyze your data at a high level: total costs and usage across all accounts
• Or Monthly, hourly, resource level granularity
• Choose an optimal Savings Plan (to lower prices on your bill)
• Forecast usage up to 12 months based on previous usage


Well Architected Framework:
6 Pillars
• 1) Operational Excellence
• 2) Security
• 3) Reliability
• 4) Performance Efficiency
• 5) Cost Optimization
• 6) Sustainability


Trusted Advisor:
• No need to install anything – high level AWS account assessment
• Analyze your AWS accounts and provides recommendation
• Core Checks and recommendations – all customers
• Can enable weekly email notification from the console
• Full Trusted Advisor – Available for Business & Enterprise support plans
• Ability to set CloudWatch alarms when reaching limits
• Programmatic Access using AWS Support API


Trusted Advisor Checks Examples:
• Cost Optimization: 
	• low utilization EC2 instances, idle load balancers, under-utilized EBS volumes… 
	• Reserved instances & savings plans optimizations, 
• Performance: 
	• High utilization EC2 instances, CloudFront CDN optimizations 
	• EC2 to EBS throughput optimizations, Alias records recommendations 
• Security: 
	• MFA enabled on Root Account, IAM key rotation, exposed Access Keys 
	• S3 Bucket Permissions for public access, security groups with unrestricted ports 
• Fault Tolerance: 
	• EBS snapshots age, Availability Zone Balance 
	• ASG Multi-AZ, RDS Multi-AZ, ELB configuration… 
• Service Limit



Other Services: Cheat Sheet:
------------------------------------------------------
Here's a quick cheat-sheet to remember all these services:
CodeCommit: service where you can store your code. Similar service is GitHub
CodeBuild: build and testing service in your CICD pipelines
CodeDeploy: deploy the packaged code onto EC2 and AWS Lambda
CodePipeline: orchestrate the actions of your CICD pipelines (build stages, manual approvals, many deploys, etc)
CloudFormation: Infrastructure as Code for AWS. Declarative way to manage, create and update resources.
ECS (Elastic Container Service): Docker container management system on AWS. Helps with creating micro-services.
ECR (Elastic Container Registry): Docker images repository on AWS. Docker Images can be pushed and pulled from there
Step Functions: Orchestrate / Coordinate Lambda functions and ECS containers into a workflow
SWF (Simple Workflow Service): Old way of orchestrating a big workflow.
EMR (Elastic Map Reduce): Big Data / Hadoop / Spark clusters on AWS, deployed on EC2 for you
Glue: ETL (Extract Transform Load) service on AWS
OpsWorks: managed Chef & Puppet on AWS
ElasticTranscoder: managed media (video, music) converter service into various optimized formats
Organizations: hierarchy and centralized management of multiple AWS accounts
Workspaces: Virtual Desktop on Demand in the Cloud. Replaces traditional on-premise VDI infrastructure
AppSync: GraphQL as a service on AWS
SSO (Single Sign On): One login managed by AWS to log in to various business SAML 2.0-compatible applications (office 365 etc)



AWS CodeCommit is a secure, highly scalable, managed source control service that hosts private Git repositories. 
It is an alternative to GitLab and GitHub.

AWS CodePipeline is a fully managed continuous delivery (CD) service that helps you automate your release 
pipeline for fast and reliable application and infrastructure updates. It automates the build, test, 
and deploy phases of your release process every time there is a code change.

AWS CodePipeline is a fully managed continuous delivery (CD) service that helps you automate your release 
pipeline for fast and reliable application and infrastructure updates. It automates the build, test, and deploy phases of your release process every time there is a code change.


CloudFormation StackSets allows you to create, update, or delete CloudFormation stacks across multiple 
AWS accounts and AWS regions with a single operation.


AWS ECS service allows you to manage a fleet of Docker containers in AWS Cloud and on-premises.

AWS CodePipeline is a fully managed continuous delivery (CD) service that helps you automate your release 
pipeline for fast and reliable application and infrastructure updates. It automates the build, test, and deploy phases of 
your release process every time there is a code change. It has direct integration with Elastic Beanstalk.

AWS CodeDeploy is a fully managed deployment service that automates software deployments to a variety of computing 
services such as EC2, Fargate, Lambda, and your on-premises servers. You can define the strategy you want to execute 
such as in-place or blue/green deployments.

AWS Step Functions is a low-code visual workflow service used to orchestrate AWS services, automate business processes, 
and build Serverless applications. It manages failures, retries, parallelization, service integrations,

Amazon EMR is managed service that makes it fast, easy, and cost-effective to run Apache Hadoop and Spark to process 
vast amounts of data.

AWS Glue is a Serverless data-preparation service for extract, transform, and load (ETL) operations.
Move data all around your AWS databases using a managed ETL service that has a metadata catalog feature

AWS OpsWorks is a configuration management service that provides managed instances of Chef and Puppet.


Amazon WorkSpaces is a fully managed, persistent desktop virtualization service that enables your 
users to access data, applications, and resources they need, anywhere, anytime, from any supported device. 
It can be used to provision either Windows or Linux desktops.


AWS AppSync is a fully managed service that makes it easy to develop GraphQL APIs by handling the heavy lifting of 
securely connecting to data sources like DynamoDB, Lambda, and more

AWS Cost Explorer enables you to view and analyze your costs and usage. You can view data for up to the last 12 months, 
forecast how much you are likely to spend for the next 12 months, and get recommendations for what EC2 reserved instances to purchase.



The Lambda function's invocation is "asynchronous", so the DLQ has to be set on the Lambda function side.





================================================= 
##FAQ
================================================= 


#Amazon EC2 Auto Scaling FAQs:
-------------------------------------------------
1)Q. Is there a limit on how large or small a subnet can be?\
The minimum size of a subnet is a /28 (or 14 IP addresses.) for IPv4. Subnets cannot be larger than the VPC in which they are created.
For IPv6, the subnet size is fixed to be a /64. Only one IPv6 CIDR block can be allocated to a subnet.

2)Q. How large of a VPC can I create?
Currently, Amazon VPC supports five (5) IP address ranges, one (1) primary and four (4) secondary for IPv4. 
Each of these ranges can be between /28 (in CIDR notation) and /16 in size. The IP address ranges of your 
VPC should not overlap with the IP address ranges of your existing network.
For IPv6, the VPC is a fixed size of /56 (in CIDR notation). A VPC can have both IPv4 and IPv6 CIDR blocks associated to it.





21) How is AWS CloudFormation different from AWS Elastic Beanstalk?
Here are some differences between AWS CloudFormation and AWS Elastic Beanstalk:
AWS CloudFormation helps you provision and describe all of the infrastructure resources that are present in your cloud environment. 
On the other hand, AWS Elastic Beanstalk provides an environment that makes it easy to deploy and run applications in the cloud.

AWS CloudFormation supports the infrastructure needs of various types of applications, like legacy applications and 
existing enterprise applications. On the other hand, AWS Elastic Beanstalk is combined with the developer tools to
help you manage the lifecycle of your applications.



22) What are the elements of an AWS CloudFormation template?
AWS CloudFormation templates are YAML or JSON formatted text files that are comprised of five essential elements, they are:
-Template parameters
-Output values
-Data tables
-Resources
-File format versi


23) What happens when one of the resources in a stack cannot be created successfully?
If the resource in the stack cannot be created, then the CloudFormation automatically rolls back and terminates all the 
resources that were created in the CloudFormation template. This is a handy feature 
when you accidentally exceed your limit of Elastic IP addresses or don’t have access to an EC2 AMI.



23) What Is Identity and Access Management (IAM) and How Is It Used?
Identity and Access Management (IAM) is a web service for securely controlling access to AWS services. 
IAM lets you manage users, security credentials such as access keys, and permissions that control which 
AWS resources users and applications can access.




24) What are the policies that you can set for your users’ passwords?
Here are some of the policies that you can set:
You can set a minimum length of the password, or you can ask the users to add at least one number or special characters in it.
You can assign requirements of particular character types, including uppercase letters, 
lowercase letters, numbers, and non-alphanumeric characters.
You can enforce automatic password expiration, prevent reuse of old passwords, and request for a password 
reset upon their next AWS sign in.
You can have the AWS users contact an account administrator when the user has allowed the password to expire. 



25) Can AWS Config aggregate data across different AWS accounts?
Yes, you can set up AWS Config to deliver configuration updates from different accounts to one S3 bucket, 
once the appropriate IAM policies are applied to the S3 bucket.



26) What happens to my Amazon EC2 instances if I delete my ASG?
If you have an EC2 Auto Scaling group (ASG) with running instances and you choose to delete the ASG, the instances will be 
terminated and the ASG will be deleted.



27) How do I know when EC2 Auto Scaling is launching or terminating the EC2 instances in an EC2 Auto Scaling group?
When you use Amazon EC2 Auto Scaling to scale your applications automatically, 
it is useful to know when EC2 Auto Scaling is launching or terminating the EC2 instances in your EC2 Auto Scaling group. 
Amazon SNS coordinates and manages the delivery or sending of notifications to subscribing clients or endpoints. 



28) What is a launch configuration?
A launch configuration is a template that an EC2 Auto Scaling group uses to launch EC2 instances.
You can specify your launch configuration with multiple EC2 Auto Scaling groups. However, you can only specify one launch configuration 
for an EC2 Auto Scaling group at a time, and you can't modify a launch configuration after you've created it. 

Therefore, if you want to change the launch configuration for your EC2 Auto Scaling group, 
you must create a launch configuration and then update your EC2 Auto Scaling group with the new launch configuration. 

When you change the launch configuration for your EC2 Auto Scaling group, any new instances are launched using the new 
configuration parameters, but existing instances are not affected. You can see the launch configurations section of the 
EC2 Auto Scaling User Guide for more details.



29) What happens if a scaling activity causes me to reach my Amazon EC2 limit of instances?
Amazon EC2 Auto Scaling cannot scale past the Amazon EC2 limit of instances that you can run. 
If you need more Amazon EC2 instances, complete the Amazon EC2 instance request form.



30) Can EC2 Auto Scaling groups span multiple AWS regions?
EC2 Auto Scaling groups are regional constructs. They can span Availability Zones, but not AWS regions.



31) How can I implement changes across multiple instances in an EC2 Auto Scaling group?
You can use AWS CodeDeploy or CloudFormation to orchestrate code changes to multiple instances in your EC2 Auto Scaling group.



32) If I have data installed in an EC2 Auto Scaling group, and a new instance is dynamically created later, is the data copied over to the new instances?
Data is not automatically copied from existing instances to new instances. You can use lifecycle hooks to copy the data, or an Amazon RDS database including replicas.



33) When I create an EC2 Auto Scaling group from an existing instance, does it create a new AMI (Amazon Machine Image)?
When you create an Auto Scaling group from an existing instance, it does not create a new AMI. 



34) What are lifecycle hooks?
Lifecycle hooks let you take action before an instance goes into service or before it gets terminated. 
This can be especially useful if you are not baking your software environment into an Amazon Machine Image (AMI). 



35) Which health check type should I select?
If you are using Elastic Load Balancing (ELB) with your group, you should select an ELB health check. 
If you’re not using ELB with your group, you should select the EC2 health check.



36) How do I control which instances Amazon EC2 Auto Scaling terminates when scaling in, and how do I protect data on an instance?
You can configure this through the use of a termination policy. You can also use instance protection to prevent 
Amazon EC2 Auto Scaling from selecting specific instances for termination when scaling in. 

If you have data on an instance, 
and you need that data to be persistent even if your instance is scaled in, then you can use a service like S3, RDS, or DynamoDB, 
to make sure that it is stored off the instance.



37) How long is the turn-around time for Amazon EC2 Auto Scaling to spin up a new instance at inService state after 
detecting an unhealthy server?
The turnaround time is within minutes. The majority of replacements happen within less than 5 minutes, and on average 
it is significantly less than 5 minutes. It depends on a variety of factors, including how long it takes to boot up the AMI 
of your instance.



38) If Elastic Load Balancing (ELB) determines that an instance is unhealthy, and moved offline, 
will the previous requests sent to the failed instance be queued and rerouted to other instances within the group?

When ELB notices that the instance is unhealthy, it will stop routing requests to it. 
However, prior to discovering that the instance is unhealthy, some requests to that instance will fail.


39) If you don’t use Elastic Load Balancing (ELB) how would users be directed to the other servers in a group if there was a failure?
You can integrate with Route53 (which Amazon EC2 Auto Scaling does not currently support out of the box, but many customers use).
You can also use your own reverse proxy, or for internal microservices, can use service discovery solutions.


================================================= 
#Security:
================================================= 

40) Are CloudWatch agents automatically installed on EC2 instances when you create an Amazon EC2 Auto Scaling group?
If your AMI contains a CloudWatch agent, it’s automatically installed on EC2 instances when you create an EC2 Auto Scaling group. With the stock Amazon Linux AMI, you need to install it (recommended, via yum).


Cost Optimization:
=================

41) Can I create a single ASG to scale instances across different purchase options?
Yes. You can provision and automatically scale EC2 capacity across different EC2 instance types, Availability Zones, and On-Demand, RIs and Spot purchase options in a single Auto Scaling Group. 


42)Can I use ASGs to launch and manage just Spot Instances or just On-Demand instances and RIs?
Yes. You can configure your ASG specifying all capacity to be only Spot instances or all capacity to be only On-Demand instances and RIs.


43) Can I specify instances of different sizes (CPU cores, memory) in my Auto Scaling group?
Yes. You can specify any instance type available in a region. Additionally, you can specify an optional weight for each instance type, which defines the capacity units that each instance would contribute to your application’s performance.


44) What are the costs for using Amazon EC2 Auto Scaling?
Amazon EC2 Auto Scaling fleet managment for EC2 instances carries no additional fees. The dynamic scaling capabilities of 
Amazon EC2 Auto Scaling are enabled by Amazon CloudWatch and also carry no additional fees. 
Amazon EC2 and Amazon CloudWatch service fees apply and are billed separately.




================================================= 
Amazon Elastic Container Registry FAQs:
================================================= 

1) What is Amazon Elastic Container Registry (Amazon ECR)?
Amazon ECR is a fully managed container registry that makes it easy for developers to share and deploy container images and artifacts. Amazon ECR is integrated with Amazon Elastic Container Service (Amazon ECS),  Amazon Elastic Kubernetes Service (Amazon EKS), and AWS Lambda, simplifying your development to production workflow.


2) Is Amazon ECR a global service?
Amazon ECR is a Regional service and is designed to give you flexibility in how images are deployed. You have the ability to push/pull images to the same AWS Region where your Docker cluster runs for the best performance. 


3) What is the difference between Amazon ECR public and private repositories?
A private repository does not offer content search capabilities and requires Amazon IAM-based authentication using 
AWS account credentials before allowing images to be pulled. A public repository has descriptive content and allows 
anyone anywhere to pull images without needing an AWS account or using IAM credentials. Public repository 
images are also available in the Amazon ECR public gallery.


4) What compliance capabilities can I enable on Amazon ECR?
You can use AWS CloudTrail on Amazon ECR to provide a history of all API actions such as who pulled an image and 
when tags were moved between images. Administrators can also find which EC2 instances pulled which images.


5) Can I access Amazon ECR inside a VPC?
Yes. You can set up AWS PrivateLink endpoints to allow your instances to pull images from your private repositories without traversing through the public internet.


6)Does Amazon ECR work with AWS Elastic Beanstalk?
Yes. AWS Elastic Beanstalk supports Amazon ECR for both single and multi-container Docker environments, allowing you to easily 
deploy container images stored in Amazon ECR with AWS Elastic Beanstalk. All you need to do is specify the 
Amazon ECR repository in your Dockerrun.aws.json configuration and attach the 
AmazonEC2ContainerRegistryReadOnly policy to your container instance role.


7) Does Amazon ECR scan container images for vulnerabilities?
You can enable Amazon ECR to automatically scan your container images for a broad range of operating system vulnerabilities. 
You can also scan images using an API command, and Amazon ECR will notify you over API and in the console when a scan completes. 
For enhanced image scanning, you can turn on Amazon Inspector.




================================================= 
Amazon EC2 Container Service FAQ:
================================================= 

1) What is Amazon Elastic Container Service?
Amazon Elastic Container Service (ECS) is a highly scalable, high performance container management service that supports 
Docker containers and allows you to easily run applications on a managed cluster of Amazon Elastic Compute Cloud (Amazon EC2) instances.

Amazon ECS makes it easy to use containers as a building block for your applications by eliminating the need for you to install, 
operate, and scale your own cluster management infrastructure. Amazon ECS lets you schedule long-running 
applications, services, and batch processes using Docker containers.



2) What is the pricing for Amazon ECS?
There is no additional charge for Amazon ECS. You pay for AWS resources (e.g. Amazon EC2 instances or EBS volumes) 
you create to store and run your application. You only pay for what you use, as you use it; there are no 
minimum fees and no upfront commitments.


3) How is Amazon ECS different from AWS Elastic Beanstalk?
AWS Elastic Beanstalk is an application management platform that helps customers easily deploy and scale web applications and services. 
It keeps the building block provisioning (e.g., EC2, Amazon RDS, Elastic Load Balancing, AWS Auto Scaling, and Amazon CloudWatch), 
application deployment, and health monitoring abstracted from the user so they can focus on writing code. You simply specify which 
container images to  deploy, the CPU and memory requirements, the port mappings, and the container links.

Elastic Beanstalk will automatically handle all the details such as provisioning an Amazon ECS cluster, balancing load, auto-scaling, 
monitoring, and container placement across your cluster. Elastic Beanstalk is ideal if you want to leverage the benefits of containers 
withthe simplicity of deploying applications from development to production by uploading a container image. You can work with 
Amazon ECS directly if you want more fine-grained control for custom application architectures.


4) Does Amazon ECS support any other container types?
No. Docker is the only container platform supported by Amazon ECS at this time.



================================================= 
Amazon EC2 Windows FAQ:
================================================= 

AWS is a member of the Microsoft Partner Network, licensed to sell Microsoft software under the Service 
Provider License Agreement (SPLA), and a Microsoft Gold Certified Hosting Partner. 
AWS is an authorized Microsoft License Mobility Partner and has an active Premier Support agreement with Microsoft.


1) Are there regional restrictions on accessing the benefit of the expanded Support agreement with Microsoft?
No, there are no regional restrictions to using this benefit.


2) What types of Microsoft software can I run on AWS?
You can run many types of Microsoft software on AWS, including but not limited to: Microsoft Office, Windows Server, SQL Server, 
Exchange, SharePoint, Skype for Business, Microsoft Dynamics products, System Center, BizTalk, and Remote Desktop Services. 
You can use license included instances that include the license for Windows Server and SQL Server on Amazon EC2 or Amazon RDS. 
AWS customers have the flexibility of bringing on-premises Microsoft volume licenses and deploying them on 
Amazon EC2 instances subject to Microsoft license terms.



3) What’s the difference between Dedicated Hosts and Dedicated Instances?
Both offerings provide instances that are dedicated to your use. However, Dedicated Hosts provide additional control 
over your instances and visibility into Host level resources and tooling that allows you to manage software that consumes 
licenses on a per-core or per-socket basis, such as Windows Server and SQL Server. In addition, AWS Config will keep a record 
of how your instances use these Dedicated Host resources which will allow you to create your own license usage reports.


4)Can I buy MSDN from AWS?
No, AWS does not sell MSDN licenses.


5)What’s new in Windows Server 2022?
The latest server OS released by Microsoft, Windows Server 2022, offers a variety of features and improvements in performance, 
connectivity and security. AWS customers can make the best out of running Windows Server 2022 on EC2 by leveraging the 
elasticity and breadth of resources offered on AWS. Customers can start using various features of Windows Server 2022 
readily by accessing the Windows AMIs offered by AWS.




================================================= 
Amazon EC2 FAQ:
================================================= 

1)What operating system environments are supported?
Amazon EC2 currently supports a variety of operating systems including: Amazon Linux, Ubuntu, Windows Server, 
Red Hat Enterprise Linux, SUSE Linux Enterprise Server, openSUSE Leap, Fedora, Fedora CoreOS, Debian, CentOS, Gentoo Linux, 
Oracle Linux, and FreeBSD. We are looking for ways to expand it to other platforms.



2)Are these On-Demand Instance vCPU-based limits regional?
Yes, the On-Demand Instance limits for an AWS account are set on a per-region basis.



3)Will these limits change over time?
Yes, limits can change over time. Amazon EC2 is constantly monitoring your usage within each region and your limits are raised 
automatically based on your use of EC2.


4)How can I view my current On-Demand Instance limits?
You can find your current On-Demand Instance limits on the EC2 Service Limits page in the Amazon EC2 console, 
or from the Service Quotas console and APIs.




================================================= 
Amazon Elastic Kubernetes Service (EKS) FAQ:
================================================= 

Kubernetes is an open-source container orchestration system allowing you to deploy and manage containerized applications at scale. 
Kubernetes arranges containers into logical groupings for management and discoverability, 
then launches them onto clusters of Amazon Elastic Compute Cloud (Amazon EC2) instances.

1) What is Amazon Elastic Kubernetes Service (Amazon EKS)?
Amazon EKS is a managed service that makes it easy for you to run Kubernetes on AWS without installing and operating 
your own Kubernetes control plane or worker nodes.



2) Which operating systems does Amazon EKS support?
Amazon EKS supports Kubernetes-compatible Linux x86, ARM, and Windows Server operating system distributions. 
Amazon EKS provides optimized AMIs for Amazon Linux 2 and Windows Server 2019. EKS- optimized AMIs for other Linux distributions, 
such as Ubuntu, are available from their respective vendors.


3) Does Amazon EKS work with AWS Fargate?
Yes. You can run Kubernetes applications as serverless containers using AWS Fargate and Amazon EKS.


4)How much does Amazon EKS cost?

You pay $0.10 per hour for each Amazon EKS cluster you create and for the AWS resources you create to run your Kubernetes worker nodes.
You only pay for what you use, as you use it; there are no minimum fees and no upfront commitments. 


================================================= 
Amazon Lightsail FAQ:
================================================= 

1) What is a Virtual Private Server?
A virtual private server, also known as an "instance", allows users to run websites and web
applications in a highly secure and available environment, while being cost effective.


2) What is Amazon Lightsail?
Amazon Lightsail is a virtual private server (VPS) provider and is the easiest way to get started with AWS for developers, 
small businesses, students, and other users who need a solution to build and host their applications on cloud. 
Lightsail provides developers compute, storage, and networking capacity and capabilities to deploy and manage 
websites and web applications in the cloud. Lightsail includes everything you need to launch your 
project quickly – virtual machines, containers, databases, CDN, load balancers, DNS management etc. 
– for a low, predictable monthly price.


3)What is a Lightsail instance?
A Lightsail instance is a virtual private server (VPS) that lives in the AWS Cloud. Use your Lightsail instances to store your data,
run your code, and build web-based applications or websites. Your instances can connect to each other and to other 
AWS resources through both public (Internet) and private (VPC) networking. You can create, manage, and 
connect easily to instances right from the Lightsail console.


4)Does Lightsail offer an API?
Yes. Everything you do in the Lightsail console is backed by a publicly available API. 




================================================= 
AWS Batch FAQ:
================================================= 

1)What is AWS Batch?
AWS Batch is a set of batch management capabilities that enables developers, scientists, and engineers to easily and efficiently 
run hundreds of thousands of batch computing jobs on AWS. AWS Batch dynamically provisions the optimal quantity and type 
of compute resources (e.g., CPU or memory optimized compute resources) based on the volume and specific resource 
requirements of the batch jobs submitted.


2)What is Batch Computing?
Batch computing is the execution of a series of programs ("jobs") on one or more computers without manual intervention. 


3)What types of batch jobs does AWS Batch support?
AWS Batch supports any job that can executed as a Docker container. Jobs specify their memory requirements and number of vCPUs.  


4)What is a Compute Resource?
An AWS Batch Compute Resource is an EC2 instance or AWS Fargate compute resource.



5)What is the pricing for AWS Batch?
There is no additional charge for AWS Batch. You only pay for the AWS Resources (e.g. EC2 instances or AWS Fargate) 
you create to store and run your batch jobs.



================================================= 
AWS Elastic Beanstalk FAQ:
================================================= 

1) What is AWS Elastic Beanstalk?
AWS Elastic Beanstalk makes it even easier for developers to quickly deploy and manage applications in the AWS Cloud. 
Developers simply upload their application, and Elastic Beanstalk automatically handles the deployment details of capacity provisioning, 
load balancing, auto-scaling, and application health monitoring.

AWS Elastic Beanstalk stores your application files and, optionally, server log files in Amazon S3.


2)Who should use AWS Elastic Beanstalk?
Those who want to deploy and manage their applications within minutes in the AWS Cloud. 
You don’t need experience with cloud computing to get started. AWS Elastic Beanstalk 
supports Java, .NET, PHP, Node.js, Python, Ruby, Go, and Docker web applications.

development stacks Also:
Apache Tomcat for Java applications
Apache HTTP Server for PHP applications
Apache HTTP Server for Python applications
Nginx or Apache HTTP Server for Node.js applications
Passenger or Puma for Ruby applications
Microsoft IIS 7.5, 8.0, and 8.5 for .NET applications



3)Why should I use IAM with AWS Elastic Beanstalk?
IAM allows you to manage users and groups in a centralized manner. 
You can control which IAM users have access to AWS Elastic Beanstalk, and limit permissions to read-only access to Elastic 
Beanstalk for operators who should not be able to perform actions against Elastic Beanstalk resources. 
All user activity within your account will be aggregated under a single AWS bill.


4)How much does AWS Elastic Beanstalk cost?
There is no additional charge for AWS Elastic Beanstalk–you pay only for the AWS resources actually used to store and run your application.



================================================= 
AWS Fargate FAQ:
================================================= 

1)What is AWS Fargate?
AWS Fargate is a serverless compute engine for containers that works with both Amazon Elastic Container Service (ECS) and 
Amazon Elastic Kubernetes Service (EKS). AWS Fargate makes it easy to focus on building your applications. 
Fargate eliminates the need to provision and manage servers.



2)What is the pricing of AWS Fargate?
With AWS Fargate, you pay only for the amount of vCPU, memory, and storage resources consumed by your containerized applications.
vCPU and memory resources are calculated from the time your container images are pulled until the 
Amazon ECS task or EKS pod terminates, rounded up to the nearest second.



3)How should I choose when to use AWS Fargate?  
Choose AWS Fargate for its isolation model and security. You should also select Fargate if you want to launch containers 
without having to provision or manage EC2 instances. If you require greater control of your 
EC2 instances or broader customization options, then use ECS or EKS without Fargate. 
Use EC2 for GPU workloads, which are not supported on Fargate today.



================================================= 
AWS Lambda FAQ:
================================================= 

1) What is serverless computing?
Serverless computing allows you to build and run applications and services without thinking about servers. 
With serverless computing, your application still runs on servers, but all the server management is done by AWS. 

At the core of serverless computing is AWS Lambda, which lets you run your code without provisioning or managing servers.


2)Can I access the infrastructure that AWS Lambda runs on?
No. AWS Lambda operates the compute infrastructure on your behalf, allowing it to perform health checks, 
apply security patches, and do other routine maintenance.



3)How does AWS Lambda secure my code?
AWS Lambda stores code in Amazon S3 and encrypts it at rest. 
AWS Lambda performs additional integrity checks while your code is in use.



4)What is an AWS Lambda function?
The code you run on AWS Lambda is uploaded as a “Lambda function”. Each function has associated configuration information, such as 
its name, description, entry point, and resource requirements. The code must be written in a “stateless” style i.e. it should assume 
there is no affinity to the underlying compute infrastructure. Local file system access, child processes, and similar artifacts 
may not extend beyond the lifetime of the request, and any persistent state should be stored in Amazon S3, Amazon DynamoDB, 
Amazon EFS, or another Internet-available storage service. Lambda functions can include libraries, even native ones.



5)Does AWS Lambda support environment variables?
Yes. You can easily create and modify environment variables from the AWS Lambda Console, CLI, or SDKs. 
To learn more about environment variables, see the documentation.



6)How do I use an AWS Lambda function to respond to Amazon CloudWatch alarms?
First, configure the alarm to send Amazon SNS notifications. Then from the AWS Lambda console, 
select a Lambda function and associate it with that Amazon SNS topic. 


7)How do I set up Amazon EFS for Lambda?
Developers can easily connect an existing EFS file system to a Lambda function via an EFS Access Point by using the console, CLI, or SDK. 
When the function is first invoked, the file system is automatically mounted and made available to function code.


6)What is AWS Lambda Extensions?
AWS Lambda Extensions lets you integrate Lambda with your favorite tools for monitoring, observability, security, and governance. 
Extensions enable you and your preferred tooling vendors to plug into Lambda’s lifecycle and integrate 
more deeply into the Lambda execution environment.




================================================= 
AWS Outposts FAQ:
================================================= 

Why would I use AWS Outposts rack instead of operating in an AWS Region?

You can use Outposts rack to support your applications that have low latency or local data processing requirements. 
These applications may need to generate near real-time responses to end user applications or need to communicate with other 
on-premises systems or control on-site equipment. These can include workloads running on factory floors for automated operations in 
manufacturing, real-time patient diagnosis or medical imaging, and content and media streaming. You can use Outposts rack to securely 
store and process customer data that needs to remain on premises or in countries where there is no AWS Region. You can run data intensive 
workloads on Outposts rack and process data locally when transmitting data to AWS Regions is expensive and 
wasteful and for better control on data analysis, backup and restore.




================================================= 
Storage FAQ:
================================================= 


Amazon EFS vs. Amazon EBS vs. Amazon S3?

AWS offers cloud storage services to support a wide range of storage workloads.

-Amazon EFS is a file storage service for use with Amazon compute (EC2, containers, serverless) and on-premises servers. 
EFS provides a file system interface, file system access semantics (such as strong consistency and file locking), 
and concurrently accessible storage for up to thousands of EC2 instances.

-Amazon Elastic Block Store (EBS) is a block-level storage service for use with EC2. Amazon EBS can deliver performance 
for workloads that require the lowest-latency access to data from a single EC2 instance.

-Amazon Simple Storage Service (S3) is an object storage service. Amazon S3 makes data available through an internet 
API that can be accessed anywhere.


1)What is AWS Storage Gateway?
The AWS Storage Gateway sits between your applications and Amazon storage services. 
AWS Storage Gateway is a hybrid cloud storage service that gives you on-premises access to virtually unlimited cloud storage. 
Storage Gateway provides a standard set of storage protocols such as iSCSI, SMB, and NFS, 
which allow you to use AWS storage without rewriting your existing applications.


2)Where can I deploy a Storage Gateway appliance?
On-premises, you can deploy a virtual machine containing the Storage Gateway software on VMware ESXi, 
Microsoft Hyper-V, or Linux KVM, or you can deploy Storage Gateway as a hardware appliance. 
You can also deploy the Storage Gateway VM in VMware Cloud on AWS, or as an AMI in Amazon EC2.



3)How does AWS Storage Gateway provide on-premises applications access to cloud storage?
Depending on your use case, Storage Gateway provides three types of storage interfaces for your on-premises 
applications: file, volume, and tape.

The Amazon S3 File Gateway enables you to store and retrieve objects in Amazon Simple Storage Service (S3) using 
file protocols such as Network File System (NFS) and Server Message Block (SMB). Objects written through S3 
File Gateway can be directly accessed in S3.

The Amazon FSx File Gateway enables you to store and retrieve files in Amazon FSx for Windows File Server using the SMB protocol. 
Files written through Amazon FSx File Gateway are directly accessible in Amazon FSx for Windows File Server.

The Volume Gateway provides block storage to your on-premises applications using iSCSI connectivity. 
Data on the volumes is stored in Amazon S3 and you can take point-in-time copies of volumes that are stored in AWS as 
Amazon EBS snapshots. You can also take copies of volumes and manage their retention using AWS Backup. 
You can restore EBS snapshots to a Volume Gateway volume or an EBS volume.

The Tape Gateway provides your backup application with an iSCSI virtual tape library (VTL) interface, consisting of a virtual media 
changer, virtual tape drives, and virtual tapes. 
Virtual tapes are stored in Amazon S3 and can be archived to Amazon S3 Glacier or Amazon S3 Glacier Deep Archive.


4)Can I use AWS Storage Gateway with AWS Direct Connect?
Yes, you can use AWS Direct Connect to increase throughput and reduce your network costs by establishing a dedicated network 
connection between your on-premises gateway and AWS. Note that AWS Storage Gateway efficiently uses your 
internet bandwidth to help speed up the upload of your on-premises application data to AWS.


5)How will I be billed for my use of AWS Storage Gateway?
There are 3 elements to how you will be billed for AWS Storage Gateway: Storage, requests, and data transfer. 



================================================= 
Amazon EBS FAQs:
================================================= 

1)Amazon EBS volumes Type?
Amazon EBS provides seven volume types: Provisioned IOPS SSD (io2 Block Express, io2, and io1), 
General Purpose SSD (gp3 and gp2), Throughput Optimized HDD (st1) and Cold HDD (sc1). 


2)Does EBS encryption support boot volumes?
Yes.


================================================= 
Amazon EFS FAQs:
================================================= 

Amazon EFS is compatible with all Linux-based AMIs for Amazon EC2.


1)What is Amazon Elastic File System?
Amazon Elastic File System (Amazon EFS) is a simple, serverless, set-and-forget elastic file system that makes it easy to set up, 
scale, and cost-optimize file storage in AWS. With a few clicks in the AWS Management Console, 
you can create file systems that are accessible to Amazon Elastic Compute Cloud (EC2) instances, 
Amazon container services (Amazon Elastic Container Service [ECS], Amazon Elastic Kubernetes Service [EKS], and AWS Fargate), and 
AWS Lambda functions through a file system interface (using standard operating system file I/O APIs). 
They also support full file system access semantics, such as strong consistency and file locking.



2)How do I access a file system from an Amazon EC2 instance?
To access your file system, mount the file system on an Amazon EC2 Linux-based instance using the standard Linux mount command and the 
 system’s DNS name. To simplify accessing your Amazon EFS file systems, we recommend using the Amazon EFS mount helper utility. 
 Once mounted, you can work with the files and directories in your file system just like you would with a local file system.
EFS uses the Network File System version 4 (NFS v4) protocol. For a step-by-step example of how to access a file system from an 
EC2 instance, see the guide here.



3)What is an Amazon EFS Access Point?
Amazon EFS Access Points simplify providing applications with access to shared datasets in an Amazon EFS file system. 
Amazon EFS Access Points work together with AWS IAM and enforce an operating system user and group, and a directory for every file 
system request made through the access point. You can create multiple access points per file system and use them to provide access 
to specific applications.



4)How do I access an Amazon EFS file system from servers in my on-premises datacenter?
To access Amazon EFS file systems from on premises, you must have an AWS Direct Connect or 
AWS VPN connection between your on-premises datacenter and your Amazon VPC.
You mount an Amazon EFS file system on your on-premises Linux server using the standard Linux mount command for mounting a file system 
using the NFS v4.1 protocol.

You can access your Amazon EFS file system concurrently from servers in your on-premises datacenter as well as Amazon EC2 instances in your Amazon VPC.


5)How much does Amazon EFS cost?

With Amazon EFS, you pay only for what you use per month.
When using the Provisioned Throughput mode, you pay for the throughput you provision per month. 
There is no minimum fee and no setup charges.



================================================= 
Amazon FSx for Lustre FAQs:
================================================= 

1)What is Amazon FSx for Lustre?
Amazon FSx for Lustre makes it easy and cost effective to launch, run, and scale the world’s most popular high-performance file system.
The open source Lustre file system is designed for applications that require fast storage – where you want your storage to keep up 
with your compute. Lustre was built to solve the problem of quickly and cheaply processing the world’s ever-growing data sets, 
and it’s the most widely used file system for the 500 fastest computers in the world.


2)What use cases does Amazon FSx for Lustre support?
Use Amazon FSx for Lustre for workloads where speed matters, such as machine learning, 
high performance computing (HPC), video processing, financial modeling, genome sequencing, and electronic design automation (EDA).


3)What is the difference between scratch and persistent deployment options?
Amazon FSx for Lustre provides two deployment options: scratch and persistent.
Scratch file systems are designed for temporary storage and shorter-term processing of data. Data is not replicated and does not persist if a file server fails.
Persistent file systems are designed for longer-term storage and workloads. The file servers are highly available, 
and data is automatically replicated within the AWS Availability Zone (AZ) that is associated with the file system. 
The data volumes attached to the file servers are replicated independently from the file servers to which they are attached.


4)What instance types and AMIs work with Amazon FSx for Lustre?
FSx for Lustre is compatible with the most popular Linux-based AMIs, including Amazon Linux, Amazon Linux 2, 
Red Hat Enterprise Linux (RHEL), CentOS, SUSE Linux and Ubuntu.


5)How do I monitor my file system’s activity?
Amazon FSx for Lustre provides native CloudWatch integration, allowing you to monitor file system health and performance 
metrics in real time.


6) How many instances can connect to a file system?
An FSx for Lustre file system can be concurrently accessed by thousands of compute instances.


7)How will I be charged and billed for my use of Amazon FSx for Lustre?
You pay only for the resources you use.
Storage capacity scaling requests are processed by adding new storage capacity to your file system. 
You will be billed for new storage capacity 
once the new file servers have been added to your file system, and the file system status changes from UPDATING to AVAILABLE.



================================================= 
Amazon FSx for Windows File Server FAQs:
================================================= 

1) What is Amazon FSx for Windows File Server?
Amazon FSx for Windows File Server provides fully managed, highly reliable, and scalable file storage that is 
accessible over the industry-standard Service Message Block (SMB) protocol. It is built on Windows Server, 
delivering a wide range of administrative features such as user quotas, end-user file restore, and 
Microsoft Active Directory (AD) integration.



2)How will I be charged and billed for my use of Amazon FSx for Windows File Server?
You pay only for the resources you use. You are billed hourly for your file systems, based on their deployment type 
(Single-AZ or Multi-AZ), storage type (SSD or HDD), storage capacity (priced per GB-month), and throughput capacity 
(priced per MBps-month). You are billed hourly for your backup storage (priced per GB-month). 
For pricing information, please visit the Amazon FSx pricing page.



================================================= 
Amazon S3 FAQ:
================================================= 

1) Does Amazon store its own data in Amazon S3?
Yes. Developers within Amazon use Amazon S3 for a wide variety of projects. Many of these projects use 
Amazon S3 as their authoritative data store and rely on it for business-critical operations.


2)Where is my data stored?
You specify an AWS Region when you create your Amazon S3 bucket. For S3 Standard, S3 Standard-IA, S3 Intelligent-Tiering, S3 Glacier 
Instant Retrieval, S3 Glacier Flexible Retrieval, and S3 Glacier Deep Archive storage classes, your objects are automatically stored 
across multiple devices spanning a minimum of three Availability Zones, each separated by miles across an AWS Region. Objects stored 
in the S3 One Zone-IA storage class are stored redundantly within a single Availability Zone in the AWS Region you select. 
For S3 on Outposts, your data is stored in your Outpost on-premises environment, unless you manually choose to transfer it to an 
AWS Region. Please refer to Regional Products and Services for details of Amazon S3 service availability by AWS Region.


3)How much does Amazon S3 cost?
With Amazon S3, you pay only for what you use. There is no minimum charge. 
You can estimate your monthly bill using the AWS Pricing Calculator.


4)What are Amazon S3 Event Notifications?
You can enable Amazon S3 Event Notifications and receive them in response to specific events in your S3 bucket, 
such as PUT, POST, COPY, and DELETE events. You can publish notifications to Amazon EventBridge, 
Amazon SNS, Amazon SQS, or directly to AWS Lambda.


Amazon S3 Event Notifications let you to run workflows, send alerts, or perform other actions in response to 
changes in your objects stored in S3. 


5) What is S3 Transfer Acceleration?
Amazon S3 Transfer Acceleration creates fast, easy, and secure transfers of files over long distances between your client and your 
Amazon S3 bucket. S3 Transfer Acceleration leverages Amazon CloudFront’s globally distributed AWS Edge Locations. 
As data arrives at an AWS Edge Location, data is routed to your Amazon S3 bucket over an optimized network path.


6)How secure is my data in Amazon S3?     
Amazon S3 is secure by default. Upon creation, only you have access to Amazon S3 buckets that you create, 
and you have complete control over who has access to your data. Amazon S3 supports user authentication to control access to data. 
You can use access control mechanisms such as bucket policies to selectively grant permissions to users and groups of users. 
The Amazon S3 console highlights your publicly accessible buckets, indicates the source of public accessibility, and also warns 
you if changes to your bucket policies or bucket ACLs would make your bucket publicly accessible. You should enable Block Public 
Access for all accounts and buckets that you do not want publicly accessible. 

You can securely upload/download your data to Amazon S3 via SSL endpoints using the HTTPS protocol. 
If you need extra security you can use the Server-Side Encryption (SSE) option to encrypt data stored at rest. 
You can configure your Amazon S3 buckets to automatically encrypt objects before storing them if the incoming storage 
requests do not have any encryption information. Alternatively, you can use your own encryption libraries to encrypt data before 
storing it in Amazon S3.



7)What is S3 Select?
S3 Select is an Amazon S3 feature that makes it easy to retrieve specific data from the contents of an object using simple 
SQL expressions without having to retrieve the entire object. S3 Select simplifies and improves the performance of scanning and 
filtering the contents of objects into a smaller, targeted dataset by up to 400%. With S3 Select, you can also perform operational 
investigations on log files in Amazon S3 without the need to operate or manage a compute cluster. 




================================================= 
AWS Backup FAQs:
================================================= 


1)What is AWS Backup?
AWS Backup is a fully managed service that enables you to centralize and automate data protection across on-premises and AWS services. 
Together with AWS Organizations, AWS Backup allows you to centrally deploy data protection (backup) policies to configure, manage, 
and govern your backup activity across your organization’s AWS accounts and resources. AWS Backup also enables you to audit and 
report on the compliance of your data protection policies with AWS Backup Audit Manager.


2)What can I back up using AWS Backup?
You can use AWS Backup to create and manage the backups of the following AWS services:
Amazon Elastic Block Store (Amazon EBS) volumes
Amazon Elastic Compute Cloud (Amazon EC2) instances (including Windows applications)
Windows Volume Shadow Copy Service (VSS) supported applications (including Windows Server, Microsoft SQL Server, 
and Microsoft Exchange Server) on Amazon EC2.
Amazon Relational Database Service (Amazon RDS) databases (including Amazon Aurora clusters)
Amazon DynamoDB tables, Amazon Elastic File System (Amazon EFS) file systems
Amazon FSx for NetApp ONTAP file systems
Amazon FSx for OpenZFS file systems
Amazon FSx for Windows File Server file systems
Amazon FSx for Lustre file systems
Amazon Neptune databases
Amazon DocumentDB (with MongoDB compatibility) databases
AWS Storage Gateway volumes
Amazon Simple Storage Service (Amazon S3).
You can also use AWS Backup to create and manage backups of Amazon Outposts, 
VMware CloudTM on AWS, and on-premises VMware virtual machines.



3)Can I use AWS Backup to back up on-premises data?
Yes, you can use AWS Backup to back up your on-premises Storage Gateway volumes and VMware virtual machines, 
providing a common way to manage the backups of your application data both on premises and on AWS.



4)What is a backup plan?
A backup plan is a policy expression that defines when and how you want to back up your AWS resources, such as DynamoDB tables or 
EFS file systems. You assign resources to backup plans and AWS Backup will then automatically make and retain backups for those 
resources according to the backup plan. Backup plans are composed of one or more backup rules. Each backup rule is composed of 
1) a backup schedule, which includes the backup frequency (Recovery Point Objective - RPO) and backup window, 
2) a lifecycle rule that specifies when to transition a backup from one storage tier to another and when to expire the recovery point, 
3) the backup vault in which to place the created recovery points, and 
4) the tags to be added to backups upon creation. For example, a backup plan might have a “daily backup rule” and a “monthly backup rule.”
 The daily rule backs up resources every day at midnight and retains the backups for one month. 
 The monthly rule takes a backup once a month on the beginning of every month and retains the backups for one year.
 
 
5)What is AWS Backup Audit Manager?
AWS Backup Audit Manager allows you to audit and report on the compliance of your data protection policies to 
help you meet your business and regulatory needs. AWS Backup enables you to centralize and automate data protection 
policies across AWS services based on organizational best practices and regulatory standards, and AWS Backup Audit 
Manager helps you maintain and demonstrate compliance to those policies.


6)What is AWS Backup Audit Manager?
AWS Backup Audit Manager allows you to audit and report on the compliance of your data protection policies to help you meet your 
business and regulatory needs. AWS Backup enables you to centralize and automate data protection policies across AWS services based 
on organizational best practices and regulatory standards, and AWS Backup Audit Manager helps you maintain and demonstrate compliance 
to those policies.


7)What is AWS Backup Vault Lock?
AWS Backup Vault Lock is a feature that enables you to prevent changes to backup lifecycle as well as prevent manual deletion of backups,
 helping you meet your compliance requirements. AWS Backup Vault Lock implements safeguards that ensure you are 
 storing your backups using a Write-Once-Read-Many (WORM) model.



================================================= 
AWS DataSync FAQs:
================================================= 

1)  What is AWS DataSync?
AWS DataSync is an online data transfer service that simplifies, automates, and accelerates copying large amounts of data between 
on-premises storage systems and AWS Storage services, as well as between AWS Storage services.

AWS DataSync reduces the complexity and cost of online data transfer, making it simple to transfer datasets 
between on-premises, edge, or other cloud storage and AWS Storage services, as well as between AWS Storage services.

2)Where can I move data to and from?
DataSync supports the following storage location types: Network File System (NFS) shares, Server Message Block (SMB) shares, 
Hadoop Distributed File Systems (HDFS), self-managed object storage, Google Cloud Storage, Azure Files, AWS Snowcone, 
Amazon Simple Storage Service (Amazon S3), Amazon Elastic File System (Amazon EFS) file systems, Amazon FSx for 
Windows File Server file systems, Amazon FSx for Lustre file systems, Amazon FSx for OpenZFS file systems, and 
Amazon FSx for NetApp ONTAP file systems.


3)How do I deploy an AWS DataSync agent?
You deploy an AWS DataSync agent to your on-premises hypervisor, in your public cloud environment, or in Amazon EC2. 
To copy data to or from an on-premises file server, you download the agent virtual machine image from the AWS Console and 
deploy to your on-premises VMware ESXi, Linux Kernel-based Virtual Machine (KVM), or Microsoft Hyper-V hypervisor. 
When a DataSync agent is used, the agent must be deployed so that it can access your file server using the NFS, SMB protocol.



4)How does AWS DataSync access my Amazon EFS file system?
AWS DataSync accesses your Amazon EFS file system using the NFS protocol. The DataSync service mounts your file system from within 
your VPC from Elastic Network Interfaces (ENIs) managed by the DataSync service. DataSync fully manages the creation, use, 
and deletion of these ENIs on your behalf. You can choose to mount your EFS file system using a mount target or an EFS Access Point.




================================================= 
AWS Snow FAQs:
================================================= 

1)What is AWS Snowball?
AWS Snowball is a service that provides secure, rugged devices, so you can bring AWS computing and storage 
capabilities to your edge environments, and transfer data into and out of AWS. 


2)Who should use Snowball Edge?
Consider Snowball Edge if you need to run computing in rugged, austere, mobile, or disconnected (or intermittently connected) environments. 
Also consider it for large-scale data transfers and migrations when bandwidth is not available for use of a 
high-speed online transfer service, such as AWS DataSync.


3)What is AWS OpsHub for Snow Family?
AWS OpsHub is an application that you can download from the Snowball resources page. 
It offers a graphical user interface for managing the AWS Snow Family devices. AWS OpsHub makes it easy to setup and manage 
AWS Snowball devices enabling you to rapidly deploy edge computing workloads and simplify data migration to the cloud. 


4) How does AWS OpsHub for Snow Family work?
AWS OpsHub is an application that you can download and install on any Windows or Mac client machine, such as a laptop. 
Once you have installed AWS OpsHub and have your AWS Snow Family device on site, open AWS OpsHub and unlock the device. 
You will then be presented with a dashboard showing your device and its system metrics. 
You can then begin deploying your edge applications or migrating your data to the device with just a few clicks.



5) What is AWS Snowmobile?
AWS Snowmobile is the first exabyte-scale data migration service that allows you to move very large datasets from on-premises to AWS. 
Each Snowmobile is a secured data truck with up to 100PB storage capacity that can be dispatched to your site and connected directly 
to your network backbone to perform high-speed data migration. You can quickly migrate an exabyte of 
data with ten Snowmobiles in parallel from a single location or multiple data centers. 
Snowmobile is offered by AWS as a managed service.



================================================= 
AWS Transfer for SFTP FAQ:
================================================= 

1) Why should I use the AWS Transfer Family?
AWS Transfer Family supports multiple protocols for business-to-business (B2B) file transfers so data can easily and securely be 
exchanged across stakeholders, third-party vendors, business partners, or customers. Without using Transfer Family, 
you have to host and manage your own file transfer service which requires you to invest in operating and managing infrastructure, 
patching servers, monitoring for uptime and availability, and building one-off mechanisms to provision users and audit their activity. 


2) How do I get started with AWS Transfer for SFTP, FTPS, and FTP?
In 3 simple steps, you get an always-on server endpoint enabled for SFTP, FTPS, and/or FTP. First, you select the protocol(s) 
you want to enable your end users to connect to your endpoint. Next, you configure user access using AWS Transfer 
Family built-in authentication manager (service managed), Microsoft Active Directory (AD), or by integrating your own or a 
third party identity provider such as Okta or Microsoft AzureAD (“BYO” authentication). Finally, select the server to access 
S3 buckets or EFS file systems. Once the protocol(s), identity provider, and the access to file systems are enabled, 
your users can continue to use their existing SFTP, FTPS, or FTP clients and configurations, 
while the data accessed is stored in the chosen file systems.


3)Which protocols should I use for securing data while in-transit over a public network?
Either SFTP or FTPS should be used for secure transfers over public networks. 
Due to the underlying security of the protocols based on SSH and TLS cryptographic algorithms, 
data and commands are transferred through a secure, encrypted channel.


4) How am I billed for use of the service?
You are billed on an hourly basis for each of the protocols enabled, from the time you create and configure your server endpoint, 
until the time you delete it. You are also billed based on the amount of data uploaded and downloaded over 
SFTP, FTPS, or FTP and number of messages exchanged over AS2.



================================================= 
Amazon Aurora FAQs:
================================================= 

1) What is Amazon Aurora?
Amazon Aurora is a modern relational database service offering performance and high availability at scale, fully open source 
MySQL- and PostgreSQL-compatible editions, and a range of developer tools for building serverless and machine 
learning (ML)-driven applications.

Aurora features a distributed, fault-tolerant, and self-healing storage system that is decoupled from compute 
resources and auto-scales up to 128 TB per database instance. It delivers high performance and availability with up to 15 low-latency 
read replicas, point-in-time recovery, continuous backup to Amazon Simple Storage Service (Amazon S3), 
and replication across three Availability Zones (AZs).


2)What does "MySQL compatible" mean?
Amazon Aurora is drop-in compatible with existing MySQL open-source databases and adds support for new releases regularly. 
This means you can easily migrate MySQL databases to and from Aurora using standard import/export tools or snapshots. 
It also means that most of the code, applications, drivers, and tools you already use with MySQL databases 
today can be used with Aurora with little or no change. When considering Aurora vs. 
MySQL, it is important to understand that the Amazon Aurora database engine is designed to be 
wire-compatible with MySQL 5.6 and 5.7 using the InnoDB storage engine. 
This makes it easy to move applications between the two engines. 
Certain MySQL features, such as the MyISAM storage engine, can’t be used with persistent tables.


3)What does “PostgreSQL compatible” mean?
This means you can easily migrate PostgreSQL databases to and from Aurora using standard import/export tools or snapshots.
It also means that most of the code, applications, drivers, and tools you already use with 
PostgreSQL databases today can be used with Aurora with little or no change. 


4)What are the minimum and maximum storage limits of an Amazon Aurora database?
The minimum storage is 10 GB. Based on your database usage, your Amazon Aurora storage will 
automatically grow, up to 128 TB, in 10 GB increments with no impact to database performance. 
There is no need to provision storage in advance.



5)How do I enable backups for my DB Instance?
Automated backups are always enabled on Amazon Aurora DB Instances. Backups do not impact database performance.


6)Can I take DB Snapshots and keep them around as long as I want?
Yes, and there is no performance impact when taking snapshots. 
Note that restoring data from DB Snapshots requires creating a new DB Instance.


7)If my database fails, what is my recovery path?
Amazon Aurora automatically maintains six copies of your data across three Availability Zones (AZs) and will 
automatically attempt to recover your database in a healthy AZ with no data loss. 
In the unlikely event your data is unavailable within Amazon Aurora storage, you can restore from a DB 
Snapshot or perform a point-in-time restore operation to a new instance. 
Note that the latest restorable time for a point-in-time restore operation can be up to five minutes in the past.


8)How do I access my Amazon Aurora database?
Amazon Aurora databases must be accessed through the database port entered on database creation. 
This provides an additional layer of security for your data. 


9)What is Amazon Aurora Serverless?
Aurora Serverless is an on-demand, auto-scaling configuration for Amazon Aurora. It enables you to run your database in the 
cloud without managing database capacity. Manually managing database capacity can take up valuable time and can lead to inefficient 
use of database resources. With Aurora Serverless, you simply create a database, specify the desired database capacity range, and 
connect your application. Aurora automatically adjusts the capacity within the range your specified based on your application’s needs. 
You pay on a per-second basis for the database capacity you use when the database is active.



10)What all Aurora features does Aurora Serverless v2 support?
Aurora Serverless v2 supports all features of provisioned Aurora, including read replica, 
multi-AZ configuration, Global Database, RDS proxy, and Performance Insights.


11)What is Amazon DevOps Guru for RDS?
Amazon DevOps Guru for RDS is a new ML-powered capability for Amazon RDS that is designed to automatically detect and 
diagnose database performance and operational issues, enabling you to resolve issues in minutes rather than days.



================================================= 
Amazon RDS FAQs:
================================================= 

1)Which relational database engines does Amazon RDS support?
Amazon RDS supports Amazon Aurora, MySQL, MariaDB, Oracle, SQL Server, and PostgreSQL database engines.


2)What is a database instance (DB instance)?
You can think of a DB instance as a database environment in the cloud with the compute and storage resources you specify. 
You can create and delete DB instances; define/refine infrastructure attributes of your DB instance(s); and control access and 
security via the AWS Management Console, Amazon RDS APIs, and AWS Command Line Interface. You can run one or more 
DB instances and each DB instance can support one or more databases or database schemas, depending on engine type.



3)How do I import data into an Amazon RDS DB instance?
There are a number of simple ways to import data into Amazon RDS, such as with the mysqldump or mysqlimport utilities for MySQL; 
Data Pump, import/export, or SQL Loader for Oracle; Import/Export wizard, full backup files (.bak files), or 
Bulk Copy Program (BCP) for SQL Server; or pg_dump for PostgreSQL


4)Can I test my DB instance with a new version before upgrading?
Yes. You can do so by creating a DB snapshot of your existing DB instance, restoring from the DB snapshot to create a new DB instance, 
and then initiating a version upgrade for the new DB instance.




================================================= 
Amazon ElastiCache FAQ:
================================================= 

1) What is Amazon ElastiCache?
Amazon ElastiCache is a web service that makes it easy to deploy and run Memcached or Redis protocol-compliant server nodes in the cloud. 
Amazon ElastiCache improves the performance of web applications by allowing you to retrieve information from a fast, managed, 
in-memory system, instead of relying entirely on slower disk-based databases. 


2)Which engines does Amazon ElastiCache support?
Amazon ElastiCache offers fully managed Redis, voted the most loved database by developers in the Stack Overflow Developer Survey for 5 years in a row, and 
Memcached for your most demanding applications that require sub-millisecond response times.



================================================= 
Amazon Redshift FAQs:
================================================= 

What is a Data Warehouse?
A data warehouse is a system that pulls together data from many different sources within an organization for reporting and analysis. 
The reports created from complex queries within a data warehouse are used to make business decisions.
A data warehouse stores historical data about your business so that you can analyze and extract insights from it. 
It does not store current information, nor is it updated in real-time.



1)What is Amazon Redshift?
Amazon Redshift is a fully managed, scalable cloud data warehouse that accelerates your time to 
insights with fast, easy, and secure analytics at scale. 
Thousands of customers rely on Amazon Redshift to analyze data from terabytes to petabytes and run complex analytical queries.



2)What is Amazon Redshift Serverless (preview)?
Amazon Redshift Serverless (preview) is a serverless option of Amazon Redshift that makes it easy to run and scale analytics in 
seconds without the need to set up and manage data warehouse infrastructure. With Redshift Serverless, any user—including data analysts, 
developers, business professionals, and data scientists—can get insights from data by simply 
loading and querying data in the data warehouse.



3)How does Amazon Redshift keep my data secure?
Amazon Redshift supports industry-leading security with built-in AWS IAM integration, identity federation for single-sign on (SSO), 
multi-factor authentication, column-level access control, row-level security, Amazon Virtual Private Cloud (Amazon VPC), and provides 
built-in AWS KMS integration to protect your data in transit and at rest. Amazon Redshift encrypts and keeps your data secure 
in transit and at rest using industry-standard encryption techniques. To keep data secure in transit, Amazon Redshift supports 
SSL-enabled connections between your client application and your Redshift data warehouse cluster. To keep your data secure at rest, 
Amazon Redshift encrypts each block using hardware-accelerated AES-256 as it is written to disk.





================================================= 
Amazon Neptune FAQs:
================================================= 
1) What is Amazon Neptune?
Amazon Neptune is a fast, reliable, fully-managed graph database service that makes it easy to build and run 
applications that work with highly connected datasets. 


2)What popular graph query languages does Amazon Neptune support?
Amazon Neptune supports both the open source Apache TinkerPop Gremlin graph traversal language and the 
W3C standard Resource Description Framework’s (RDF) SPARQL query language.



3)How do I access my Amazon Neptune database?
Access to Amazon Neptune databases must be done through the HTTP port entered on database creation within your VPC. 



================================================= 
AWS Database Migration Service FAQs:
================================================= 


1)What sources and targets does AWS Database Migration Service support?
AWS Database Migration Service (DMS) supports a range of homogeneous and heterogeneous data replications.
Either the source or the target database (or both) need to reside in RDS or on EC2. 
Replication between on-premises to on-premises databases is not supported.


=====================================================
Amazon DocumentDB (with MongoDB compatibility) FAQs:
=====================================================

1)What is Amazon DocumentDB (with MongoDB compatibility)?
Amazon DocumentDB (with MongoDB compatibility) is a fast, scalable, highly available, and fully managed document database service 
that supports MongoDB workloads. As a document database, Amazon DocumentDB makes it easy to store, query, and index JSON data. 
Developers can use the same MongoDB application code, drivers, and tools as they do 
today to run, manage, and scale workloads on Amazon DocumentDB.



2)What does "MongoDB-compatible" mean?
MongoDB compatible” means that Amazon DocumentDB interacts with the Apache 2.0 open source 
MongoDB 3.6 and 4.0 APIs. As a result, you can use the 
same MongoDB drivers, applications, and tools with Amazon DocumentDB with little or no changes. 




================================================= 
Amazon VPC FAQs:
================================================= 

1)Why should I use Amazon VPC?
Amazon VPC enables you to build a virtual network in the AWS cloud - no VPNs, hardware, or physical datacenters required. 
You can define your own network space, and control how your network and the Amazon EC2 resources inside 
your network are exposed to the Internet. You can also leverage the enhanced security options in 
Amazon VPC to provide more granular access to and from the Amazon EC2 instances in your virtual network.



2)What are the components of Amazon VPC?
-A Virtual Private Cloud: A logically isolated virtual network in the AWS cloud. 
You define a VPC’s IP address space from ranges you select.

-Subnet: A segment of a VPC’s IP address range where you can place groups of isolated resources.

-Internet Gateway: The Amazon VPC side of a connection to the public Internet.

-NAT Gateway: A highly available, managed Network Address Translation (NAT) service for your 
resources in a private subnet to access the Internet.

-Virtual private gateway: The Amazon VPC side of a VPN connection.

-Peering Connection: A peering connection enables you to route traffic via private IP addresses between two peered VPCs.

-VPC Endpoints: Enables private connectivity to services hosted in AWS, from within your 
VPC without using an Internet Gateway, VPN, Network Address Translation (NAT) devices, or firewall proxies.

A VPC endpoint allows you to privately connect to other services in AWS. These endpoints include gateway endpoints, 
interface endpoints, and load balancer endpoints.
AWS VPC endpoints allow you to connect to S3 (for instance) from your network without the use of the 
internet (internet gateway, NAT gateway).


-Egress-only Internet Gateway: A stateful gateway to provide egress only access for IPv6 traffic from the VPC to the Internet.


3) How will I be charged and billed for my use of Amazon VPC?
There are no additional charges for creating and using the VPC itself. 
If you connect your VPC to your corporate datacenter using the optional hardware VPN connection, pricing is per VPN connection-hour 
(the amount of time you have a VPN connection in the "available" state.) Partial hours are billed as full hours. 
Data transferred over VPN connections will be charged at standard AWS Data Transfer rates.


4)What are the connectivity options for my Amazon VPC?
-The internet (via an internet gateway)
-Your corporate data center using an AWS Site-to-Site VPN connection (via the virtual private gateway)
-Both the internet and your corporate data center (utilizing both an internet gateway and a virtual private gateway)
-Other AWS services (via internet gateway, NAT, virtual private gateway, or VPC endpoints)
-Other Amazon VPCs (via VPC peering connections)


5) When is an IP address considered a Public IP address?
Any IP address that is assigned to an instance or a service hosted in a VPC that can be accessed over the internet is considered a 
public IP address. Only public IPv4 addresses, including Elastic IP addresses (EIPs) and IPv6 GUA can be routable on the internet. 
To do so, you would need to first connect the VPC to the internet and then update the route table to 
make them reachable to/from the internet.


6)How do instances without public IP addresses access the Internet?
Instances without public IP addresses can access the Internet in one of two ways:
Instances without public IP addresses can route their traffic through a NAT gateway or a NAT instance to access the Internet. 
These instances use the public IP address of the NAT gateway or NAT instance to traverse the Internet. 
The NAT gateway or NAT instance allows outbound communication but doesn’t allow machines on the Internet to initiate a 
connection to the privately addressed instances.

For VPCs with a hardware VPN connection or Direct Connect connection, instances can route their Internet traffic down the 
virtual private gateway to your existing datacenter. From there, it can access the Internet via your existing egress points 
and network security/monitoring devices.


7)Can I connect to my VPC using a software VPN?
Yes. You may use a third-party software VPN to create a site to site or remote access 
VPN connection with your VPC via the Internet gateway.



8)Does traffic go over the internet when two instances communicate using public IP addresses, or when instances 
communicate with a public AWS service endpoint?
No. When using public IP addresses, all communication between instances and services hosted in AWS use AWS's private network. 
Packets that originate from the AWS network with a destination on the AWS network stay on the AWS global network, except 
traffic to or from AWS China Regions.
 
In addition, all data flowing across the AWS global network that interconnects our data centers and Regions is 
automatically encrypted at the physical layer before it leaves our secured facilities. 
Additional encryption layers exist as well; for example, all VPC cross-region peering traffic, 
and customer or service-to-service Transport Layer Security (TLS) connections. 


9)What IP address ranges are assigned to a default Amazon VPC?
Default VPCs are assigned a CIDR range of 172.31.0.0/16. Default subnets within a default VPC are assigned /20 
netblocks within the VPC CIDR range. 



10)How large of a VPC can I create?
Currently, Amazon VPC supports five (5) IP address ranges, one (1) primary and four (4) secondary for IPv4. 
Each of these ranges can be between /28 (in CIDR notation) and /16 in size. 
The IP address ranges of your VPC should not overlap with the IP address ranges of your existing network.
For IPv6, the VPC is a fixed size of /56 (in CIDR notation). A VPC can have both IPv4 and IPv6 CIDR blocks associated to it.



11)Is there a limit on how large or small a subnet can be?
The minimum size of a subnet is a /28 (or 14 IP addresses.) for IPv4. Subnets cannot be larger than the VPC in which they are created.
For IPv6, the subnet size is fixed to be a /64. Only one IPv6 CIDR block can be allocated to a subnet.


12)Can I assign any IP address to an instance?
You can assign any IP address to your instance as long as it is:
Part of the associated subnet's IP address range
Not reserved by Amazon for IP networking purposes
Not currently assigned to another interfac
Not currently assigned to another interfac




13)Can I assign multiple IP addresses to an instance?
Yes. You can assign one or more secondary private IP addresses to an Elastic Network Interface or an EC2 instance in Amazon VPC. 
The number of secondary private IP addresses you can assign depends on the instance type. 


14) Can I monitor the network traffic in my VPC?
Yes. You can use Amazon VPC traffic mirroring and Amazon VPC flow logs features to monitor the network traffic in your Amazon VPC.

VPC flow logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC. 
Flow logs data can be published to either Amazon CloudWatch Logs or Amazon S3.


15)What is Amazon VPC traffic mirroring?
Amazon VPC traffic mirroring makes it easy for customers to replicate network traffic to and from an 
Amazon EC2 instance and forward it to out-of-band security and monitoring appliances for use-cases such as content inspection, 
threat monitoring, and troubleshooting. 



16)Can a VPC span multiple Availability Zones?
Yes. 


17) Can a subnet span Availability Zones?
No. A subnet must reside within a single Availability Zone.


18)Am I charged for network bandwidth between instances in different subnets?
If the instances reside in subnets in different Availability Zones, you will be charged $0.01 per GB for data transfer.



19)What are instance hostnames?
When you launch an instance, it is assigned a hostname. There are two options available, an IP based name or a Resource based name, 
and this parameter is configurable at instance launch. 
The IP based name uses a form of the Private IPv4 address while the Resource based name uses a form of the instance-id.

 you can change the hostname of an instance form IP based to 
 Resource based or vice versa by stopping the instance and then changing the resource based naming options.
 the instance hostname can be used as DNS hostnames.



20)What is AWS PrivateLink?
AWS PrivateLink enables customers to access services hosted on AWS in a highly available and scalable manner, 
while keeping all the network traffic within the AWS network.



21)How many VPCs, subnets,  es, and internet gateways can I create?

Five Amazon VPCs per AWS account per region
Two hundred subnets per Amazon VPC
Five Amazon VPC Elastic IP addresses per AWS account per region
One internet gateway per Amazon VPC




================================================= 
Amazon CloudFront FAQs:
================================================= 

With CloudFront, your files are delivered to end-users using a global network of edge locations.
Amazon CloudFront is a good choice for distribution of frequently accessed static content that benefits from edge 
delivery—like popular website images, videos, media files or software downloads.

1)How does Amazon CloudFront speed up my entire website?
Amazon CloudFront uses standard cache control headers you set on your files to identify static and dynamic content. 
Delivering all your content using a single Amazon CloudFront distribution helps you make sure that performance 
optimizations are applied to your entire website or web application.


2)What types of content does Amazon CloudFront support?
Amazon CloudFront supports content that can be sent using the HTTP or WebSocket protocols. 
This includes dynamic web pages and applications, such as HTML or PHP pages or WebSocket-based applications, 
and any popular static files that are a part of your web application, such as website images, audio, video, 
media files or software downloads. Amazon CloudFront also supports delivery of live or on-demand media streaming over HTTP.


3)What types of HTTP requests are supported by Amazon CloudFront?
Amazon CloudFront currently supports GET, HEAD, POST, PUT, PATCH, DELETE and OPTIONS reques

4)What are WebSockets?
WebSocket is a real-time communication protocol that provides bidirectional communication between a client and a server 
over a long-held TCP connection.



4) What is streaming? Why would I want to stream?
Generally, streaming refers to delivering audio and video to end users over the Internet without having to download the 
media file prior to playback. The protocols used for streaming include those that use HTTP for delivery such as Apple’s 
HTTP Live Streaming (HLS), MPEG Dynamic Adaptive Streaming over HTTP (MPEG-DASH), 
Adobe’s HTTP Dynamic Streaming (HDS) and Microsoft’s Smooth Streaming.



5)What is Lambda@Edge?
Lambda@Edge  is an extension of AWS Lambda allowing you to run code at global edge locations without provisioning or managing servers. 
Lambda@Edge offers powerful and flexible serverless computing for complex functions and full application logic closer to your viewers. 




================================================= 
Amazon Route 53 FAQs:
================================================= 

1) What is Private DNS?
Private DNS is a Route 53 feature that lets you have authoritative DNS within your VPCs without exposing your 
DNS records (including the name of the resource and its IP address(es) to the Internet.




================================================= 
Amazon API Gateway FAQs:
================================================= 


1)What is Amazon API Gateway?
Amazon API Gateway is a fully managed service that makes it easy for developers to publish, maintain, monitor, 
and secure APIs at any scale.


2)How am I charged for using Amazon API Gateway?
Amazon API Gateway bills per million API calls, plus the cost of data transfer out, in gigabytes. 
If you choose to provision a cache for your API, hourly rates apply. For WebSocket APIs, API Gateway 
bills based on messages sent and received and the number of minutes a client is connected to the API.



================================================= 
AWS App Mesh FAQs:
================================================= 

1)What is AWS App Mesh?

AWS App Mesh makes it easy to monitor, control, and debug the communications between services.

App Mesh makes it easier to get visibility, security, and control over the communications between your services 
without writing new code or running additional AWS infrastructure. Using App Mesh, you can standardize 
how services communicate, implement rules for communications between services, and capture metrics, logs, 
and traces directly into AWS services and third-party tools of your choice.


App Mesh uses the open source Envoy proxy, making it compatible with a wide range 
of AWS partner and open source tools.




================================================= 
AWS Direct Connect FAQs:
================================================= 

What is AWS Direct Connect?
AWS Direct Connect is a networking service that provides an alternative to using the internet to connect to AWS. 
Using AWS Direct Connect, data that would have previously been transported over the internet is delivered through a 
private network connection between your facilities and AWS. In many circumstances, private network connections can reduce costs, 
increase bandwidth, and provide a more consistent network experience than internet-based connections. All AWS services, including 
Amazon Elastic Compute Cloud (EC2), Amazon Virtual Private Cloud (VPC), Amazon Simple Storage Service (S3), and 
Amazon DynamoDB can be used with AWS Direct Connect.



2)What is an AWS Direct Connect gateway?
An AWS Direct Connect gateway is a grouping of virtual private gateways (VGWs) and private virtual interfaces (VIFs). 
An AWS Direct Connect gateway is a globally available resource. You can create the AWS Direct Connect gateway in any 
Region and access it from all other Regions. 


3)What is a virtual interface (VIF)?

A virtual interface (VIF) is necessary to access AWS services, and is either public or private. 
A public virtual interface enables access to public services, such as Amazon S3. A private virtual interface enables access to your VPC. 


4) What is a virtual private gateway (VGW)?
A virtual private gateway (VGW) is part of a VPC that provides edge routing for AWS managed VPN connections and AWS 
Direct Connect connections. You associate an AWS Direct Connect gateway with the virtual private gateway for the VPC. 


5)What is AWS Direct Connect SiteLink?
When the AWS Direct Connect SiteLink feature is enabled at two or more AWS Direct Connect locations, 
you can send data between those locations, bypassing AWS Regions. 
AWS Direct Connect SiteLink works with both hosted and dedicated connections.


6)How will I be charged and billed for my use of AWS Direct Connect?
AWS Direct Connect has two separate charges: port hours and data transfer. Pricing is per port-hour consumed for each port type. 
Partial port hours consumed are billed as full hours. The account that owns the port will be charged the port hour charges.
Data transfer through AWS Direct Connect will be billed in the same month in which the usage occurred.



7)How does AWS Direct Connect differ from an IPsec VPN Connection?
VPN connections use IPsec to establish encrypted network connectivity between your intranet and an Amazon VPC over the public internet. 
VPN Connections can be configured in minutes and are a good solution if you have an immediate need, have low to modest 
bandwidth requirements, and can tolerate the inherent variability of internet-based connectivity. AWS Direct Connect bypasses 
the internet; instead, it uses dedicated, private network connections between your network and AWS.



8)Does MACsec replace other encryption technologies I currently use in my network?
MACsec is not intended as a replacement for any specific encryption technology. For simplicity, and for defense in depth,
you should continue to use any encryption technologies that you already use. We offer MACsec as an encryption option you can 
integrate into your network in addition to other encryption technologies you currently use.



================================================= 
Elastic Load Balancing FAQs:
================================================= 

1)How do I decide which load balancer to select for my application?
Elastic Load Balancing (ELB) supports four types of load balancers. 
You can select the appropriate load balancer based on your application needs. 
If you need to load balance HTTP requests, we recommend you use the Application Load Balancer (ALB). 
For network/transport protocols (layer4 – TCP, UDP) load balancing, and for extreme performance/low latency 
applications we recommend using Network Load Balancer. If your application is built within the Amazon Elastic Compute Cloud (Amazon EC2) 
Classic network, you should use Classic Load Balancer. If you need to deploy and run 
third-party virtual appliances, you can use Gateway Load Balancer.


2)Can Network Load Balancer process both TCP and UDP protocol traffic on the same port?
Yes. To achieve this, you can use a TCP+UDP listener. For example, for a DNS service using both TCP and UDP, 
you can create a TCP+UDP listener on port 53, and the load balancer will process traffic for both UDP and TCP requests on that port. 
You must associate a TCP+UDP listener with a TCP+UDP target group.



3) How does Gateway Load Balancer pricing work?
You are charged for each hour or partial hour that a Gateway Load Balancer is running and the number of Load Balancer 
Capacity Units (LCU) used by Gateway Load Balancer per hour.



================================================= 
Amazon Athena FAQs:
================================================= 

1)What is Amazon Athena?
Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. 
Athena is serverless, so there is no infrastructure to setup or manage, and you can start analyzing data immediately. 
You don’t even need to load your data into Athena, it works directly with data stored in S3. To get started, 
just log into the Athena Management Console, define your schema, and start querying. 

Amazon Athena uses Presto with full standard SQL support and works with a variety of standard data formats, 
including CSV, JSON, ORC, Avro, and Parquet. Athena can handle complex analysis, including large joins, window functions, and arrays. 



2)How do you access Amazon Athena?
Amazon Athena can be accessed via the AWS Management Console, an API, or an ODBC or JDBC driver. 
You can programmatically run queries, add tables or partitions using the ODBC or JDBC driver.



3) What is federated query?
If you have data in sources other than Amazon S3, you can use Athena to query the data in place or build pipelines that extract 
data from multiple data sources and store them in Amazon S3. With Athena Federated Query, you can run SQL queries across data 
stored in relational, non-relational, object, and custom data sources.


4)How is Amazon Athena priced?
Amazon Athena is priced per query and charges based on the amount of data scanned by the query. 
You can store data in a variety of formats on Amazon S3. If you compress your data, partition, 
or convert it to columnar storage formats, you pay less because you scan less data. 
Converting data to the columnar format allows Athena to read only the columns it needs to process the query.

No, you are not charged for failed queries.
Amazon Athena queries data directly from Amazon S3, so your source data is billed at S3 rates. When Amazon Athena runs a query, 
it stores the results in an S3 bucket of your choice and you are billed at standard S3 rates for these result sets. 




================================================= 
Amazon EMR FAQs:
================================================= 


1)What is Amazon EMR?
Amazon EMR is the industry-leading cloud big data platform for data processing, interactive analysis, and machine learning using 
open source frameworks such as Apache Spark, Apache Hive, and Presto. With EMR you can run petabyte-scale analysis at less than 
half of the cost of traditional on-premises solutions and over 1.7x faster than standard Apache Spark.


2)What is EMR Studio?
EMR Studio is an integrated development environment (IDE) that makes it easy for data scientists and data engineers to develop, 
visualize, and debug data engineering and data science applications written in R, Python, Scala, and PySpark.


3)How is EMR Studio different from SageMaker Studio?
You can use both EMR Studio and SageMaker Studio with Amazon EMR. EMR Studio provides an integrated development environment (IDE) 
that makes it easy for you to develop, visualize, and debug data engineering and data science applications written in R, Python, 
Scala, and PySpark. Amazon SageMaker Studio provides a single, web-based visual interface where you can perform all 
machine learning development steps. 


4)How much does Amazon EMR cost?
Amazon EMR pricing is simple and predictable: you pay a per-second rate for every second you use, with a one-minute minimum. 
You can estimate your bill using the AWS Pricing Calculator. Usage for other Amazon Web Services including 
Amazon EC2 is billed separately from Amazon EMR.



5)What is Amazon EMR Serverless?

Amazon EMR Serverless is a new deployment option in Amazon EMR that allows you to run big data frameworks such as 
Apache Spark and Apache Hive without configuring, managing, and scaling clusters.



================================================= 
Amazon CloudSearch FAQs:
================================================= 

1)What is Amazon CloudSearch?
Amazon CloudSearch is a fully-managed service in the AWS Cloud that makes it easy to set up, manage, and scale a 
search solution for your website or application.


2)What benefits does Amazon CloudSearch offer?
Amazon CloudSearch is a fully managed search service that automatically scales with the volume of data and complexity of search 
requests to deliver fast and accurate results. Amazon CloudSearch lets customers add search capability without needing to 
manage hosts, traffic and data scaling, redundancy, or software packages. Users pay low hourly rates only for the resources consumed. 
Amazon CloudSearch can offer significantly lower total cost of ownership compared to operating and managing your own search environment.



================================================= 
Amazon Kinesis Data Streams FAQs:
================================================= 

1)What is Amazon Kinesis Data Streams?
With Amazon Kinesis Data Streams, you can build custom applications that process or analyze streaming data for specialized needs. 
You can add various types of data such as clickstreams, application logs, and social media to a Kinesis data 
stream from hundreds of thousands of sources. 
Within seconds, the data will be available for your applications to read and process from the stream.


2)When I use Kinesis Data Streams, how secure is my data?

Amazon Kinesis is secure by default. Only the account and data stream owners have access to the Kinesis resources they create. 
Kinesis supports user authentication to control access to data. You can use AWS IAM policies to selectively grant 
permissions to users and groups of users. 
You can securely put and get your data from Kinesis through SSL endpoints using the HTTPS protocol.



3)How does Amazon Kinesis Data Streams pricing work?

Kinesis Data Streams uses simple pay-as-you-go pricing. There are no upfront costs or minimum fees, and you pay only for the 
resources you use. Kinesis Data Streams has two capacity modes—on-demand and provisioned—and both come with specific billing options.




=================================================
#Security, Identity, & Compliance
=================================================



AWS Identity and Access Management FAQ:
======================================

1) What is AWS Identity and Access Management (IAM)?
IAM provides fine-grained access control across all of AWS. With IAM, you can control access to services and resources under specific 
conditions. 
IAM provides authentication and authorization for AWS services. A service evaluates if an AWS request is allowed or denied. 
Access is denied by default and is allowed only when a policy explicitly grants access. You can attach policies to roles and 
resources to control access across AWS.


2)What are IAM roles and how do they work?
AWS Identity and Access Management (IAM) roles provide a way to access AWS by relying on temporary security credentials. 
Each role has a set of permissions for making AWS service requests, and a role is not associated with a specific user or group. 
Instead, trusted entities such as identity providers or AWS services assume roles.



3)What are IAM policies?
IAM policies define permissions for the entities you attach them to. For example, to grant access to an IAM role, 
attach a policy to the role. The permissions defined in the policy determine whether requests are allowed or denied. 
You also can attach policies to some resources, such as Amazon S3 buckets, to grant direct, cross-account access.



4)What are customer managed policies and when should I use them?
To grant only the permissions required to perform tasks, you can create customer managed policies that are specific to your use 
cases and resources. Use customer managed policies to continue refining permissions for your specific requirements.


5)What are inline policies and when should I use them?
Inline policies are embedded in and inherent to specific IAM roles. Use inline policies if you want to maintain a strict 
one-to-one relationship between a policy and the identity to which it is applied. 
For example, you can grant administrative permissions to ensure they are not attached to other roles. 


6)What are resource-based policies and when should I use them?
Resource-based policies are permissions policies that are attached to resources. For example, you can attach resource-based policies 
to Amazon S3 buckets, Amazon SQS queues, VPC endpoints, and AWS Key Management Service encryption keys. 
For a list of services that support resource-based policies, see AWS services that work with IAM. 
Use resource-based policies to grant direct, cross-account access. 
With resource-based policies, you can define who has access to a resource and which actions they can perform with it.



AWS IAM Identity Center (Successor to AWS Single Sign-On) FAQs:
==============================================================

1) What is AWS IAM Identity Center (successor to AWS Single Sign-On)?
IAM Identity Center is built on top of AWS Identity and Access Management (IAM) to simplify access management to multiple AWS accounts, 
AWS applications, and other SAML-enabled cloud applications. In IAM Identity Center, you create, or connect, 
your workforce users for use across AWS. You can choose to manage access just to your AWS accounts, just to your cloud applications, 
or to both. You can create users directly in IAM Identity Center, or you can bring them from your existing workforce directory. 
With IAM Identity Center, you get a unified administration experience to define, customize, and assign fine-grained access. 
Your workforce users get a user portal to access their assigned AWS accounts or cloud applications.



================================================= 
Amazon Cognito FAQs:
================================================= 

1)What is Amazon Cognito?
Amazon Cognito lets you easily add user sign-up and authentication to your mobile and web apps. 
Amazon Cognito also enables you to authenticate users through an external identity provider and provides 
temporary security credentials to access your app’s backend resources in AWS or any service behind Amazon API Gateway. 
Amazon Cognito works with external identity providers that support SAML or OpenID Connect, social 
identity providers (such as Facebook, Twitter, Amazon) and you can also integrate your own identity provider.



2)What is a User Pool?
A User Pool is your user directory that you can configure for your web and mobile apps. 
A User Pool securely stores your users’ profile attributes. You can create and manage a 
User Pool using the AWS console, AWS CLI, or AWS SDK.


3)What are unauthenticated users?
Unauthenticated users are users who do not authenticate with any identity provider, but instead access your app as a guest. 
You can define a separate IAM role for these users to provide limited permissions to access your backend resources.


4)How much does Cognito Identity cost?
With Amazon Cognito, you pay only for what you use. There are no minimum fees and no upfront commitments.
If you are using the Cognito Identity to create a User Pool, you pay based on your monthly active users (MAUs) only. 
A user is counted as a MAU if, within a calendar month, there is an identity operation related to that user, such as sign-up, 
sign-in, token refresh, password change, or a user account attribute is updated. You are not charged for subsequent sessions or 
for inactive users with in that calendar month. Separate charges apply for optional use of SMS messaging as described below.



================================================= 
AWS Directory Service FAQ:
================================================= 

1)What can I do with AWS Directory Service?
AWS Directory Service makes it easy for you to setup and run directories in the AWS cloud, or connect your AWS resources 
with an existing on-premises Microsoft Active Directory. Once your directory is created, you can use it to manage users and groups, 
provide single sign-on to applications and services, create and apply group policy, join Amazon EC2 instances to a domain, 
as well as simplify the deployment and management of cloud-based Linux and Microsoft Windows workloads. 
AWS Directory Service enables your end users to use their existing corporate credentials when accessing AWS applications, 
such as Amazon WorkSpaces, Amazon WorkDocs and Amazon WorkMail, as well as directory-aware Microsoft workloads, 
including custom .NET and SQL Server-based applications. Finally, you can use your existing corporate credentials to 
administer AWS resources via AWS Identity and Access Management (IAM) role-based access to the AWS Management Console, 
so you do not need to build out more identity federation infrastructure.


2)How do I create an AWS Managed Microsoft AD directory?
You can launch the AWS Directory Service console from the AWS Management Console to create an AWS Managed Microsoft AD directory. 
Alternatively, you can use the AWS SDK or AWS CLI.


3)Which applications are compatible with AWS Managed Microsoft AD?
Amazon Chime
Amazon Connect
Amazon EC2 Instances
Amazon FSx for Windows File Server
Amazon QuickSight
Amazon RDS for MySQL
Amazon RDS for Oracle
Amazon RDS for PostgreSQL
Amazon RDS for SQL Server
Amazon Single Sign On
Amazon WorkDocs
Amazon WorkMail
Amazon WorkSpaces
AWS Client VPN
AWS Management Console
Note that not all configurations of these applications may be supported.


================================================= 
AWS Key Management Service FAQs:
================================================= 

1) What is AWS Key Management Service (KMS)?
AWS KMS is a managed service that enables you to easily create and control the keys used for cryptographic operations. 
The service provides a highly available key generation, storage, management, and auditing solution for you to encrypt or 
digitally sign data within your own applications or control the encryption of data across AWS services.


2) How will I be charged and billed for my use of AWS KMS?
With AWS KMS, you pay only for what you use, there is no minimum fee. There are no set-up fees or commitments to begin using the service. 
At the end of the month, your credit card will automatically be charged for that month’s usage.



3)Who can use and manage my keys in AWS KMS?
AWS KMS enforces usage and management policies that you define. You choose to allow AWS Identity and 
Access Management (IAM) users and roles from your account or other accounts to use and manage your keys.



================================================= 
AWS Organizations FAQs:
================================================= 


1)What is AWS Organizations?
An organization is a collection of AWS accounts that you can organize into a hierarchy and manage centrally.

AWS Organizations helps you centrally govern your environment as you scale your workloads on AWS. 
Whether you are a growing startup or a large enterprise, Organizations helps you to programmatically create new accounts and 
allocate resources, simplify billing by setting up a single payment method for all of your accounts, create groups of accounts 
to organize your workflows, and apply policies to these groups for governance. 
In addition, AWS Organizations is integrated with other AWS services so you can define central 
configurations, security mechanisms, and resource sharing across accounts in your organization.



2)Which central governance and management capabilities does AWS Organizations enable?

Automate AWS account creation and management, and provision resources with AWS CloudFormation Stacksets
Maintain a secure environment with policies and management of AWS security services
Govern access to AWS services, resources, and regions
Centrally manage policies across multiple AWS accounts
Audit your environment for compliance 
View and manage costs with consolidated billing 
Configure AWS services across multiple accounts



3)What is AWS Control Tower?
AWS Control Tower, built on AWS services such as AWS Organizations, offers the easiest way to set up and govern a new, secure, 
multi-account AWS environment. It establishes a landing zone, which is a well-architected, multi-account environment based on 
best-practice blueprints, and enables governance using guardrails you can choose.



4)What does AWS Organizations cost?
AWS Organizations is offered at no additional charge.



================================================= 
Amazon MQ FAQs
================================================= 

1) What is Amazon MQ?
Amazon MQ is a managed message broker service for Apache ActiveMQ and RabbitMQ that makes it easy to set up and operate message 
brokers in the cloud. You get direct access to the ActiveMQ and RabbitMQ consoles and industry standard APIs and protocols for 
messaging, including JMS, NMS, AMQP 1.0 and 0.9.1, STOMP, MQTT, and WebSocket. You can easily move from any message 
broker that uses these standards to Amazon MQ because you don’t have to rewrite any messaging code in your applications.



2)Who should use Amazon MQ?
Amazon MQ is suitable for enterprise IT pros, developers, and architects who are managing a message broker themselves–whether 
on-premises or in the cloud–and want to move to a fully managed cloud service without rewriting the messaging code in their applications.


3)Who should use Amazon MQ?
Amazon MQ is suitable for enterprise IT pros, developers, and architects who are managing a message broker themselves–whether 
on-premises or in the cloud–and want to move to a fully managed cloud service without rewriting the messaging code in their applications.



4)How does Amazon MQ work with other AWS services?

Any application that runs on an AWS compute service, such as Amazon EC2, Amazon ECS, or AWS Lambda, can use Amazon MQ. 
Amazon MQ is also integrated with the following AWS services:

Amazon CloudWatch - monitor metrics and generate alarms
Amazon CloudWatch Logs - publish logs from your Amazon MQ brokers to Amazon CloudWatch Logs
AWS CloudTrail - log, continously monitor, and retain Amazon MQ API calls
AWS CloudFormation - automate the process of creating, updating, and deleting message brokers
AWS Identity and Access Management (IAM) - authentication and authorization of the service API
AWS Key Management Service (KMS) - create and control the keys used to encrypt your data




================================================= 
Amazon SQS FAQs:
================================================= 

1) How is Amazon SQS different from Amazon Simple Notification Service (SNS)?
Amazon SNS allows applications to send time-critical messages to multiple subscribers through a “push” mechanism, 
eliminating the need to periodically check or “poll” for updates. Amazon SQS is a message queue service used by distributed 
applications to exchange messages through a polling model, and can be used to decouple sending and receiving components. 



2)How is Amazon SQS different from Amazon MQ?
If you're using messaging with existing applications, and want to move your messaging to the cloud quickly and easily, 
we recommend you consider Amazon MQ. 
It supports industry-standard APIs and protocols so you can switch from any standards-based message broker to 
Amazon MQ without rewriting the messaging code in your applications. If you are building brand new applications in the cloud, 
we recommend you consider Amazon SQS and Amazon SNS. Amazon SQS and SNS are lightweight, fully managed message queue and topic 
services that scale almost infinitely and provide simple, easy-to-use APIs.



3)How is Amazon SQS different from Amazon Kinesis Streams?
Amazon SQS offers a reliable, highly-scalable hosted queue for storing messages as they travel between applications or microservices. 
It moves data between distributed application components and helps you decouple these components. 
Amazon SQS provides common middleware constructs such as dead-letter queues and poison-pill management. 
It also provides a generic web services API and can be accessed by any programming language that the AWS SDK supports. 
Amazon SQS supports both standard and FIFO queues.

Amazon Kinesis Streams allows real-time processing of streaming big data and the ability to read and replay records to multiple 
Amazon Kinesis Applications. The Amazon Kinesis Client Library (KCL) delivers all records for a given partition key to the same 
record processor, making it easier to build multiple applications that read from the same 
Amazon Kinesis stream (for example, to perform counting, aggregation, and filtering).


4)How much does Amazon SQS cost?

You pay only for what you use, and there is no minimum fee.
The cost of Amazon SQS is calculated per request, plus data transfer charges for data transferred out of 
Amazon SQS (unless data is transferred to Amazon Elastic Compute Cloud (EC2) 
instances or to AWS Lambda functions within the same region). 



5)Can I use Amazon SQS with other AWS services?
Yes. You can make your applications more flexible and scalable by using Amazon SQS with compute services such as 
Amazon EC2, Amazon Elastic Container Service (ECS), and AWS Lambda, as well as with storage and database services such as 
Amazon Simple Storage Service (Amazon S3) and Amazon DynamoDB.


6) What is Amazon SQS long polling?
Amazon SQS long polling is a way to retrieve messages from your Amazon SQS queues. 
While the regular short polling returns immediately, even if the message queue being polled is empty, 
long polling doesn’t return a response until a message arrives in the message queue, 
or the long poll times out.

Long polling makes it inexpensive to retrieve messages from your Amazon SQS queue as soon as the messages are available. 
Using long polling might reduce the cost of using SQS, because you can reduce the number of empty receives



7) How does Amazon SQS handle messages that can't be processed?
In Amazon SQS, you can use the API or the console to configure dead letter queues, which receive messages from other source queues. 
When configuring a dead letter queue, you are required to set appropriate permissions for the dead letter queue 
redrive using RedriveAllowPolicy.
RedriveAllowPolicy includes the parameters for the dead-letter queue redrive permission. 
It defines which source queues can specify dead-letter queues as a JSON object.
Once you make a dead letter queue, it receives messages after a maximum number of processing attempts cannot be completed. You can use dead letter queues to isolate messages that can't be processed for later analysis.



8) What is a visibility timeout?
The visibility timeout is a period of time during which Amazon SQS prevents other consuming components 
from receiving and processing a message.



9)What are message groups?
Messages are grouped into distinct, ordered "bundles" within a FIFO queue. For each message group ID, all messages are sent and 
received in strict order. However, messages with different message group ID values might be sent and received out of order. 
You must associate a message group ID with a message. If you don't provide a message group ID, the action fails.

If multiple hosts (or different threads on the same host) send messages with the same message group ID are sent to a FIFO queue, 
Amazon SQS delivers the messages in the order in which they arrive for processing. 
To ensure that Amazon SQS preserves the order in which messages are sent and received, ensure that multiple senders send each 
message with a unique message group ID.



================================================= 
Amazon SNS FAQs:
================================================= 

1)What is Amazon Simple Notification Service (Amazon SNS)?
Amazon Simple Notification Service (Amazon SNS) is a web service that makes it easy to set up, operate, and send notifications from 
the cloud. It provides developers with a highly scalable, flexible, and cost-effective capability to publish messages 
from an application and immediately deliver them to subscribers or other applications. 

It is designed to make web-scale computing easier for developers. Amazon SNS follows the “publish-subscribe” (pub-sub) messaging paradigm,
 with notifications being delivered to clients using a “push” mechanism that eliminates the need to 
 periodically check or “poll” for new information and updates. 
 
With simple APIs requiring minimal up-front development effort, 
 no maintenance or management overhead and pay-as-you-go pricing, Amazon SNS gives developers an easy mechanism to 
 incorporate a powerful notification system with their applications.


2)What are the benefits of using Amazon SNS?
Instantaneous, push-based delivery (no polling)
Simple APIs and easy integration with applications
Flexible message delivery over multiple transport protocols
Inexpensive, pay-as-you-go model with no up-front costs
Web-based AWS Management Console offers the simplicity of a point-and-click interface



3)What are some example uses for Amazon SNS notifications?
The Amazon SNS service can support a wide variety of needs including event notification, monitoring applications, 
workflow systems, time-sensitive information updates, mobile applications, and any other application that generates or 
consumes notifications. 

For example, Amazon SNS can be used in workflow systems to relay events among distributed computer 
applications, move data between data stores or update records in business systems. Event updates and notifications 
concerning validation, approval, inventory changes and shipment status are immediately delivered to relevant system components as 
well as end-users. A common pattern is to use SNS to publish messages to Amazon SQS message queues to reliably send messages to one or 
many system components asynchronously. 

Another example use for Amazon SNS is to relay time-critical events to mobile applications and 
devices. Since Amazon SNS is both highly reliable and scalable, it provides significant advantages to developers who build 
applications that rely on real-time events.


4)How much does Amazon SNS cost?
With Amazon SNS, there is no minimum fee and you pay only for what you use. Users pay $0.50 per 1 million 
Amazon SNS Requests, $0.06 per 100,000 notification deliveries over HTTP, and $2.00 per 100,000 notification deliveries over email. 
For SMS messaging, charges vary by destination country.


5) Are there quotas for the number of topics or number of subscribers per topic?
By default, SNS offers 10 million subscriptions per topic, and 100,000 topics per account.



================================================= 
Amazon SWF FAQs:
================================================= 

1)What is Amazon SWF?
Amazon Simple Workflow Service (SWF) is a web service that makes it easy to coordinate work across distributed application components. 
Amazon SWF enables applications for a range of use cases, including media processing, web application back-ends, 
business process workflows, and analytics pipelines, to be designed as a coordination of tasks.


2)What can I do with Amazon SWF?
Amazon SWF can be used to address many challenges that arise while building applications with distributed components. 
For example, you can use Amazon SWF and the accompanying AWS Flow Framework for:

-Writing your applications as asynchronous programs using simple programming constructs that abstract details such as initiating tasks to run remotely and tracking the program’s runtime state.

-Maintaining your application’s execution state (e.g. which steps have completed, which ones are running, etc.). 
You do not have to use databases, custom systems, or ad hoc solutions to keep execution state.
Communicating and managing the flow of work between your application components. 

With Amazon SWF, you do not need to design a messaging protocol or worry about lost and duplicated tasks.

Centralizing the coordination of steps in your application. Your coordination logic does not have to be scattered across 
different components, but can be encapsulated in a single program.

Integrating a range of programs and components, including legacy systems and 3rd party cloud services, into your applications. 

By allowing your application flexibility in where and in what combination the application components are deployed, 

Amazon SWF helps you gradually migrate application components from private data centers to public cloud infrastructure 
without disrupting the application availability or performance.

Automating workflows that include long-running human tasks (e.g. approvals, reviews, investigations, etc.) 
Amazon SWF reliably tracks the status of processing steps that run up to several days or months.

Building an application layer on top of Amazon SWF to support domain specific languages for your end users. 

Since Amazon SWF gives you full flexibility in choosing your programming language, you can conveniently build interpreters for 
specialized languages (e.g. XPDL) and customized user-interfaces including modeling tools.

Getting detailed audit trails and visibility into all running instances of your applications. 

You can also incorporate visibility capabilities provided by Amazon SWF into your own user interfaces using the 
APIs provided by Amazon SWF.

ustomers have used Amazon SWF to build applications for video encoding, social commerce, infrastructure provisioning, 
MapReduce pipelines, business process management, and several other use cases. For more details on use cases, please see What are some use cases that can be solved with SWF?. To see how customers are using Amazon SWF today, please read our case studies.




================================================= 
Amazon AppFlow FAQs:
================================================= 

1)What is Amazon AppFlow?
Amazon AppFlow is a fully managed integration service that enables you to securely transfer data between Software-as-a-Service 
(SaaS) applications like Salesforce, Marketo, Slack, and ServiceNow, and AWS services like Amazon S3 and Amazon Redshift, 
in just a few clicks. With AppFlow, you can run data flows at nearly any scale at the frequency you choose - on a schedule, 
in response to a business event, or on demand. You can configure powerful data transformation capabilities like filtering and 
validation to generate rich, ready-to-use data as part of the flow itself, without additional steps. AppFlow automatically encrypts 
data in motion, and allows users to restrict data from flowing over the public Internet for SaaS applications that are 
integrated with AWS PrivateLink, reducing exposure to security threats.




================================================= 
AWS Step Functions FAQs:
================================================= 

1)What is AWS Step Functions?
AWS Step Functions is a fully managed service that makes it easy to coordinate the components of distributed applications and 
microservices using visual workflows. 

Building applications from individual components that each perform a discrete 
function lets you scale easily and change applications quickly. 

Step Functions is a reliable way to coordinate 
components and step through the functions of your application. Step Functions provides a graphical console 
to arrange and visualize the components of your application as a series of steps. 

This makes it simple to build and run multi-step applications. Step Functions automatically triggers and tracks each step, and 
retries when there are errors, so your application executes in order and as expected. 

Step Functions logs the state of each step, so when things do go wrong, you can diagnose and debug problems quickly. 
You can change and add steps without even writing code, so you can easily evolve your application and innovate faster.



2)What are some common AWS Step Functions use cases?
AWS Step Functions helps with any computational problem or business process that can be subdivided into a series of steps. 
It’s also useful for creating end-to-end workflows to manage jobs with interdependencies. 
Common use cases include:

Data processing: consolidate data from multiple databases into unified reports, refine and reduce large data sets into useful formats, 
or coordinate multi-step analytics and machine learning workflows

DevOps and IT automation: build tools for continuous integration and continuous deployment, or create event-driven applications 
that automatically respond to changes in infrastructure

E-commerce: automate mission-critical business processes, such as order fulfillment and inventory tracking

Web applications: implement robust user registration processes and sign-on authentication



================================================= 
Amazon CloudWatch FAQs:
================================================= 


1)What is Amazon CloudWatch?
Amazon CloudWatch is a monitoring service for AWS cloud resources and the applications you run on AWS. 
You can use Amazon CloudWatch to collect and track metrics, collect and monitor log files, and set alarms. 
Amazon CloudWatch can monitor AWS resources such as Amazon EC2 instances, Amazon DynamoDB tables, and Amazon RDS DB instances, 
as well as custom metrics generated by your applications and services, and any log files your applications generate. 
You can use Amazon CloudWatch to gain system-wide visibility into resource utilization, application performance, and 
operational health. You can use these insights to react and keep your application running smoothly.



2)What can I use to access CloudWatch?
Amazon CloudWatch can be accessed via API, command-line interface, AWS SDKs, and the AWS Management Console.


3)How do I retrieve my log data?
You can retrieve any of your log data using the CloudWatch Logs console or through the CloudWatch Logs CLI. 
Log events are retrieved based on the Log Group, Log Stream and time with which they are associated. 
The CloudWatch Logs API for retrieving log events is GetLogEvents.

You can use the CLI to retrieve your log events and search through them using command line grep or similar search functions.


4)What is CloudWatch Dashboards?
Amazon CloudWatch Dashboards allow you to create, customize, interact with, and save graphs of AWS resources and custom metrics.


5)What is CloudWatch Events?
Amazon CloudWatch Events (CWE) is a stream of system events describing changes in your AWS resources. 
The events stream augments the existing CloudWatch Metrics and Logs streams to provide a more complete picture of the 
health and state of your applications. You write declarative rules to associate events of interest with automated actions to be taken.


6) What services emit CloudWatch Events?
Currently, Amazon EC2, Auto Scaling, and AWS CloudTrail are supported. 
Via AWS CloudTrail, mutating API calls (i.e., all calls except Describe*, List*, and Get*) across all services are 
visible in CloudWatch Events.


================================================= 
AWS Auto Scaling FAQs:
================================================= 
1)What is AWS Auto Scaling? 
AWS Auto Scaling helps you configure consistent and congruent scaling policies across the full infrastructure stack 
backing your application. AWS Auto Scaling will automatically scale resources as needed to align to your selected scaling strategy, 
so you maintain performance and pay only for the resources you actually need.


2)When should I use AWS Auto Scaling?
You should use AWS Auto Scaling if you have an application that uses one or more scalable resources and experiences variable load. 
A good example would be an e-commerce web application that receives variable traffic through the day. 
It follows a standard three tier architecture with Elastic Load Balancing for distributing incoming traffic, 
Amazon EC2 for the compute layer, and DynamoDB for the data layer. 
In this case, AWS Auto Scaling will scale one or more EC2 Auto Scaling groups and DynamoDB tables that are powering the application 
in response to the demand curve.



3) How much does AWS Auto Scaling cost?
Similar to Auto Scaling on individual AWS resources, AWS Auto Scaling is free to use. 
AWS Auto Scaling is enabled by Amazon CloudWatch, so service fees apply for CloudWatch and your application resources 
(such as Amazon EC2 instances, Elastic Load Balancing load balancers, etc.).




================================================= 
AWS CloudFormation FAQs:
================================================= 

1)What is AWS CloudFormation?
CloudFormation introduces four concepts: A template is a JSON or YAML declarative code file that describes the intended 
state of all the resources you need to deploy your application


AWS CloudFormation is a service that gives developers and businesses an easy way to create a collection of related AWS and 
third-party resources, and provision and manage them in an orderly and predictable fashion.

Developers can deploy and update compute, database, and many other resources in a simple, declarative style that 
abstracts away the complexity of specific resource APIs. 


2)How is CloudFormation different from AWS Elastic Beanstalk?
These services are designed to complement each other. AWS Elastic Beanstalk provides an environment where you can easily 
deploy and run applications in the cloud. It is integrated with developer tools and provides a one-stop experience for 
managing application lifecycle. If your application workloads can be managed as Elastic Beanstalk workloads, 
you can enjoy a more turn-key experience in creating and updating applications. Behind the scenes, 
Elastic Beanstalk uses CloudFormation to create and maintain resources. If your application requirements dictate more custom control, 
the additional functionality of CloudFormation gives you more options to control your workloads.

AWS CloudFormation is a convenient provisioning mechanism for a broad range of AWS and third-party resources. 
It supports the infrastructure needs of many different types of applications such as existing enterprise applications, 
legacy applications, applications built using a variety of AWS resources, and container-based solutions 
(including those built using AWS Elastic Beanstalk).



3)How much does AWS CloudFormation cost?
There is no additional charge for using AWS CloudFormation with resource providers in the following 
namespaces: AWS::*, Alexa::*, and Custom::*. In this case, you pay for AWS resources (such as Amazon EC2 instances, 
Elastic Load Balancing load balancers, etc.) created using AWS CloudFormation just as if you had created them manually. 
You only pay for what you use, as you use it; there are no minimum fees and no required upfront commitments.



================================================= 
AWS CloudTrail FAQs:
================================================= 

1)What is AWS CloudTrail?
AWS CloudTrail enables auditing, security monitoring, and operational troubleshooting by tracking user activity and API usage. 
CloudTrail logs, continuously monitors, and retains account activity related to actions across your AWS infrastructure, 
giving you control over storage, analysis, and remediation actions.


2)Who should use CloudTrail?
You should use CloudTrail if you need to audit activity, monitor security, or troubleshoot operational issues.


3)How can I secure my CloudTrail log files?
By default, CloudTrail log files are encrypted using Amazon S3 Server Side Encryption (SSE) and placed into your S3 bucket. 
You can control access to log files by applying IAM or S3 bucket policies. You can add an additional layer of security by 
enabling S3 Multi Factor Authentication (MFA) Delete on your S3 bucket. 


4)Why should I use AWS CloudTrail Lake?
CloudTrail Lake allows you to examine incidents by querying all actions logged by CloudTrail. 
It simplifies incident logging by helping to remove operational dependencies and provides tools that can help reduce 
your reliance on complex data process pipelines that span across teams. 


5)How do I get charged for AWS CloudTrail?
AWS CloudTrail allows you to view, search, and download the last 90 days of your account’s management events for free. 
You can deliver one copy of your ongoing management events to Amazon S3 for free by creating a trail. 
Once a CloudTrail trail is set up, Amazon S3 charges apply based on your usage.



================================================= 
AWS Config FAQs:
================================================= 

1)What is AWS Config?
AWS Config is a fully managed service that provides you with an AWS resource inventory, configuration history, 
and configuration change notifications to enable security and governance. With AWS Config you can discover existing AWS resources, 
export a complete inventory of your AWS resources with all configuration details, and determine how a resource was 
configured at any point in time. These capabilities enable compliance auditing, security analysis, 
resource change tracking, and troubleshooting.


2)How will I be charged for AWS Config?
With AWS Config, you are charged based on the number of configuration items recorded, the number of active AWS Config rule 
evaluations and the number of conformance pack evaluations in your account. A configuration item is a record of the 
configuration state of a resource in your AWS account. An AWS Config rule evaluation is a compliance state evaluation of a 
resource by a AWS Config rule in your AWS account, and a conformance pack evaluation is the evaluation of a resource by a 
AWS Config rule within the conformance pack. 





=================================================
#Exam content         
================================================= 



#Exam Content Outline
-------------------------------------------------

The exam contains five main sections, which are:

-Design Resilient Architectures (34%)
-Define Performant Architecture (24%)
-Specify Secure Applications and Architectures (24%)
-Design Cost-Optimized Architectures (10%)
-Define Operationally Excellent Architectures (6%)Now


Design Resilient Architecture:
-How to choose reliable and resilient storage using services like AWS S3, AWS Glacier, and AWS EBS
-How to design decoupling mechanisms using AWS services like AWS SNS
-How to create a multi-tier architecture
-How to architect for high availability and fault-tolerance


Define Performant Architectures:
How to choose performant storage and databases using AWS RDS, AWS Redshift, and AWS DynamoDB
How to improve performance using AWS Elasticache
How to design elastic and scalable solutions through AWS Lambda, AWS CloudWatch, and AWS Data Pipeline



Specify Secure Applications and Architectures:
How to secure applications using AWS Inspector, AWS CloudTrail, and AWS IAM
How to secure data using AWS CloudHSM and AWS Macie
How to define the network infrastructure with AWS CloudFront, AWS VPC, and Elastic Load Balancer


Design Cost-Optimized Architectures:
How to design cost-optimized compute solutions using AWS EC2, AWS Elastic Beanstalk, AWS Lambda, and Aws Lightsail
Design cost-effective storage solutions using AWS S3, AWS Glacier, AWS EBS, and AWS Elastic File System


Define Operationally Excellent Architectures:
Perform operations as code
Annotate documentation
Make frequent, small, and reversible changes
Anticipate and tackle failures



================================================= 
#Exam | Question | Mock
================================================= 

1) How do you choose aws region ?

2) How do you upgrade or downgrade a system with near-zero downtime?
You can upgrade or downgrade a system with near-zero downtime using the following steps of migration:
-Open EC2 console
-Choose Operating System AMI
-Launch an instance with the new instance type
-Install all the updates
-Install applications
-Test the instance to see if it’s working
-If working, deploy the new instance and replace the older instance
-Once it’s deployed, you can upgrade or downgrade the system with near-zero downtime.


3) Is there any other alternative tool to log into the cloud environment other than console?
The that can help you log into the AWS resources are:
Putty
AWS CLI for Linux
AWS CLI for Windows
AWS CLI for Windows CMD
AWS SDK
Eclipse


4) What services can be used to create a centralized logging solution?
The essential services that you can use are Amazon CloudWatch Logs, store them in Amazon S3, and then use 
Amazon Elastic Search to visualize them. You can use Amazon Kinesis Firehose to move the data from Amazon S3 to Amazon ElasticSearch.




5) Name some of the AWS services that are not region-specific ?
AWS services that are not region-specific are:
IAM
Route 53
Web Application Firewall 
CloudFront



6) What is CloudWatch?
The Amazon CloudWatch has the following features:
Depending on multiple metrics, it participates in triggering alarms.
Helps in monitoring the AWS environments like CPU utilization, EC2, Amazon RDS instances, Amazon SQS, S3, Load Balancer, SNS, etc.




7) What is an Elastic Transcoder?
To support multiple devices with various resolutions like laptops, tablets, and smartphones, we need to change the resolution and 
format of the video. This can be done easily by an AWS Service tool called the Elastic Transcoder, 
which is a media transcoding in the cloud that exactly lets us do the needful. It is easy to use, cost-effective, 
and highly scalable for businesses and developers.


8) How do you set up SSH agent forwarding so that you do not have to copy the key every time you log in?
Here’s how you accomplish this:
Go to your PuTTY Configuration
Go to the category SSH -> Auth
Enable SSH agent forwarding to your instance



9) How do you configure CloudWatch to recover an EC2 instance?
Here’s how you can configure them:
Create an Alarm using Amazon CloudWatch
In the Alarm, go to Define Alarm -> Actions tab
Choose Recover this instance option



10) What are the common types of AMI designs?
There are many types of AMIs, but some of the common AMIs are:
Fully Baked AMI
Just Enough Baked AMI (JeOS AMI)
Hybrid AMI




11) What are Key-Pairs in AWS?
The Key-Pairs are password-protected login credentials for the Virtual Machines that are used to prove our 
identity while connecting the Amazon EC2 instances. The Key-Pairs are made up of a Private Key and a 
Public Key which lets us connect to the instances.



12) How can you recover/login to an EC2 instance for which you have lost the key?
Follow the steps provided below to recover an EC2 instance if you have lost the key:

Verify that the EC2Config service is running
Detach the root volume for the instance
Attach the volume to a temporary instance
Modify the configuration file
Restart the original instance



13) How do you monitor Amazon VPC?
You can monitor VPC by using:
-CloudWatch and CloudWatch logs
-VPC Flow Logs



14) When Would You Prefer Provisioned IOPS over Standard Rds Storage?
You would use Provisioned IOPS when you have batch-oriented workloads. 
]Provisioned IOPS delivers high IO rates, but it is also expensive. 
However, batch processing workloads do not require manual intervention. 



15) How Do Amazon Rds, Dynamodb, and Redshift Differ from Each Other?
Amazon RDS is a database management service for relational databases. It manages patching, upgrading, and data backups automatically. 
It’s a database management service for structured data only. On the other hand, DynamoDB is a NoSQL 
database service for dealing with unstructured data. Redshift is a data warehouse product used in data analysis.



16) What are the factors to consider while migrating to Amazon Web Services?
Here are the factors to consider during AWS migration:
Operational Costs - These include the cost of infrastructure, ability to match demand and supply, transparency, and others.
Workforce Productivity 
Cost avoidance
Operational resilience
Business agility




17) What is RTO and RPO in AWS?
RTO or Recovery Time Objective is the maximum time your business or organization is willing to wait for a recovery to 
complete in the wake of an outage. On the other hand, RPO or Recovery Point Objective is the 
maximum amount of data loss your company is willing to accept as measured in time.





18) What are the advantages of AWS IAM?
AWS IAM allows an administrator to provide multiple users and groups with granular access. Various user groups and users may require 
varying levels of access to the various resources that have been developed. 
We may assign roles to users and create roles with defined access levels using IAM.

It further gives us Federated Access, which allows us to grant applications and users access to 
resources without having to create IAM Roles.


19) Explain Connection Draining ?
Connection Draining is an AWS service that allows us to serve current requests on the servers that are either being 
decommissioned or updated.

By enabling this Connection Draining, we let the Load Balancer make an outgoing instance finish its 
existing requests for a set length of time before sending it any new requests. A departing instance will 
immediately go off if Connection Draining is not enabled, and all pending requests will fail.


20) What is Power User Access in AWS?
The AWS Resources owner is identical to an Administrator User. ,
The Administrator User can build, change, delete, and inspect resources, as well as grant permissions to other AWS users.

Administrator Access without the ability to control users and permissions is provided to a Power User. 
A Power User Access user cannot provide permissions to other users but has the ability to modify, remove, view, and create resources.


================================================= 
#ExamAdv | advice
================================================= 

#Section 8: High Availability and Scalability: ELB & ASG
-------------------------------------------------

-Gateway Load balancer
1)If you see GENEvE protocol on port 608
=> Think it is Gateway Load balancer


2)Any Time you see multiple SSL certificates 
=> Think this is ALB or NLB, Using  SNI (Server Name Indication) make it work.

-ASG solution Architeure
3)Lifecycle Hooks of ASG are doing something before instance gos Termination
Like backup loggs or DB dump



#Section 15: CloudFront & AWS Global Accelerator
-------------------------------------------------
1)Global Acceleration come to in exam



#On Section 23: Identity and Access Management (IAM) - Advanced
-------------------------------------------------

-Identity Federation
1)If taking about AusumeRole or Cross-Account 
 => Its about STS (AWS Security Token Service)


-Directory Services 
2)May exam ask about MS-AD from of High level of 3type AWS AD 
 => AWS AD Manager | AD Connector  with proxy | Simple AD



-Resource Access Manager (RAM)
3)Subnet or Other resource sharing with other account (Generally exam asking about Subnet)
 => Resource Access Manager (RAM)



#Section 24: AWS Security & Encryption: KMS, SSM Parameter Store, CloudHSM, Shield, WAF
-------------------------------------------------

-KMS Overview
4)Exam generally asking about Symmetric (AES-256 Keys) Encryption.


-AWS Secrets Manager - Overview
5)Exam ask you about Store thing about its AWS Secrets Manager

-WAF
6)Need to remember that WAF (Web Application Firewall) Deploy onlyh three thing 
  Application Load Balancer | API Gateway | CloudFont

-GuardDuty 
7)GuardDuty protect you against CryptoCurrency Attacks


-Shared Responsibility Model
8)Exam ask you about your responsibility and AwS Responsibility
Need to undestanding clearly about Shared Responsibility



#Section 26: Disaster Recovery & Migrations
-------------------------------------------------
1)Exam ask you base on scenario base question, what should is choose the solution for.








=================================================
#Billing | Pricing | Cost 
=================================================

EC2:
===
EC2 Instances Purchasing Options:
• On-Demand Instances – short workload, predictable pricing, pay by second
• Reserved (1 & 3 years)
• Reserved Instances – long workloads
• Convertible Reserved Instances – long workloads with flexible instances
• Savings Plans (1 & 3 years) –commitment to an amount of usage, long workload
• Spot Instances – short workloads, cheap, can lose instances (less reliable)
• Dedicated Hosts – book an entire physical server, control instance placement
• Dedicated Instances – no other customers will share your hardware
• Capacity Reservations – reserve capacity in a specific AZ for any duration
1)Elastic IP Addresses cost money when not use it !
2)ENI not cost any Money.
3)There is no charge applicable to Security Groups in Amazon EC2 / Amazon VPC.


Route53:
========
1)Route53 Not free, 12dolar per year.
2)You pay $0.50 per month per hosted zone.



RDS:
====
1)In AWS there’s a network cost when data goes from one AZ to another
2)For RDS Read Replicas within the same region, you don’t pay that fee, for diffenent region have Network cost.

EFS:
===
1)Only pay for uses size only (No free tier)

IAM:
===
1) Bit cost for storage of snapshots of the IAM size.(NO free tier)
2)AWS Identity and Access Management (IAM) and AWS Security Token Service (AWS STS) are features of your 
3)AWS account offered at no additional charge. 
4)You are charged only when you access other AWS services using your IAM users or AWS STS temporary security credentials.

ELB:
====
1)There is no additional charge for AWS Auto Scaling. 
You pay only for the AWS resources needed to run your applications and Amazon CloudWatch monitoring fees.
2)No charge for ASG Launch Templates and Launch.
3)Application load balancer on available on free tirs NLB are not.
4)ASG are free (you only pay for the underlying EC2 instances)


S3:
===
1)The AWS Free Tier includes 5 GB of Amazon S3 standard storage, which offers the highest Amazon S3 durability.
2)Empty S3 buckets don't cost anything. 
You are only charged by the size of the objects in the bucket, the storage class and the access.

CloudFont:
===========
1)Global excelarater not free.
2)Global Excelaration pricing model of the region.


Transit gateway:
===============
You are charged hourly for each attachment on a transit gateway, and you are charged for the amount of 
traffic processed on the transit gateway. 




API Gateway:
============
1)Caching is charged by the hour and is not eligible for the AWS Free Tier.


Transit Gateway:
================
AWS Transit Gateway is a networking service that uses a hub-and-spoke model to connect on-premises data centres and 
Amazon Virtual Private Clouds (VPCs) to a single gateway.

Through a central hub, it connects your Amazon Virtual Private Clouds (VPCs) and on-premises networks. 
This simplifies your network and eliminates complicated peering relationships. 
It functions as a cloud router, establishing new connections only once.



Beanstalk:
There is no additional charge for Elastic Beanstalk.


SQS, Kinesis:
============
1)Kinesis is not free tiar.


ECS:
====
1)Fargate lunch type ask about a lot.
2)From ECS rolling update one question only.


(IAM) - Advanced:
=============================
1)AWS Config no free tier, $0.003 per configuration item recorded per region,
$0.001 per config rule evaluation per region

Security:
=========
1)CloudHSM no free tiar.
2)WAP and Shelc are not free.


vpc:
=====
1)VPC Rechability not free 
2)3Q come from of Site-to-Site VPN Connections operation
3)AWS VPC starts to cost money when you utilize Site-to-Site VPN connections, 
PrivateLinks (VPC endpoints), NAT gateways, and traffic mirroring.



================================================= 
#EXMQ
================================================= 

IAM:
====
1)User,Group and Policy.
2)MFA (Multi Factor Authentication) login with password and a sacurity device together.



Route53:
========
1)About Geoproximity routeing policy

RDS
=====
1)RDS is Dynamicly auto-scales.
2)Undestanding Read Replica and Multi-AZ
3)Is The Read Replicas be setup as Multi AZ for Disaster Recovery, Yes(CommonQ) 
Lotof question come from ReadReplica and Multi-AZ.
4)How do you make database go from Single-AZ to Multi-AZ ?
5)If the master is not encrypted, the read replicas cannot be encrypted.(CommonQ)
6)Write end point for Master and Read End Point for multiple Read Replica with loadbalanceing on connection point.
7)ElastiCache – Redis vs Memcached
8)Multi-AZ helps when you plan a disaster recovery for an entire AZ going down. 
  If you plan against an entire AWS Region going down, you should use backups and replication across AWS Regions.
9)ElastiCache and RDS Read Replicas do indeed help with scaling reads.
10)You would like to ensure you have a replica of your database available in another 
AWS Region if a disaster happens to your main AWS Region. 
Which database do you recommend to implement this easily?
Aurora Global Databases allows you to have an Aurora Replica in another AWS Region, with up to 5 secondary regions.


EBS
====
1)High permormance hrdware I/O thing its EC2 Instance Store.
2)Need to remaind EBS Volume Types in higlevel (General Purpose SSD, Provisioned IOPS (PIOPS) SSD, Hard Disk Drives (HDD))
 like EBS –Volume Types Summary.
3)Exam ask you EFS – Performance & Storage Classes.
4)When should you use EFS, What option should you set of EFS Network file system for ensure the validate and complement the requirment.
5)EBS root valume by default delete when ec2 terminate, we can peserved it after terminate EC2, Exam may ask about a scenario.
6)EBS volumes are created in a specific AZ and can only be attached to one EC2 instance at a time. 
It is not help any way for stateless app.

 
ELB
====
1)If exam talking about the Uses GENEVE protocol on port 6081, thing its GLB.
2)A web application hosted on a fleet of EC2 instances managed by an Auto Scaling Group. 
You are exposing this application through an Application Load Balancer. Both the EC2 instances and the ALB are deployed on a 
VPC with the following CIDR 192.168.0.0/18. 
How do you configure the EC2 instances' security group to ensure only the ALB can access them on port 80?

Ans:Add an Inbound Rule with port 80  and ALB's Security Group as the source.
This is the most secure way of ensuring only the ALB can access the EC2 instances. 
Referencing by security groups in rules is an extremely powerful rule and many questions at the exam rely on it. 
Make sure you fully master the concepts behind it!

3)An application is deployed with an Application Load Balancer and an Auto Scaling Group. Currently, you manually 
scale the ASG and you would like to define a Scaling Policy that will ensure the average number of connections to your 
EC2 instances is around 1000. Which Scaling Policy should you use?
Ans:Target Scaling Group.


S3:
=====
1)Exam will asked which Encryption one are adapted to which situation base on scenario.
2)Cross S3 will come with simple way.
3)If a client does a cross-origin request on our S3 bucket, we need to enable the correct CORS headers.
4)If you don’t specify or configure a default region, then us-east-1 will be chosen by default using SDK.
5)You and your colleague are working on an application that's interacting with some AWS services through making API calls. 
Your colleague can run the application on his machine without issues, while you get API Authorization Exceptions. 
What should you do?
Ans:Chack both IAM using AWS Policy Simulator.

2)Filtering of server side data of S3 get List thing its S3 Select & Glacier Select

3) Your application on EC2 creates images thumbnails after profile photos
are uploaded to Amazon S3. These thumbnails can be easily recreated,
and only need to be kept for 45 days. The source images should be able
to be immediately retrieved for these 45 days, and afterwards, the user
can wait up to 6 hours. How would you design this?
S3 source images can be on STANDARD, with a lifecycle configuration
to transition them to GLACIER after 45 days.
S3 thumbnails can be on ONEZONE_IA, with a lifecycle configuration
to expire them (delete them) after 45 days. 

4)
• A rule in your company states that you should be able to recover your
deleted S3 objects immediately for 15 days, although this may happen rarely.
After this time, and for up to 365 days, deleted objects should be recoverable
within 48 hours.
• You need to enable S3 versioning in order to have object versions, so that
“deleted objects” are in fact hidden by a “delete marker” and can be
recovered
• You can transition these “noncurrent versions” of the object to S3_IA
• You can transition afterwards these “noncurrent versions” to
DEEP_ARCHIVE

5)S3Request pay
6)analyze data in S3 using serverless SQL, its Athena

7)Create an application that will traverse the S3 bucket, issue a Byte Range 
Fetch for the first 250 bytes, and store that information in RDS


CloudFront:
============
1) Global excelaration come to exam.
2) CloudFront pay model


AWS Storage:
============
1)Snowball cannot import to Glacier directly.
You must use Amazon S3 first, in combination with an S3 lifecycle policy
2)FSx its thard party fully manage file system (Luster/Windows)
3)Seamless integration with S3 Can “read/write S3” as a file system (through FSx)
4)FSx where we use Scratch/Persistent file system
5)You need to know the differences between all 3 storage gateway.
Lot of scenario bas on it.
6)Volume Gateway two type, Cached volume and Storage volume.


SQS, Kinesis:
=============
1)If you see decoupling, think its SQS Que.
2)What is needed to publish coess account SQS or event notification from S3.
3)SQS Message visibility time concept with ChangeMessageVisible to take more time to process.
4)API call wait for reduce the latency think it SQS long puling.
5)Request-Response Systems Que how to work it.
6)SNS and FiFo Queue.



Serverless:
===========
1)Exam does serverless havily !
2)Lamda limetation
3)Lambda@Edge at high level.
4)DynamoDB High level view adn few advance feature.
5)DynamoDB Read/Write capacity must be come.
6) if “Sig v4” think its IAM permission for API Gateway
7)If need Custom Authorization, Its Lambd Authorizer


AWS Databases:
=============
1)If see In-memory data store, sub-millisecond this is ElastiCache
2)High performance data analysis its Redshift
3)Anytime you see "search",partial matches think ElasticSearch.


CloudWatch/CloudTrail/Config:
============================
1)Cross Acccount access and Ausume role, Its STS
2)Exam ask about high level of 3Type AD
3)Any time you see multiple acc login with SAML, Its SSO


(IAM) - Advanced:
================
1)Cross Acccount access and Ausume role, Its STS
2)Exam ask about high level of 3Type AD
3)Any time you see multiple acc login with SAML, Its SSO


Security:
========

1)If see, rotation of key, secrect storage, integration wtih RDS then Its Scerect Manager
2)WAP only for Deploy on Application Load Balancer, API Gateway, CloudFront
3)WAP Protects from common attack - SQL injection and Cross-Site Scripting (XSS)
4)Two or Three question about SharedResponsability



vpc:
=====
1)Conndct DX need more then 1 month
2)Direct Connect - Resiliency | H/M
3)AWS PrivateLink (VPC Endpoint Services)
4)IP Multicast its mean Transit Gateway
5)Transit Gateway: Site-to-Site VPN ECMP
6)Transit Gateway: throughput with ECMP
7)IPv6 Troubleshooting May Ipv4 are finish


DisasterRecoveryMigrations:
===========================
1)Exam will provide scnereio and what is best for backup Pilot Light, Warm Standby ,Multi Site or All AWS Multi Region.




MoreSolutionArch:
================
1)If you want to every envent from s3 then make sure of s3 object versioning.
2)S3 enent only three reciver - SQS, SNS and Lamda, you can make many of Arcturacture from of this three.

User -> CloudFront->APIGateway->Application->Database->DBCaching(MemCach, Radis,DX)
3)CloudFron is Cashinc in EdgeLocation, API Gateway cacheing on Regional location
For Updated data in CloudFron add TTL time for updated data.
Application not cash, Database not cash, S3 not Cash.
4)High Performance Computing (HPC) come to exam.
5)Elastic Network Adapter (ENA) up to 100 Gbps for high permormance
6)Diffentent of enA, ENI, EFI, or any




OtherService:
=============
1)Multi account region deployment its StackSets
2)state machine,workflow(Job), orkstation, lamda if in exam its Step Functions.
2)Hadoop clusters (Big Data) processing its EMR.
3)In the exam: Chef & Puppet needed => AWS Opsworks
4)VDI (Virtual Desktop Infrastructure) its WorkSpaces
5)across mobile and web apps in real-time and GraphQL its AppSync.
6)Full Trusted Advisor – Available for Business & Enterprise support plans






#Domain Of Exam
-------------------------------------------------
Domain 1: Design Resilient Architectures 30%
Domain 2: Design High-Performing Architectures 28%
Domain 3: Design Secure Applications and Architectures 24%
Domain 4: Design Cost-Optimized Architectures 18%
TOTAL 100%


Domain 1: Design Resilient Architectures

	1.1 Design a multi-tier architecture solution
		 Determine a solution design based on access patterns.
		 Determine a scaling strategy for components used in a design.
		 Select an appropriate database based on requirements.
		 Select an appropriate compute and storage service based on requirements.
	1.2 Design highly available and/or fault-tolerant architectures
		 Determine the amount of resources needed to provide a fault-tolerant architecture across
		Availability Zones.
		 Select a highly available configuration to mitigate single points of failure.
		 Apply AWS services to improve the reliability of legacy applications when application changes
		are not possible.
		 Select an appropriate disaster recovery strategy to meet business requirements.
		 Identify key performance indicators to ensure the high availability of the solution.
	1.3 Design decoupling mechanisms using AWS services
		 Determine which AWS services can be leveraged to achieve loose coupling of components.
		 Determine when to leverage serverless technologies to enable decoupling.
	1.4 Choose appropriate resilient storage
		 Define a strategy to ensure the durability of data.
		 Identify how data service consistency will affect the operation of the application.
		 Select data services that will meet the access requirements of the application.
		 Identify storage services that can be used with hybrid or non-cloud-native applications.


Domain 2: Design High-Performing Architectures

	2.1 Identify elastic and scalable compute solutions for a workload
		 Select the appropriate instance(s) based on compute, storage, and networking requirements.
		 Choose the appropriate architecture and services that scale to meet performance
		requirements.
		 Identify metrics to monitor the performance of the solution. 
	2.2 Select high-performing and scalable storage solutions for a workload
		 Select a storage service and configuration that meets performance demands.
		 Determine storage services that can scale to accommodate future needs.
	2.3 Select high-performing networking solutions for a workload
		 Select appropriate AWS connectivity options to meet performance demands.
		 Select appropriate features to optimize connectivity to AWS public services.
		 Determine an edge caching strategy to provide performance benefits.
		 Select appropriate data transfer service for migration and/or ingestion.
	2.4 Choose high-performing database solutions for a workload
		 Select an appropriate database scaling strategy.
		 Determine when database caching is required for performance improvement.
		 Choose a suitable database service to meet performance needs.
		
		
Domain 3: Design Secure Applications and Architectures

	3.1 Design secure access to AWS resources
		 Determine when to choose between users, groups, and roles.
		 Interpret the net effect of a given access policy.
		 Select appropriate techniques to secure a root account.
		 Determine ways to secure credentials using features of AWS IAM.
		 Determine the secure method for an application to access AWS APIs.
		 Select appropriate services to create traceability for access to AWS resources.
	3.2 Design secure application tiers
		 Given traffic control requirements, determine when and how to use security groups and
		network ACLs.
		 Determine a network segmentation strategy using public and private subnets.
		 Select the appropriate routing mechanism to securely access AWS service endpoints or
		internet-based resources from Amazon VPC.
		 Select appropriate AWS services to protect applications from external threats.
	3.3 Select appropriate data security options
		 Determine the policies that need to be applied to objects based on access patterns.
		 Select appropriate encryption options for data at rest and in transit for AWS services.
		 Select appropriate key management options based on requirements.
		
		
Domain 4: Design Cost-Optimized Architectures

	4.1 Identify cost-effective storage solutions
		 Determine the most cost-effective data storage options based on requirements.
		 Apply automated processes to ensure that data over time is stored on storage tiers that
		minimize costs.
	4.2 Identify cost-effective compute and database services
		 Determine the most cost-effective Amazon EC2 billing options for each aspect of the
		workload.
		 Determine the most cost-effective database options based on requirements.
		 Select appropriate scaling strategies from a cost perspective.
		 Select and size compute resources that are optimally suited for the workload.
		 Determine options to minimize total cost of ownership (TCO) through managed services and
		serverless architectures.
	4.3 Design cost-optimized network architectures
		 Identify when content delivery can be used to reduce costs.
		 Determine strategies to reduce data transfer costs within AWS.
		 Determine the most cost-effective connectivity options between AWS and on-premises
		environments.
		
	
	
#Which key tools, technologies, and concepts might be covered on the exam?
-------------------------------------------------
	 Compute
	 Cost management
	 Database
	 Disaster recovery
	 High availability
	 Management and governance
	 Microservices and component decoupling
	 Migration and data transfer
	 Networking, connectivity, and content delivery
	 Security
	 Serverless design principles
	 Storage


#AWS services and features
-------------------------------------------------
Analytics:
	 Amazon Athena
	 Amazon Elasticsearch Service (Amazon ES)
	 Amazon EMR
	 AWS Glue
	 Amazon Kinesis
	 Amazon QuickSight


AWS Billing and Cost Management:
	 AWS Budgets
	 Cost Explorer
	
Application Integration:
	 Amazon Simple Notification Service (Amazon SNS)
	 Amazon Simple Queue Service (Amazon SQS)
	
Compute:
	 Amazon EC2
	 AWS Elastic Beanstalk
	 Amazon Elastic Container Service (Amazon ECS)
	 Amazon Elastic Kubernetes Service (Amazon EKS)
	 Elastic Load Balancing
	 AWS Fargate
	 AWS Lambda
	
Database:
	 Amazon Aurora
	 Amazon DynamoDB
	 Amazon ElastiCache
	 Amazon RDS
	 Amazon Redshift
	
Management and Governance:
	 AWS Auto Scaling
	 AWS Backup
	 AWS CloudFormation
	 AWS CloudTrail
	 Amazon CloudWatch
	 AWS Config
	 Amazon EventBridge (Amazon CloudWatch Events)
	 AWS Organizations
	 AWS Resource Access Manager
	 AWS Systems Manager
	 AWS Trusted Advisor
	
Migration and Transfer:
	 AWS Database Migration Service (AWS DMS)
	 AWS DataSync
	 AWS Migration Hub
	 AWS Server Migration Service (AWS SMS)
	 AWS Snowball
	 AWS Transfer Family
	
Networking and Content Delivery:
	 Amazon API Gateway
	 Amazon CloudFront
	 AWS Direct Connect
	 AWS Global Accelerator
	 Amazon Route 53
	 AWS Transit Gateway
	 Amazon VPC (and associated features)
	
Security, Identity, and Compliance:
	 AWS Certificate Manager (ACM)
	 AWS Directory Service
	 Amazon GuardDuty
	 AWS Identity and Access Management (IAM)
	 Amazon Inspector
	 AWS Key Management Service (AWS KMS)
	 Amazon Macie
	 AWS Secrets Manager
	 AWS Shield
	 AWS Single Sign-On
	 AWS WAF
	
Storage:
	 Amazon Elastic Block Store (Amazon EBS)
	 Amazon Elastic File System (Amazon EFS)
	 Amazon FSx
	 Amazon S3
	 Amazon S3 Glacier
	 AWS Storage Gateway
	 
 






