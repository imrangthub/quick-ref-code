#################################################
#                 AWS                          #
#################################################


=>sudo chmod 777 -R destinationFolder/*
Allow ec2 file permission


=>/mnt/efs/fs1
MetadataURL is http://169.254.169.254/latest/meta-data


#!/bin/bash
yum update -y
yum install -y httpd
systemctl start httpd
systemctl enable httpd
echo "<h1>Hello World from $(hostname -f)</h1>" > /var/www/html/index.html


We all know Route53, IAM, CloudFront, WAF are Global.
=================================================
#General | Basic | Info      
================================================= 

AWS- 
  -Region
  -VPC
	-AvailableZone
	-Router 
      -NACL*
	-Subnet
      -SecurityGroup	
	  -EC2
 -Gateway




#BasicInfo
-------------------------------------------------

Amazon web service is an online platform that provides scalable and cost-effective cloud computing solutions.
AWS has 80 Availability Zones across 25 geographic regions global data centers. 

AWS:
-Security: AWS provides a secure and durable platform that provides end-to-end security and storage.
-Experience: The skills and infrastructure management born from Amazon’s many years of experience can be very valuable.
-Flexibility: It allows users to select the operating systems, language, database, and other services as per their requirements.
-Easy to use: AWS lets you host your applications quickly and securely, regardless of whether it’s an existing or new application.
-Scalable: The applications you use can be scaled up or down, depending on your requirements.
-Cost savings: You only pay for the compute power, storage, and other resources that you use, without any long-term commitments.
-Scheduling: This enables you to start and stop AWS services at predetermined times
-Reliability: AWS takes multiple backups at servers at multiple physical locations


#Cloud Computing?           
-------------------------------------------------
Cloud computing is a computing service made available over the internet.
Cloud computing is a pay-as-you-go model for delivering IT resources.
You pay only for what you use.


Cloud computing differs from a traditional, on-premises environment in many ways,
including flexible, global, and scalable capacity, managed services, built-in security,
options for cost optimization, and various operating models.

=============================================== 

#Region        
-------------------------------------------------
How to choose  and AWS Region?
 - Compliance
 - Proximity
 - Available Service
 - Pricing
 
 
#Availabiulity Zones |  AZ | az      
-------------------------------------------------
Each region hase many availabuility zone.
Usually 3, Min 2 and Max 6;





=================================================
#Subnet | cidr             
================================================= 
Subnet create on availablity zone not on Region. Same subnet can not take more then one.


Public Subnet
 - If a subnet traffic is routed to on Internet Gateway it is public with a public IP.
Private Subnet
 - If a Subnet dosnot have a route to the internet Gateway then it is private.
   When you create a VPC you must specify on IPv4 CIDR blica for the 
   , The allowed blick size is betwwn /16 to /28 netmask.
   The first four and last IP address of Subnet cannot be assigned.
    - Suppose a IP => 10.0.0.0/16
	- 10.0.0.0 Network address
	- 10.0.0.1 Reserved by AWS for VPC Route
	- 10.0.0.2 Reserved By AWS for DNS server
	- 10.0.0.3 Reserved By AWS for future use
	- 10.0.0.255 Brodycast Address
	AWS do not support brodcust in a VPC but reserve the address.
	
	


=================================================
#VPC | vpc | virtual private cloud                
================================================= 
Vpc is a Virtual Network or DataCenter inside AWS for one Client.
A VPC can span multiple availability zones in a region.
The CIDR block for the default VPC is always a 16 subnet mask; in this example, it's 172.31.0.0/16. It means this VPC can provide up to 65,536 IP addresses.

- It is logical Isolated from Other virtual Network in the AWS.
- Max 5 VPC can be created and 200 subnet in 1 VPC.
- We Can allocate Max 5 Elastic IP.
- Once a VPC created DHCP, NACL and Sucurity Group will be created automatically.
- A VPC is confied to on AWS Region and dos not extend between Region.

- Onece the VPC is created, Its CIDR block range cant change.
  (You can create another CIDR make it primary and older one will be Secondary then you can delete fitst one)
- If you need a diffenent CIDR size, create a new VPC.
- The different subnets wihtin a VPC cannot overlap.
- You can expend yor vpc CIDR by adding new/extra IP address.


VPC create on Region not available zone. all property of VPC are Regional becaus its exists on Region. 
One vpc cant extend more then one Region.


VPC Two Type

- Default Vpc
- Custom Vpc

Primary diffent of of both, default vpc has internet gatway custom vpc has not, you can added.

Default VPC has default CIDR, Security Group, NACL and Route table setting.
In Custom VPC has to be create considering its CIDR dos not have Internet Getwat be default.
 

VPC Peering: 
A peering connection can be made between your own VPCs or with a VPC in another AWS account, as long as it is in the same region.

VPCs, but transitive peering is not supported. In other words, VPC A can connect to B and C in the above diagram, 
but C cannot communicate with B unless directly paired.

We can have up to 200 Subnets per Amazon Virtual Private Cloud (VPC).

Monitor VPC by using:
CloudWatch and CloudWatch logs
VPC Flow Logs

#Component of VPC
-------------------------------------------------

 - CIDR and IP address subnet
 - Implied router and Routing Table
 - Internet Gatway
 - Security Group
 - Network ACL
 - Virtual Private gatway 
 - Peering connections
 - Elastic IP




-Each VPC has a main route table, by default
-Main route table has a default route enabling communication between resources in all subnets in a VPC
-Default route rule CANNOT be deleted/edited
-HOWEVER you can add/edit/delete other routing rules to the main route table


#Subnet 
-------------------------------------------------
Amazon defines a route table as a set of rules, called routes, which are used to determine where network traffic is directed.
Each subnet has to be linked to a route table, and a subnet can only be linked to one route table. On the other hand, one route table can have associations with multiple subnets. 


-Each subnet can have its own route table OR share its route table with the VPC
-If a subnet does not have a route table associated with it, it implicitly uses the route table of its VPC
-Multiple subnets can share a route table
-HOWEVER at any point in time, a subnet can be associated with one route table ONLY


Public Subnet:
-Communication is allowed from subnet to internet
-Communication is allowed from internet to subnet


Security products and features:
-Security groups - This acts as a firewall for the EC2 instances, controlling inbound and outbound traffic at the instance level.
-Network access control lists - It acts as a firewall for the subnets, controlling inbound and outbound traffic at the subnet level.
-Flow logs - These capture the inbound and outbound traffic from the network interfaces in your VPC.



=================================================
#VPN | VPNs | vpn             
================================================= 
VPCs can also serve as a bridge between your corporate data center and the AWS cloud. 
With a VPC Virtual Private Network (VPN), your VPC becomes an extension of your on-prem environment.



=================================================
#DirectConnect    | DX       | DC
================================================= 
AWS Direct Connect Private dedicated network connection from on-premises to AWS


Direct Connect is an AWS service that establishes a dedicated network connection between your premises and AWS. 
You can create this private connectivity to reduce network costs, increase bandwidth, and provide more consistent network experience 
compared to regular internet-based connections.

The use case for Direct Connect is high throughput workloads or if you need a stable or reliable connection


DirectConnect connects your on-prem with your VPC through a non-public tunnel.

=================================================
#NAT Gateway        
================================================= 
A Network Address Translation (NAT) device can be used to enable instances in a private subnet to connect to the internet 
or the AWS services, but this prevents the internet from initiating connections with the instances in a private subnet.

However, your private subnet database instance might still need internet access or the ability to connect to other AWS resources. 
You can use a NAT device to do so. 

The NAT device directs traffic from your private subnet to either the internet or other AWS services. 
It then sends the response back to your instances. When traffic is directed to the internet, the source 
IP address of your instance is replaced with the NAT device address, and when the internet traffic returns, 
the NAT device translates the address to your instance’s private IP address.


NAT device is added to the public subnet to get internet connectivity.



AWS provides two kinds of NAT devices:
NAT gateway 
NAT instance 


NAT Gateway
A NAT gateway must be launched in a public subnet because it needs internet connectivity. 
It also requires an elastic IP address, which you can select at the time of launch.

Once created, you need to update the route table associated with your private subnet to point internet-bound traffic to the NAT gateway. 
This way, the instances in your private subnet can communicate with the internet.

=================================================
#AWS PrivateLink
=================================================
In summary, Global Accelerator is a fast/reliable pipeline between user and application.

AWS PrivateLink simplifies the security of data shared with cloud-based applications by eliminating the exposure of 
data to the public Internet. AWS PrivateLink provides private connectivity between different VPCs, AWS services, 
and on-premises applications, securely on the Amazon network.

This is useful because different AWS services often talk to each other over the internet. 
If you do not want that behavior and instead want AWS services to only communicate within the AWS network, use AWS PrivateLink. 



Summary: AWS PrivateLink connects your AWS services with other AWS services through a non-public tunnel.



Network ACL:
-------------------------------------------------

Every subnet in your VPC must be associated with an ACL, failing which the subnet gets automatically associated with your default ACL.

One subnet can only be linked with one ACL. On the other hand, an ACL can be linked to multiple subnets.


Amazon VPC can contain anywhere from 16 to 65,536 IP addresses. You can select your CIDR block according to the number of instances needed.

VPC limits:

Five VPCs per region
200 subnets per VPC
200 route tables per VPC
500 security groups per VPC
50 inbound and outbound rules per VPC


Amazon VPC costs:
If you opt to create a hardware VPN connection associated with your VPC using Virtual Private Gateway, you will have to pay for each 
VPN connection hour that your VPN connection is provisioned and available. Each partial VPN connection hour consumed is 
billed as a full hour. You'll also incur standard AWS data transfer charges for all data transferred via the VPN connection. 

If you create a NAT gateway in your VPC, Charges are levied for each NAT gateway hour that your NAT gateway is 
provisioned and available for. Data processing charges apply for each gigabyte processed through a NAT gateway. 
Each partial NAT gateway hour consumed is billed as a full hour.



=================================================
#BGP |   Border Gateway Protocol          
================================================= 
BGP is a means by which all junction points on the internet (routers) communicate with each other to dynamically establish the 
correct (and correctly weighted) paths that network packets should follow to traverse the global networking

=================================================
#AWS Global Accelerator
=================================================

In summary, Global Accelerator is a fast/reliable pipeline between user and application.


AWS Global Accelerator accelerates connectivity to improve performance and availability for users. 
Global Accelerator sits on top of the AWS backbone and directs traffic to optimal endpoints worldwide. 
By default, Global Accelerator provides you two static IP addresses that you can make use of.

Global Accelerator also provides fast regional failover.




=================================================
#Internet Protocol (IP    
================================================= 
The Internet Protocol (IP) uses three types of addressing schemes: Unicast, Multicast, and Anycast.

Unicast:
A Unicast address is used to identify a single unique host. It is used to send data to a single destination. 
In computer networking, unicast communication is a one-to-one transmission from one point in the network to another. 


Multicast:
A Multicast address is used to deliver data to a group of destinations (a one-to-many transmission). 
IP multicast group addresses are represented by class-D IP addresses reserved specifically for multicast 
communications, ranging from 224.0.0.0 through 239.255.255.255. Any IP packet sent to a multicast address is 
delivered to only those hosts that have joined that particular IP Multicast group, resulting in less network traffic, 
thereby reducing bandwidth and network overhead. If the host hasn’t joined the group, the receiver ignores the packets 
at the hardware level, eliminating platform software resource consumption in that network element. 
IPv6 multicast replaces broadcast addresses that were supported in IPv4. 

Anycast:
Anycast, also known as IP Anycast or Anycast routing, is an IP network addressing scheme that allows multiple servers to share the same 
IP address, allowing for multiple physical destination servers to be logically identified by a single IP address. 
Based on the location of the user request, the anycast routers send it to the server in the network based on a least-cost analysis 
that includes assessing the number of hops, shortest distance, lowest transit cost, and minimum latency measurements to optimize the 
selection of a destination server.




=================================================
#Security Groups    
================================================= 

Security Groups are used to control access (SSH, HTTP, RDP, etc.) with EC2. 
They act as a virtual firewall for your instances to control inbound and outbound traffic. 
When you launch an instance in a VPC, you can assign up to five security groups to 
the instance and security groups act at the instance level, not the subnet level.

Security groups are specific to a single VPC, so you can't share a Security Group 
between multiple VPCs. However, you can copy a Security Group to create a new Security Group with the same 
rules in another VPC for the same AWS Account.


Security Groups are regional and can span AZs, but can't be cross-regional.

You can specify the source of your security group (basically who is allowed to bypass the virtual firewall) 
to be a single /32 IP address, an IP range, or even a separate security group.

You cannot block specific IP addresses with Security Groups (use NACLs instead)


Security Groups are stateful:
-If an outgoing request is allowed, the incoming response for it is automatically allowed.
-If an incoming request is allowed, an outgoing response for it is automatically allowed


=================================================
#WAF   | Web Application Firewall (WAF)
================================================= 
AWS WAF is a web application that lets you allow or block the HTTP(s) requests that are bound for CloudFront, 
API Gateway, Application Load Balancers, EC2, and other Layer 7 entry points into your AWS environment. 
AWS WAF gives you control over how traffic reaches your applications by enabling you to create security 
rules that block common attack patterns, such as SQL injection or cross-site scripting, 
and rules that filter out specific traffic patterns that you can define. 



=================================================
#Storage | DB | db | s3 | efs | ebs           
================================================= 
There are several categories of databases:
Relational (OLTP and OLAP), Document, Key Value, Graph, In Memory among others

Buckets:
-Buckets are a universal namespace, i.e., the bucket names must be unique.
-If uploading of an object to S3 bucket is successful, we receive a HTTP 200 code.
-S3, S3-IA, S3 Reduced Redundancy Storage are the storage classes.
-Encryption is of two types, i.e., Client Side Encryption and Server Side Encryption
-Access to the buckets can be controlled by using either ACL (Access Control List) or bucket policies.
-By default buckets are private and all the objects stored in a bucket are also private.


Amazon Simple Storage Service (Amazon S3) is an object storage service that offers 
industry-leading scalability, data availability, security, and performance.

Amazon S3 is an object storage service that stores data as objects within buckets. 
An object is a file and any metadata that describes the file. A bucket is a container for objects.
Each object has a key (or key name), which is the unique identifier for the object within the bucket.

Access control lists (ACLs)
You can use ACLs to grant read and write permissions to authorized users for individual buckets and objects. 
Each bucket and object has an ACL attached to it as a subresource. 
The ACL defines which AWS accounts or groups are granted access and the type of access.

Amazon S3 cloud storage is an object-based storage service. You cannot install an operating system when you use 
Amazon S3 storage because data cannot be accessed on the block level as it is required by an operating system.


AWS Storage:
  - Simple Storage service (S3)
  - Elastic file system (EFS)
  - Elastic Block Storage (EBS)
  - Glacier
  - Snowball
  
  
   
#Simple Storage Service (S3)
--------------------------------------------------
S3 Object base storage, its able to access via http/https.
Its a distrubute database and data keep in bucket.

TypeOfS3:
 - S3 Standard
 - Amazon Glacher
 - Glacher Deep archive
 - Standart infrequental acc
 - One zone IA
 - Intelactual
 
 
 
#Amazon S3 Replication
--------------------------------------------------

Replication is the automatic, asynchronous copying of objects across buckets in the same or different AWS Regions. 
Replication copies newly created objects and object updates from a source bucket to a destination bucket or buckets. 

When you configure replication, you add replication rules to the source bucket. 
Replication rules define which source bucket objects to replicate and the destination bucket or buckets where 
the replicated objects are stored. You can create a rule to replicate all the objects in a bucket or a subset of 
objects with a specific key name prefix, one or more object tags, or both. A destination bucket can be in the 
same AWS account as the source bucket, or it can be in a different account.

If you specify an object version ID to delete, Amazon S3 deletes that object version in the source bucket. 
But it doesn't replicate the deletion in the destination bucket. In other words, 
it doesn't delete the same object version from the destination bucket. This protects data from malicious deletions.

When you add a replication rule to a bucket, the rule is enabled by default, so it starts working as soon as you save it.

Amazon Simple Storage Service (S3) Replication is an elastic, fully managed, low cost feature that replicates objects 
between buckets. S3 Replication offers the most flexibility and functionality in cloud storage, giving you the controls you need to 
meet your data sovereignty and other business needs.

With Amazon S3 Replication, you can configure Amazon S3 to automatically replicate S3 objects across different 
AWS Regions by using S3 Cross-Region Replication (CRR) or between buckets in the same AWS Region by using S3 
Same-Region Replication (SRR). S3 Replication offers the flexibility of replicating to multiple destination buckets in the same, 
or different AWS Regions. S3 Replication supports two-way replication between two or more buckets in the same or different AWS Regions. 


 
#S3 security
-------------------------------------------------

Security responsibility includes the following areas:

-Managing your data, including object ownership and encryption.
-Classifying your assets.
-Managing access to your data using IAM roles and other service configurations to apply the appropriate permissions.
-Enabling detective controls such as AWS CloudTrail or Amazon GuardDuty for Amazon S3.


Server-Side Encryption:
Request Amazon S3 to encrypt your object before saving it on disks in its data centers and then decrypt it 
when you download the objects. Server-side encryption can help reduce risk to your data by encrypting 
the data with a key that is stored in a different mechanism than the mechanism that stores the data itself.

Amazon S3 provides these server-side encryption options:
-Server-side encryption with Amazon S3‐managed keys (SSE-S3).
-Server-side encryption with KMS key stored in AWS Key Management Service (SSE-KMS).
-Server-side encryption with customer-provided keys (SSE-C).


Client-Side Encryption:

Encrypt data client-side and upload the encrypted data to Amazon S3. In this case, you manage the encryption process, 
the encryption keys, and related tools. As with server-side encryption, client-side encryption can help 
reduce risk by encrypting the data with a key that is stored in a different mechanism than the mechanism that stores the data itself.




#EFS - Elastic File System
-------------------------------------------------
Simple, serverless, set-and-forget, elastic file system

Amazon Elastic File System (Amazon EFS) provides a simple, serverless, set-and-forget elastic file system for use with AWS Cloud services and on-premises resources. 

Amazon EFS provides a simple, serverless, set-and-forget elastic file system.
With Amazon EFS, you can create a file system, mount the file system on an Amazon EC2 instance,
and then read and write data to and from your file system. 
You can mount an Amazon EFS file system in your virtual private cloud (VPC), through the 
Network File System versions 4.0 and 4.1 (NFSv4) protocol.



How do I access a file system from an Amazon EC2 instance?
To access your file system, you mount the file system on an Amazon EC2 Linux-based instance using the standard Linux mount command 
and the file system’s DNS name. 
Once you’ve mounted, you can work with the files and directories in your file system just like you would with a local file system.



#Object Lifecycle Management
------------------------------------------------
A lifecycle configuration is a set of rules that define actions that AWS S3 applies to a group of objects. There are two types of actions:

Transaction Actions
This action defines objects’ transition from one storage class to another.

Expiration Actions
This action deletes objects in the Amazon S3 bucket.


#Amazon DynamoDB
------------------------------------------------
Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. 
It's a fully managed, multiregion, multimaster, durable non-SQL database. It comes with built-in security, backup and restore, 
and in-memory caching for internet-scale applications.


The main components of DynamoDB are:

-a collection which serves as the foundational table
-a document which is equivalent to a row in a SQL database
-key-value pairs which are the fields within the document or row

Amazon DynamoDB is a Serverless key-value and document database that delivers single-digit millisecond performance at any scale. 
It's a fully managed, multiregion, multimaster, durable database with built-in security, backup and restore, 
and in-memory caching for internet-scale applications. DynamoDB can handle more than 10 trillion requests 
per day and can support peaks of more than 20 million requests per second.

Amazon DynamoDB is a fully managed, serverless, key-value NoSQL database designed to run high-performance applications at any scale. 
DynamoDB offers built-in security, continuous backups, automated multi-Region replication, in-memory caching, and data export tools.


DynamoDB is accessible via an HTTP API and performs authentication & authorization via IAM roles, 
making it a perfect fit for building Serverless applications.


DAX:
===
DynamoDB Accelerator (DAX) delivers fast response times for accessing eventually consistent data. Certain usecases requires microseconds response times




#Amazon Aurora
------------------------------------------------
Amazon Aurora (Aurora) is a fully managed relational database engine that's compatible with MySQL and PostgreSQL.
The underlying storage grows automatically as needed. An Aurora cluster volume can grow to a maximum size of 128 tebibytes (TiB). 


Amazon Aurora is a relational database management system (RDBMS) built for the cloud with full MySQL and PostgreSQL compatibility. 
Aurora gives you the performance and availability of commercial-grade databases at one-tenth the cost.

Amazon Aurora provides built-in security, continuous backups, serverless compute, 
up to 15 read replicas, automated multi-Region replication, and integrations with other AWS services.

Amazon Aurora is a modern relational database service offering performance and high availability at scale, fully open source 
MySQL- and PostgreSQL-compatible editions, and a range of developer tools for 
building serverless and machine learning (ML)-driven applications.

Aurora features a distributed, fault-tolerant, and self-healing storage system that is 
decoupled from compute resources and auto-scales up to 128 TB per database instance. 
It delivers high performance and availability with up to 15 low-latency read replicas, 
point-in-time recovery, continuous backup to Amazon Simple Storage Service (Amazon S3), 
and replication across three Availability Zones (AZs).


#Data Warehouse
-------------------------------------------------
A data warehouse is a specialized type of relational database, which is optimized for
analysis and reporting of large amounts of data. It can be used to combine
transactional data from disparate sources (such as user behavior in a web application,
data from your finance and billing system, or customer relationship management or
CRM) to make them available for analysis and decision-making.

Traditionally, setting up, running, and scaling a data warehouse has been complicated
and expensive. On AWS, you can leverage Amazon Redshift, a managed data
warehouse service that is designed to operate at less than a tenth the cost of
traditional solutions.


#Redshift
-------------------------------------------------
Fastest, easiest, and most widely used cloud data warehouse

Amazon Redshift is a fully managed data warehouse service in the cloud. Its datasets range from 
100s of gigabytes to a petabyte. The initial process to create a data warehouse is to launch a 
set of compute resources called nodes, which are organized into groups called cluster. 
After that you can process your queries.

Amazon Redshift is a fully managed, scalable cloud data warehouse that accelerates your time to insights with fast, easy, 
and secure analytics at scale. Thousands of customers rely on 
Amazon Redshift to analyze data from terabytes to petabytes and run complex analytical queries.

Redshift is not multi-AZ, if you want multi-AZ you will need to spin up a separate cluster ingesting the same input. 
You can also manually restore snapshots to a new AZ in the event of an outage.


What are the differences between a database and a data warehouse? 
A database is any collection of data organized for storage, accessibility, and retrieval. 

A data warehouse is a type of database the integrates copies of transaction data 
from disparate source systems and provisions them for analytical use.


#Storage Gateway 
-------------------------------------------------
AWS Storage Gateway - Summary:
Key to look for : Hybrid storage (cloud + on premise)

File share (NFS or SMB) + Looking for S3 features and integrations => AWS Storage File Gateway

Tapes on cloud => AWS Storage Tape Gateway

Volumes on cloud (Block Storage) => AWS Storage Volume Gateway

High performance => Stored
Otherwise => Cached

Needs additional setup on-premises

VM image with AWS Storage Gateway software deployed on-premises or on EC2 instance





=================================================
# Edge locations             
================================================= 
Edge locations are AWS data centers designed to deliver services with the lowest latency possible.
Amazon has dozens of these data centers spread across the world. They’re closer to users than Regions or Availability Zones, 
often in major cities, so responses can be fast and snappy. 

UserCase:
A subset of services where latency matters, they use edge locations, they are:-

CloudFront:
which uses edge locations to cache copies of the content that it serves, so the content is closer to users and can be delivered to them faster.
Route 53: 
which serves DNS responses from edge locations, so that DNS queries that originate nearby can resolve faster (and, contrary to what you might think, is also Amazon’s premier database).
Web Application Firewall and AWS Shield:
which filter traffic in edge locations to stop unwanted traffic as soon as possible.




=================================================
#ElastiCache  | Elasti Cache      
================================================= 
Amazon ElastiCache is a fully managed, in-memory caching service supporting flexible, real-time use cases. 
You can use ElastiCache for caching, which accelerates application and database performance, 
or as a primary data store for use cases that don't require durability like session stores, gaming leaderboards, 
streaming, and analytics. 
ElastiCache is compatible with Redis and Memcached. 


Use cases:
Accelerate application performance
Ease backend database load
Build low-latency data stores


Amazon ElastiCache is a web service that makes it easy to deploy and run Memcached or Redis 
protocol-compliant server nodes in the cloud. Amazon ElastiCache improves the performance of web applications by 
allowing you to retrieve information from a fast, managed, in-memory system, instead of relying entirely on slower disk-based databases. 


=================================================
#CloudFront  | cludFont      
================================================= 
Global Accelerator and CloudFront both use the AWS global network and its edge locations around the world.

CloudFront is Amazon’s content delivery network that is primarily used to speed up websites. 
It’s particularly useful for large, static assets—like images and videos. CloudFront sits in front of an “origin” server 
(which serves the original content), and caches it at the edge locations around the world.

When a user visits a site, they’re routed to the nearest edge location using DNS. 
CloudFront looks to see if the page they requested is cached. If it is, the page is served directly from the cache. 
If it isn’t, CloudFront fetches the page from the origin, stores it in the cache, and serves it to the user. 
The next user to hit the same edge location will get the page served from the cache.


The AWS CDN service is called CloudFront. It serves up cached content and assets for the increased global performance of your application. 
The main components of CloudFront are the edge locations (cache endpoints), the origin (original source of truth to be cached 
such as an EC2 instance, an S3 bucket, an Elastic Load Balancer or a Route 53 config), and the distribution (the arrangement 
of edge locations from the origin or basically the network itself).


Performance
CloudFront improves performance for both cacheable content (such as images and videos) and 
dynamic content (such as API acceleration and dynamic site delivery).

Global Accelerator improves performance for a wide range of applications over TCP or UDP by proxying packets at the 
edge to applications running in one or more AWS Regions.

Use Cases:
CloudFront is a good fit for HTTP use cases
Global Accelerator is a good fit for non-HTTP use cases, such as gaming (UDP), IoT (MQTT), or VoIP, 
as well as for HTTP use cases that require static IP addresses or deterministic, fast regional failover.

Caching:
CloudFront supports Edge caching
Global Accelerator does not support Edge Caching.

Geo-Targeting:
Geo-Targeting is a concept where businesses can show personalized content to their audience based on their geographic location 
without changing the URL. This helps you create customized content for the audience of a specific 
geographical area, keeping their needs in the forefront.



=================================================
#AWS Global Accelerator (AMS SSPS)          
================================================= 
Global Accelerator, your users' traffic is moved off the internet and onto Amazon’s private global network 
through 90+ global edge locations, then directed to your application origins. 

AWS Global Accelerator is a networking service that 
improves the performance of your users’ traffic by up to 60% using Amazon Web Services’ global network infrastructure.

Global Accelerator, you are provided two global static public IPs that act as a fixed entry point to your application, 
improving availability. On the back end, add or remove your AWS application endpoints, such as Application Load Balancers, 
Network Load Balancers, EC2 Instances, and Elastic IPs without making user-facing changes.

Global Accelerator automatically re-routes your traffic to your nearest healthy available endpoint to mitigate endpoint failure.


It provides static IP addresses that act as a fixed entry point to application endpoints in a single or multiple AWS Regions, 
such as Application Load Balancers, Network Load Balancers or EC2 instances.



=================================================
#Amazon FSx       
================================================= 
Amazon FSx for Windows File Server provides a fully managed native Microsoft File System.
You can use Microsoft Active Directory to authenticate into the file system.


Amazon FSx is a fully managed third-party file system solution. It uses SSD storage to provide fast performance with low latency.
There are four available FSx solutions available in AWS:

Amazon FSx makes it easy and cost effective to launch, run, and scale feature-rich, high-performance file systems in the cloud. 
It supports a wide range of workloads with its reliability, security, scalability, and broad set of capabilities. 

Four widely-used file systems: 
 - NetApp ONTAP, 
 - OpenZFS, 
 - Windows File Server 
 - Lustre.
 
 
NetApp ONTAP
In collaboration with NetApp, AWS has launched Amazon FSx for NetApp ONTAP, a new cloud-based managed shared file and block storage 
service that brings the best of both worlds to their customers.
FSx for ONTAP delivers NFS, SMB and iSCSI storage powered by NetApp’s advanced data management system
 
Lustre
Amazon FSx for Lustre offers fully-managed storage built especially to provide high-performance at scale for compute workloads. 
It is ideal for machine learning, video rendering, high performance computing and financial simulations.



Windows File Server:
FSx for Windows File Server provides fully managed Microsoft Windows file servers, that are backed by a fully native Windows file system. 


FSx has two key differentiators compared to other Amazon’s previous file service offerings such as Elastic File Service (EFS). 
It comes with a complete file server built in, and it offers superior performance for demanding use cases.

Amazon FSx offers file systems designed for a variety of workload types. 
You can use AWS FSx as storage for Windows applications, machine learning (ML) and high-performance computing (HPC). 
FSx can also help with electronic design automation.


You can deploy your Amazon FSx for Windows in a single AZ or in a Multi-AZ configuration.
By default, all data is encrypted at rest.


=================================================
#Amazon FSx for Lustre
================================================= 
Amazon FSx for Lustre makes it easy and cost effective to launch and run the open source Lustre file system for high-performance 
computing applications. With FSx for Lustre, you can launch and run a file system that can process massive data sets at up to 
hundreds of gigabytes per second of throughput, millions of IOPS, and sub-millisecond latencies.


FSx for Lustre is compatible with the most popular Linux-based AMIs, including Amazon Linux, 
Amazon Linux 2, Red Hat Enterprise Linux (RHEL), CentOS, SUSE Linux and Ubuntu.

Since the Lustre file system is designed for high-performance computing workloads that typically run on compute clusters, 
choose EFS for normal Linux file system if your requirements don't match this use case.

FSx Lustre has the ability to store and retrieve data directly on S3 on its own.



=================================================
#SQS, SNS, MQ and Amazon Kinesis        
=================================================
Anytime multiple services need to receive the same event, you should consider SNS rather than SQS.

#AWS SQS
--------------------------------------------------
 The entire service is based on sending messages to the queue and allowing for applications (ex. ECS containers, Lambda functions)
 to poll for messages and process them. 
 The message stays in the queue until some application picks it up, processes it, and deletes the message when it’s done. 
 

#AWS SNS(Simple Notification Service)
--------------------------------------------------
It provides much more functionality than just the ability to send push notifications (emails, SMS, and mobile push). 
In fact, it’s a serverless publish-subscribe messaging system allowing to send events to multiple applications (subscribers) at the same time (fan-out), 
including SQS queues, Lambda functions, Kinesis Data Streams, and generic HTTP endpoints. 


In order to use the service, we only need to:
create a topic,
subscribe to a topic,
confirm the subscription,
start sending events to a topic to deliver them to all subscribers (potentially multiple applications and people).

#Amazon Kinesis 
--------------------------------------------------

Amazon Kinesis Data Streams is a serverless streaming data service that makes it easy to capture, process, 
and store data streams at any scale.
Amazon Kinesis makes it easy to collect, process, and analyze real-time, streaming data so you can get timely 
insights and react quickly to new information.

Amazon Kinesis is a managed, scalable, cloud-based service that allows real-time processing of streaming large amount of data per second. 
It is designed for real-time applications and allows developers to take in any amount of data from several sources, 
scaling up and down that can be run on EC2 instances.

It is used to capture, store, and process data from large, distributed streams such as event logs and social media feeds. 
After processing the data, Kinesis distributes it to multiple consumers simultaneously.

The producers continually push data to Kinesis Data Streams, and the consumers process the data in real time. 
Consumers (such as a custom application running on Amazon EC2 or an Amazon Kinesis Data Firehose delivery stream) 
can store their results using an AWS service such as Amazon DynamoDB, Amazon Redshift, or Amazon S3.

#MQ
--------------------------------------------------
Amazon MQ is a managed message broker service for Apache ActiveMQ and RabbitMQ that makes 
it easy to set up and operate message brokers in the cloud. 
You get direct access to the ActiveMQ and RabbitMQ consoles and industry standard APIs and protocols for messaging, 
including JMS, NMS, AMQP 1.0 and 0.9.1, STOMP, MQTT, and WebSocket. 
You can easily move from any message broker that uses these standards to Amazon MQ because you 
don’t have to rewrite any messaging code in your applications.



How do I migrate if I'm using a different message broker instead of ActiveMQ or RabbitMQ?
Amazon MQ provides compatibility with the most common messaging APIs, such as Java Message Service (JMS) and 
.NET Message Service (NMS), and protocols, including AMQP, STOMP, MQTT, and WebSocket. 
This makes it easy to switch from any standards-based message broker to Amazon MQ without rewriting the messaging code in your applications. 
In most cases, you can simply update the endpoints of your Amazon MQ broker to connect to your existing applications, 
and start sending messages.





=================================================
#CloudWatch  | cloudWatch      
================================================= 
Amazon CloudWatch is a monitoring and observability service built for DevOps engineers, developers, site reliability engineers (SREs), 
IT managers, and product owners. 

CloudWatch provides you with data and actionable insights to monitor your applications, respond to system-wide performance changes, 
and optimize resource utilization. 

CloudWatch collects monitoring and operational data in the form of logs, metrics, and events. 

CloudWatch collects monitoring and operational data in the form of logs, metrics, and events.
You can use CloudWatch to detect anomalous behavior in your environments, set alarms, visualize logs and metrics side by side, 
take automated actions, troubleshoot issues, and discover insights to keep your applications running smoothly.

You get a unified view of operational health and gain complete visibility of your AWS resources, applications, and services running on 
AWS and on-premises. You can use CloudWatch to detect anomalous behavior in your environments, set alarms, 
visualize logs and metrics side by side, take automated actions, troubleshoot issues, and discover insights to 
keep your applications running smoothly.
 
 - Use a single platform for observability
 - Collect metrics on AWS and on premises
 - Improve operational performance and resource optimization
 - Get operational visibility and insight
 - Derive actionable insights from logs
 
 
You can use metrics to calculate statistics and then present the data graphically in the CloudWatch console. 
You can configure alarm actions to stop, start, or terminate an Amazon EC2 instance when certain criteria are met.
 
Use cases:
-Monitor Amazon EC2
-Monitor Other Amazon Web Services Resources
-Monitor Custom Metrics
-Monitor and Store Logs
-Set Alarms
-Monitor and React to Resource Changes


CloudWatch is NOT CloudTrail so it is important to know that only CloudTrail can monitor AWS access for security and auditing reasons. 
CloudWatch is all about performance. CloudTrail is all about auditing.


=================================================
#AWS Event and EventBridge
================================================= 

#Event
--------------------------------------------------
An event indicates a change in an environment such as an AWS environment, a SaaS partner service or application, 
or one of your applications or services. The following are examples of events:

Amazon EC2 generates an event when the state of an instance changes from pending to running.

Amazon EC2 Auto Scaling generates events when it launches or terminates instances.

AWS CloudTrail publishes events when you make API calls.

You can also set up scheduled events that are generated on a periodic basis.

Events are represented as JSON objects and they all have a similar structure, and the same top-level fields.


#EventBridge
--------------------------------------------------
EventBridge is an event bus for messages that you want to propagate across your (micro)services. 
Those events can come from state changes of AWS services, other AWS accounts, or external 
applications like Auth0, Shopify, and others. You can, of course, also send your custom messages.

EventBridge delivers a stream of real-time data from event sources such as Zendesk or Shopify to targets like 
AWS Lambda and other SaaS applications. You can set up routing rules to determine 
where to send your data to build application architectures that react in real-time to your data sources 
with event publisher and consumer completely decoupled.

EventBridge sends metrics to Amazon CloudWatch every minute for everything from the number of 
matched events to the number of times a target is invoked by a rule.


In event-driven architecture, services interact with each other through events. 
An event is something that happened in your application (for example, an item was put into a cart, a new order was placed). 
Events are JSON objects that tell you information about something that happened in your application. 
In event-driven architecture, each component of the application raises an event whenever anything changes. 
Other components listen and decide what to do with it and how they would like to react.


#Glue
--------------------------------------------------

AWS Glue is a fully managed ETL (extract, transform, and load) service that makes it simple and cost-effective to categorize your data, 
clean it, enrich it, and move it reliably between various data stores and data streams. 


Athena uses the AWS Glue Data Catalog to store and retrieve table metadata for the Amazon S3 data in your Amazon Web Services account. 
The table metadata lets the Athena query engine know how to find, read, and process the data that you want to query.


Data integration is the process of preparing and combining data for analytics, machine learning, and application development. 
It involves multiple tasks, such as discovering and extracting data from various sources; enriching, cleaning, normalizing, 
and combining data; and loading and organizing data in databases, data warehouses, and data lakes. These tasks are 
often handled by different types of users that each use different products.

AWS Glue provides both visual and code-based interfaces to make data integration easier. 
Users can easily find and access data using the AWS Glue Data Catalog.


Glue uses ETL jobs to extract data from a combination of other Amazon Web Services and incorporates 
it into data lakes and data warehouses. It uses application programming interfaces (APIs) to transform the e
xtracted data set for integration, and to help users monitor jobs.

Users can put ETL jobs on a schedule or pick events that will trigger a job. 
Once triggered, Glue extracts the data, transforms it based on code that Glue generates automatically, 
and loads it into Amazon S3 or Amazon Redshift. Glue then writes metadata from the job into the AWS Glue Data Catalog.


AWS Glue DataBrew is a visual data preparation tool that enables users to clean and normalize data without writing any code. 

Simplify data preparation (capturing metadata) for analytics:
Connect AWS Glue to your data on AWS (Aurora, RDS, Redshift, S3 etc)
AWS Glue creates a AWS Glue Data Catalog with metadata abstracted from your data
Your data is ready for searching and querying



#CloudTrail 
--------------------------------------------------
CloudTrail logs actions inside your AWS environment.CloudTrail has a feature called CloudTrail Insights.
Insights let you detect unusual API activities on your account by automation.

AWS CloudTrail is an AWS service that helps you enable governance, compliance, and operational and risk auditing of your AWS account. 
Actions taken by a user, role, or an AWS service are recorded as events in CloudTrail. 
Events include actions taken in the AWS Management Console, AWS Command Line Interface, and AWS SDKs and APIs.

CloudTrail is enabled on your AWS account when you create it. 
When activity occurs in your AWS account, that activity is recorded in a CloudTrail event. 
You can easily view recent events in the CloudTrail console by going to Event history. 
For an ongoing record of activity and events in your AWS account, create a trail.



AWS CloudTrail is a service that automatically records events such as AWS API calls. 
You can create EventBridge rules that use the information from CloudTrail. 
For more information about CloudTrail, see What is AWS CloudTrail?.


There are two types of events that can be logged in CloudTrail: management events and data events.

Management events provide information about management operations that are performed on resources in your AWS account.
Think of Management events as things normally done by people when they are in AWS. 

Examples:
a user sign in
a policy changed
a newly created security configuration
a logging rule deletion


Data events provide information about the resource operations performed on or in a resource.
Think of Data events as things normally done by software when hitting various AWS endpoints. 

Examples:
S3 object-level API activity
Lambda function execution activity
By default, CloudTrail logs management events, but not data events.



Track events, API calls, changes made to your AWS resources:
-Who made the request?
-What action was performed?
-What are the parameters used?
-What was the end result?


#AWS Config | AWSConfig 
-------------------------------------------------
This helps you understand the configuration changes that happen in your environment. 
This service provides an AWS inventory that includes configuration history, configuration change notification, 
and relationships between AWS resources. It can also be configured to send information via AWS SNS when new logs are delivered.



#CloudFront
-------------------------------------------------

Amazon Cloudfront is a content delivery network (AWS CDN) that retrieves data stored in the 
Amazon S3 bucket and distributes it to numerous edge locations across the world. 
Edge locations are the network of data centers distributed worldwide through which content is delivered.

If the content is already cached in the edge location, CloudFront delivers it immediately with the lowest latency possible.
If the content is not present in the edge location, CloudFront retrieves it from the origin 
(like Amazon S3 bucket, a MediaPackage channel, or an HTTP server) that has been identified for your content.

Securely deliver content with low latency and high transfer speeds

Amazon CloudFront speeds up distribution of your static and dynamic web content, such as .html, .css, .php, image, and media files. 
When users request your content, CloudFront delivers it through a worldwide network of 
edge locations that provide low latency and high performance.

AWS CloudFront is a globally-distributed network offered by Amazon Web Services, 
which securely transfers content such as software, SDKs, videos, etc., to the clients, with high transfer speed.


Use cases:
-Deliver fast, secure websites
-Accelerate dynamic content delivery and APIs
-Stream live and on-demand video
-Distribute patches and updates



Scenario: Restrict content to users in certain countries
	Enable CloudFront Geo restriction
	Configure White list(countries to be allowed) and Blacklist(countries to be blocked)


=================================================
#Route 53 | Route53 
================================================= 
Amazon Route 53 is a highly available and scalable Domain Name System (DNS) service. 
You can use Route 53 to perform three main functions in any combination: domain registration, DNS routing, and health checking.


When you buy a domain name, every DNS address starts with an SOA (Start of Authority) record. 
The SOA record stores information about the name of the server that kicked off the transfer of ownership, 
the administrator who will now use the domain, the current metadata available, and the default number of seconds or TTL.

NS records, or Name Server records, are used by the Top Level Domain hosts (.org, .com, .uk, etc.) 
to direct traffic to the Content servers. The Content DNS servers contain the authoritative DNS records.

Browsers talk to the Top Level Domains whenever they are queried and encounter domain name that they do not recognize.
Browsers will ask for the authoritative DNS records associated with the domain.

Because the Top Level Domain contains NS records, the TLD can in turn queries the Name Servers for their own SOA.
Within the SOA, there will be the requested information.

Once this information is collected, it will then be returned all the way back to the original browser asking for it.



In summary: Browser -> TLD -> NS -> SOA -> DNS record. The pipeline reverses when the correct DNS record is found.


The routing policies available are:
-Simple Routing
-Weighted Routing
-Latency-based Routing
-Failover Routing
-Geolocation Routing
-Geo-proximity Routing
-Multivalue Answer Routing



================================================= 
# Encryption  | Security
================================================= 

#Symmetric Key 
-------------------------------------------------
Symmetric encryption algorithms use the same key for encryption and decryption
-Key Factor 1: Choose the right encryption algorithm
-Key Factor 2: How do we secure the encryption key?
-Key Factor 3: How do we share the encryption key?


#Asymmetric Key Encryption
-------------------------------------------------

Two Keys : Public Key and Private Key Also called Public Key Cyptography.

Encrypt data with Public Key and decrypt with Private Key
Share Public Key with everybody and keep the Private Key with you(YEAH, ITS PRIVATE!)

No crazy questions:
Will somebody not figure out private key using the public key?

How do you create Asymmetric Keys



#Amazon Cognito
-------------------------------------------------

Amazon Cognito provides authentication, authorization, and user management for your web and mobile apps. 
Your users can sign in directly with a user name and password, or through a third party such as Facebook, Amazon, Google or Apple.


Amazon Cognito lets you add user sign-up, sign-in, and access control to your web and mobile apps quickly and easily. 
Amazon Cognito scales to millions of users and supports sign-in with social identity providers, such as Apple, Facebook, Google, 
and Amazon, and enterprise identity providers via SAML 2.0 and OpenID Connect.  


Amazon Cognito is designed for developers who want to add user management and sync functionality to their mobile and web apps. 
Developers can use Cognito Identity to add sign-up and sign-in to their apps and to enable their 
users to securely access their app’s resources. 
Cognito also enables developers to sync data across devices, platforms, and applications.



#STS(Security Token Service)
-------------------------------------------------
WS STS is an AWS service that allows you to request temporary security credentials for your AWS resources, 
for IAM authenticated users and users that are authenticated in AWS such as federated users via OpenID or SAML2.0.

You use STS to provide trusted users with temporary access to resources via API calls, your AWS console or 
the AWS command line interface (CLI)


AWS provides AWS Security Token Service (AWS STS) as a web service that enables you to request temporary, 
limited-privilege credentials for AWS Identity and Access Management (IAM) users or for users you authenticate (federated users).

Recording API requests:
AWS STS supports AWS CloudTrail, a service that records AWS calls for your AWS account and delivers log files to an Amazon S3 bucket. 
By using information collected by CloudTrail, you can determine the requests successfully sent to AWS STS, 
as well as who sent the request, and when it was sent.

The STS token lifecycle is determined by you and can be anywhere from 15 minutes to 36 hours.

External web identities can be authenticated by a third party online identity manager like amazon, google, 
facebook or any other open-id connect compatible service. This web identity federation also removes the need to 
distribute long-term security credentials to 
facilitate access to your AWS resources.


Use-Case:
-Identity Federation Use-Case
-Cross-Account Access using AWS STS
-EC2 Instance STS Credentials


#KMS AWS Key Management Service 
-------------------------------------------------

Amazon S3 uses AWS KMS keys to encrypt your Amazon S3 objects. AWS KMS encrypts only the object data.
 
 
KMS is a managed service that makes it easy for you to create and control the encryption keys used to encrypt your data. 
AWS KMS uses Hardware Security Modules (HSMs) to protect the security of your keys. 
You can use AWS KMS to protect your data in AWS services and in your applications

AWS Key Management Service (AWS KMS) makes it easy for you to create and manage cryptographic keys and control their use across a 
wide range of AWS services and in your applications.

AWS KMS is integrated with AWS CloudTrail to provide you with logs of all key usage to help meet your regulatory and compliance needs.

AWS KMS Key Management Service is a useful and very beneficial service while dealing with sensitive data and it 
also makes it easy for you to create and manage cryptographic keys.




AWS KMS keys must be in the same Region as the bucket.




Features of AWS KMS:
----------------------------------------------------
It is an easy way to control and access your data using managed encryption.

With AWS Key Management Service, the process of key management is reduced to a few simple clicks.

It is also integrated with other AWS services including Amazon EBS, Amazon S3, and Amazon RedShift to 
simplify the encryption of your data within these services.

AWS KMS enables you to create, rotate, disable, enable, and define usage policies for master keys and audit their usage.

It is a centralized key management

It is secure and compliant.



CloudHSM
----------------------------------------------------
AWS CloudHSM is a cloud-based hardware security module (HSM) that enables you to easily 
generate and use your own encryption keys on the AWS Cloud. 

CloudHSM is standards-compliant and enables you to export all of your keys to most other
commercially-available HSMs, subject to your configurations. 


The AWS CloudHSM service helps you meet corporate, contractual, and regulatory compliance 
requirements for data security by using dedicated 
Hardware Security Module (HSM) instances within the AWS cloud. 

AWS CloudHSM provides hardware security modules in the AWS Cloud. A hardware security module (HSM) is a computing device that 
processes cryptographic operations and provides secure storage for cryptographic keys.



When you use an HSM from AWS CloudHSM, you can perform a variety of cryptographic tasks:

Generate, store, import, export, and manage cryptographic keys, including symmetric keys and asymmetric key pairs.

Use symmetric and asymmetric algorithms to encrypt and decrypt data.

Use cryptographic hash functions to compute message digests and hash-based message authentication codes (HMACs).

Cryptographically sign data (including code signing) and verify signatures.

Generate cryptographically secure random data.



Q: What is a Hardware Security Module (HSM)?

A Hardware Security Module (HSM) provides secure key storage and cryptographic operations within a 
tamper-resistant hardware device. HSMs are designed to securely store cryptographic 
key material and use the key material without exposing it outside the cryptographic boundary of the hardware.


Q: What can I do with CloudHSM?

You can use the CloudHSM service to support a variety of use cases and applications, 
such as database encryption, Digital Rights Management (DRM), Public Key Infrastructure (PKI), 
authentication and authorization, document signing, and transaction processing.

There are no upfront costs to use AWS CloudHSM. With CloudHSM, you pay an hourly fee for each HSM you launch until you terminate the HSM.


#Transit Gateway (TGW)
-------------------------------------------------
AWS Transit Gateway connects your Amazon Virtual Private Clouds (VPCs) and on-premises networks 
through a central hub. This simplifies your network and puts an end to complex peering relationships. 
It acts as a cloud router – each new connection is only made once.

Transit Gateway is a Regional resource and can connect thousands of VPCs within the same AWS Region. 
You can create multiple Transit Gateway instances per Region, and you can 
connect to a maximum of three Transit Gateway instances over a single Direct Connect connection for hybrid connectivity.



As your cloud infrastructure expands globally you need to find out a way to connect your resources which are in different VPCs. 
A Transit Gateway is a network hub that you can use to interconnect your virtual private clouds (VPCs) and on-premises networks. 

It is like a hub and spoke design or star topology design for connecting VPCs and on-premises networks. 
Transit Gateway allows customers to connect thousands of VPCs together. It is a regional service. 
It gives you simplified connectivity to the multiple VPC as compared to a complex VPC peering connection. 

Traffic between VPC and Transit Gateway remains on the AWS global private network and is not exposed to the public internet. 
Transit Gateways in different regions can peer with each other to enable VPC communications across regions. 
Transit Gateway inter-Region peering encrypts all traffic, with no single point of 
failure or bandwidth bottleneck which helps you to get improved security.


#ECMP
-------------------------------------------------
Equal-cost multi-path routing (ECMP) is a routing strategy where packet forwarding to a single destination can occur over multiple best paths with equal routing priority. 
Multi-path routing can be used in conjunction with most routing protocols because it is a per-hop local decision made independently at each router.


AWS Transit Gateway VPN supports ECMP protocol that can load balance traffic across multiple VPN tunnels. 
The question is, can Transit Gateway ECMP be used to deploy a transit DMZ as shown in the diagram below?


#Traffic Mirroring
-------------------------------------------------

Traffic Mirroring copies inbound and outbound traffic from the network interfaces that are attached to your instances. 
You can send the mirrored traffic to the network interface of another instance, a Network Load Balancer that has a UDP listener, 
or a Gateway Load Balancer that has a UDP listener. The traffic mirror source and the traffic mirror target (monitoring appliance) 
can be in the same VPC. Or they can be in a different VPCs that are connected 
through intra-Region VPC peering, a transit gateway, or by a Gateway Load Balancer endpoint to connect to a 
Gateway Load Balancer in a different VPC.


Traffic Mirroring is an Amazon VPC feature that you can use to copy network traffic from an elastic network interface of type interface. 
You can then send the traffic to out-of-band security and monitoring appliances for:

-Content inspection
-Threat monitoring
-Troubleshooting




#Directory Service
-------------------------------------------------
Microsoft AD is a Microsoft Active Directory hosted on the AWS Cloud. It integrates most Active Directory features with AWS applications.



AWS Directory Service lets you run Microsoft Active Directory (AD) as a managed service. 
AWS Directory Service for Microsoft Active Directory, also referred to as AWS Managed Microsoft AD, 
is powered by Windows Server 2012 R2. When you select and launch this directory type, 
it is created as a highly available pair of domain controllers connected to your virtual 
private cloud (VPC). The domain controllers run in different Availability Zones in a Region of your choice. 
Host monitoring and recovery, data replication, snapshots, and software updates are automatically configured and managed for you.

With AWS Managed Microsoft AD, you can run directory-aware workloads in the AWS Cloud, 
including Microsoft SharePoint and custom .NET and SQL Server-based applications. 
You can also configure a trust relationship between AWS Managed Microsoft AD in the AWS Cloud and your existing 
on-premises Microsoft Active Directory, providing users and groups with access to resources in either domain, 
using single sign-on (SSO).

AWS Directory Service makes it easy to set up and run directories in the AWS Cloud, or connect your 
AWS resources with an existing on-premises Microsoft Active Directory. Once your directory is created, 
you can use it for a variety of tasks:

-Manage users and groups
-Provide single sign-on to applications and services
-Create and apply group policy
-Simplify the deployment and management of cloud-based Linux and Microsoft Windows workloads
-You can use AWS Managed Microsoft AD to enable multi-factor authentication by integrating with your 
 existing RADIUS-based MFA infrastructure to provide an additional layer of security when users access AWS applications.
-Securely connect to Amazon EC2 Linux and Windows instances




AWS introduced AWS Directory Service for Microsoft Active Directory (Standard Edition), 
also known as AWS Microsoft AD (Standard Edition), which is managed Microsoft Active Directory (AD) 
that is performance optimized for small and midsize businesses. AWS Microsoft AD (Standard Edition) 
offers you a highly available and cost-effective primary directory in the 
AWS Cloud that you can use to manage users, groups, and computers.


AWS Managed Microsoft AD makes it easy to migrate AD-dependent applications and Windows workloads to AWS. 
With AWS Managed Microsoft AD, you can use Group Policies to manage EC2 instances and run AD-dependent applications in the 
AWS Cloud without the need to deploy your own AD infrastructure


#AWS Workspaces
-------------------------------------------------
-Desktop-as-a-Service (DaaS)
--Provision Windows or Linux desktops in minutes
-Eliminate traditional desktop management - Virtual Desktop Infrastructure (VDI)



#AWS Shield
-------------------------------------------------

Shields from Distributed Denial of Service (DDoS) attacks
Disrupt normal traffic of a server by overwhelming it with a flood of Internet traffic
Protect
-Amazon Route 53
-Amazon CloudFront
-AWS Global Accelerator
-Amazon Elastic Compute Cloud (EC2) instances
-Elastic Load Balancers (ELB)

=================================================
#HPC ENA, EFA      
=================================================

 
#Elastic Fabric Adapter (EFA)
-------------------------------------------------
An Elastic Fabric Adapter (EFA) is a network device that you can attach to your Amazon EC2 instance to accelerate 
High Performance Computing (HPC) and machine learning applications. EFA enables you to achieve the application performance of 
an on-premises HPC cluster, with the scalability, flexibility, and elasticity provided by the AWS Cloud.

Elastic Fabric Adapter (EFA) is a network interface for Amazon EC2 instances that enables customers to run applications 
requiring high levels of inter-node communications at scale on AWS. 
Its custom-built operating system (OS) bypass hardware interface enhances the performance of inter-instance communications, 
which is critical to scaling these applications. 


You can create, use, and manage an EFA much like any other elastic network interface in Amazon EC2. 
However, unlike elastic network interfaces, EFAs cannot be attached to or detached from an instance in a running state.

Elastic Fabric Adapter (EFA) is a network interface for Amazon EC2 instances that enables customers to run HPC applications 
requiring high levels of inter-instance communications, like computational fluid dynamics, weather modeling, 
and reservoir simulation, at scale on AWS. It uses a custom-built operating system bypass technique to enhance the 
performance of inter-instance communications, which is critical to scaling HPC applications. With EFA, 
HPC applications using popular HPC technologies like Message Passing Interface (MPI) can scale to thousands of CPU cores. 
EFA supports industry-standard libfabric APIs, so applications that use a supported MPI library can be 
migrated to AWS with little or no modification.

 
#ENA (Elastic Network Adapter)
-------------------------------------------------
The Elastic Network Adapter (ENA) is designed to improve operating system health and reduce 
the chances of long-term disruption because of unexpected hardware behavior and or failures. 
The ENA architecture keeps device or driver failures as transparent to the system as possible. 
This topic provides troubleshooting information for ENA.

#With EC2
The Elastic Network Adapter (ENA) driver publishes network performance metrics from the instances where they are enabled. 
You can use these metrics to troubleshoot instance performance issues, choose the right instance size for a workload, 
plan scaling activities proactively, and benchmark applications to determine whether they maximize the
performance available on an instance.

#HPC (High Performance Computing)
-------------------------------------------------
Run your large, complex simulations and deep learning workloads in the cloud with a complete suite of 
high performance computing (HPC) products and services on AWS. Gain insights faster, and quickly
move from idea to market with virtually unlimited compute capacity, a high-performance file system,
and high-throughput networking.


#ParallelCluster 
-------------------------------------------------
AWS ParallelCluster is an open source cluster management tool that makes it easy for you to deploy and manage High Performance 
Computing (HPC) clusters on AWS. ParallelCluster uses a simple text file to model and provision all the 
resources needed for your HPC applications in an automated and secure manner. It also supports multiple 
instance types and job submission queues, and job schedulers like AWS Batch and Slurm.

AWS ParallelCluster is built on the popular open source CfnCluster project and is released via the Python Package Index (PyPI). 
ParallelCluster's source code is hosted on the Amazon Web Services repository on GitHub. 
AWS ParallelCluster is available at no additional charge, and you pay only for the AWS resources needed to run your applications.




=================================================
#CloudFormation         
=================================================
AWS CloudFormation is an AWS service that uses template files to automate the setup of AWS resources.

An AWS CloudFormation template is a formatted text file in JSON or YAML language that describes your AWS infrastructure. 
To create, view and modify templates, you can use AWS CloudFormation Designer or any text editor tool. 
An AWS CloudFormation template consists of nine main objects:

Format version: Format version defines the capability of a template.
Description: Any comments about your template can be specified in the description.
Metadata: Metadata can be used in the template to provide further information using JSON or YAML objects. 

CloudFormation is an infrastructure automation platform for AWS that deploys AWS resources in a repeatable, testable and auditable manner.
AWS CloudFormation provides users with a simple way to create and manage a collection of Amazon Web Services (AWS)
resources by provisioning and updating them in a predictable way. AWS CloudFormation enables you to manage your 
complete infrastructure or AWS resources in a text file.



You can use CloudFormation to automate the configuration of workloads that run on the most popular AWS services, 
like the EC2 compute service, the S3 storage service, and the IAM service for configuring access control.

You can also apply CloudFormation templates to AWS services that cater to niche use cases, like Ground Station, 
the AWS satellite management solution.

In general, if a service runs on AWS, it is a safe bet that you can use CloudFormation to automate its configuration and deployment.

It is worth noting that CloudFormation is not the only way to configure and deploy services on AWS. 
You can handle these processes manually using the AWS command-line interface, API, or Web console. 

AWS CloudFormation enables you to manage your complete infrastructure or AWS resources in a text file, or template. 
A collection of AWS resources is called a stack. AWS resources can be created or updated by using a stack.


HowToWork:
-Create or use an existing CloudFormation template using JSON or YAML format.
-Save the code in an S3 bucket, which serves as a repository for the code.
-Use AWS CloudFormation to call the bucket and create a stack on your template. 
-CloudFormation reads the file and understands the services that are called, their order, 
the relationship between the services, and provisions the services one after the other.



#StepFunction
-------------------------------------------------

AWS Step Functions is a serverless orchestration service that lets developers create and manage multi-step 
application workflows in the cloud. By using the service’s drag-and-drop visual editor, 
teams can easily assemble individual microservices into unified workflows. At each step of a given workflow, 
Step Functions manages input, output, error handling, and retries, so that developers can focus on 
higher-value business logic for their applications.




#EMR
-------------------------------------------------

With it, organizations can process and analyze massive amounts of data.

Amazon Elastic MapReduce (Amazon EMR) is a web service that makes it easy to quickly and cost-effectively process vast amounts of data.
Amazon EMR is the industry-leading cloud big data platform for processing vast amounts of data using 
open source tools such as Apache Spark, Apache Hive, Apache HBase, Apache Flink, Apache Hudi, and Presto. 
Amazon EMR makes it easy to set up, operate, and scale your big data environments 
by automating time-consuming tasks like provisioning capacity and tuning clusters and uses 
Hadoop, an open source framework, to distribute your data and processing across a resizable cluster of Amazon EC2 instances. 


#GraphQL:
------------------------------------------------
GraphQL is a query language for APIs and a runtime for fulfilling those queries with your existing data. 
GraphQL provides a complete and understandable description of the data in your API, gives clients the 
power to ask for exactly what they need and nothing more, makes it easier to evolve APIs over time, 
and enables powerful developer tools.

Send a GraphQL query to your API and get exactly what you need, nothing more and nothing less. 
GraphQL queries always return predictable results. Apps using GraphQL are fast and stable because they 
control the data they get, not the server.


#AppSync
-------------------------------------------------
The fundamental idea of a GraphQL API is that all API functionality is available via a unified query language 
(the Graph Query Language) under a single endpoint. Rather than making requests to various endpoints to get 
different parts of the data needed to build a webpage, developers can issue a single request to a 
GraphQL API and immediately get back all the data they need. This model reduces the complexity of 
web applications and improves the experience for website visitors with faster load times.

AWS AppSync is a fully managed GraphQL API layer developed by Amazon Web Services. 
AppSync allows developers to build GraphQL APIs without much of the usual work; it handles the parsing and resolution of 
requests as well as connecting to other AWS services like AWS Lambda, NoSQL and SQL data stores, and HTTP APIs to gather 
backend data for the API.

AWS AppSync is a fully managed AWS serverless service for real-time data queries, synchronization, and communications. 
In AppSync, AWS has a GraphQL-as-a-Service offering that makes it easy to build scalable and resilient GraphQL APIs in the cloud.

With AppSync, you can build scalable applications, including those requiring real-time updates, on a range of data sources 
such as NoSQL data stores, relational databases, HTTP APIs, and your custom data sources with Amazon Lambda. 
For mobile and web apps, AppSync additionally provides local data access when devices go offline, 
and data synchronization with customizable conflict resolution, when they are back online.

AWS AppSync is a serverless GraphQL and Pub/Sub API service that simplifies building modern web and mobile applications.

AWS AppSync GraphQL APIs simplify application development by providing a single endpoint to securely query or update data from multiple databases, microservices, and APIs.

AWS AppSync Pub/Sub APIs make it easy to create engaging real-time experiences by automatically publishing data updates to subscribed API clients via serverless WebSockets connections. 



=================================================
#Elastic Beanstalk        
================================================= 
AWS Elastic Beanstalk allows you to quickly deploy applications and services without having to worry about 
configuring underlying resources, services, operating systems or web servers.

Elastic Beanstalk takes care of the hosting infrastructure, coding language interpreter, operating system, 
security, https service and application layer. All you need to worry about is writing your code.

You can develop code in a number of languages which is then zipped up and the zip file is used 
when instantiating a new elastic beanstalk instance.

Elastic Beanstalk is a complete application management solution, and manages all infrastructure and platform tasks on your behalf.

When using Elastic Beanstalk as your deployment solution, simply upload your source code and 
Elastic Beanstalk will provision and operate all necessary infrastructure, including servers, databases, 
load balancers, networks, and auto scaling groups. Although these resources are created on your behalf, 
you retain full control of these resources, allowing developers to customize as needed.

AWS Elastic Beanstalk, you can quickly deploy and manage applications in the AWS Cloud without worrying about the 
infrastructure that runs those applications. AWS Elastic Beanstalk reduces management complexity 
without restricting choice or control. You simply upload your application, and AWS Elastic Beanstalk 
automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring.


Elastic Beanstalk is the fastest and simplest way to deploy your application on AWS. You simply use the AWS Management Console, 
a Git repository, or an integrated development environment (IDE) such as Eclipse or Visual Studio to upload your application, 
and Elastic Beanstalk automatically handles the deployment details of capacity 
provisioning, load balancing, auto-scaling, and application health monitoring. 
Within minutes, your application will be ready to use without any infrastructure or resource configuration work on your part.



AWS Elastic Beanstalk provides an environment that makes it easy to deploy and run applications in the cloud.
AWS Elastic Beanstalk is combined with the developer tools to help you manage the lifecycle of your applications.


=================================================
#Global Accelerator      
================================================= 
Global Accelerator is a network layer service in which you create accelerators to improve availability and performance for internet applications used by a global audience. 


With Global Accelerator, you are provided two global static public IPs that act as a fixed entry point to your application, 
improving availability. On the back end, add or remove your AWS application endpoints, such as Application Load Balancers, 
Network Load Balancers, EC2 Instances, and Elastic IPs without making user-facing changes. 
Global Accelerator automatically re-routes your traffic to your nearest healthy available endpoint to mitigate endpoint failure.


It provides static IP addresses that act as a fixed entry point to application endpoints in a 
single or multiple AWS Regions, such as Application Load Balancers, Network Load Balancers or EC2 instances.



Uses the AWS global network to optimize the path from users to applications, improving the performance of TCP and UDP traffic.

AWS Global Accelerator continually monitors the health of application endpoints and will detect an unhealthy endpoint and redirect 
traffic to healthy endpoints in less than 1 minute.


#RTO and RPO
-------------------------------------------------
Among the components of a DR plan are two key parameters that define how long your business can afford to be offline and how
much data loss it can tolerate. These are the Recovery Time Objective (RTO) and Recovery Point Objective (RPO).


RTO:
RTO is the goal your organization sets for the maximum length of time it should take to restore normal 
operations following an outage or data loss.

RTO refers to how much time an application can be down without causing significant damage to the business. 
Some applications can be down for days without significant consequences. 

RTO is not simply the duration of time between loss and recovery. The objective also accounts for the steps 
IT must take to restore the application and its data.



RPO:
RPO is your goal for the maximum amount of data the organization can tolerate losing. 
This parameter is measured in time: from the moment a failure occurs to your last valid data backup. 
For example, if you experience a failure now and your last full data backup was 24 hours ago, the RPO is 24 hours. 


Recovery point objectives refer to your company’s loss tolerance: the amount of data that can be lost before significant 
harm to the business occurs. 

If you back up all or most of your data in regularly scheduled 24-hour increments, then in the worst-case scenario you 
will lose 24 hours’ worth of data.

For example, if you have a 4-hour RPO for an application then you will have a maximum 4-hour gap between backup and data loss.
Having a 4-hour RPO does not necessarily mean you will lose 4 hours’ worth of data.


=================================================
#AWS Organizations  
================================================= 
Organizations typically have multiple AWS accounts
Different business units
Different environments

How do you centralize your management (billing, access control, compliance and security) across multiple AWS accounts?
Welcome AWS Organizations!
Organize accounts into Organizational Units (OU)
Provides API to automate creation of new accounts


Features:
-One consolidated bill for all AWS accounts
-Centralized compliance management for AWS Config Rules
-Send AWS CloudTrail data to one S3 bucket (across accounts)
-AWS Firewall Manager to manage firewall rules (across accounts)
-AWS WAF, AWS Shield Advanced protections and Security Groups
-Use Service control policies (SCPs) to define restrictions for actions (across accounts):
-Prevent users from disabling AWS Config or changing its rules
-Require Amazon EC2 instances to use a specific type
-Require MFA to stop an Amazon EC2 instance
-Require a tag upon resource creation



=================================================
#AWS Trusted Advisor
================================================= 
Recommendations for cost optimization, performance, security and fault tolerance
-Red - Action recommended Yellow - investigate and Green - Good to go
-All AWS customers get 4 checks for free:
-Service limits (usage > 80%)
-Security groups having unrestricted access (0.0.0.0/0)
-Proper use of IAM
-MFA on Root Account
-Business or Enterprise AWS support plan provides over 50 checks
-Disable those you are not interested in
-How much will you save by using Reserved Instances?
-How does your resource utilization look like? Are you right sized?


AWS Directory Service
=================================================
#AWS Well-Architected  | Architected Framework      
================================================= 

The AWS Well-Architected Framework is based on five pillars:
-operational excellence, 
-security, 
-reliability, 
-performance efficiency
-cost optimization


DevOps 



=================================================
#DevOps | CI  |CD | cicd | DevOps - CI, CD Tools       
================================================= 
Tools:
-CodeCommit - Private source control (Git)
-CodePipeline - Orchestrate CI/CD pipelines
-CodeBuild - Build and Test Code (application packages and containers)
-CodeDeploy - Automate Deployment (EC2, ECS, Elastic Beanstalk, EKS, Lambda etc)






=================================================
#Managed Services8 | IAAS| PAAS       
================================================= 

AWS Managed Service Offerings:

-Elastic Load Balancing - Distribute incoming traffic across multiple targets
-AWS Elastic Beanstalk - Run and Manage Web Apps
-Amazon Elastic Container Service (ECS) - Containers orchestration on AWS
-AWS Fargate - Serverless compute for containers
-Amazon Elastic Kubernetes Service (EKS) - Run Kubernetes on AWS
-Amazon RDS - Relational Databases - MySQL, Oracle, SQL Server etc



IAAS:

IAAS (Infrastructure as a Service):
IAAS (Infrastructure as a Service) is all about using only infrastructure from cloud provider. It is also called “Lift and Shift”. Example: Using EC2 to deploy your applications or databases

With IAAS, you are responsible for:

Application Code and Runtime
Configuring load balancing
Auto scaling
OS upgrades and patches
Availability




PAAS:
PAAS (Platform as a Service)
PAAS (Platform as a Service) is all about using a platform provided by cloud

Cloud provider is responsible for:

OS (incl. upgrades and patches)
Application Runtime
Auto scaling, Availability & Load balancing etc..
You are responsible for:

Application code
Configuration

Examples of PAAS
CAAS (Container as a Service): Containers instead of Applications
FAAS (Function as a Service) or Serverless: Functions instead of Applications



=================================================
#Shared Responsibility Model         
================================================= 
Security & Compliance is shared responsibility between AWS and customer


Amazon EC2 instances is Infrastructure as a Service (IaaS).
You are responsible for:
Guest OS (incl. security patches)
Application software installed
Configuring Security Groups (or firewalls)


AWS is responsible for infrastructure layer only.


Amazon S3 & DynamoDB are managed services.

AWS manages infrastructure layer, OS, and platform.

You are responsible for
-Managing your data
-Managing security of data at rest(encryption)
-Managing security of data in transit
-Mandating SSL/HTTPS
-Using the right network - AWS global network or dedicated private network when possible
-Managing access to the service
-Configure right permissions (IAM users/roles/user policies/resource policies)
(FOR AWS RDS) Managing in database users
-Configuring the right security groups (control inbound and outbound traffic)
-Disabling external access (public vs private)


=================================================
# API Gateway       
================================================= 
How about a fully managed service with auto scaling that can act as a “front door” to your APIs? Welcome “Amazon API Gateway”

Amazon API Gateway helps you to “publish, maintain, monitor, and secure APIs at any scale”

You can authorize users by integrating with:

AWS IAM (for AWS users using signature version 4)
Amazon Cognito
Lambda authorizer (custom authorization with JWT tokens or SAML)

Features:
-Integrates with AWS Lambda, Amazon EC2, Amazon ECS or any web application
-Supports HTTP(S) and WebSockets (two way communication - chat apps and streaming dashboards)
-Serverless. Pay for use (API calls and connection duration)
-Provides API Lifecycle Management for RESTful APIs and WebSocket APIs
-You can Run multiple versions of the same API
-Supports Rate Limits(request quota limits), throttling and fine-grained access permissions using API Keys for Third-Party Developers
-Lifecycle management for REST APIs
-Versioning and multiple environments
-API keys - Generate API keys to monitor usage
-Implement plans and quota limits for external applications (or developer)
-WARNING - Do NOT use API keys for Authorization
-Enable caching for API calls with TTL
-Protect backends by throttling requests
-Integrates with
-Amazon CloudWatch - Performance metrics, API calls, latency data and error rates
-Amazon CloudWatch Logs - Debug logging
-WS CloudTrail - Complete history of changes to your REST API




=================================================
#Amazon Elastic Container Service (ECS)      
================================================= 
Amazon Elastic Container Service (Amazon ECS) is a fully managed service for container orchestration.

Amazon ECR is a Fully-managed Docker container registry provided by AWS. Its an alternative to Docker Hub.


AWS Fargate is a serverless option.

-Service: Allows you to run and maintain a specified number (the “desired count”) of tasks
-ECS cluster: Grouping of one or more container instances (EC2 instances) where you run your tasks
-Container Instance - EC2 instance in the cluster running a container agent (helps it communicate with the cluster)
-AWS provides ECS ready AMIs with container agents pre-installed.


Remember:
-AWS Fargate does NOT give you visibility into the EC2 instances in the cluster.
-You can use On-Demand instances or Spot instances to create your cluster.
-You can load balance using Application Load Balancers
-Two features of ALB are important for ECS:
-Dynamic host port mapping: Multiple tasks from the same service are allowed per EC2 (container) instance
-Path-based routing: Multiple services can use the same listener port on same ALB and be routed based on path (www.app.com/microservice-a and www.app.com/microservice-b)


Elastic Beanstalk:
Single container or multiple containers in same EC2 instance
Recommended for simple web applications

Amazon ECS:
AWS specific solution for container orchestration
Ideal for microservices


Amazon Fargate:
Serverless version of Amazon ECS
You want to run microservices and you don’t want to manage the cluster


Amazon EKS:
AWS managed service for Kubernetes
Recommended if you are already using Kubernetes and would want to move the workload to AWS



=================================================
#ML | machine-learning
================================================= 

#SageMaker
-------------------------------------------------
Amazon SageMaker is a cloud-based machine-learning platform that helps users create, design, train, tune, and deploy machine-learning models in a production-ready hosted environment. The AWS SageMaker comes with a pool of advantages (know all about it in the next section)




=================================================
#AWS Services Scope | Area
================================================= 
IAM
	Users, Groups, Roles, Accounts – Global
		Same AWS accounts, users, groups and roles can be used in all regions
	Key Pairs – Global or Regional
		Amazon EC2 created key pairs are specific to the region
		RSA key pair can be created and uploaded that can be used in all regions
Virtual Private Cloud
	VPC – Regional
		VPC are created within a region
	Subnet – Availability Zone
		Subnet can span only a single Availability Zone
	Security groups – Regional
		A security group is tied to a region and can be assigned only to instances in the same region.
	VPC Endpoints – Regional
		You cannot create an endpoint between a VPC and an AWS service in a different region.
	VPC Peering – Regional
		VPC Peering can be performed across VPC in the same account of different AWS accounts but only within the same region. They cannot span across regions
		VPC Peering can now span inter-region
	Elastic IP Address – Regional
		Elastic IP address created within the region can be assigned to instances within the region only
EC2
	Resource Identifiers – Regional
		Each resource identifier, such as an AMI ID, instance ID, EBS volume ID, or EBS snapshot ID, is tied to its region and can be used only in the region where you created the resource.
	Instances – Availability Zone
		An instance is tied to the Availability Zones in which you launched it. However, note that its instance ID is tied to the region.
	EBS Volumes – Availability Zone
		Amazon EBS volume is tied to its Availability Zone and can be attached only to instances in the same Availability Zone.
	EBS Snapshot – Regional
		An EBS snapshot is tied to its region and can only be used to create volumes in the same region and has to be copied from One region to other if needed

AMIs – Regional
	AMI provides templates to launch EC2 instances
	AMI is tied to the Region where its files are located with Amazon S3. For using AMI in different regions, the AMI can be copied to other regions
Auto Scaling – Regional
	Auto Scaling spans across multiple Availability Zones within the same region but cannot span across regions
Elastic Load Balancer – Regional
	Elastic Load Balancer distributes traffic across instances in multiple Availability Zones in the same region
Cluster Placement Groups – Availability Zone
	Cluster Placement groups can be span across Instances within the same Availability Zones

S3 – Global but Data is Regional
	S3 buckets are created within the selected region
	Objects stored are replicated across Availability Zones to provide high durability but are not cross region replicated unless done explicitly
Route53 – Global
	Route53 services are offered at AWS edge locations and are global
DynamoDb – Regional
	All data objects are stored within the same region and replicated across multiple Availability Zones in the same region
	Data objects can be explicitly replicated across regions using cross-region replication
WAF – Global
	Web Application Firewall (WAF) services protects web applications from common web exploits are offered at AWS edge locations and are global
CloudFront – Global
	CloudFront is the global content delivery network (CDN) services are offered at AWS edge locations
Storage Gateway – Regional
	AWS Storage Gateway stores volume, snapshot, and tape data in the AWS region in which the gateway is activated
AWS Config – Regional
AWS GuardDuty – Regional



=================================================
#AWS Security, Identity, & Compliance services
================================================= 
Category | Use cases | AWS service

Identity & access management:
	Securely manage access to services and resources
	AWS Identity & Access Management (IAM)
	
	Cloud single-sign-on (SSO) service
	AWS IAM Identity Center (successor to AWS SSO)
	
	Identity management for your apps
	Amazon Cognito
	
	Managed Microsoft Active Directory
	AWS Directory Service
	
	Simple, secure service to share AWS resources
	AWS Resource Access Manager
	
	Central governance and management across AWS accounts
	AWS Organizations
	
Detection:
	Automate AWS security checks and centralize security alerts
	AWS Security Hub
	
	Protect AWS accounts with intelligent threat detection
	Amazon GuardDuty
	
	Automate vulnerability management
	Amazon Inspector
	
	Record and evaluate configurations of your AWS resources
	AWS Config
	
	Track user activity and API usage
	AWS CloudTrail
	
	Security management for IoT devices
	AWS IoT Device Defender
	
Network and application protection:
	Network security
	AWS Network Firewall
	
	DDoS protection
	AWS Shield
	
	Filter and control outbound DNS traffic for your VPCs
	Amazon Route 53 Resolver DNS Firewall
	
	Filter malicious web traffic
	WAF icon AWS Web Application Firewall (WAF)
	
	Central management of firewall rules
	AWS Firewall Manager
	
Data protection:
	Discover and protect your sensitive data at scale
	Amazon Macie
	
	Key storage and management
	AWS Key Management Service (KMS)
	
	Hardware based key storage for regulatory compliance
	AWS CloudHSM
	
	Provision, manage, and deploy public and private SSL/TLS certificates
	AWS Certificate Manager
	
	Rotate, manage, and retrieve secrets
	AWS Secrets Manager
	
Incident response:
	Investigate potential security issues
	Amazon Detective
	Scalable, cost-effective application recovery to AWS
	AWS Elastic Disaster Recovery
Compliance:
	No cost, self-service portal for on-demand access to AWS’ compliance reports
	AWS Artifact
	
	Continuously audit your AWS usage to simplify how you assess risk and compliance
	AWS Audit Manager


  