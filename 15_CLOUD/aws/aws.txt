#################################################
#                 AWS                          #
#################################################


Tags=>This helps to identify instances more quickly. 

Stopping => Temporarily shutting down the system
Terminating => Returning control to Amazon


Private key: The user downloads the private key
Public key: AWS uses the public key to confirm the identity of the user. 



=================================================
#To Do           
================================================= 

https://aws.amazon.com/vpc/faqs/
https://aws.amazon.com/faqs/



RTO), Recovery Point Objectives (RPO), and compliance


Whitepapers:4
 - Architecting for the cloud : AWS best practices
 - AWS Well-Architected Framework
 - AWS Disaster Recovery

-Inportant services FAQ
-AWS Community
-AWS Conference(Youtube)





=================================================
#Documentation | doc | Info            
================================================= 


AWS- 
  -Region
  -VPC
	-AvailableZone
	-Router 
      -NACL*
	-Subnet
      -SecurityGroup	
	  -EC2
 -Gateway




Amazon web service is an online platform that provides scalable and cost-effective cloud computing solutions.
AWS has 80 Availability Zones across 25 geographic regions global data centers. 

AWS:
-Security: AWS provides a secure and durable platform that provides end-to-end security and storage.
-Experience: The skills and infrastructure management born from Amazon’s many years of experience can be very valuable.
-Flexibility: It allows users to select the operating systems, language, database, and other services as per their requirements.
-Easy to use: AWS lets you host your applications quickly and securely, regardless of whether it’s an existing or new application.
-Scalable: The applications you use can be scaled up or down, depending on your requirements.
-Cost savings: You only pay for the compute power, storage, and other resources that you use, without any long-term commitments.
-Scheduling: This enables you to start and stop AWS services at predetermined times
-Reliability: AWS takes multiple backups at servers at multiple physical locations


=================================================
#Cloud Computing?           
================================================= 

Cloud computing is a computing service made available over the internet.
Cloud computing is a pay-as-you-go model for delivering IT resources.
You pay only for what you use.


Cloud computing differs from a traditional, on-premises environment in many ways,
including flexible, global, and scalable capacity, managed services, built-in security,
options for cost optimization, and various operating models.



=================================================
#Region        
================================================= 

How to choose  and AWS Region?
 - Compliance
 - Proximity
 - Available Service
 - Pricing
 
=================================================
#Availabiulity Zones |  AZ | az      
================================================= 

Each region hase many availabuility zone.
Usually 3, Min 2 and Max 6;

=================================================
#Subnet | cidr             
================================================= 
Subnet create on availablity zone not on Region. Same subnet can not take more then one.


Public Subnet
 - If a subnet traffic is routed to on Internet Gateway it is public with a public IP.
Private Subnet
 - If a Subnet dosnot have a route to the internet Gateway then it is private.
   When you create a VPC you must specify on IPv4 CIDR blica for the 
   , The allowed blick size is betwwn /16 to /28 netmask.
   The first four and last IP address of Subnet cannot be assigned.
    - Suppose a IP => 10.0.0.0/16
	- 10.0.0.0 Network address
	- 10.0.0.1 Reserved by AWS for VPC Route
	- 10.0.0.2 Reserved By AWS for DNS server
	- 10.0.0.3 Reserved By AWS for future use
	- 10.0.0.255 Brodycast Address
	AWS do not support brodcust in a VPC but reserve the address.
	
	


=================================================
#VPC | vpc | virtual private cloud                
================================================= 

Vpc is a Virtual Network or DataCenter inside AWS for one Client.
A VPC can span multiple availability zones in a region.
The CIDR block for the default VPC is always a 16 subnet mask; in this example, it's 172.31.0.0/16. It means this VPC can provide up to 65,536 IP addresses.

- It is logical Isolated from Other virtual Network in the AWS.
- Max 5 VPC can be created and 200 subnet in 1 VPC.
- We Can allocate Max 5 Elastic IP.
- Once a VPC created DHCP, NACL and Sucurity Group will be created automatically.
- A VPC is confied to on AWS Region and dos not extend between Region.

- Onece the VPC is created, Its CIDR block range cant change.
  (You can create another CIDR make it primary and older one will be Secondary then you can delete fitst one)
- If you need a diffenent CIDR size, create a new VPC.
- The different subnets wihtin a VPC cannot overlap.
- You can expend yor vpc CIDR by adding new/extra IP address.


VPC create on Region not available zone. all property of VPC are Regional becaus its exists on Region. 
One vpc cant extend more then one Region.


VPC Two Type

- Default Vpc
- Custom Vpc

Primary diffent of of both, default vpc has internet gatway custom vpc has not, you can added.

Default VPC has default CIDR, Security Group, NACL and Route table setting.
In Custom VPC has to be create considering its CIDR dos not have Internet Getwat be default.
 

VPC Peering: 
A peering connection can be made between your own VPCs or with a VPC in another AWS account, as long as it is in the same region.

VPCs, but transitive peering is not supported. In other words, VPC A can connect to B and C in the above diagram, 
but C cannot communicate with B unless directly paired.

We can have up to 200 Subnets per Amazon Virtual Private Cloud (VPC).

Monitor VPC by using:
CloudWatch and CloudWatch logs
VPC Flow Logs

#Component of VPC
-------------------------------------------------

 - CIDR and IP address subnet
 - Implied router and Routing Table
 - Internet Gatway
 - Security Group
 - Network ACL
 - Virtual Private gatway 
 - Peering connections
 - Elastic IP




-Each VPC has a main route table, by default
-Main route table has a default route enabling communication between resources in all subnets in a VPC
-Default route rule CANNOT be deleted/edited
-HOWEVER you can add/edit/delete other routing rules to the main route table


#Subnet 
-------------------------------------------------
Amazon defines a route table as a set of rules, called routes, which are used to determine where network traffic is directed.
Each subnet has to be linked to a route table, and a subnet can only be linked to one route table. On the other hand, one route table can have associations with multiple subnets. 


-Each subnet can have its own route table OR share its route table with the VPC
-If a subnet does not have a route table associated with it, it implicitly uses the route table of its VPC
-Multiple subnets can share a route table
-HOWEVER at any point in time, a subnet can be associated with one route table ONLY


Public Subnet:
-Communication is allowed from subnet to internet
-Communication is allowed from internet to subnet


Security products and features:
-Security groups - This acts as a firewall for the EC2 instances, controlling inbound and outbound traffic at the instance level.
-Network access control lists - It acts as a firewall for the subnets, controlling inbound and outbound traffic at the subnet level.
-Flow logs - These capture the inbound and outbound traffic from the network interfaces in your VPC.



=================================================
#VPN | VPNs | vpn             
================================================= 
VPCs can also serve as a bridge between your corporate data center and the AWS cloud. 
With a VPC Virtual Private Network (VPN), your VPC becomes an extension of your on-prem environment.



=================================================
#DirectConnect    | DX       | DC
================================================= 
AWS Direct Connect Private dedicated network connection from on-premises to AWS


Direct Connect is an AWS service that establishes a dedicated network connection between your premises and AWS. 
You can create this private connectivity to reduce network costs, increase bandwidth, and provide more consistent network experience 
compared to regular internet-based connections.

The use case for Direct Connect is high throughput workloads or if you need a stable or reliable connection


DirectConnect connects your on-prem with your VPC through a non-public tunnel.

=================================================
#NAT Gateway        
================================================= 
A Network Address Translation (NAT) device can be used to enable instances in a private subnet to connect to the internet 
or the AWS services, but this prevents the internet from initiating connections with the instances in a private subnet.

However, your private subnet database instance might still need internet access or the ability to connect to other AWS resources. 
You can use a NAT device to do so. 

The NAT device directs traffic from your private subnet to either the internet or other AWS services. 
It then sends the response back to your instances. When traffic is directed to the internet, the source 
IP address of your instance is replaced with the NAT device address, and when the internet traffic returns, 
the NAT device translates the address to your instance’s private IP address.


NAT device is added to the public subnet to get internet connectivity.



AWS provides two kinds of NAT devices:
NAT gateway 
NAT instance 


NAT Gateway
A NAT gateway must be launched in a public subnet because it needs internet connectivity. 
It also requires an elastic IP address, which you can select at the time of launch.

Once created, you need to update the route table associated with your private subnet to point internet-bound traffic to the NAT gateway. 
This way, the instances in your private subnet can communicate with the internet.

=================================================
#AWS PrivateLink
=================================================
In summary, Global Accelerator is a fast/reliable pipeline between user and application.

AWS PrivateLink simplifies the security of data shared with cloud-based applications by eliminating the exposure of 
data to the public Internet. AWS PrivateLink provides private connectivity between different VPCs, AWS services, 
and on-premises applications, securely on the Amazon network.

This is useful because different AWS services often talk to each other over the internet. 
If you do not want that behavior and instead want AWS services to only communicate within the AWS network, use AWS PrivateLink. 



Summary: AWS PrivateLink connects your AWS services with other AWS services through a non-public tunnel.



Network ACL:
-------------------------------------------------

Every subnet in your VPC must be associated with an ACL, failing which the subnet gets automatically associated with your default ACL.

One subnet can only be linked with one ACL. On the other hand, an ACL can be linked to multiple subnets.


Amazon VPC can contain anywhere from 16 to 65,536 IP addresses. You can select your CIDR block according to the number of instances needed.

VPC limits:

Five VPCs per region
200 subnets per VPC
200 route tables per VPC
500 security groups per VPC
50 inbound and outbound rules per VPC


Amazon VPC costs:
If you opt to create a hardware VPN connection associated with your VPC using Virtual Private Gateway, you will have to pay for each 
VPN connection hour that your VPN connection is provisioned and available. Each partial VPN connection hour consumed is 
billed as a full hour. You'll also incur standard AWS data transfer charges for all data transferred via the VPN connection. 

If you create a NAT gateway in your VPC, Charges are levied for each NAT gateway hour that your NAT gateway is 
provisioned and available for. Data processing charges apply for each gigabyte processed through a NAT gateway. 
Each partial NAT gateway hour consumed is billed as a full hour.



=================================================
#BGP |   Border Gateway Protocol          
================================================= 
BGP is a means by which all junction points on the internet (routers) communicate with each other to dynamically establish the 
correct (and correctly weighted) paths that network packets should follow to traverse the global networking

=================================================
#AWS Global Accelerator
=================================================

In summary, Global Accelerator is a fast/reliable pipeline between user and application.


AWS Global Accelerator accelerates connectivity to improve performance and availability for users. 
Global Accelerator sits on top of the AWS backbone and directs traffic to optimal endpoints worldwide. 
By default, Global Accelerator provides you two static IP addresses that you can make use of.

Global Accelerator also provides fast regional failover.




=================================================
#Internet Protocol (IP    
================================================= 
The Internet Protocol (IP) uses three types of addressing schemes: Unicast, Multicast, and Anycast.

Unicast:
A Unicast address is used to identify a single unique host. It is used to send data to a single destination. 
In computer networking, unicast communication is a one-to-one transmission from one point in the network to another. 


Multicast:
A Multicast address is used to deliver data to a group of destinations (a one-to-many transmission). 
IP multicast group addresses are represented by class-D IP addresses reserved specifically for multicast 
communications, ranging from 224.0.0.0 through 239.255.255.255. Any IP packet sent to a multicast address is 
delivered to only those hosts that have joined that particular IP Multicast group, resulting in less network traffic, 
thereby reducing bandwidth and network overhead. If the host hasn’t joined the group, the receiver ignores the packets 
at the hardware level, eliminating platform software resource consumption in that network element. 
IPv6 multicast replaces broadcast addresses that were supported in IPv4. 

Anycast:
Anycast, also known as IP Anycast or Anycast routing, is an IP network addressing scheme that allows multiple servers to share the same 
IP address, allowing for multiple physical destination servers to be logically identified by a single IP address. 
Based on the location of the user request, the anycast routers send it to the server in the network based on a least-cost analysis 
that includes assessing the number of hops, shortest distance, lowest transit cost, and minimum latency measurements to optimize the 
selection of a destination server.




=================================================
#EC2    
================================================= 

EC2 is short for Elastic Compute Cloud, and it provides scalable computing capacity.
EC2 provides virtual computing environments called “instances.”


t2.micro:
	t - Instance Family
	2 - generation. Improvements with each generation.
	micro - size. (nano < micro < small < medium < large < xlarge < …..)

EC2 provides two important services to get details:
-Instance Metadata Service
-Dynamic Data Service

URL: http://169.254.169.254/latest/meta-data/

URL: http://169.254.169.254/latest/dynamic/
Example: http://169.254.169.254/latest/dynamic/instance-identity/document



#EC2 capacity reservation
-------------------------------------------------

Capacity Reservations enable you to reserve compute capacity for your Amazon EC2 instances in a specific 
Availability Zone for any duration. This gives you the ability to create and manage Capacity Reservations 
independently from the billing discounts offered by Savings Plans or Regional Reserved Instances.


Capacity Reservations enable you to reserve capacity for your Amazon EC2 instances in a specific Availability Zone for any duration. 
This gives you the flexibility to selectively add capacity reservations and still get the Regional RI discounts for that usage. 
By creating Capacity Reservations, you ensure that you always have access to Amazon EC2 capacity when you need it, 
for as long as you need it. 



Creating copies of EC2 instances: Old instance -> Snapshot -> Image (AMI) -> New instance


Elastic IP Addresses:
How do you get a constant public IP address for a EC2 instance? Quick and dirty way is to use an Elastic IP!

An Elastic IP can be switched to another EC2 instance within the same region. Elastic IP remains attached even if you stop the instance. 
You have to manually detach it.



Bootstrapping: 
Install OS patches or software when an EC2 instance is launched.

Example:

#!/bin/bash
yum update -y
yum install -y httpd
systemctl start httpd
systemctl enable httpd
curl -s http://169.254.169.254/latest/dynamic/instance-identity/document > /var/www/html/index.html


You can call http://169.254.169.254/latest/user-data/ from inside the EC2 instance to lookup user data configured for that instance.

Termination Protection:

Remember:EC2 Termination Protection is not effective for terminations from a) Auto Scaling Groups (ASG) b) Spot Instances c) OS Shutdown

Launch Templates - Pre-configured templates (AMI ID, instance type, and network settings) simplifying the creation of EC2 instances.

Key Pairs:
-Public key cryptography (Key Pairs) used to protect your EC2 instances
-You need private key with right permissions (chmod 400) to connect to your EC2 instance. 
(Windows EC2 instances only) You need admin password also.
-Security group should allow SSH(22) or RDP(3389)


#AMI  | Amazon Machine Image
-------------------------------------------------
You need to choose the Amazon Machine Image or AMI based on what operating system and what software do you want on the EC2 instance.

AMIs are stored in Amazon S3 (region specific).

AMIs contain:
-Root volume block storage (OS and applications)
-Block device mappings for non-root volumes
-You can configure launch permissions on an AMI

Who can use the AMI?
You can share your AMIs with other AWS accounts

Best Practice: Backup upto date AMIs in multiple regions
Critical for Disaster Recovery


Three AMI sources:
-Provided by AWS
-AWS Market Place: Online store for customized AMIs. Per hour billing
-Customized AMIs: Created by you.



#Monitoring EC2 instances
-------------------------------------------------
Amazon CloudWatch is used to monitor EC2 instances.

There are two types of monitoring:
(FREE) Basic monitoring (“Every 5 minutes”) provided for all EC2 instance types
(\(\)) EC2 Detailed Monitoring can be enabled for detailed metrics every 1 minute


#EC2 Tenancy - Shared vs Dedicated
--------------------------------------------------

There are two Dedicated EC2 options:

-EC2 Dedicated Instances
-EC2 Dedicated Hosts


EC2 Dedicated Instances are Virtualized instances on hardware dedicated to one customer:
You do NOT have visibility into the hardware of underlying host

EC2 Dedicated Hosts are Physical servers dedicated to one customer:
You have visibility into the hardware of underlying host (sockets and physical cores)
(Use cases) Regulatory needs or server-bound software licenses like Windows Server, SQL Server


#EC2 Pricing Models:
--------------------------------------------------

-On Demand: Request when you want it	Flexible and Most Expensive
-Spot: Quote the maximum price	Cheapest (upto 90% off) BUT NO Guarantees
-Reserved: Reserve ahead of time	Upto 75% off. 1 or 3 years reservation.
-Savings Plans: Commit spending $X per hour on (EC2 or AWS Fargate or Lambda)	Upto 66% off. No restrictions. 1 or 3 years reservation.


INP:
====
-You want to update the EC2 instance to a new AMI updated with latest patches
Relaunch a new instance with an updated AMI

-Create EC2 instances based on on-premise Virtual Machine (VM) images	
Use VM Import/Export. You are responsible for licenses.


-You are installing a lot of software using user data slowing down instance launch. How to make it faster?	
Create an AMI from the EC2 instance and use it for launching new instances

-I’ve stopped my EC2 instance. Will I be billed for it?	
ZERO charge for a stopped instance (If you have storage attached, you have to pay for storage)



=================================================
#Placement Group        
================================================= 

Placement groups balance the tradeoff between risk tolerance and network performance when it comes to your fleet of EC2 instances.
The more you care about risk, the more isolated you want your instances to be from each other. 
The more you care about performance, the more conjoined you want your instances to be with each other.


Placing the things at one place or gathering the things together at one place is nothing but placement groups.

When you launch multiple EC2 instances on AWS, the EC2 service makes sure that all of your EC2 instances 
are spread across different physical machines to minimize the failure of the entire system. 
But AWS EC2 also provides the customers the ability to put the EC2 instance according to their need. 
Placement groups are used to determine how the EC2 instances are launched on the underlying hardware. 


Why use Placement Group?

Placement groups help us to launch a bunch of EC2 instances close to each other physically within the same AZ. 
Being close physically and within the same AZ helps it take advantage of high-speed connectivity to 
provide low latency, high throughput access.

Placement groups strategies:

Cluster placement group: It groups instances into low latency clusters in a single available zone(AZ).
Spread placement group: It spread the instances across underlying hardware.
Partition placement group: It spreads the instances across many different partitions within an AZ.


Cluster Placement Group:
In the cluster placement group, all the instances are in the same rack in a single availability zone. 
Cluster placement groups are designed for high speed performance and low network latency applications as 
EC2 instances are physically on the same rack and it causes low latency between the EC2 instances in the same cluster placement group. 
It usually supports up to 10Gbps network. As the EC2 instances in the cluster placement group are in the same physical rack so the 
problem with cluster placement groups is if the rack fails, all the instances will fail at the same time 
compromising the high availability of the application.
  
Spread Placement Group:
In the spread placement group, all EC2 instances are located on different hardware racks in a single availability zone. 
Each rack is isolated from others and has its own power and networks to reduce the failure of all the instances in the 
spread placement group at a time. You can create up to 7 EC2 instances per availability zone per spread placement group. 
Unlike Cluster placement groups, EC2 instances in the spread placement group exist on different hardware within 
the single availability zone minimizing the failure of all the EC2 instances at a time while making sure of the low latency. 
Spread placement groups are designed for applications that require maximum high availability and where 
each instance must be isolated from failure from each other.



Partition Placement Group:
In the partition placement group, instances are launched into different partitions on different hardware racks to 
make sure of high availability. It can span across multiple AZs in the same region. 
The instances in a partition do not share racks with the instances in the other partitions. 
A partition failure can affect many EC2 instances in the same partition but won’t affect the EC2 instances on the other partitions. 
Partition placement groups are designed for applications that require maximum high availability. 
Partition placement groups are used for big application deployment and are ideal 
for large distributed and replicated workloads such as kafka, hadoop and cassandra etc.


=================================================
#ENI |  Elastic network interfaces             
================================================= 
An elastic network interface is a logical networking component in a VPC that represents a virtual network card. 
It can include the following attributes:

A primary private IPv4 address from the IPv4 address range of your VPC
One or more secondary private IPv4 addresses from the IPv4 address range of your VPC
One Elastic IP address (IPv4) per private IPv4 address
One public IPv4 address
One or more IPv6 addresses
One or more security groups
A MAC address
A source/destination check flag
A description

ENIs are virtual network cards you can attach to your EC2 instances. They are used to enable network connectivity for your instances, 
and having more than one of them connected to your instance allows it to communicate on two different subnets.


A common use case for ENIs is the creation of management networks. 
This allows you to have public-facing applications like web servers in a public subnet but lock down SSH access 
down to a private subnet on a secondary network interface. In this scenario, you would connect using a 
VPN to the private management subnet, then administrate your servers as usual.




Each EC2 instance is connected to primary network interface (eth0). 
You can create and attach a secondary network interface - eth1.

This allows an instance to be dual homed - present in two subnets in a VPC. 
It can be used to create a management network or a low budget high availability solution.


Important terminology with ENI:
Hot attach: Attaching ENI when EC2 instance is running
Warm attach: Attaching ENI when EC2 instance is stopped
Cold attach: Attaching ENI at launch time of EC2 instance


=================================================
#Security Groups    
================================================= 

Security Groups are used to control access (SSH, HTTP, RDP, etc.) with EC2. 
They act as a virtual firewall for your instances to control inbound and outbound traffic. 
When you launch an instance in a VPC, you can assign up to five security groups to 
the instance and security groups act at the instance level, not the subnet level.

Security groups are specific to a single VPC, so you can't share a Security Group 
between multiple VPCs. However, you can copy a Security Group to create a new Security Group with the same 
rules in another VPC for the same AWS Account.


Security Groups are regional and can span AZs, but can't be cross-regional.

You can specify the source of your security group (basically who is allowed to bypass the virtual firewall) 
to be a single /32 IP address, an IP range, or even a separate security group.

You cannot block specific IP addresses with Security Groups (use NACLs instead)


Security Groups are stateful:
-If an outgoing request is allowed, the incoming response for it is automatically allowed.
-If an incoming request is allowed, an outgoing response for it is automatically allowed


=================================================
#WAF   | Web Application Firewall (WAF)
================================================= 
AWS WAF is a web application that lets you allow or block the HTTP(s) requests that are bound for CloudFront, 
API Gateway, Application Load Balancers, EC2, and other Layer 7 entry points into your AWS environment. 
AWS WAF gives you control over how traffic reaches your applications by enabling you to create security 
rules that block common attack patterns, such as SQL injection or cross-site scripting, 
and rules that filter out specific traffic patterns that you can define. 



=================================================
#Storage | DB | db | s3 | efs | ebs           
================================================= 
There are several categories of databases:
Relational (OLTP and OLAP), Document, Key Value, Graph, In Memory among others

Buckets:
-Buckets are a universal namespace, i.e., the bucket names must be unique.
-If uploading of an object to S3 bucket is successful, we receive a HTTP 200 code.
-S3, S3-IA, S3 Reduced Redundancy Storage are the storage classes.
-Encryption is of two types, i.e., Client Side Encryption and Server Side Encryption
-Access to the buckets can be controlled by using either ACL (Access Control List) or bucket policies.
-By default buckets are private and all the objects stored in a bucket are also private.


Amazon Simple Storage Service (Amazon S3) is an object storage service that offers 
industry-leading scalability, data availability, security, and performance.

Amazon S3 is an object storage service that stores data as objects within buckets. 
An object is a file and any metadata that describes the file. A bucket is a container for objects.
Each object has a key (or key name), which is the unique identifier for the object within the bucket.

Access control lists (ACLs)
You can use ACLs to grant read and write permissions to authorized users for individual buckets and objects. 
Each bucket and object has an ACL attached to it as a subresource. 
The ACL defines which AWS accounts or groups are granted access and the type of access.

Amazon S3 cloud storage is an object-based storage service. You cannot install an operating system when you use 
Amazon S3 storage because data cannot be accessed on the block level as it is required by an operating system.


AWS Storage:
  - Simple Storage service (S3)
  - Elastic file system (EFS)
  - Elastic Block Storage (EBS)
  - Glacier
  - Snowball
  
  
   
#Simple Storage Service (S3)
--------------------------------------------------
S3 Object base storage, its able to access via http/https.
Its a distrubute database and data keep in bucket.

TypeOfS3:
 - S3 Standard
 - Amazon Glacher
 - Glacher Deep archive
 - Standart infrequental acc
 - One zone IA
 - Intelactual
 
 
 
#Amazon S3 Replication
--------------------------------------------------

Replication is the automatic, asynchronous copying of objects across buckets in the same or different AWS Regions. 
Replication copies newly created objects and object updates from a source bucket to a destination bucket or buckets. 

When you configure replication, you add replication rules to the source bucket. 
Replication rules define which source bucket objects to replicate and the destination bucket or buckets where 
the replicated objects are stored. You can create a rule to replicate all the objects in a bucket or a subset of 
objects with a specific key name prefix, one or more object tags, or both. A destination bucket can be in the 
same AWS account as the source bucket, or it can be in a different account.

If you specify an object version ID to delete, Amazon S3 deletes that object version in the source bucket. 
But it doesn't replicate the deletion in the destination bucket. In other words, 
it doesn't delete the same object version from the destination bucket. This protects data from malicious deletions.

When you add a replication rule to a bucket, the rule is enabled by default, so it starts working as soon as you save it.

Amazon Simple Storage Service (S3) Replication is an elastic, fully managed, low cost feature that replicates objects 
between buckets. S3 Replication offers the most flexibility and functionality in cloud storage, giving you the controls you need to 
meet your data sovereignty and other business needs.

With Amazon S3 Replication, you can configure Amazon S3 to automatically replicate S3 objects across different 
AWS Regions by using S3 Cross-Region Replication (CRR) or between buckets in the same AWS Region by using S3 
Same-Region Replication (SRR). S3 Replication offers the flexibility of replicating to multiple destination buckets in the same, 
or different AWS Regions. S3 Replication supports two-way replication between two or more buckets in the same or different AWS Regions. 


 
#S3 security
-------------------------------------------------

Security responsibility includes the following areas:

-Managing your data, including object ownership and encryption.
-Classifying your assets.
-Managing access to your data using IAM roles and other service configurations to apply the appropriate permissions.
-Enabling detective controls such as AWS CloudTrail or Amazon GuardDuty for Amazon S3.


Server-Side Encryption:
Request Amazon S3 to encrypt your object before saving it on disks in its data centers and then decrypt it 
when you download the objects. Server-side encryption can help reduce risk to your data by encrypting 
the data with a key that is stored in a different mechanism than the mechanism that stores the data itself.

Amazon S3 provides these server-side encryption options:
-Server-side encryption with Amazon S3‐managed keys (SSE-S3).
-Server-side encryption with KMS key stored in AWS Key Management Service (SSE-KMS).
-Server-side encryption with customer-provided keys (SSE-C).


Client-Side Encryption:

Encrypt data client-side and upload the encrypted data to Amazon S3. In this case, you manage the encryption process, 
the encryption keys, and related tools. As with server-side encryption, client-side encryption can help 
reduce risk by encrypting the data with a key that is stored in a different mechanism than the mechanism that stores the data itself.

#Elastic Block Storage (EBS)
-------------------------------------------------
 - One EBS for one EC2
 - EBS Volume locked at the AZ level, EC2 and EBS have to be same zone to attach.
 - Make shapshots to sent data volum to AZ or region.
 - Using EBS Multi-Attach, you can attach the same EBS volume to multiple EC2 instances in the same AZ (with Full rw).


#EFS - Elastic File System
-------------------------------------------------
Simple, serverless, set-and-forget, elastic file system

Amazon Elastic File System (Amazon EFS) provides a simple, serverless, set-and-forget elastic file system for use with AWS Cloud services and on-premises resources. 

Amazon EFS provides a simple, serverless, set-and-forget elastic file system.
With Amazon EFS, you can create a file system, mount the file system on an Amazon EC2 instance,
and then read and write data to and from your file system. 
You can mount an Amazon EFS file system in your virtual private cloud (VPC), through the 
Network File System versions 4.0 and 4.1 (NFSv4) protocol.


#Object Lifecycle Management
------------------------------------------------
A lifecycle configuration is a set of rules that define actions that AWS S3 applies to a group of objects. There are two types of actions:

Transaction Actions
This action defines objects’ transition from one storage class to another.

Expiration Actions
This action deletes objects in the Amazon S3 bucket.


#Amazon DynamoDB
------------------------------------------------
Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. 
It's a fully managed, multiregion, multimaster, durable non-SQL database. It comes with built-in security, backup and restore, 
and in-memory caching for internet-scale applications.


The main components of DynamoDB are:

-a collection which serves as the foundational table
-a document which is equivalent to a row in a SQL database
-key-value pairs which are the fields within the document or row

Amazon DynamoDB is a Serverless key-value and document database that delivers single-digit millisecond performance at any scale. 
It's a fully managed, multiregion, multimaster, durable database with built-in security, backup and restore, 
and in-memory caching for internet-scale applications. DynamoDB can handle more than 10 trillion requests 
per day and can support peaks of more than 20 million requests per second.

Amazon DynamoDB is a fully managed, serverless, key-value NoSQL database designed to run high-performance applications at any scale. 
DynamoDB offers built-in security, continuous backups, automated multi-Region replication, in-memory caching, and data export tools.


DynamoDB is accessible via an HTTP API and performs authentication & authorization via IAM roles, 
making it a perfect fit for building Serverless applications.


DAX:
===
DynamoDB Accelerator (DAX) delivers fast response times for accessing eventually consistent data. Certain usecases requires microseconds response times




#Amazon Aurora
------------------------------------------------
Amazon Aurora (Aurora) is a fully managed relational database engine that's compatible with MySQL and PostgreSQL.
The underlying storage grows automatically as needed. An Aurora cluster volume can grow to a maximum size of 128 tebibytes (TiB). 


Amazon Aurora is a relational database management system (RDBMS) built for the cloud with full MySQL and PostgreSQL compatibility. 
Aurora gives you the performance and availability of commercial-grade databases at one-tenth the cost.

Amazon Aurora provides built-in security, continuous backups, serverless compute, 
up to 15 read replicas, automated multi-Region replication, and integrations with other AWS services.

Amazon Aurora is a modern relational database service offering performance and high availability at scale, fully open source 
MySQL- and PostgreSQL-compatible editions, and a range of developer tools for 
building serverless and machine learning (ML)-driven applications.

Aurora features a distributed, fault-tolerant, and self-healing storage system that is 
decoupled from compute resources and auto-scales up to 128 TB per database instance. 
It delivers high performance and availability with up to 15 low-latency read replicas, 
point-in-time recovery, continuous backup to Amazon Simple Storage Service (Amazon S3), 
and replication across three Availability Zones (AZs).


#Data Warehouse
-------------------------------------------------
A data warehouse is a specialized type of relational database, which is optimized for
analysis and reporting of large amounts of data. It can be used to combine
transactional data from disparate sources (such as user behavior in a web application,
data from your finance and billing system, or customer relationship management or
CRM) to make them available for analysis and decision-making.

Traditionally, setting up, running, and scaling a data warehouse has been complicated
and expensive. On AWS, you can leverage Amazon Redshift, a managed data
warehouse service that is designed to operate at less than a tenth the cost of
traditional solutions.


#Redshift
-------------------------------------------------
Fastest, easiest, and most widely used cloud data warehouse

Amazon Redshift is a fully managed data warehouse service in the cloud. Its datasets range from 
100s of gigabytes to a petabyte. The initial process to create a data warehouse is to launch a 
set of compute resources called nodes, which are organized into groups called cluster. 
After that you can process your queries.

Amazon Redshift is a fully managed, scalable cloud data warehouse that accelerates your time to insights with fast, easy, 
and secure analytics at scale. Thousands of customers rely on 
Amazon Redshift to analyze data from terabytes to petabytes and run complex analytical queries.

Redshift is not multi-AZ, if you want multi-AZ you will need to spin up a separate cluster ingesting the same input. 
You can also manually restore snapshots to a new AZ in the event of an outage.


What are the differences between a database and a data warehouse? 
A database is any collection of data organized for storage, accessibility, and retrieval. 

A data warehouse is a type of database the integrates copies of transaction data 
from disparate source systems and provisions them for analytical use.


#Storage Gateway 
-------------------------------------------------



AWS Storage Gateway - Summary:
Key to look for : Hybrid storage (cloud + on premise)

File share (NFS or SMB) + Looking for S3 features and integrations => AWS Storage File Gateway

Tapes on cloud => AWS Storage Tape Gateway

Volumes on cloud (Block Storage) => AWS Storage Volume Gateway

High performance => Stored
Otherwise => Cached

Needs additional setup on-premises

VM image with AWS Storage Gateway software deployed on-premises or on EC2 instance




=================================================
#RDS       
================================================= 

RDS is a managed service that makes it easy to set up, operate, and scale a relational database in AWS. 
It provides cost-efficient and resizable capacity while automating or outsourcing time-consuming administration tasks such as 
hardware provisioning, database setup, patching and backups.

RDS comes in six different flavors:
SQL Server
Oracle
MySQL Server
PostgreSQL
MariaDB
Aurora


RDS has two key features when scaling out:
-Read replication for improved performance
-Multi-AZ for high availability


You cannot SSH into an RDS instance so therefore you cannot patch the OS. 
This means that AWS is responsible for the security and maintenance of RDS. 

You can provision an EC2 instance as a database if you need or want to manage the underlying server yourself, 
but not with an RDS engine.

Multi-AZ is supported for all DB flavors except aurora. This is because Aurora is completely fault-tolerant on its own.
Multi-AZ feature allows for high availability across availability zones and not regions.

During a failover, the recovered former primary becomes the new secondary and the promoted secondary becomes primary. 
Once the original DB is recovered, there will be a sync process kicked off where the two 
DBs mirror each other once to sync up on the new data that the failed former primary might have missed out on.



Read Replication is exclusively used for performance enhancement.

You can promote read replicas to be their very own production database if needed.
Read replicas are supported for all six flavors of DB on top of RDS.
Each Read Replica will have its own DNS endpoint.
Automated backups must be enabled in order to use read replicas.



RDS Backups:

When it comes to RDS, there are two kinds of backups:
-automated backups
-database snapshots

Automated backups allow you to recover your database to any point in time within a retention period (between one and 35 days). 
Automated backups will take a full daily snapshot and will also store transaction logs throughout the day. 
When you perform a DB recovery, RDS will first choose the most recent daily backup and apply the relevant transaction logs from that day. 


DB snapshots are done manually by the administrator. A key different from automated backups is that they are retained even 
after the original RDS instance is terminated. With automated backups, the backed up data in 
S3 is wiped clean along with the RDS engine. 


A relational database system does not scale well for the following reasons:
It normalizes data and stores it on multiple tables that require multiple queries to write to disk.
It generally incurs the performance costs of an ACID-compliant transaction system.
It uses expensive joins to reassemble required views of query results.


Scenario:
=========
-You want full control of OS or need elevated permissions	Consider going for a custom installation (EC2 + EBS)
-You want to migrate data from an on-premise database to cloud database of the same type	Consider using AWS Database Migration Service
-You want to migrate data from one database engine to another (Example : Microsoft SQL Server to Amazon Aurora)	Consider using AWS Schema Conversion Tool
-What are retained when you delete a RDS database instance?	All automatic backups are deleted
-All manual snapshots are retained (until explicit deletion)
(Optional) Take a final snapshot
-How do you reduce global latency and improve disaster recovery?	Use multi region read replicas
-How do you select the subnets a RDS instance is launched into?	Create DB Subnet groups
-How can you add encryption to an unencrypted database instance?	Create a DB snapshot
-Encrypt the database snapshot using keys from KMS
-Create a database from the encrypted snapshot
-Are you billed if you stop your DB instance?	You are billed for storage, IOPS, backups and snapshots. You are NOT billed for DB instance hours
-I will need RDS for at least one year. How can I reduce costs?	Use Amazon RDS reserved instances.
-Efficiently manage database connections	Use Amazon RDS Proxy
-Sits between client applications (including lambdas) and RDS




Let’s add a standby database in the second data center with replication.

Let’s consider some challenges:

Challenge 1 (SOLVED): Your database will go down if the data center crashes
You can switch to the standby database
Challenge 2 (SOLVED): You will lose data if the database crashes
Challenge 3 (SOLVED): Database will be slow when you take snapshots
Take snapshots from standby.
Applications connecting to master will get good performance always



=================================================
#Elastic Load Blancer |  | elb | ELB          
=================================================

Elastic Load Balancer are used to distribute traffic across EC2 instances in one or more AZs in a single region. 

The AWS ELB is an AWS service for automatic distribution of incoming application traffic across its components like Amazon EC2 instances, AWS Lambda, and containers..

A load balancer accepts incoming traffic from clients and routes requests to its registered targets (such as EC2) in one or more AZ. 
The load balancer also monitors the health of its registered targets and ensures that it routes traffic only to healthy targets.
When the load balancer detects an unhealthy target, it stops routing traffic to that target. 
It then resumes routing traffic to that target when it detects that the target is healthy again.

TypeOfLoadBalancer:
 - Application Load Balancers
 - Network Load Balancers
 - Gateway Load Balancers
 - Classic Load Balancers
 
Load balancers are a regional service. They do not balance load across different regions. 
You must provision a new ELB in each region that you operate out of.


With Application Load Balancers, Network Load Balancers, and Gateway Load Balancers, 
you register targets in target groups, and route traffic to the target groups. 
With Classic Load Balancers,you register instances with the load balancer.


Cross-zone load balancing is enable then every target equal load, if disable the  every AZ are equal. 
Application Load Balancers, cross-zone load balancing is always enabled.
With Network Load Balancers and Gateway Load Balancers, cross-zone load balancing is disabled by default. 


Elastic Load Balancing works with the following services:

 - Amazon EC2 — Virtual servers that run your applications in the cloud. 
 - Amazon EC2 Auto Scaling — Ensures that you are running your desired number of instances, even if an instance fails. 
 - AWS Certificate Manager — When you create an HTTPS listener, you can specify certificates provided by ACM. The load balancer uses certificates to terminate connections and decrypt requests from clients.
 - Amazon CloudWatch — Enables you to monitor your load balancer and to take action as needed. 
 - Amazon ECS — Enables you to run, stop, and manage Docker containers on a cluster of EC2 instances. 
 - AWS Global Accelerator — Improves the availability and performance of your application. Use an accelerator to distribute traffic across multiple load balancers in one or more AWS Regions.
 - Route 53 — Provides a reliable and cost-effective way to route visitors to websites by translating domain names into the numeric IP addresses that computers use to connect to each other.
 - AWS WAF — You can use AWS WAF (Web Application Firewall) with your Application Load Balancer to allow or block requests based on the rules in a web access control list (web ACL).

 
#AWS Classic Load Balancer:
------------------------------------------------- 
This simple load balancer operates both at the request level and the connection level and was originally used for classic EC2 instances. 
Classic Load balancer in AWS is used on EC2-classic instances. This is the previous generation’s load balancer and also it doesn’t allow host-based or path based routing.
Mostly it is used to route traffic to one single URL.


#NLB
------------------------------------------------- 
AWS recommends AWS Network Load Balancer (NLB) if the application needs to achieve static IP and extreme performance.
AWS network load balancers also avoid DNS caching problems and work with existing firewall security policies of users thanks to its static and resilient IP addresses. 
And AWS load balancer TLS termination is only possible with NLB.


#Create a ELB
------------------------------------------------- 
Select which type of AWS load balancer to use
Complete basic configuration
Configure a security group
Configure a target group
Register targets
Create a load balancer and test it
Get more details on how to configure AWS load balancers


#GateWay Load Balancer
-------------------------------------------------

Gateway Load Balancer helps you easily deploy, scale, and manage your third-party virtual appliances. 
It gives you one gateway for distributing traffic across multiple virtual appliances while scaling them up or down, 
based on demand. This decreases potential points of failure in your network and increases availability.

You can find, test, and buy virtual appliances from third-party vendors directly in AWS Marketplace.  

Use cases:
Centralize your third-party virtual appliances
Consolidating your third-party virtual appliances with Gateway Load Balancer can reduce operational overhead and costs.

=================================================
#ASG Auto Scaling | Auto Scaling groups        
================================================= 

An Auto Scaling group contains a collection of EC2 instances that are treated as a 
logical grouping for the purposes of automatic scaling and management. 
An Auto Scaling group also enables you to use Amazon EC2 Auto Scaling features such as health check replacements and scaling policies.

When you create an EC2 Auto Scaling group, you must specify a launch configuration. You can specify your launch configuration with multiple EC2 Auto Scaling groups. 

AutoScaling happend in a available zone.


Component of Authscaling
 - Launch configuation
 - AutoScaling Group
 - Scaling Policy
 
 
AutoScalingPolicies
 - Manual
 - Daynamic
 
 
You cannot modify a launch configuration after you've created it. If you want to change the launch 
configuration for an Auto Scaling group, you must create a new launch configuration and update your Auto Scaling group to 
inherit this new launch configuration.

The default termination policy for an Auto Scaling Group is to automatically terminate a stopped instance, 
so unless you've configured it to do otherwise, stopping an instance will result in termination regardless 
if you wanted that to happen or not. 
A new instance will be spun up in its place.
 
 


#Amazon EC2 Auto Scaling vs. AWS Auto Scaling           
--------------------------------------------------

AWS Auto Scaling:
You should use AWS Auto Scaling to manage scaling for multiple resources across multiple services. 
AWS Auto Scaling lets you define dynamic scaling policies for multiple EC2 Auto Scaling groups or other resources using predefined scaling strategies. Using AWS Auto Scaling to configure scaling policies for all of the scalable resources 
in your application is faster than managing scaling policies for each resource via its individual service console

Amazon EC2 Auto Scaling :
You should use EC2 Auto Scaling if you only need to scale Amazon EC2 Auto Scaling groups, or if you are only interested 
in maintaining the health of your EC2 fleet. You should also use EC2 Auto Scaling if you need 
to create or configure Amazon EC2 Auto Scaling groups, or if you need to set up scheduled or step 
scaling policies (as AWS Auto Scaling supports only target tracking scaling policies)



Predictive Scaling of AWS Auto Scaling pla:
Predictive Scaling Policy brings the similar prediction algorithm offered through AWS Auto Scaling 
plan as a native scaling policy in EC2 Auto Scaling. 
If you have predictable load changes, you can use Predictive Scaling policy to proactively increase capacity ahead of upcoming demand. 
Amazon EC2 Auto Scaling enables you to run your Amazon EC2 fleet at optimal utilization.



What is fleet management and how is it different from dynamic scaling:

Fleet management refers to the functionality that automatically replaces unhealthy instances and maintains your fleet at the 
desired capacity. Amazon EC2 Auto Scaling fleet management ensures that your application is able to receive traffic 
and that the instances themselves are working properly. 
When Auto Scaling detects a failed health check, it can replace the instance automatically.


The dynamic scaling capabilities of Amazon EC2 Auto Scaling refers to the functionality that automatically increases 
or decreases capacity based on load or other metrics. For example, if your CPU spikes above 80% (and you have an alarm setup)
 Amazon EC2 Auto Scaling can add a new instance dynamically.

=================================================
# Edge locations             
================================================= 

Edge locations are AWS data centers designed to deliver services with the lowest latency possible.


UserCase:

CloudFront, 
which uses edge locations to cache copies of the content that it serves, so the content is closer to users and can be delivered to them faster.
Route 53, 
which serves DNS responses from edge locations, so that DNS queries that originate nearby can resolve faster (and, contrary to what you might think, is also Amazon’s premier database).
Web Application Firewall and AWS Shield, 
which filter traffic in edge locations to stop unwanted traffic as soon as possible.




=================================================
#ElastiCache  | Elasti Cache      
================================================= 

Amazon ElastiCache is a fully managed, in-memory caching service supporting flexible, real-time use cases. 
You can use ElastiCache for caching, which accelerates application and database performance, 
or as a primary data store for use cases that don't require durability like 
session stores, gaming leaderboards, streaming, and analytics. ElastiCache is compatible with Redis and Memcached. 


Use cases:
Accelerate application performance
Ease backend database load
Build low-latency data stores


Amazon ElastiCache is a web service that makes it easy to deploy and run Memcached or Redis 
protocol-compliant server nodes in the cloud. Amazon ElastiCache improves the performance of web applications by 
allowing you to retrieve information from a fast, managed, in-memory system, instead of relying entirely on slower disk-based databases. 

=================================================
#CloudFront  | cludFont      
================================================= 
Global Accelerator and CloudFront both use the AWS global network and its edge locations around the world.

CloudFront is Amazon’s content delivery network that is primarily used to speed up websites. 
It’s particularly useful for large, static assets—like images and videos. CloudFront sits in front of an “origin” server 
(which serves the original content), and caches it at the edge locations around the world.

When a user visits a site, they’re routed to the nearest edge location using DNS. 
CloudFront looks to see if the page they requested is cached. If it is, the page is served directly from the cache. 
If it isn’t, CloudFront fetches the page from the origin, stores it in the cache, and serves it to the user. 
The next user to hit the same edge location will get the page served from the cache.


The AWS CDN service is called CloudFront. It serves up cached content and assets for the increased global performance of your application. 
The main components of CloudFront are the edge locations (cache endpoints), the origin (original source of truth to be cached 
such as an EC2 instance, an S3 bucket, an Elastic Load Balancer or a Route 53 config), and the distribution (the arrangement 
of edge locations from the origin or basically the network itself).


Performance
CloudFront improves performance for both cacheable content (such as images and videos) and dynamic content (such as API acceleration and dynamic site delivery).
Global Accelerator improves performance for a wide range of applications over TCP or UDP by proxying packets at the edge to applications running in one or more AWS Regions.

Use Cases
CloudFront is a good fit for HTTP use cases
Global Accelerator is a good fit for non-HTTP use cases, such as gaming (UDP), IoT (MQTT), or VoIP, as well as for HTTP use cases that require static IP addresses or deterministic, fast regional failover.

Caching
CloudFront supports Edge caching
Global Accelerator does not support Edge Caching.

Geo-Targeting:
Geo-Targeting is a concept where businesses can show personalized content to their audience based on their geographic location 
without changing the URL. This helps you create customized content for the audience of a specific 
geographical area, keeping their needs in the forefront.



=================================================
#AWS Global Accelerator (AMS SSPS)          
================================================= 

Global Accelerator, your users' traffic is moved off the internet and onto Amazon’s private global network through 90+ global edge locations, then directed to your application origins. 

AWS Global Accelerator is a networking service that 
improves the performance of your users’ traffic by up to 60% using Amazon Web Services’ global network infrastructure.

Global Accelerator, you are provided two global static public IPs that act as a fixed entry point to your application, 
improving availability. On the back end, add or remove your AWS application endpoints, such as Application Load Balancers, 
Network Load Balancers, EC2 Instances, and Elastic IPs without making user-facing changes.

Global Accelerator automatically re-routes your traffic to your nearest healthy available endpoint to mitigate endpoint failure.


It provides static IP addresses that act as a fixed entry point to application endpoints in a single or multiple AWS Regions, such as Application Load Balancers, Network Load Balancers or EC2 instances.



=================================================
#Amazon FSx       
================================================= 

Amazon FSx for Windows File Server provides a fully managed native Microsoft File System.
You can use Microsoft Active Directory to authenticate into the file system.


Amazon FSx is a fully managed third-party file system solution. It uses SSD storage to provide fast performance with low latency.
There are four available FSx solutions available in AWS:

Amazon FSx makes it easy and cost effective to launch, run, and scale feature-rich, high-performance file systems in the cloud. 
It supports a wide range of workloads with its reliability, security, scalability, and broad set of capabilities. 

Four widely-used file systems: 
 - NetApp ONTAP, 
 - OpenZFS, 
 - Windows File Server 
 - Lustre.
 
 
NetApp ONTAP
In collaboration with NetApp, AWS has launched Amazon FSx for NetApp ONTAP, a new cloud-based managed shared file and block storage 
service that brings the best of both worlds to their customers.
FSx for ONTAP delivers NFS, SMB and iSCSI storage powered by NetApp’s advanced data management system
 
Lustre
Amazon FSx for Lustre offers fully-managed storage built especially to provide high-performance at scale for compute workloads. 
It is ideal for machine learning, video rendering, high performance computing and financial simulations.



Windows File Server:
FSx for Windows File Server provides fully managed Microsoft Windows file servers, that are backed by a fully native Windows file system. 


FSx has two key differentiators compared to other Amazon’s previous file service offerings such as Elastic File Service (EFS). 
It comes with a complete file server built in, and it offers superior performance for demanding use cases.

Amazon FSx offers file systems designed for a variety of workload types. 
You can use AWS FSx as storage for Windows applications, machine learning (ML) and high-performance computing (HPC). 
FSx can also help with electronic design automation.


You can deploy your Amazon FSx for Windows in a single AZ or in a Multi-AZ configuration.
By default, all data is encrypted at rest.


=================================================
#Amazon FSx for Lustre
================================================= 
Amazon FSx for Lustre makes it easy and cost effective to launch and run the open source Lustre file system for high-performance 
computing applications. With FSx for Lustre, you can launch and run a file system that can process massive data sets at up to 
hundreds of gigabytes per second of throughput, millions of IOPS, and sub-millisecond latencies.


FSx for Lustre is compatible with the most popular Linux-based AMIs, including Amazon Linux, 
Amazon Linux 2, Red Hat Enterprise Linux (RHEL), CentOS, SUSE Linux and Ubuntu.

Since the Lustre file system is designed for high-performance computing workloads that typically run on compute clusters, 
choose EFS for normal Linux file system if your requirements don't match this use case.

FSx Lustre has the ability to store and retrieve data directly on S3 on its own.



=================================================
#SQS, SNS, MQ and Amazon Kinesis        
================================================= 


Anytime multiple services need to receive the same event, you should consider SNS rather than SQS.

#AWS SQS
--------------------------------------------------
 The entire service is based on sending messages to the queue and allowing for applications (ex. ECS containers, Lambda functions)
 to poll for messages and process them. 
 The message stays in the queue until some application picks it up, processes it, and deletes the message when it’s done. 
 

#AWS SNS(Simple Notification Service)
--------------------------------------------------
It provides much more functionality than just the ability to send push notifications (emails, SMS, and mobile push). 
In fact, it’s a serverless publish-subscribe messaging system allowing to send events to multiple applications (subscribers) at the same time (fan-out), 
including SQS queues, Lambda functions, Kinesis Data Streams, and generic HTTP endpoints. 


In order to use the service, we only need to:
create a topic,
subscribe to a topic,
confirm the subscription,
start sending events to a topic to deliver them to all subscribers (potentially multiple applications and people).

#Amazon Kinesis 
--------------------------------------------------

Amazon Kinesis Data Streams is a serverless streaming data service that makes it easy to capture, process, 
and store data streams at any scale.
Amazon Kinesis makes it easy to collect, process, and analyze real-time, streaming data so you can get timely 
insights and react quickly to new information.

Amazon Kinesis is a managed, scalable, cloud-based service that allows real-time processing of streaming large amount of data per second. 
It is designed for real-time applications and allows developers to take in any amount of data from several sources, 
scaling up and down that can be run on EC2 instances.

It is used to capture, store, and process data from large, distributed streams such as event logs and social media feeds. 
After processing the data, Kinesis distributes it to multiple consumers simultaneously.

The producers continually push data to Kinesis Data Streams, and the consumers process the data in real time. 
Consumers (such as a custom application running on Amazon EC2 or an Amazon Kinesis Data Firehose delivery stream) 
can store their results using an AWS service such as Amazon DynamoDB, Amazon Redshift, or Amazon S3.

#MQ
--------------------------------------------------
Amazon MQ is a managed message broker service for Apache ActiveMQ and RabbitMQ that makes 
it easy to set up and operate message brokers in the cloud. 
You get direct access to the ActiveMQ and RabbitMQ consoles and industry standard APIs and protocols for messaging, 
including JMS, NMS, AMQP 1.0 and 0.9.1, STOMP, MQTT, and WebSocket. 
You can easily move from any message broker that uses these standards to Amazon MQ because you 
don’t have to rewrite any messaging code in your applications.



How do I migrate if I'm using a different message broker instead of ActiveMQ or RabbitMQ?
Amazon MQ provides compatibility with the most common messaging APIs, such as Java Message Service (JMS) and 
.NET Message Service (NMS), and protocols, including AMQP, STOMP, MQTT, and WebSocket. 
This makes it easy to switch from any standards-based message broker to Amazon MQ without rewriting the messaging code in your applications. 
In most cases, you can simply update the endpoints of your Amazon MQ broker to connect to your existing applications, 
and start sending messages.





=================================================
#CloudWatch  | cloudWatch      
================================================= 

Amazon CloudWatch is a monitoring and observability service built for DevOps engineers, developers, site reliability engineers (SREs), 
IT managers, and product owners. 

CloudWatch provides you with data and actionable insights to monitor your applications, respond to system-wide performance changes, 
and optimize resource utilization. 

CloudWatch collects monitoring and operational data in the form of logs, metrics, and events. 

CloudWatch collects monitoring and operational data in the form of logs, metrics, and events.
You can use CloudWatch to detect anomalous behavior in your environments, set alarms, visualize logs and metrics side by side, 
take automated actions, troubleshoot issues, and discover insights to keep your applications running smoothly.

You get a unified view of operational health and gain complete visibility of your AWS resources, applications, and services running on 
AWS and on-premises. You can use CloudWatch to detect anomalous behavior in your environments, set alarms, 
visualize logs and metrics side by side, take automated actions, troubleshoot issues, and discover insights to 
keep your applications running smoothly.
 
 - Use a single platform for observability
 - Collect metrics on AWS and on premises
 - Improve operational performance and resource optimization
 - Get operational visibility and insight
 - Derive actionable insights from logs
 
 
You can use metrics to calculate statistics and then present the data graphically in the CloudWatch console. 
You can configure alarm actions to stop, start, or terminate an Amazon EC2 instance when certain criteria are met.
 
Use cases:
-Monitor Amazon EC2
-Monitor Other Amazon Web Services Resources
-Monitor Custom Metrics
-Monitor and Store Logs
-Set Alarms
-Monitor and React to Resource Changes


CloudWatch is NOT CloudTrail so it is important to know that only CloudTrail can monitor AWS access for security and auditing reasons. 
CloudWatch is all about performance. CloudTrail is all about auditing.


=================================================
#AWS Event and EventBridge
================================================= 

#Event
--------------------------------------------------
An event indicates a change in an environment such as an AWS environment, a SaaS partner service or application, 
or one of your applications or services. The following are examples of events:

Amazon EC2 generates an event when the state of an instance changes from pending to running.

Amazon EC2 Auto Scaling generates events when it launches or terminates instances.

AWS CloudTrail publishes events when you make API calls.

You can also set up scheduled events that are generated on a periodic basis.

Events are represented as JSON objects and they all have a similar structure, and the same top-level fields.


#EventBridge
--------------------------------------------------
EventBridge is an event bus for messages that you want to propagate across your (micro)services. 
Those events can come from state changes of AWS services, other AWS accounts, or external 
applications like Auth0, Shopify, and others. You can, of course, also send your custom messages.

EventBridge delivers a stream of real-time data from event sources such as Zendesk or Shopify to targets like 
AWS Lambda and other SaaS applications. You can set up routing rules to determine 
where to send your data to build application architectures that react in real-time to your data sources 
with event publisher and consumer completely decoupled.

EventBridge sends metrics to Amazon CloudWatch every minute for everything from the number of 
matched events to the number of times a target is invoked by a rule.


In event-driven architecture, services interact with each other through events. 
An event is something that happened in your application (for example, an item was put into a cart, a new order was placed). 
Events are JSON objects that tell you information about something that happened in your application. 
In event-driven architecture, each component of the application raises an event whenever anything changes. 
Other components listen and decide what to do with it and how they would like to react.


#Glue
--------------------------------------------------

AWS Glue is a fully managed ETL (extract, transform, and load) service that makes it simple and cost-effective to categorize your data, 
clean it, enrich it, and move it reliably between various data stores and data streams. 


Athena uses the AWS Glue Data Catalog to store and retrieve table metadata for the Amazon S3 data in your Amazon Web Services account. 
The table metadata lets the Athena query engine know how to find, read, and process the data that you want to query.


Data integration is the process of preparing and combining data for analytics, machine learning, and application development. 
It involves multiple tasks, such as discovering and extracting data from various sources; enriching, cleaning, normalizing, 
and combining data; and loading and organizing data in databases, data warehouses, and data lakes. These tasks are 
often handled by different types of users that each use different products.

AWS Glue provides both visual and code-based interfaces to make data integration easier. 
Users can easily find and access data using the AWS Glue Data Catalog.


Glue uses ETL jobs to extract data from a combination of other Amazon Web Services and incorporates 
it into data lakes and data warehouses. It uses application programming interfaces (APIs) to transform the e
xtracted data set for integration, and to help users monitor jobs.

Users can put ETL jobs on a schedule or pick events that will trigger a job. 
Once triggered, Glue extracts the data, transforms it based on code that Glue generates automatically, 
and loads it into Amazon S3 or Amazon Redshift. Glue then writes metadata from the job into the AWS Glue Data Catalog.


AWS Glue DataBrew is a visual data preparation tool that enables users to clean and normalize data without writing any code. 

Simplify data preparation (capturing metadata) for analytics:
Connect AWS Glue to your data on AWS (Aurora, RDS, Redshift, S3 etc)
AWS Glue creates a AWS Glue Data Catalog with metadata abstracted from your data
Your data is ready for searching and querying






#CloudTrail 
--------------------------------------------------
CloudTrail logs actions inside your AWS environment.CloudTrail has a feature called CloudTrail Insights.
Insights let you detect unusual API activities on your account by automation.

AWS CloudTrail is an AWS service that helps you enable governance, compliance, and operational and risk auditing of your AWS account. 
Actions taken by a user, role, or an AWS service are recorded as events in CloudTrail. 
Events include actions taken in the AWS Management Console, AWS Command Line Interface, and AWS SDKs and APIs.

CloudTrail is enabled on your AWS account when you create it. 
When activity occurs in your AWS account, that activity is recorded in a CloudTrail event. 
You can easily view recent events in the CloudTrail console by going to Event history. 
For an ongoing record of activity and events in your AWS account, create a trail.



AWS CloudTrail is a service that automatically records events such as AWS API calls. 
You can create EventBridge rules that use the information from CloudTrail. 
For more information about CloudTrail, see What is AWS CloudTrail?.


There are two types of events that can be logged in CloudTrail: management events and data events.

Management events provide information about management operations that are performed on resources in your AWS account.
Think of Management events as things normally done by people when they are in AWS. 

Examples:
a user sign in
a policy changed
a newly created security configuration
a logging rule deletion


Data events provide information about the resource operations performed on or in a resource.
Think of Data events as things normally done by software when hitting various AWS endpoints. 

Examples:
S3 object-level API activity
Lambda function execution activity
By default, CloudTrail logs management events, but not data events.



Track events, API calls, changes made to your AWS resources:
-Who made the request?
-What action was performed?
-What are the parameters used?
-What was the end result?


#AWS Config | AWSConfig 
-------------------------------------------------
This helps you understand the configuration changes that happen in your environment. 
This service provides an AWS inventory that includes configuration history, configuration change notification, 
and relationships between AWS resources. It can also be configured to send information via AWS SNS when new logs are delivered.




#CloudFront
-------------------------------------------------

Amazon Cloudfront is a content delivery network (AWS CDN) that retrieves data stored in the 
Amazon S3 bucket and distributes it to numerous edge locations across the world. 
Edge locations are the network of data centers distributed worldwide through which content is delivered.

If the content is already cached in the edge location, CloudFront delivers it immediately with the lowest latency possible.
If the content is not present in the edge location, CloudFront retrieves it from the origin 
(like Amazon S3 bucket, a MediaPackage channel, or an HTTP server) that has been identified for your content.

Securely deliver content with low latency and high transfer speeds

Amazon CloudFront speeds up distribution of your static and dynamic web content, such as .html, .css, .php, image, and media files. 
When users request your content, CloudFront delivers it through a worldwide network of 
edge locations that provide low latency and high performance.

AWS CloudFront is a globally-distributed network offered by Amazon Web Services, 
which securely transfers content such as software, SDKs, videos, etc., to the clients, with high transfer speed.


Use cases:
-Deliver fast, secure websites
-Accelerate dynamic content delivery and APIs
-Stream live and on-demand video
-Distribute patches and updates



Scenario: Restrict content to users in certain countries
	Enable CloudFront Geo restriction
	Configure White list(countries to be allowed) and Blacklist(countries to be blocked)


=================================================
#Route 53 | Route53 
================================================= 
Amazon Route 53 is a highly available and scalable Domain Name System (DNS) service. 
You can use Route 53 to perform three main functions in any combination: domain registration, DNS routing, and health checking.


When you buy a domain name, every DNS address starts with an SOA (Start of Authority) record. 
The SOA record stores information about the name of the server that kicked off the transfer of ownership, 
the administrator who will now use the domain, the current metadata available, and the default number of seconds or TTL.

NS records, or Name Server records, are used by the Top Level Domain hosts (.org, .com, .uk, etc.) 
to direct traffic to the Content servers. The Content DNS servers contain the authoritative DNS records.

Browsers talk to the Top Level Domains whenever they are queried and encounter domain name that they do not recognize.
Browsers will ask for the authoritative DNS records associated with the domain.

Because the Top Level Domain contains NS records, the TLD can in turn queries the Name Servers for their own SOA.
Within the SOA, there will be the requested information.

Once this information is collected, it will then be returned all the way back to the original browser asking for it.



In summary: Browser -> TLD -> NS -> SOA -> DNS record. The pipeline reverses when the correct DNS record is found.


The routing policies available are:
-Simple Routing
-Weighted Routing
-Latency-based Routing
-Failover Routing
-Geolocation Routing
-Geo-proximity Routing
-Multivalue Answer Routing

=================================================
#Security | IAM |       
================================================= 

IAM is a global AWS services that is not limited by regions. Any user, group, role or policy is accessible globally.

IAM Entities:
Users - any individual end user such as an employee, system architect, CTO, etc.

Groups - any collection of similar people with shared permissions such as system administrators, HR employees, finance teams, etc. 
Each user within their specified group will inherit the permissions set for the group.

Roles - any software service that needs to be granted permissions to do its job, 
e.g- AWS Lambda needing write permissions to S3 or a fleet of EC2 instances needing read permissions from a RDS MySQL database.

Policies - the documented rule sets that are applied to grant or limit access. 
In order for users, groups, or roles to properly set permissions, they use policies. 
Policies are written in JSON and you can either use custom policies for your specific needs or use the default policies set by AWS.


#IAM
-------------------------------------------------
AWS IAM is also called AWS Identity and Access Management.

It helps you securely manage AWS resources and services.

IAM features are:
    AWS account root user
	IAM Users
	IAM policy
	IAM groups
	IAM roles
	Multi-factor authentication
	
	
IAM user represents an entity (person or an application) that interacts with AWS resources and services.


================================================= 
# Encryption  | Security
================================================= 

#Symmetric Key 
-------------------------------------------------
Symmetric encryption algorithms use the same key for encryption and decryption
-Key Factor 1: Choose the right encryption algorithm
-Key Factor 2: How do we secure the encryption key?
-Key Factor 3: How do we share the encryption key?


#Asymmetric Key Encryption
-------------------------------------------------

Two Keys : Public Key and Private Key Also called Public Key Cyptography.

Encrypt data with Public Key and decrypt with Private Key
Share Public Key with everybody and keep the Private Key with you(YEAH, ITS PRIVATE!)

No crazy questions:
Will somebody not figure out private key using the public key?

How do you create Asymmetric Keys



#Amazon Cognito
-------------------------------------------------

Amazon Cognito provides authentication, authorization, and user management for your web and mobile apps. 
Your users can sign in directly with a user name and password, or through a third party such as Facebook, Amazon, Google or Apple.


Amazon Cognito lets you add user sign-up, sign-in, and access control to your web and mobile apps quickly and easily. 
Amazon Cognito scales to millions of users and supports sign-in with social identity providers, such as Apple, Facebook, Google, 
and Amazon, and enterprise identity providers via SAML 2.0 and OpenID Connect.  


Amazon Cognito is designed for developers who want to add user management and sync functionality to their mobile and web apps. 
Developers can use Cognito Identity to add sign-up and sign-in to their apps and to enable their 
users to securely access their app’s resources. 
Cognito also enables developers to sync data across devices, platforms, and applications.



#STS(Security Token Service)
-------------------------------------------------
WS STS is an AWS service that allows you to request temporary security credentials for your AWS resources, 
for IAM authenticated users and users that are authenticated in AWS such as federated users via OpenID or SAML2.0.

You use STS to provide trusted users with temporary access to resources via API calls, your AWS console or 
the AWS command line interface (CLI)


AWS provides AWS Security Token Service (AWS STS) as a web service that enables you to request temporary, 
limited-privilege credentials for AWS Identity and Access Management (IAM) users or for users you authenticate (federated users).

Recording API requests:
AWS STS supports AWS CloudTrail, a service that records AWS calls for your AWS account and delivers log files to an Amazon S3 bucket. 
By using information collected by CloudTrail, you can determine the requests successfully sent to AWS STS, 
as well as who sent the request, and when it was sent.

The STS token lifecycle is determined by you and can be anywhere from 15 minutes to 36 hours.

External web identities can be authenticated by a third party online identity manager like amazon, google, 
facebook or any other open-id connect compatible service. This web identity federation also removes the need to 
distribute long-term security credentials to 
facilitate access to your AWS resources.


Use-Case:
-Identity Federation Use-Case
-Cross-Account Access using AWS STS
-EC2 Instance STS Credentials


#KMS AWS Key Management Service 
-------------------------------------------------

Amazon S3 uses AWS KMS keys to encrypt your Amazon S3 objects. AWS KMS encrypts only the object data.
 
 
KMS is a managed service that makes it easy for you to create and control the encryption keys used to encrypt your data. 
AWS KMS uses Hardware Security Modules (HSMs) to protect the security of your keys. 
You can use AWS KMS to protect your data in AWS services and in your applications

AWS Key Management Service (AWS KMS) makes it easy for you to create and manage cryptographic keys and control their use across a 
wide range of AWS services and in your applications.

AWS KMS is integrated with AWS CloudTrail to provide you with logs of all key usage to help meet your regulatory and compliance needs.

AWS KMS Key Management Service is a useful and very beneficial service while dealing with sensitive data and it 
also makes it easy for you to create and manage cryptographic keys.




AWS KMS keys must be in the same Region as the bucket.




Features of AWS KMS:
----------------------------------------------------
It is an easy way to control and access your data using managed encryption.

With AWS Key Management Service, the process of key management is reduced to a few simple clicks.

It is also integrated with other AWS services including Amazon EBS, Amazon S3, and Amazon RedShift to 
simplify the encryption of your data within these services.

AWS KMS enables you to create, rotate, disable, enable, and define usage policies for master keys and audit their usage.

It is a centralized key management

It is secure and compliant.



CloudHSM
----------------------------------------------------
AWS CloudHSM is a cloud-based hardware security module (HSM) that enables you to easily 
generate and use your own encryption keys on the AWS Cloud. 

CloudHSM is standards-compliant and enables you to export all of your keys to most other
commercially-available HSMs, subject to your configurations. 


The AWS CloudHSM service helps you meet corporate, contractual, and regulatory compliance 
requirements for data security by using dedicated 
Hardware Security Module (HSM) instances within the AWS cloud. 

AWS CloudHSM provides hardware security modules in the AWS Cloud. A hardware security module (HSM) is a computing device that 
processes cryptographic operations and provides secure storage for cryptographic keys.



When you use an HSM from AWS CloudHSM, you can perform a variety of cryptographic tasks:

Generate, store, import, export, and manage cryptographic keys, including symmetric keys and asymmetric key pairs.

Use symmetric and asymmetric algorithms to encrypt and decrypt data.

Use cryptographic hash functions to compute message digests and hash-based message authentication codes (HMACs).

Cryptographically sign data (including code signing) and verify signatures.

Generate cryptographically secure random data.



Q: What is a Hardware Security Module (HSM)?

A Hardware Security Module (HSM) provides secure key storage and cryptographic operations within a 
tamper-resistant hardware device. HSMs are designed to securely store cryptographic 
key material and use the key material without exposing it outside the cryptographic boundary of the hardware.


Q: What can I do with CloudHSM?

You can use the CloudHSM service to support a variety of use cases and applications, 
such as database encryption, Digital Rights Management (DRM), Public Key Infrastructure (PKI), 
authentication and authorization, document signing, and transaction processing.

There are no upfront costs to use AWS CloudHSM. With CloudHSM, you pay an hourly fee for each HSM you launch until you terminate the HSM.


#Transit Gateway (TGW)
-------------------------------------------------
AWS Transit Gateway connects your Amazon Virtual Private Clouds (VPCs) and on-premises networks 
through a central hub. This simplifies your network and puts an end to complex peering relationships. 
It acts as a cloud router – each new connection is only made once.

Transit Gateway is a Regional resource and can connect thousands of VPCs within the same AWS Region. 
You can create multiple Transit Gateway instances per Region, and you can 
connect to a maximum of three Transit Gateway instances over a single Direct Connect connection for hybrid connectivity.



As your cloud infrastructure expands globally you need to find out a way to connect your resources which are in different VPCs. 
A Transit Gateway is a network hub that you can use to interconnect your virtual private clouds (VPCs) and on-premises networks. 

It is like a hub and spoke design or star topology design for connecting VPCs and on-premises networks. 
Transit Gateway allows customers to connect thousands of VPCs together. It is a regional service. 
It gives you simplified connectivity to the multiple VPC as compared to a complex VPC peering connection. 

Traffic between VPC and Transit Gateway remains on the AWS global private network and is not exposed to the public internet. 
Transit Gateways in different regions can peer with each other to enable VPC communications across regions. 
Transit Gateway inter-Region peering encrypts all traffic, with no single point of 
failure or bandwidth bottleneck which helps you to get improved security.


#ECMP
-------------------------------------------------
Equal-cost multi-path routing (ECMP) is a routing strategy where packet forwarding to a single destination can occur over multiple best paths with equal routing priority. 
Multi-path routing can be used in conjunction with most routing protocols because it is a per-hop local decision made independently at each router.


AWS Transit Gateway VPN supports ECMP protocol that can load balance traffic across multiple VPN tunnels. 
The question is, can Transit Gateway ECMP be used to deploy a transit DMZ as shown in the diagram below?


#Traffic Mirroring
-------------------------------------------------

Traffic Mirroring copies inbound and outbound traffic from the network interfaces that are attached to your instances. 
You can send the mirrored traffic to the network interface of another instance, a Network Load Balancer that has a UDP listener, 
or a Gateway Load Balancer that has a UDP listener. The traffic mirror source and the traffic mirror target (monitoring appliance) 
can be in the same VPC. Or they can be in a different VPCs that are connected 
through intra-Region VPC peering, a transit gateway, or by a Gateway Load Balancer endpoint to connect to a 
Gateway Load Balancer in a different VPC.


Traffic Mirroring is an Amazon VPC feature that you can use to copy network traffic from an elastic network interface of type interface. 
You can then send the traffic to out-of-band security and monitoring appliances for:

-Content inspection
-Threat monitoring
-Troubleshooting




#Directory Service
-------------------------------------------------
Microsoft AD is a Microsoft Active Directory hosted on the AWS Cloud. It integrates most Active Directory features with AWS applications.



AWS Directory Service lets you run Microsoft Active Directory (AD) as a managed service. 
AWS Directory Service for Microsoft Active Directory, also referred to as AWS Managed Microsoft AD, 
is powered by Windows Server 2012 R2. When you select and launch this directory type, 
it is created as a highly available pair of domain controllers connected to your virtual 
private cloud (VPC). The domain controllers run in different Availability Zones in a Region of your choice. 
Host monitoring and recovery, data replication, snapshots, and software updates are automatically configured and managed for you.

With AWS Managed Microsoft AD, you can run directory-aware workloads in the AWS Cloud, 
including Microsoft SharePoint and custom .NET and SQL Server-based applications. 
You can also configure a trust relationship between AWS Managed Microsoft AD in the AWS Cloud and your existing 
on-premises Microsoft Active Directory, providing users and groups with access to resources in either domain, 
using single sign-on (SSO).

AWS Directory Service makes it easy to set up and run directories in the AWS Cloud, or connect your 
AWS resources with an existing on-premises Microsoft Active Directory. Once your directory is created, 
you can use it for a variety of tasks:

-Manage users and groups
-Provide single sign-on to applications and services
-Create and apply group policy
-Simplify the deployment and management of cloud-based Linux and Microsoft Windows workloads
-You can use AWS Managed Microsoft AD to enable multi-factor authentication by integrating with your 
 existing RADIUS-based MFA infrastructure to provide an additional layer of security when users access AWS applications.
-Securely connect to Amazon EC2 Linux and Windows instances




AWS introduced AWS Directory Service for Microsoft Active Directory (Standard Edition), 
also known as AWS Microsoft AD (Standard Edition), which is managed Microsoft Active Directory (AD) 
that is performance optimized for small and midsize businesses. AWS Microsoft AD (Standard Edition) 
offers you a highly available and cost-effective primary directory in the 
AWS Cloud that you can use to manage users, groups, and computers.


AWS Managed Microsoft AD makes it easy to migrate AD-dependent applications and Windows workloads to AWS. 
With AWS Managed Microsoft AD, you can use Group Policies to manage EC2 instances and run AD-dependent applications in the 
AWS Cloud without the need to deploy your own AD infrastructure


#AWS Workspaces
-------------------------------------------------
-Desktop-as-a-Service (DaaS)
--Provision Windows or Linux desktops in minutes
-Eliminate traditional desktop management - Virtual Desktop Infrastructure (VDI)



#AWS Shield
-------------------------------------------------

Shields from Distributed Denial of Service (DDoS) attacks
Disrupt normal traffic of a server by overwhelming it with a flood of Internet traffic
Protect
-Amazon Route 53
-Amazon CloudFront
-AWS Global Accelerator
-Amazon Elastic Compute Cloud (EC2) instances
-Elastic Load Balancers (ELB)

=================================================
#HPC ENA, EFA      
=================================================

 
#Elastic Fabric Adapter (EFA)
-------------------------------------------------
An Elastic Fabric Adapter (EFA) is a network device that you can attach to your Amazon EC2 instance to accelerate 
High Performance Computing (HPC) and machine learning applications. EFA enables you to achieve the application performance of 
an on-premises HPC cluster, with the scalability, flexibility, and elasticity provided by the AWS Cloud.

Elastic Fabric Adapter (EFA) is a network interface for Amazon EC2 instances that enables customers to run applications 
requiring high levels of inter-node communications at scale on AWS. 
Its custom-built operating system (OS) bypass hardware interface enhances the performance of inter-instance communications, 
which is critical to scaling these applications. 


You can create, use, and manage an EFA much like any other elastic network interface in Amazon EC2. 
However, unlike elastic network interfaces, EFAs cannot be attached to or detached from an instance in a running state.

Elastic Fabric Adapter (EFA) is a network interface for Amazon EC2 instances that enables customers to run HPC applications 
requiring high levels of inter-instance communications, like computational fluid dynamics, weather modeling, 
and reservoir simulation, at scale on AWS. It uses a custom-built operating system bypass technique to enhance the 
performance of inter-instance communications, which is critical to scaling HPC applications. With EFA, 
HPC applications using popular HPC technologies like Message Passing Interface (MPI) can scale to thousands of CPU cores. 
EFA supports industry-standard libfabric APIs, so applications that use a supported MPI library can be 
migrated to AWS with little or no modification.

 
#ENA (Elastic Network Adapter)
-------------------------------------------------
The Elastic Network Adapter (ENA) is designed to improve operating system health and reduce 
the chances of long-term disruption because of unexpected hardware behavior and or failures. 
The ENA architecture keeps device or driver failures as transparent to the system as possible. 
This topic provides troubleshooting information for ENA.

#With EC2
The Elastic Network Adapter (ENA) driver publishes network performance metrics from the instances where they are enabled. 
You can use these metrics to troubleshoot instance performance issues, choose the right instance size for a workload, 
plan scaling activities proactively, and benchmark applications to determine whether they maximize the
performance available on an instance.

#HPC (High Performance Computing)
-------------------------------------------------
Run your large, complex simulations and deep learning workloads in the cloud with a complete suite of 
high performance computing (HPC) products and services on AWS. Gain insights faster, and quickly
move from idea to market with virtually unlimited compute capacity, a high-performance file system,
and high-throughput networking.


#ParallelCluster 
-------------------------------------------------
AWS ParallelCluster is an open source cluster management tool that makes it easy for you to deploy and manage High Performance 
Computing (HPC) clusters on AWS. ParallelCluster uses a simple text file to model and provision all the 
resources needed for your HPC applications in an automated and secure manner. It also supports multiple 
instance types and job submission queues, and job schedulers like AWS Batch and Slurm.

AWS ParallelCluster is built on the popular open source CfnCluster project and is released via the Python Package Index (PyPI). 
ParallelCluster's source code is hosted on the Amazon Web Services repository on GitHub. 
AWS ParallelCluster is available at no additional charge, and you pay only for the AWS resources needed to run your applications.




=================================================
#CloudFormation         
=================================================

AWS CloudFormation is an AWS service that uses template files to automate the setup of AWS resources.

An AWS CloudFormation template is a formatted text file in JSON or YAML language that describes your AWS infrastructure. 
To create, view and modify templates, you can use AWS CloudFormation Designer or any text editor tool. 
An AWS CloudFormation template consists of nine main objects:

Format version: Format version defines the capability of a template.
Description: Any comments about your template can be specified in the description.
Metadata: Metadata can be used in the template to provide further information using JSON or YAML objects. 

CloudFormation is an infrastructure automation platform for AWS that deploys AWS resources in a repeatable, testable and auditable manner.
AWS CloudFormation provides users with a simple way to create and manage a collection of Amazon Web Services (AWS)
resources by provisioning and updating them in a predictable way. AWS CloudFormation enables you to manage your 
complete infrastructure or AWS resources in a text file.



You can use CloudFormation to automate the configuration of workloads that run on the most popular AWS services, 
like the EC2 compute service, the S3 storage service, and the IAM service for configuring access control.

You can also apply CloudFormation templates to AWS services that cater to niche use cases, like Ground Station, 
the AWS satellite management solution.

In general, if a service runs on AWS, it is a safe bet that you can use CloudFormation to automate its configuration and deployment.

It is worth noting that CloudFormation is not the only way to configure and deploy services on AWS. 
You can handle these processes manually using the AWS command-line interface, API, or Web console. 

AWS CloudFormation enables you to manage your complete infrastructure or AWS resources in a text file, or template. 
A collection of AWS resources is called a stack. AWS resources can be created or updated by using a stack.


HowToWork:
-Create or use an existing CloudFormation template using JSON or YAML format.
-Save the code in an S3 bucket, which serves as a repository for the code.
-Use AWS CloudFormation to call the bucket and create a stack on your template. 
-CloudFormation reads the file and understands the services that are called, their order, 
the relationship between the services, and provisions the services one after the other.



#StepFunction
-------------------------------------------------

AWS Step Functions is a serverless orchestration service that lets developers create and manage multi-step 
application workflows in the cloud. By using the service’s drag-and-drop visual editor, 
teams can easily assemble individual microservices into unified workflows. At each step of a given workflow, 
Step Functions manages input, output, error handling, and retries, so that developers can focus on 
higher-value business logic for their applications.




#EMR
-------------------------------------------------

With it, organizations can process and analyze massive amounts of data.

Amazon Elastic MapReduce (Amazon EMR) is a web service that makes it easy to quickly and cost-effectively process vast amounts of data.
Amazon EMR is the industry-leading cloud big data platform for processing vast amounts of data using 
open source tools such as Apache Spark, Apache Hive, Apache HBase, Apache Flink, Apache Hudi, and Presto. 
Amazon EMR makes it easy to set up, operate, and scale your big data environments 
by automating time-consuming tasks like provisioning capacity and tuning clusters and uses 
Hadoop, an open source framework, to distribute your data and processing across a resizable cluster of Amazon EC2 instances. 




3
#GraphQL:
------------------------------------------------
GraphQL is a query language for APIs and a runtime for fulfilling those queries with your existing data. 
GraphQL provides a complete and understandable description of the data in your API, gives clients the 
power to ask for exactly what they need and nothing more, makes it easier to evolve APIs over time, 
and enables powerful developer tools.

Send a GraphQL query to your API and get exactly what you need, nothing more and nothing less. 
GraphQL queries always return predictable results. Apps using GraphQL are fast and stable because they 
control the data they get, not the server.


#AppSync
-------------------------------------------------


The fundamental idea of a GraphQL API is that all API functionality is available via a unified query language 
(the Graph Query Language) under a single endpoint. Rather than making requests to various endpoints to get 
different parts of the data needed to build a webpage, developers can issue a single request to a 
GraphQL API and immediately get back all the data they need. This model reduces the complexity of 
web applications and improves the experience for website visitors with faster load times.

AWS AppSync is a fully managed GraphQL API layer developed by Amazon Web Services. 
AppSync allows developers to build GraphQL APIs without much of the usual work; it handles the parsing and resolution of 
requests as well as connecting to other AWS services like AWS Lambda, NoSQL and SQL data stores, and HTTP APIs to gather 
backend data for the API.

AWS AppSync is a fully managed AWS serverless service for real-time data queries, synchronization, and communications. 
In AppSync, AWS has a GraphQL-as-a-Service offering that makes it easy to build scalable and resilient GraphQL APIs in the cloud.

With AppSync, you can build scalable applications, including those requiring real-time updates, on a range of data sources 
such as NoSQL data stores, relational databases, HTTP APIs, and your custom data sources with Amazon Lambda. 
For mobile and web apps, AppSync additionally provides local data access when devices go offline, 
and data synchronization with customizable conflict resolution, when they are back online.

AWS AppSync is a serverless GraphQL and Pub/Sub API service that simplifies building modern web and mobile applications.

AWS AppSync GraphQL APIs simplify application development by providing a single endpoint to securely query or update data from multiple databases, microservices, and APIs.

AWS AppSync Pub/Sub APIs make it easy to create engaging real-time experiences by automatically publishing data updates to subscribed API clients via serverless WebSockets connections. 



=================================================
#Elastic Beanstalk        
================================================= 

AWS Elastic Beanstalk allows you to quickly deploy applications and services without having to worry about 
configuring underlying resources, services, operating systems or web servers.

Elastic Beanstalk takes care of the hosting infrastructure, coding language interpreter, operating system, 
security, https service and application layer. All you need to worry about is writing your code.

You can develop code in a number of languages which is then zipped up and the zip file is used 
when instantiating a new elastic beanstalk instance.

Elastic Beanstalk is a complete application management solution, and manages all infrastructure and platform tasks on your behalf.

When using Elastic Beanstalk as your deployment solution, simply upload your source code and 
Elastic Beanstalk will provision and operate all necessary infrastructure, including servers, databases, 
load balancers, networks, and auto scaling groups. Although these resources are created on your behalf, 
you retain full control of these resources, allowing developers to customize as needed.

AWS Elastic Beanstalk, you can quickly deploy and manage applications in the AWS Cloud without worrying about the 
infrastructure that runs those applications. AWS Elastic Beanstalk reduces management complexity 
without restricting choice or control. You simply upload your application, and AWS Elastic Beanstalk 
automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring.


Elastic Beanstalk is the fastest and simplest way to deploy your application on AWS. You simply use the AWS Management Console, 
a Git repository, or an integrated development environment (IDE) such as Eclipse or Visual Studio to upload your application, 
and Elastic Beanstalk automatically handles the deployment details of capacity 
provisioning, load balancing, auto-scaling, and application health monitoring. 
Within minutes, your application will be ready to use without any infrastructure or resource configuration work on your part.



AWS Elastic Beanstalk provides an environment that makes it easy to deploy and run applications in the cloud.
AWS Elastic Beanstalk is combined with the developer tools to help you manage the lifecycle of your applications.


=================================================
#Global Accelerator      
================================================= 

Global Accelerator is a network layer service in which you create accelerators to improve availability and performance for internet applications used by a global audience. 


With Global Accelerator, you are provided two global static public IPs that act as a fixed entry point to your application, 
improving availability. On the back end, add or remove your AWS application endpoints, such as Application Load Balancers, 
Network Load Balancers, EC2 Instances, and Elastic IPs without making user-facing changes. 
Global Accelerator automatically re-routes your traffic to your nearest healthy available endpoint to mitigate endpoint failure.


It provides static IP addresses that act as a fixed entry point to application endpoints in a 
single or multiple AWS Regions, such as Application Load Balancers, Network Load Balancers or EC2 instances.



Uses the AWS global network to optimize the path from users to applications, improving the performance of TCP and UDP traffic.

AWS Global Accelerator continually monitors the health of application endpoints and will detect an unhealthy endpoint and redirect 
traffic to healthy endpoints in less than 1 minute.


#RTO and RPO
-------------------------------------------------
Among the components of a DR plan are two key parameters that define how long your business can afford to be offline and how
much data loss it can tolerate. These are the Recovery Time Objective (RTO) and Recovery Point Objective (RPO).


RTO:
RTO is the goal your organization sets for the maximum length of time it should take to restore normal 
operations following an outage or data loss.

RTO refers to how much time an application can be down without causing significant damage to the business. 
Some applications can be down for days without significant consequences. 

RTO is not simply the duration of time between loss and recovery. The objective also accounts for the steps 
IT must take to restore the application and its data.



RPO:
RPO is your goal for the maximum amount of data the organization can tolerate losing. 
This parameter is measured in time: from the moment a failure occurs to your last valid data backup. 
For example, if you experience a failure now and your last full data backup was 24 hours ago, the RPO is 24 hours. 


Recovery point objectives refer to your company’s loss tolerance: the amount of data that can be lost before significant 
harm to the business occurs. 

If you back up all or most of your data in regularly scheduled 24-hour increments, then in the worst-case scenario you 
will lose 24 hours’ worth of data.

For example, if you have a 4-hour RPO for an application then you will have a maximum 4-hour gap between backup and data loss.
Having a 4-hour RPO does not necessarily mean you will lose 4 hours’ worth of data.


=================================================
#AWS Organizations  
================================================= 
Organizations typically have multiple AWS accounts
Different business units
Different environments

How do you centralize your management (billing, access control, compliance and security) across multiple AWS accounts?
Welcome AWS Organizations!
Organize accounts into Organizational Units (OU)
Provides API to automate creation of new accounts


Features:
-One consolidated bill for all AWS accounts
-Centralized compliance management for AWS Config Rules
-Send AWS CloudTrail data to one S3 bucket (across accounts)
-AWS Firewall Manager to manage firewall rules (across accounts)
-AWS WAF, AWS Shield Advanced protections and Security Groups
-Use Service control policies (SCPs) to define restrictions for actions (across accounts):
-Prevent users from disabling AWS Config or changing its rules
-Require Amazon EC2 instances to use a specific type
-Require MFA to stop an Amazon EC2 instance
-Require a tag upon resource creation



=================================================
#AWS Trusted Advisor
================================================= 
Recommendations for cost optimization, performance, security and fault tolerance
-Red - Action recommended Yellow - investigate and Green - Good to go
-All AWS customers get 4 checks for free:
-Service limits (usage > 80%)
-Security groups having unrestricted access (0.0.0.0/0)
-Proper use of IAM
-MFA on Root Account
-Business or Enterprise AWS support plan provides over 50 checks
-Disable those you are not interested in
-How much will you save by using Reserved Instances?
-How does your resource utilization look like? Are you right sized?


AWS Directory Service
=================================================
#AWS Well-Architected  | Architected Framework      
================================================= 

The AWS Well-Architected Framework is based on five pillars:
-operational excellence, 
-security, 
-reliability, 
-performance efficiency
-cost optimization


DevOps 



=================================================
#DevOps | CI  |CD | cicd | DevOps - CI, CD Tools       
================================================= 

Tools:
-CodeCommit - Private source control (Git)
-CodePipeline - Orchestrate CI/CD pipelines
-CodeBuild - Build and Test Code (application packages and containers)
-CodeDeploy - Automate Deployment (EC2, ECS, Elastic Beanstalk, EKS, Lambda etc)






=================================================
#Managed Services8 | IAAS| PAAS       
================================================= 

AWS Managed Service Offerings:

-Elastic Load Balancing - Distribute incoming traffic across multiple targets
-AWS Elastic Beanstalk - Run and Manage Web Apps
-Amazon Elastic Container Service (ECS) - Containers orchestration on AWS
-AWS Fargate - Serverless compute for containers
-Amazon Elastic Kubernetes Service (EKS) - Run Kubernetes on AWS
-Amazon RDS - Relational Databases - MySQL, Oracle, SQL Server etc



IAAS:

IAAS (Infrastructure as a Service):
IAAS (Infrastructure as a Service) is all about using only infrastructure from cloud provider. It is also called “Lift and Shift”. Example: Using EC2 to deploy your applications or databases

With IAAS, you are responsible for:

Application Code and Runtime
Configuring load balancing
Auto scaling
OS upgrades and patches
Availability






PAAS:

PAAS (Platform as a Service)
PAAS (Platform as a Service) is all about using a platform provided by cloud

Cloud provider is responsible for:

OS (incl. upgrades and patches)
Application Runtime
Auto scaling, Availability & Load balancing etc..
You are responsible for:

Application code
Configuration

Examples of PAAS
CAAS (Container as a Service): Containers instead of Applications
FAAS (Function as a Service) or Serverless: Functions instead of Applications



=================================================
#Shared Responsibility Model         
================================================= 
Security & Compliance is shared responsibility between AWS and customer



Amazon EC2 instances is Infrastructure as a Service (IaaS).
You are responsible for:
Guest OS (incl. security patches)
Application software installed
Configuring Security Groups (or firewalls)


AWS is responsible for infrastructure layer only.


Amazon S3 & DynamoDB are managed services.

AWS manages infrastructure layer, OS, and platform.

You are responsible for
-Managing your data
-Managing security of data at rest(encryption)
-Managing security of data in transit
-Mandating SSL/HTTPS
-Using the right network - AWS global network or dedicated private network when possible
-Managing access to the service
-Configure right permissions (IAM users/roles/user policies/resource policies)
(FOR AWS RDS) Managing in database users
-Configuring the right security groups (control inbound and outbound traffic)
-Disabling external access (public vs private)


=================================================
# API Gateway       
================================================= 
How about a fully managed service with auto scaling that can act as a “front door” to your APIs? Welcome “Amazon API Gateway”

Amazon API Gateway helps you to “publish, maintain, monitor, and secure APIs at any scale”

You can authorize users by integrating with:

AWS IAM (for AWS users using signature version 4)
Amazon Cognito
Lambda authorizer (custom authorization with JWT tokens or SAML)

Features:
-Integrates with AWS Lambda, Amazon EC2, Amazon ECS or any web application
-Supports HTTP(S) and WebSockets (two way communication - chat apps and streaming dashboards)
-Serverless. Pay for use (API calls and connection duration)
-Provides API Lifecycle Management for RESTful APIs and WebSocket APIs
-You can Run multiple versions of the same API
-Supports Rate Limits(request quota limits), throttling and fine-grained access permissions using API Keys for Third-Party Developers
-Lifecycle management for REST APIs
-Versioning and multiple environments
-API keys - Generate API keys to monitor usage
-Implement plans and quota limits for external applications (or developer)
-WARNING - Do NOT use API keys for Authorization
-Enable caching for API calls with TTL
-Protect backends by throttling requests
-Integrates with
-Amazon CloudWatch - Performance metrics, API calls, latency data and error rates
-Amazon CloudWatch Logs - Debug logging
-WS CloudTrail - Complete history of changes to your REST API




=================================================
#Amazon Elastic Container Service (ECS)      
================================================= 

Amazon Elastic Container Service (Amazon ECS) is a fully managed service for container orchestration.


Amazon ECR is a Fully-managed Docker container registry provided by AWS. Its an alternative to Docker Hub.



AWS Fargate is a serverless option.

-Service: Allows you to run and maintain a specified number (the “desired count”) of tasks
-ECS cluster: Grouping of one or more container instances (EC2 instances) where you run your tasks
-Container Instance - EC2 instance in the cluster running a container agent (helps it communicate with the cluster)
-AWS provides ECS ready AMIs with container agents pre-installed.


Remember:
-AWS Fargate does NOT give you visibility into the EC2 instances in the cluster.
-You can use On-Demand instances or Spot instances to create your cluster.
-You can load balance using Application Load Balancers
-Two features of ALB are important for ECS:
-Dynamic host port mapping: Multiple tasks from the same service are allowed per EC2 (container) instance
-Path-based routing: Multiple services can use the same listener port on same ALB and be routed based on path (www.app.com/microservice-a and www.app.com/microservice-b)


Elastic Beanstalk:
Single container or multiple containers in same EC2 instance
Recommended for simple web applications

Amazon ECS:
AWS specific solution for container orchestration
Ideal for microservices


Amazon Fargate:
Serverless version of Amazon ECS
You want to run microservices and you don’t want to manage the cluster


Amazon EKS:
AWS managed service for Kubernetes
Recommended if you are already using Kubernetes and would want to move the workload to AWS



=================================================
#ML | machine-learning
================================================= 


#SageMaker
-------------------------------------------------
Amazon SageMaker is a cloud-based machine-learning platform that helps users create, design, train, tune, and deploy machine-learning models in a production-ready hosted environment. The AWS SageMaker comes with a pool of advantages (know all about it in the next section)






=================================================
#Exam content         
================================================= 



#Exam Content Outline
-------------------------------------------------

The exam contains five main sections, which are:

-Design Resilient Architectures (34%)
-Define Performant Architecture (24%)
-Specify Secure Applications and Architectures (24%)
-Design Cost-Optimized Architectures (10%)
-Define Operationally Excellent Architectures (6%)Now


Design Resilient Architecture:
-How to choose reliable and resilient storage using services like AWS S3, AWS Glacier, and AWS EBS
-How to design decoupling mechanisms using AWS services like AWS SNS
-How to create a multi-tier architecture
-How to architect for high availability and fault-tolerance


Define Performant Architectures:
How to choose performant storage and databases using AWS RDS, AWS Redshift, and AWS DynamoDB
How to improve performance using AWS Elasticache
How to design elastic and scalable solutions through AWS Lambda, AWS CloudWatch, and AWS Data Pipeline



Specify Secure Applications and Architectures:
How to secure applications using AWS Inspector, AWS CloudTrail, and AWS IAM
How to secure data using AWS CloudHSM and AWS Macie
How to define the network infrastructure with AWS CloudFront, AWS VPC, and Elastic Load Balancer


Design Cost-Optimized Architectures:
How to design cost-optimized compute solutions using AWS EC2, AWS Elastic Beanstalk, AWS Lambda, and Aws Lightsail
Design cost-effective storage solutions using AWS S3, AWS Glacier, AWS EBS, and AWS Elastic File System


Define Operationally Excellent Architectures:
Perform operations as code
Annotate documentation
Make frequent, small, and reversible changes
Anticipate and tackle failures




#Question
-------------------------------------------------

1) How do you choose aws region ?

2) How do you upgrade or downgrade a system with near-zero downtime?

You can upgrade or downgrade a system with near-zero downtime using the following steps of migration:
-Open EC2 console
-Choose Operating System AMI
-Launch an instance with the new instance type
-Install all the updates
-Install applications
-Test the instance to see if it’s working
-If working, deploy the new instance and replace the older instance
-Once it’s deployed, you can upgrade or downgrade the system with near-zero downtime.


3) Is there any other alternative tool to log into the cloud environment other than console?

The that can help you log into the AWS resources are:
Putty
AWS CLI for Linux
AWS CLI for Windows
AWS CLI for Windows CMD
AWS SDK
Eclipse


4) What services can be used to create a centralized logging solution?

The essential services that you can use are Amazon CloudWatch Logs, store them in Amazon S3, and then use 
Amazon Elastic Search to visualize them. You can use Amazon Kinesis Firehose to move the data from Amazon S3 to Amazon ElasticSearch.




5) Name some of the AWS services that are not region-specific ?

AWS services that are not region-specific are:
IAM
Route 53
Web Application Firewall 
CloudFront



6) What is CloudWatch?
The Amazon CloudWatch has the following features:

Depending on multiple metrics, it participates in triggering alarms.
Helps in monitoring the AWS environments like CPU utilization, EC2, Amazon RDS instances, Amazon SQS, S3, Load Balancer, SNS, etc.




7) What is an Elastic Transcoder?

To support multiple devices with various resolutions like laptops, tablets, and smartphones, we need to change the resolution and 
format of the video. This can be done easily by an AWS Service tool called the Elastic Transcoder, 
which is a media transcoding in the cloud that exactly lets us do the needful. It is easy to use, cost-effective, 
and highly scalable for businesses and developers.


8) How do you set up SSH agent forwarding so that you do not have to copy the key every time you log in?

Here’s how you accomplish this:
Go to your PuTTY Configuration
Go to the category SSH -> Auth
Enable SSH agent forwarding to your instance



9) How do you configure CloudWatch to recover an EC2 instance?

Here’s how you can configure them:
Create an Alarm using Amazon CloudWatch
In the Alarm, go to Define Alarm -> Actions tab
Choose Recover this instance option



10) What are the common types of AMI designs?

There are many types of AMIs, but some of the common AMIs are:
Fully Baked AMI
Just Enough Baked AMI (JeOS AMI)
Hybrid AMI




11) What are Key-Pairs in AWS?

The Key-Pairs are password-protected login credentials for the Virtual Machines that are used to prove our 
identity while connecting the Amazon EC2 instances. The Key-Pairs are made up of a Private Key and a 
Public Key which lets us connect to the instances.



12) How can you recover/login to an EC2 instance for which you have lost the key?

Follow the steps provided below to recover an EC2 instance if you have lost the key:

Verify that the EC2Config service is running
Detach the root volume for the instance
Attach the volume to a temporary instance
Modify the configuration file
Restart the original instance






13) How do you monitor Amazon VPC?

You can monitor VPC by using:
-CloudWatch and CloudWatch logs
-VPC Flow Logs



14) When Would You Prefer Provisioned IOPS over Standard Rds Storage?

You would use Provisioned IOPS when you have batch-oriented workloads. 
]Provisioned IOPS delivers high IO rates, but it is also expensive. 
However, batch processing workloads do not require manual intervention. 



15) How Do Amazon Rds, Dynamodb, and Redshift Differ from Each Other?

Amazon RDS is a database management service for relational databases. It manages patching, upgrading, and data backups automatically. 
It’s a database management service for structured data only. On the other hand, DynamoDB is a NoSQL 
database service for dealing with unstructured data. Redshift is a data warehouse product used in data analysis.



16) What are the factors to consider while migrating to Amazon Web Services?

Here are the factors to consider during AWS migration:

Operational Costs - These include the cost of infrastructure, ability to match demand and supply, transparency, and others.
Workforce Productivity 
Cost avoidance
Operational resilience
Business agility






17) What is RTO and RPO in AWS?

RTO or Recovery Time Objective is the maximum time your business or organization is willing to wait for a recovery to 
complete in the wake of an outage. On the other hand, RPO or Recovery Point Objective is the 
maximum amount of data loss your company is willing to accept as measured in time.





18) What are the advantages of AWS IAM?

AWS IAM allows an administrator to provide multiple users and groups with granular access. Various user groups and users may require 
varying levels of access to the various resources that have been developed. 
We may assign roles to users and create roles with defined access levels using IAM.

It further gives us Federated Access, which allows us to grant applications and users access to 
resources without having to create IAM Roles.


19) Explain Connection Draining ?

Connection Draining is an AWS service that allows us to serve current requests on the servers that are either being 
decommissioned or updated.

By enabling this Connection Draining, we let the Load Balancer make an outgoing instance finish its 
existing requests for a set length of time before sending it any new requests. A departing instance will 
immediately go off if Connection Draining is not enabled, and all pending requests will fail.


20) What is Power User Access in AWS?

The AWS Resources owner is identical to an Administrator User. ,
The Administrator User can build, change, delete, and inspect resources, as well as grant permissions to other AWS users.

Administrator Access without the ability to control users and permissions is provided to a Power User. 
A Power User Access user cannot provide permissions to other users but has the ability to modify, remove, view, and create resources.



Amazon EC2 Auto Scaling FAQs:
==============================

General:
========

21) How is AWS CloudFormation different from AWS Elastic Beanstalk?

Here are some differences between AWS CloudFormation and AWS Elastic Beanstalk:

AWS CloudFormation helps you provision and describe all of the infrastructure resources that are present in your cloud environment. 
On the other hand, AWS Elastic Beanstalk provides an environment that makes it easy to deploy and run applications in the cloud.
AWS CloudFormation supports the infrastructure needs of various types of applications, like legacy applications and 
existing enterprise applications. On the other hand, AWS Elastic Beanstalk is combined with the developer tools to
help you manage the lifecycle of your applications.



22) What are the elements of an AWS CloudFormation template?

AWS CloudFormation templates are YAML or JSON formatted text files that are comprised of five essential elements, they are:

-Template parameters
-Output values
-Data tables
-Resources
-File format versi


23) What happens when one of the resources in a stack cannot be created successfully?

If the resource in the stack cannot be created, then the CloudFormation automatically rolls back and terminates all the 
resources that were created in the CloudFormation template. This is a handy feature 
when you accidentally exceed your limit of Elastic IP addresses or don’t have access to an EC2 AMI.



23) What Is Identity and Access Management (IAM) and How Is It Used?

Identity and Access Management (IAM) is a web service for securely controlling access to AWS services. 
IAM lets you manage users, security credentials such as access keys, and permissions that control which 
AWS resources users and applications can access.





24) What are the policies that you can set for your users’ passwords?

Here are some of the policies that you can set:

You can set a minimum length of the password, or you can ask the users to add at least one number or special characters in it.
You can assign requirements of particular character types, including uppercase letters, 
lowercase letters, numbers, and non-alphanumeric characters.
You can enforce automatic password expiration, prevent reuse of old passwords, and request for a password 
reset upon their next AWS sign in.
You can have the AWS users contact an account administrator when the user has allowed the password to expire. 



25) Can AWS Config aggregate data across different AWS accounts?

Yes, you can set up AWS Config to deliver configuration updates from different accounts to one S3 bucket, 
once the appropriate IAM policies are applied to the S3 bucket.



26) What happens to my Amazon EC2 instances if I delete my ASG?

If you have an EC2 Auto Scaling group (ASG) with running instances and you choose to delete the ASG, the instances will be 
terminated and the ASG will be deleted.


27) How do I know when EC2 Auto Scaling is launching or terminating the EC2 instances in an EC2 Auto Scaling group?

When you use Amazon EC2 Auto Scaling to scale your applications automatically, 
it is useful to know when EC2 Auto Scaling is launching or terminating the EC2 instances in your EC2 Auto Scaling group. 
Amazon SNS coordinates and manages the delivery or sending of notifications to subscribing clients or endpoints. 



28) What is a launch configuration?

A launch configuration is a template that an EC2 Auto Scaling group uses to launch EC2 instances.
You can specify your launch configuration with multiple EC2 Auto Scaling groups. However, you can only specify one launch configuration 
for an EC2 Auto Scaling group at a time, and you can't modify a launch configuration after you've created it. 
Therefore, if you want to change the launch configuration for your EC2 Auto Scaling group, 
you must create a launch configuration and then update your EC2 Auto Scaling group with the new launch configuration. 
When you change the launch configuration for your EC2 Auto Scaling group, any new instances are launched using the new 
configuration parameters, but existing instances are not affected. You can see the launch configurations section of the 
EC2 Auto Scaling User Guide for more details.



29) What happens if a scaling activity causes me to reach my Amazon EC2 limit of instances?

Amazon EC2 Auto Scaling cannot scale past the Amazon EC2 limit of instances that you can run. If you need more Amazon EC2 instances, 
complete the Amazon EC2 instance request form.


30) Can EC2 Auto Scaling groups span multiple AWS regions?

EC2 Auto Scaling groups are regional constructs. They can span Availability Zones, but not AWS regions.



31) How can I implement changes across multiple instances in an EC2 Auto Scaling group?

You can use AWS CodeDeploy or CloudFormation to orchestrate code changes to multiple instances in your EC2 Auto Scaling group.




32) If I have data installed in an EC2 Auto Scaling group, and a new instance is dynamically created later, is the data copied over to the new instances?

Data is not automatically copied from existing instances to new instances. You can use lifecycle hooks to copy the data, or an Amazon RDS database including replicas.




33) When I create an EC2 Auto Scaling group from an existing instance, does it create a new AMI (Amazon Machine Image)?
When you create an Auto Scaling group from an existing instance, it does not create a new AMI. 



34) What are lifecycle hooks?
Lifecycle hooks let you take action before an instance goes into service or before it gets terminated. 
This can be especially useful if you are not baking your software environment into an Amazon Machine Image (AMI). 


35) Which health check type should I select?
If you are using Elastic Load Balancing (ELB) with your group, you should select an ELB health check. 
If you’re not using ELB with your group, you should select the EC2 health check.


36) How do I control which instances Amazon EC2 Auto Scaling terminates when scaling in, and how do I protect data on an instance?
You can configure this through the use of a termination policy. You can also use instance protection to prevent 
Amazon EC2 Auto Scaling from selecting specific instances for termination when scaling in. If you have data on an instance, 
and you need that data to be persistent even if your instance is scaled in, then you can use a service like S3, RDS, or DynamoDB, 
to make sure that it is stored off the instance.


37) How long is the turn-around time for Amazon EC2 Auto Scaling to spin up a new instance at inService state after 
detecting an unhealthy server?

The turnaround time is within minutes. The majority of replacements happen within less than 5 minutes, and on average 
it is significantly less than 5 minutes. It depends on a variety of factors, including how long it takes to boot up the AMI 
of your instance.


38) If Elastic Load Balancing (ELB) determines that an instance is unhealthy, and moved offline, 
will the previous requests sent to the failed instance be queued and rerouted to other instances within the group?

When ELB notices that the instance is unhealthy, it will stop routing requests to it. 
However, prior to discovering that the instance is unhealthy, some requests to that instance will fail.


39) If you don’t use Elastic Load Balancing (ELB) how would users be directed to the other servers in a group if there was a failure?
You can integrate with Route53 (which Amazon EC2 Auto Scaling does not currently support out of the box, but many customers use).
You can also use your own reverse proxy, or for internal microservices, can use service discovery solutions.



Security:
=========

40) Are CloudWatch agents automatically installed on EC2 instances when you create an Amazon EC2 Auto Scaling group?
If your AMI contains a CloudWatch agent, it’s automatically installed on EC2 instances when you create an EC2 Auto Scaling group. With the stock Amazon Linux AMI, you need to install it (recommended, via yum).


Cost Optimization:
=================

41) Can I create a single ASG to scale instances across different purchase options?
Yes. You can provision and automatically scale EC2 capacity across different EC2 instance types, Availability Zones, and On-Demand, RIs and Spot purchase options in a single Auto Scaling Group. 


42)Can I use ASGs to launch and manage just Spot Instances or just On-Demand instances and RIs?
Yes. You can configure your ASG specifying all capacity to be only Spot instances or all capacity to be only On-Demand instances and RIs.


43) Can I specify instances of different sizes (CPU cores, memory) in my Auto Scaling group?
Yes. You can specify any instance type available in a region. Additionally, you can specify an optional weight for each instance type, which defines the capacity units that each instance would contribute to your application’s performance.


44) What are the costs for using Amazon EC2 Auto Scaling?
Amazon EC2 Auto Scaling fleet managment for EC2 instances carries no additional fees. The dynamic scaling capabilities of 
Amazon EC2 Auto Scaling are enabled by Amazon CloudWatch and also carry no additional fees. 
Amazon EC2 and Amazon CloudWatch service fees apply and are billed separately.



Amazon Elastic Container Registry FAQs:
========================================

1) What is Amazon Elastic Container Registry (Amazon ECR)?
Amazon ECR is a fully managed container registry that makes it easy for developers to share and deploy container images and artifacts. Amazon ECR is integrated with Amazon Elastic Container Service (Amazon ECS),  Amazon Elastic Kubernetes Service (Amazon EKS), and AWS Lambda, simplifying your development to production workflow.


2) Is Amazon ECR a global service?
Amazon ECR is a Regional service and is designed to give you flexibility in how images are deployed. You have the ability to push/pull images to the same AWS Region where your Docker cluster runs for the best performance. 


3) What is the difference between Amazon ECR public and private repositories?
A private repository does not offer content search capabilities and requires Amazon IAM-based authentication using 
AWS account credentials before allowing images to be pulled. A public repository has descriptive content and allows 
anyone anywhere to pull images without needing an AWS account or using IAM credentials. Public repository 
images are also available in the Amazon ECR public gallery.


4) What compliance capabilities can I enable on Amazon ECR?
You can use AWS CloudTrail on Amazon ECR to provide a history of all API actions such as who pulled an image and 
when tags were moved between images. Administrators can also find which EC2 instances pulled which images.


5) Can I access Amazon ECR inside a VPC?
Yes. You can set up AWS PrivateLink endpoints to allow your instances to pull images from your private repositories without traversing through the public internet.


6)Does Amazon ECR work with AWS Elastic Beanstalk?
Yes. AWS Elastic Beanstalk supports Amazon ECR for both single and multi-container Docker environments, allowing you to easily 
deploy container images stored in Amazon ECR with AWS Elastic Beanstalk. All you need to do is specify the 
Amazon ECR repository in your Dockerrun.aws.json configuration and attach the 
AmazonEC2ContainerRegistryReadOnly policy to your container instance role.


7) Does Amazon ECR scan container images for vulnerabilities?
You can enable Amazon ECR to automatically scan your container images for a broad range of operating system vulnerabilities. 
You can also scan images using an API command, and Amazon ECR will notify you over API and in the console when a scan completes. 
For enhanced image scanning, you can turn on Amazon Inspector.





Amazon EC2 Container Service FAQ:
================================

1) What is Amazon Elastic Container Service?
Amazon Elastic Container Service (ECS) is a highly scalable, high performance container management service that supports 
Docker containers and allows you to easily run applications on a managed cluster of Amazon Elastic Compute Cloud (Amazon EC2) instances.

Amazon ECS makes it easy to use containers as a building block for your applications by eliminating the need for you to install, 
operate, and scale your own cluster management infrastructure. Amazon ECS lets you schedule long-running 
applications, services, and batch processes using Docker containers.



2) What is the pricing for Amazon ECS?
There is no additional charge for Amazon ECS. You pay for AWS resources (e.g. Amazon EC2 instances or EBS volumes) 
you create to store and run your application. You only pay for what you use, as you use it; there are no 
minimum fees and no upfront commitments.


3) How is Amazon ECS different from AWS Elastic Beanstalk?
AWS Elastic Beanstalk is an application management platform that helps customers easily deploy and scale web applications and services. 
It keeps the building block provisioning (e.g., EC2, Amazon RDS, Elastic Load Balancing, AWS Auto Scaling, and Amazon CloudWatch), 
application deployment, and health monitoring abstracted from the user so they can focus on writing code. You simply specify which 
container images to  deploy, the CPU and memory requirements, the port mappings, and the container links.

Elastic Beanstalk will automatically handle all the details such as provisioning an Amazon ECS cluster, balancing load, auto-scaling, 
monitoring, and container placement across your cluster. Elastic Beanstalk is ideal if you want to leverage the benefits of containers 
withthe simplicity of deploying applications from development to production by uploading a container image. You can work with 
Amazon ECS directly if you want more fine-grained control for custom application architectures.


4) Does Amazon ECS support any other container types?
No. Docker is the only container platform supported by Amazon ECS at this time.








#Domain % of Exam
-------------------------------------------------
Domain 1: Design Resilient Architectures 30%
Domain 2: Design High-Performing Architectures 28%
Domain 3: Design Secure Applications and Architectures 24%
Domain 4: Design Cost-Optimized Architectures 18%
TOTAL 100%


Domain 1: Design Resilient Architectures

	1.1 Design a multi-tier architecture solution
		 Determine a solution design based on access patterns.
		 Determine a scaling strategy for components used in a design.
		 Select an appropriate database based on requirements.
		 Select an appropriate compute and storage service based on requirements.
	1.2 Design highly available and/or fault-tolerant architectures
		 Determine the amount of resources needed to provide a fault-tolerant architecture across
		Availability Zones.
		 Select a highly available configuration to mitigate single points of failure.
		 Apply AWS services to improve the reliability of legacy applications when application changes
		are not possible.
		 Select an appropriate disaster recovery strategy to meet business requirements.
		 Identify key performance indicators to ensure the high availability of the solution.
	1.3 Design decoupling mechanisms using AWS services
		 Determine which AWS services can be leveraged to achieve loose coupling of components.
		 Determine when to leverage serverless technologies to enable decoupling.
	1.4 Choose appropriate resilient storage
		 Define a strategy to ensure the durability of data.
		 Identify how data service consistency will affect the operation of the application.
		 Select data services that will meet the access requirements of the application.
		 Identify storage services that can be used with hybrid or non-cloud-native applications.


Domain 2: Design High-Performing Architectures

	2.1 Identify elastic and scalable compute solutions for a workload
		 Select the appropriate instance(s) based on compute, storage, and networking requirements.
		 Choose the appropriate architecture and services that scale to meet performance
		requirements.
		 Identify metrics to monitor the performance of the solution. 
	2.2 Select high-performing and scalable storage solutions for a workload
		 Select a storage service and configuration that meets performance demands.
		 Determine storage services that can scale to accommodate future needs.
	2.3 Select high-performing networking solutions for a workload
		 Select appropriate AWS connectivity options to meet performance demands.
		 Select appropriate features to optimize connectivity to AWS public services.
		 Determine an edge caching strategy to provide performance benefits.
		 Select appropriate data transfer service for migration and/or ingestion.
	2.4 Choose high-performing database solutions for a workload
		 Select an appropriate database scaling strategy.
		 Determine when database caching is required for performance improvement.
		 Choose a suitable database service to meet performance needs.
		
		
Domain 3: Design Secure Applications and Architectures

	3.1 Design secure access to AWS resources
		 Determine when to choose between users, groups, and roles.
		 Interpret the net effect of a given access policy.
		 Select appropriate techniques to secure a root account.
		 Determine ways to secure credentials using features of AWS IAM.
		 Determine the secure method for an application to access AWS APIs.
		 Select appropriate services to create traceability for access to AWS resources.
	3.2 Design secure application tiers
		 Given traffic control requirements, determine when and how to use security groups and
		network ACLs.
		 Determine a network segmentation strategy using public and private subnets.
		 Select the appropriate routing mechanism to securely access AWS service endpoints or
		internet-based resources from Amazon VPC.
		 Select appropriate AWS services to protect applications from external threats.
	3.3 Select appropriate data security options
		 Determine the policies that need to be applied to objects based on access patterns.
		 Select appropriate encryption options for data at rest and in transit for AWS services.
		 Select appropriate key management options based on requirements.
		
		
Domain 4: Design Cost-Optimized Architectures

	4.1 Identify cost-effective storage solutions
		 Determine the most cost-effective data storage options based on requirements.
		 Apply automated processes to ensure that data over time is stored on storage tiers that
		minimize costs.
	4.2 Identify cost-effective compute and database services
		 Determine the most cost-effective Amazon EC2 billing options for each aspect of the
		workload.
		 Determine the most cost-effective database options based on requirements.
		 Select appropriate scaling strategies from a cost perspective.
		 Select and size compute resources that are optimally suited for the workload.
		 Determine options to minimize total cost of ownership (TCO) through managed services and
		serverless architectures.
	4.3 Design cost-optimized network architectures
		 Identify when content delivery can be used to reduce costs.
		 Determine strategies to reduce data transfer costs within AWS.
		 Determine the most cost-effective connectivity options between AWS and on-premises
		environments.
		
	
	
#Which key tools, technologies, and concepts might be covered on the exam?
-------------------------------------------------
	 Compute
	 Cost management
	 Database
	 Disaster recovery
	 High availability
	 Management and governance
	 Microservices and component decoupling
	 Migration and data transfer
	 Networking, connectivity, and content delivery
	 Security
	 Serverless design principles
	 Storage


#AWS services and features
-------------------------------------------------
Analytics:
	 Amazon Athena
	 Amazon Elasticsearch Service (Amazon ES)
	 Amazon EMR
	 AWS Glue
	 Amazon Kinesis
	 Amazon QuickSight


AWS Billing and Cost Management:
	 AWS Budgets
	 Cost Explorer
	
Application Integration:
	 Amazon Simple Notification Service (Amazon SNS)
	 Amazon Simple Queue Service (Amazon SQS)
	
Compute:
	 Amazon EC2
	 AWS Elastic Beanstalk
	 Amazon Elastic Container Service (Amazon ECS)
	 Amazon Elastic Kubernetes Service (Amazon EKS)
	 Elastic Load Balancing
	 AWS Fargate
	 AWS Lambda
	
Database:
	 Amazon Aurora
	 Amazon DynamoDB
	 Amazon ElastiCache
	 Amazon RDS
	 Amazon Redshift
	
Management and Governance:
	 AWS Auto Scaling
	 AWS Backup
	 AWS CloudFormation
	 AWS CloudTrail
	 Amazon CloudWatch
	 AWS Config
	 Amazon EventBridge (Amazon CloudWatch Events)
	 AWS Organizations
	 AWS Resource Access Manager
	 AWS Systems Manager
	 AWS Trusted Advisor
	
Migration and Transfer:
	 AWS Database Migration Service (AWS DMS)
	 AWS DataSync
	 AWS Migration Hub
	 AWS Server Migration Service (AWS SMS)
	 AWS Snowball
	 AWS Transfer Family
	
Networking and Content Delivery:
	 Amazon API Gateway
	 Amazon CloudFront
	 AWS Direct Connect
	 AWS Global Accelerator
	 Amazon Route 53
	 AWS Transit Gateway
	 Amazon VPC (and associated features)
	
Security, Identity, and Compliance:
	 AWS Certificate Manager (ACM)
	 AWS Directory Service
	 Amazon GuardDuty
	 AWS Identity and Access Management (IAM)
	 Amazon Inspector
	 AWS Key Management Service (AWS KMS)
	 Amazon Macie
	 AWS Secrets Manager
	 AWS Shield
	 AWS Single Sign-On
	 AWS WAF
	
Storage:
	 Amazon Elastic Block Store (Amazon EBS)
	 Amazon Elastic File System (Amazon EFS)
	 Amazon FSx
	 Amazon S3
	 Amazon S3 Glacier
	 AWS Storage Gateway
	 
 

 

 
 





  