#################################################
#                 AWS                          #
#################################################


Tags=>This helps to identify instances more quickly. 

Stopping => Temporarily shutting down the system
Terminating => Returning control to Amazon


Private key: The user downloads the private key
Public key: AWS uses the public key to confirm the identity of the user. 

KeyWord:
Latency:বিলম্ব, দেরী,  বাধা, 
Availability:প্রাপ্যতা, লভ্যতা
Provision:বন্দোবস্ত,ব্যবস্থা
Consistent:সামঞ্জস্যপূর্ণ,সমান
Retained:ধরে রাখা

=>sudo chmod 777 -R destinationFolder/*
Allow ec2 file permission

=================================================
#To Do           
================================================= 


Transition of S3 objects


HandOnRemain:
============
InstanceStorage
-------------------------------------------------
1) EFS serive


ELB
---------
1)ALB Sticky Session
2)NLB
3)CrossZone
4)ConnectionDraining
5)AuthScalingPolicy


RDS | Aurora| ElastiCache
-------------------------
Need to Review this section
1)Complate cource hanson remain

Route53
-----------
Need to review

S3/IAM/SDK/AdvanceS3
Need to handson



=================================================
#Billing | Const 
=================================================

EC2:
===
EC2 Instances Purchasing Options:
• On-Demand Instances – short workload, predictable pricing, pay by second
• Reserved (1 & 3 years)
• Reserved Instances – long workloads
• Convertible Reserved Instances – long workloads with flexible instances
• Savings Plans (1 & 3 years) –commitment to an amount of usage, long workload
• Spot Instances – short workloads, cheap, can lose instances (less reliable)
• Dedicated Hosts – book an entire physical server, control instance placement
• Dedicated Instances – no other customers will share your hardware
• Capacity Reservations – reserve capacity in a specific AZ for any duration
1)Elastic IP Addresses cost money when not use it !
2)ENI not cost any Money.
3)There is no charge applicable to Security Groups in Amazon EC2 / Amazon VPC.

RDS:
====
1)In AWS there’s a network cost when data goes from one AZ to another
2)For RDS Read Replicas within the same region, you don’t pay that fee, for diffenent region have Network cost.

EFS:
===
1)Only pay for uses size only (No free tier)

IAM:
===
1) Bit cost for storage of snapshots of the IAM size.(NO free tier)
2)AWS Identity and Access Management (IAM) and AWS Security Token Service (AWS STS) are features of your 
3)AWS account offered at no additional charge. 
4)You are charged only when you access other AWS services using your IAM users or AWS STS temporary security credentials.

ELB:
====
1)There is no additional charge for AWS Auto Scaling. 
You pay only for the AWS resources needed to run your applications and Amazon CloudWatch monitoring fees.
2)No charge for ASG Launch Templates and Launch.
3)Application load balancer on available on free tirs NLB are not.
4)ASG are free (you only pay for the underlying EC2 instances)

CloudFont:
===========
1)Global excelarater not free.
2)Global Excelaration pricing model of the region.


================================================= 
#EXMQ
================================================= 

IAM:
====
1)User,Group and Policy.
2)MFA (Multi Factor Authentication) login with password and a sacurity device together.



RDS
=====
1)RDS is Dynamicly auto-scales.
2)Undestanding Read Replica and Multi-AZ
3)Is The Read Replicas be setup as Multi AZ for Disaster Recovery, Yes(CommonQ) 
Lotof question come from ReadReplica and Multi-AZ.
4)How do you make database go from Single-AZ to Multi-AZ ?
5)If the master is not encrypted, the read replicas cannot be encrypted.(CommonQ)
6)Write end point for Master and Read End Point for multiple Read Replica with loadbalanceing on connection point.
7)ElastiCache – Redis vs Memcached
8)Multi-AZ helps when you plan a disaster recovery for an entire AZ going down. 
  If you plan against an entire AWS Region going down, you should use backups and replication across AWS Regions.
9)ElastiCache and RDS Read Replicas do indeed help with scaling reads.
10)You would like to ensure you have a replica of your database available in another 
AWS Region if a disaster happens to your main AWS Region. 
Which database do you recommend to implement this easily?
Aurora Global Databases allows you to have an Aurora Replica in another AWS Region, with up to 5 secondary regions.


EBS
====
1)High permormance hrdware I/O thing its EC2 Instance Store.
2)Need to remaind EBS Volume Types in higlevel (General Purpose SSD, Provisioned IOPS (PIOPS) SSD, Hard Disk Drives (HDD))
 like EBS –Volume Types Summary.
3)Exam ask you EFS – Performance & Storage Classes.
4)When should you use EFS, What option should you set of EFS Network file system for ensure the validate and complement the requirment.
5)EBS root valume by default delete when ec2 terminate, we can peserved it after terminate EC2, Exam may ask about a scenario.
6)EBS volumes are created in a specific AZ and can only be attached to one EC2 instance at a time. 
It is not help any way for stateless app.

 
ELB
====
1)If exam talking about the Uses GENEVE protocol on port 6081, thing its GLB.
2)A web application hosted on a fleet of EC2 instances managed by an Auto Scaling Group. 
You are exposing this application through an Application Load Balancer. Both the EC2 instances and the ALB are deployed on a 
VPC with the following CIDR 192.168.0.0/18. 
How do you configure the EC2 instances' security group to ensure only the ALB can access them on port 80?

Ans:Add an Inbound Rule with port 80  and ALB's Security Group as the source.
This is the most secure way of ensuring only the ALB can access the EC2 instances. 
Referencing by security groups in rules is an extremely powerful rule and many questions at the exam rely on it. 
Make sure you fully master the concepts behind it!

3)An application is deployed with an Application Load Balancer and an Auto Scaling Group. Currently, you manually 
scale the ASG and you would like to define a Scaling Policy that will ensure the average number of connections to your 
EC2 instances is around 1000. Which Scaling Policy should you use?
Ans:Target Scaling Group.

S3:
=====
1)Exam will asked which Encryption one are adapted to which situation base on scenario.
2)Cross S3 will come with simple way.
3)If a client does a cross-origin request on our S3 bucket, we need to enable the correct CORS headers.
4)If you don’t specify or configure a default region, then us-east-1 will be chosen by default using SDK.
5)You and your colleague are working on an application that's interacting with some AWS services through making API calls. 
Your colleague can run the application on his machine without issues, while you get API Authorization Exceptions. 
What should you do?
Ans:Chack both IAM using AWS Policy Simulator.

2)Filtering of server side data of S3 get List thing its S3 Select & Glacier Select

3) Your application on EC2 creates images thumbnails after profile photos
are uploaded to Amazon S3. These thumbnails can be easily recreated,
and only need to be kept for 45 days. The source images should be able
to be immediately retrieved for these 45 days, and afterwards, the user
can wait up to 6 hours. How would you design this?
S3 source images can be on STANDARD, with a lifecycle configuration
to transition them to GLACIER after 45 days.
S3 thumbnails can be on ONEZONE_IA, with a lifecycle configuration
to expire them (delete them) after 45 days. 

4)
• A rule in your company states that you should be able to recover your
deleted S3 objects immediately for 15 days, although this may happen rarely.
After this time, and for up to 365 days, deleted objects should be recoverable
within 48 hours.
• You need to enable S3 versioning in order to have object versions, so that
“deleted objects” are in fact hidden by a “delete marker” and can be
recovered
• You can transition these “noncurrent versions” of the object to S3_IA
• You can transition afterwards these “noncurrent versions” to
DEEP_ARCHIVE

5)S3Request pay
6)analyze data in S3 using serverless SQL, its Athena

7)Create an application that will traverse the S3 bucket, issue a Byte Range 
Fetch for the first 250 bytes, and store that information in RDS


CloudFront:
============
1) Global excelaration come to exam.
2) CloudFront pay model


=================================================
#General | Basic | Info      
================================================= 

AWS- 
  -Region
  -VPC
	-AvailableZone
	-Router 
      -NACL*
	-Subnet
      -SecurityGroup	
	  -EC2
 -Gateway




#BasicInfo
-------------------------------------------------

Amazon web service is an online platform that provides scalable and cost-effective cloud computing solutions.
AWS has 80 Availability Zones across 25 geographic regions global data centers. 

AWS:
-Security: AWS provides a secure and durable platform that provides end-to-end security and storage.
-Experience: The skills and infrastructure management born from Amazon’s many years of experience can be very valuable.
-Flexibility: It allows users to select the operating systems, language, database, and other services as per their requirements.
-Easy to use: AWS lets you host your applications quickly and securely, regardless of whether it’s an existing or new application.
-Scalable: The applications you use can be scaled up or down, depending on your requirements.
-Cost savings: You only pay for the compute power, storage, and other resources that you use, without any long-term commitments.
-Scheduling: This enables you to start and stop AWS services at predetermined times
-Reliability: AWS takes multiple backups at servers at multiple physical locations


#Cloud Computing?           
-------------------------------------------------

Cloud computing is a computing service made available over the internet.
Cloud computing is a pay-as-you-go model for delivering IT resources.
You pay only for what you use.


Cloud computing differs from a traditional, on-premises environment in many ways,
including flexible, global, and scalable capacity, managed services, built-in security,
options for cost optimization, and various operating models.


=================================================
#FOR | SAA-C02         
================================================= 

#Region        
-------------------------------------------------
How to choose  and AWS Region?
 - Compliance
 - Proximity
 - Available Service
 - Pricing
 
 
#Availabiulity Zones |  AZ | az      
-------------------------------------------------
Each region hase many availabuility zone.
Usually 3, Min 2 and Max 6;



=================================================
##Section 4: IAM & AWS CLI        
================================================= 


#IAM | User | Group | Policy
-------------------------------------------------
IAM is a global AWS services that is not limited by regions. 
Any user, group, role or policy is accessible globally.

AWS IAM is also called AWS Identity and Access Management.
It helps you securely manage AWS resources and services.
IAM user represents an entity (person or an application) that interacts with AWS resources and services.


IAM Entities:
-------------
-Users - any individual end user such as an employee, system architect, CTO, etc.
-Groups - any collection of similar people with shared permissions such as system administrators, HR employees, finance teams, etc. 
 Each user within their specified group will inherit the permissions set for the group.
-Roles - any software service that needs to be granted permissions to do its job, 
 e.g- AWS Lambda needing write permissions to S3 or a fleet of EC2 instances needing read permissions from a RDS MySQL database.
-Policies - the documented rule sets that are applied to grant or limit access. 
 In order for users, groups, or roles to properly set permissions, they use policies. 
 Policies are written in JSON and you can either use custom policies for your specific needs or use the default policies set by AWS.


IAM features are:
    AWS account root user
	IAM Users
	IAM policy
	IAM groups
	IAM roles
	Multi-factor authentication
	

User can access AWS Three way:
-AwS Management Console
-AWS Command Line Interface(CLI)
-AWS Softeare Developer Kit (SDK)
Also CloudShell like CLI	


ConfigCLI:
==========
Download/Install CLI Software you Operstion system
=>aws --version
=>aws configure
Then Enter access ID and Key
=>aws iam list-users


CloudShell:
AWS CloudShell is a browser-based shell that gives you command-line access to your AWS resources in the selected AWS region. 
AWS CloudShell comes pre-installed with popular tools for resource management and creation.


IAM Role:
Some aws service will need to perform action on your behalf, to do so, we will\
assign permissions to AWS services with IAM Roles.



IAM Security Tools:
==================
• IAM Credentials Report (account-level)
-A report that lists all your account's users and the status of their various credentials.

• IAM Access Advisor (user-level)
-Access advisor shows the service permissions granted to a user and when those services were last accessed.
-You can use this information to revise your policies.



IAM Guidelines & Best Practices:
• Don’t use the root account except for AWS account setup
• One physical user = One AWS user
• Assign users to groups and assign permissions to groups
• Create a strong password policy
• Use and enforce the use of Multi Factor Authentication (MFA)
• Create and use Roles for giving permissions to AWS services
• Use Access Keys for Programmatic Access (CLI / SDK)
• Audit permissions of your account with the IAM Credentials Report



IAM Section – Summary:
• Users: mapped to a physical user, has a password for AWS Console
• Groups: contains users only
• Policies: JSON document that outlines permissions for users or groups
• Roles: for EC2 instances or AWS services
• Security: MFA + Password Policy
• Access Keys: access AWS using the CLI or SDK
• Audit: IAM Credential Reports & IAM Access Advisor


Golden AMI is an image that contains all your software, dependencies, and configurations, so that future 
EC2 instances can boot up quickly from that AMI.


=================================================
##Section 5: EC2 Fundamentals
=================================================
Ec2=> Elastic Compute Cloud (Infrastructure as a Service on AWS)
EC2 provides virtual computing environments called “instances.”
Copy of EC2 instances: Old instance -> Snapshot -> Image (AMI) -> New instance


The following EC2-related resources don't generate charges when used or provisioned to an account:
-Virtual private clouds (VPCs)
-Security groups
-Key pairs
-Elastic network interfaces
-Auto Scaling groups


IAM Roles are the right way to provide credentials and permissions to an EC2 instance.

#EC2    
-------------------------------------------------
t2.micro:
	t - Instance Family
	2 - generation. Improvements with each generation.
	micro - size. (nano < micro < small < medium < large < xlarge < …..)


EC2 provides two important services to get details:
-Instance Metadata Service
-Dynamic Data Service

MetadataURL: 
http://169.254.169.254/latest/meta-data/
http://169.254.169.254/latest/dynamic/
http://169.254.169.254/latest/dynamic/instance-identity/document



#AWS EC2 Instance Metadata
-------------------------------------------------
AWS EC2 Instance Metadata is powerful but one of the least known features to developers
• It allows AWS EC2 instances to ”learn about themselves” without using an IAM Role for that purpose.
• The URL is http://169.254.169.254/latest/meta-data
• You can retrieve the IAM Role name from the metadata, but you CANNOT retrieve the IAM Policy.
• Metadata = Info about the EC2 instance
• Userdata = launch script of the EC2 instance
• Let’s practice and see what we can do with it!



#Bootstrapping: 
-------------------------------------------------
Install OS patches or software when an EC2 instance is launched.

Example:
#!/bin/bash
yum update -y
yum install -y httpd
systemctl start httpd
systemctl enable httpd
echo "<h1>Hello World from $(hostname -f)</h1>" > /var/www/html/index.html
OR
#!/bin/bash
yum update -y
yum install -y httpd
systemctl start httpd
systemctl enable httpd
curl -s http://169.254.169.254/latest/dynamic/instance-identity/document > /var/www/html/index.html
You can call http://169.254.169.254/latest/user-data/ from inside the EC2 instance to lookup user data configured for that instance.

http://3.110.83.133/home.html

#Security Groups:
-------------------------------------------------
Security groups are acting as a “firewall” on EC2 instances 
They regulate: 
• Access to Ports 
• Authorised IP ranges – IPv4 and IPv6 
• Control of inbound network (from other to the instance) 
• Control of outbound network (from the instance to other)

Security Groups can be attached to multiple EC2 instances within the same AWS Region/VPC.


-Security Groups are the fundamental of network security in AWS
-They control how traffic is allowed into or out of our EC2 Instances
-Security groups only contain allow rules
-Security groups rules can reference by IP or by security group



General Ports:
22    = SSH (Secure Shell) - log into a Linux instance
21    = FTP (File Transfer Protocol) – upload files into a file share
22    = SFTP (Secure File Transfer Protocol) – upload files using SSH
80    = HTTP – access unsecured websites
443   = HTTPS – access secured websites
3389  = RDP (Remote Desktop Protocol) – log into a Windows instance


Key Pairs:
-Public key cryptography (Key Pairs) used to protect your EC2 instances
-You need private key with right permissions (chmod 400) to connect to your EC2 instance. 
(Windows EC2 instances only) You need admin password also.
-Security group should allow SSH(22) or RDP(3389)


SSH:
-------------------------------------------------

Windows PowerShell:
Go to .pem file directory then
=>ssh -i .\MyKeFile.pem ec2-user@ec2PublicIP


=>whoami
=>ping google.com




#EC2 Tenancy - Shared vs Dedicated
--------------------------------------------------

There are two Dedicated EC2 options:
-EC2 Dedicated Instances
-EC2 Dedicated Hosts


EC2 Dedicated Instances are Virtualized instances on hardware dedicated to one customer:
You do NOT have visibility into the hardware of underlying host

EC2 Dedicated Hosts are Physical servers dedicated to one customer:
You have visibility into the hardware of underlying host (sockets and physical cores)
(Use cases) Regulatory needs or server-bound software licenses like Windows Server, SQL Server


#EC2 Pricing Models:
--------------------------------------------------
-On Demand: Request when you want it	Flexible and Most Expensive
-Spot: Quote the maximum price	Cheapest (upto 90% off) BUT NO Guarantees
-Reserved: Reserve ahead of time	Upto 75% off. 1 or 3 years reservation.
-Savings Plans: Commit spending $X per hour on (EC2 or AWS Fargate or Lambda)	Upto 66% off. No restrictions. 1 or 3 years reservation.




Compute Optimized:
Compute Optimized EC2 instances are great for compute-intensive workloads requiring high-performance 
processors (e.g., batch processing, media transcoding, high-performance computing, scientific modeling & 
machine learning, and dedicated gaming servers).


Memory Optimized:
Memory Optimized EC2 instances are great for workloads requiring large data sets in memory.


Storage Optimized:
Storage Optimized EC2 instances are great for workloads requiring high, sequential read/write access to large data sets on local storage.



Dedicated Hosts:
Dedicated Hosts are good for companies with strong compliance needs or for software that have complicated licensing models. 
This is the most expensive EC2 Purchasing Option available.


#Monitoring EC2 instances
-------------------------------------------------
Amazon CloudWatch is used to monitor EC2 instances.

There are two types of monitoring:
(FREE) Basic monitoring (“Every 5 minutes”) provided for all EC2 instance types
(\(\)) EC2 Detailed Monitoring can be enabled for detailed metrics every 1 minute



VM Import/Export:
VM Import/Export enables you to easily import virtual machine images from your existing environment to Amazon EC2 instances. 
This service allows you to leverage your existing investments in the virtual machines that you have built to meet your IT security, 
configuration management, and compliance requirements by bringing those virtual machines into Amazon EC2 as ready-to-use instances. 
If you are planning to use your own Microsoft licenses, use the ImportImage tool made available by the 
VM Import/Export service to import your own Microsoft media.
The VM Import/Export service is available at no additional charge beyond standard usage charges for Amazon EC2 and Amazon S3.



Termination Protection:
Remember:EC2 Termination Protection is not effective for terminations from a) Auto Scaling Groups (ASG) b) Spot Instances c) OS Shutdown
Launch Templates - Pre-configured templates (AMI ID, instance type, and network settings) simplifying the creation of EC2 instances.



INP:
====
-You want to update the EC2 instance to a new AMI updated with latest patches
Relaunch a new instance with an updated AMI

-Create EC2 instances based on on-premise Virtual Machine (VM) images	
Use VM Import/Export. You are responsible for licenses.


-You are installing a lot of software using user data slowing down instance launch. How to make it faster?	
Create an AMI from the EC2 instance and use it for launching new instances

-I’ve stopped my EC2 instance. Will I be billed for it?	
ZERO charge for a stopped instance (If you have storage attached, you have to pay for storage)






=================================================
##Section 6: EC2 - Solutions Architect Associate Level
=================================================

Spot Fleet is a set of Spot Instances and optionally On-demand Instances. 
It allows you to automatically request Spot Instances with the lowest price.


#Elastic IP Addresses:
-------------------------------------------------
How do you get a constant public IP address for a EC2 instance? Quick and dirty way is to use an Elastic IP!
An Elastic IP can be switched to another EC2 instance within the same region. Elastic IP remains attached even if you stop the instance. 
You have to manually detach it.



#Placement Group        
-------------------------------------------------
Placement groups balance the tradeoff between risk tolerance and network performance when it comes to your fleet of EC2 instances.
The more you care about risk, the more isolated you want your instances to be from each other. 
The more you care about performance, the more conjoined you want your instances to be with each other.


Placing the things at one place or gathering the things together at one place is nothing but placement groups.

When you launch multiple EC2 instances on AWS, the EC2 service makes sure that all of your EC2 instances 
are spread across different physical machines to minimize the failure of the entire system. 
But AWS EC2 also provides the customers the ability to put the EC2 instance according to their need. 
Placement groups are used to determine how the EC2 instances are launched on the underlying hardware. 


Why use Placement Group?
Placement groups help us to launch a bunch of EC2 instances close to each other physically within the same AZ. 
Being close physically and within the same AZ helps it take advantage of high-speed connectivity to 
provide low latency, high throughput access.


Placement groups strategies:
-Cluster placement group: It groups instances into low latency clusters in a single available zone(AZ).
-Spread placement group: It spread the instances across underlying hardware.
-Partition placement group: It spreads the instances across many different partitions within an AZ.


Cluster Placement Group:
In the cluster placement group, all the instances are in the same rack in a single availability zone. 
Cluster placement groups are designed for high speed performance and low network latency applications as 
EC2 instances are physically on the same rack and it causes low latency between the EC2 instances in the same cluster placement group. 
It usually supports up to 10Gbps network. As the EC2 instances in the cluster placement group are in the same physical rack so the 
problem with cluster placement groups is if the rack fails, all the instances will fail at the same time 
compromising the high availability of the application.
  
Spread Placement Group:
Spread Placement Group places your EC2 instances on different physical hardware across different AZs.

In the spread placement group, all EC2 instances are located on different hardware racks in a single availability zone. 
Each rack is isolated from others and has its own power and networks to reduce the failure of all the instances in the 
spread placement group at a time. You can create up to 7 EC2 instances per availability zone per spread placement group. 
Unlike Cluster placement groups, EC2 instances in the spread placement group exist on different hardware within 
the single availability zone minimizing the failure of all the EC2 instances at a time while making sure of the low latency. 
Spread placement groups are designed for applications that require maximum high availability and where 
each instance must be isolated from failure from each other.

Partition Placement Group:
In the partition placement group, instances are launched into different partitions on different hardware racks to 
make sure of high availability. It can span across multiple AZs in the same region. 
The instances in a partition do not share racks with the instances in the other partitions. 
A partition failure can affect many EC2 instances in the same partition but won’t affect the EC2 instances on the other partitions. 
Partition placement groups are designed for applications that require maximum high availability. 
Partition placement groups are used for big application deployment and are ideal 
for large distributed and replicated workloads such as kafka, hadoop and cassandra etc.




#ENI |  Elastic network interfaces             
-------------------------------------------------
An elastic network interface is a logical networking component in a VPC that represents a virtual network card. 
Elastic Network Interfaces (ENIs) are bounded to a specific AZ. You can not attach an ENI to an EC2 instance in a different AZ.
ENIs have security groups, just like EC2 instances, which act as a built in firewall.


It can include the following attributes:
A primary private IPv4 address from the IPv4 address range of your VPC
One or more secondary private IPv4 addresses from the IPv4 address range of your VPC
One Elastic IP address (IPv4) per private IPv4 address
One public IPv4 address
One or more IPv6 addresses
One or more security groups
A MAC address
A source/destination check flag
A description

ENIs are virtual network cards you can attach to your EC2 instances. They are used to enable network connectivity for your instances, 
and having more than one of them connected to your instance allows it to communicate on two different subnets.


A common use case for ENIs is the creation of management networks. 
This allows you to have public-facing applications like web servers in a public subnet but lock down SSH access 
down to a private subnet on a secondary network interface. In this scenario, you would connect using a 
VPN to the private management subnet, then administrate your servers as usual.




Each EC2 instance is connected to primary network interface (eth0). 
You can create and attach a secondary network interface - eth1.

This allows an instance to be dual homed - present in two subnets in a VPC. 
It can be used to create a management network or a low budget high availability solution.


Important terminology with ENI:
Hot attach: Attaching ENI when EC2 instance is running
Warm attach: Attaching ENI when EC2 instance is stopped
Cold attach: Attaching ENI at launch time of EC2 instance


ENI make you more control over the provate network, and IP.


EC2 Hibernate:
For enable EC2 Hibernation root ebs volume must be encrypted and must enought size to fit RAM in Root EBS.
To enable EC2 Hibernate, the EC2 Instance Root Volume type must be an EBS volume and must be encrypted to ensure the 
protection of sensitive content.


EC2 Nitro:
• Underlying Platform for the next generation of EC2 instances
• New virtualization technology
• Allows for better performance:
• Better networking options (enhanced networking, HPC, IPv6)
• Higher Speed EBS (Nitro is necessary for 64,000 EBS IOPS – max 32,000 on non-Nitro)
• Better underlying security
• Instance types example:
• Virtualized:A1, C5,C6gn, D3, G4, I3en, Inf1, M5,M5a,M5n.
• Bare metal: a1.metal, c5.metal, c5d.metal, c5n.metal, c6g.metal, c6gd.metal.



#EC2 capacity reservation:
-------------------------------------------------
Capacity Reservations enable you to reserve capacity for your Amazon EC2 instances in a specific Availability Zone for any duration. 
This gives you the flexibility to selectively add capacity reservations and still get the Regional RI discounts for that usage. 
By creating Capacity Reservations, you ensure that you always have access to Amazon EC2 capacity when you need it, 
for as long as you need it. 

You must reserve the instance only for 1 or 3 years, not any time in between.



=================================================
##Section 7: EC2 Instance Storage
=================================================

The OS provides access to block-level storage via open, write, and read system calls.

The simplified flow of a read request goes like this:
-An application wants to read the file /path/to/file.txt and makes a read system call.
-The OS forwards the read request to the file system.
-The file system translates /path/to/file.txt to the block on the disk where the data is stored.



#Elastic Block Storage (EBS)
-------------------------------------------------
AWS provides two kinds of block-level storage: network-attached storage (NAS) and instance storage. 
NAS is (like iSCSI) attached to your EC2 instance via a network connection, whereas instance storage is a normal hard 
disk that the host system provides to your EC2 instance. 

You can use block-level storage only in combination with an EC2 instance where the OS runs.
The OS provides access to block-level storage via open, write, and read system calls.

Aren’t part of your EC2 instances; they’re attached to your EC2 instance via a network connection. 
If you terminate your EC2 instance, the EBS volumes remain.

EBS Volumes are created for a specific AZ. It is possible to migrate them between different AZs using EBS Snapshots.

What’s an EBS Volume?
• An EBS (Elastic Block Store) Volume is a network drive you can attach
to your instances while they run
• It allows your instances to persist data, even after their termination
• They can only be mounted to one instance at a time (at the CCP level)
• They are bound to a specific availability zone
• Analogy: Think of them as a “network USB stick”
• Free tier: 30 GB of free EBS storage of type General Purpose (SSD) or
Magnetic per month


 - One EBS for one EC2
 - EBS Volume locked at the AZ level, EC2 and EBS have to be same zone to attach.
 - Make shapshots to sent data volum to AZ or region.
 - Using EBS Multi-Attach, you can attach the same EBS volume to multiple EC2 instances in the same AZ (with Full rw).
 - EBS Multi-Attach volume can be attached to multiple EC2 instances

When creating EC2 instances, you can only use the following EBS volume types as boot volumes: gp2, gp3, io1, io2, and Magnetic (Standard).
Using EBS Multi-Attach, you can attach the same EBS volume to multiple EC2 instances in the same AZ. 
Each EC2 instance has full read/write permissions.



#Instance stores:
-------------------------------------------------
An instance store provides block-level storage like a normal hard disk. Instance store is part of an EC2 instance and available only 
if your instance is running; it won’t persist your data if you stop or terminate the instance. Therefore you don’t pay 
separately for an instance store; instance store charges are included in the EC2 instance price.

64,000 is the maximum IOPS you can achieve when you're using EBS io1 or io2 volume types.


You are running a high-performance database that requires an IOPS of 310,000 for its underlying storage. What do you recommend?
You can run a database on an EC2 instance that uses an Instance Store, but you'll have a problem that the data will be lost if the 
EC2 instance is stopped (it can be restarted without problems). 
One solution is that you can set up a replication mechanism on another EC2 instance with an Instance Store to have a standby copy. 
Another solution is to set up backup mechanisms for your data. 
It's all up to you how you want to set up your architecture to validate your requirements. 
In this use case, it's around IOPS, so we have to choose an EC2 Instance Store.

 
Spot Instances:
===============
Spot instances are cost-effective when you can be flexible with your application’s availability and when your 
applications can be interrupted after a two-minute warning notification. 

Spot instances are ideal for stateless, error-tolerant, or flexible applications like data analysis, batch jobs, 
background processing, and optional tasks.

A Spot Price is the hourly rate for a Spot instance. 


There are two types of spot requests:
-A one-time spot request stays active until Amazon EC2 runs the spot instance, you cancel the request, or it expires. 
If the spot price goes above your bid or capacity is unavailable, Amazon terminates the spot instance and closes the spot request.

-A persistent spot instance request stays active until you cancel it or until it expires—even if Amazon carried out the request. 
If the spot price goes above your bid or capacity becomes unavailable, the spot instance is interrupted. 
Amazon keeps the request open, and if conditions change and a spot instance does become available, it starts or 
resumes the spot instance



#AMI  | Amazon Machine Image
-------------------------------------------------
You need to choose the Amazon Machine Image or AMI based on what operating system and what software do you want on the EC2 instance.
AMIs are stored in Amazon S3 (region specific).

AMIs are built for a specific AWS Region, they're unique for each AWS Region. You can't launch an EC2 instance using an 
AMI in another AWS Region, but you can copy the AMI to the target AWS Region and then use it to create your EC2 instances.


AMIs contain:
-Root volume block storage (OS and applications)
-Block device mappings for non-root volumes
-You can configure launch permissions on an AMI

Who can use the AMI?
You can share your AMIs with other AWS accounts

Best Practice: Backup upto date AMIs in multiple regions
Critical for Disaster Recovery


Three AMI sources:
-Provided by AWS
-AWS Market Place: Online store for customized AMIs. Per hour billing
-Customized AMIs: Created by you.


AMI Process (from an EC2 instance):
• Start an EC2 instance and customize it
• Stop the instance (for data integrity)
• Build an AMI – this will also create EBS snapshots
• Launch instances from other AMIs


EC2 Instance Store:
Instance store volumes, unlike EBS volumes, cannot be detached or attached to another instance


• EBS volumes are network drives with good but “limited” performance
• If you need a high-performance hardware disk, use EC2 Instance Store
• Better I/O performance
• EC2 Instance Store lose their storage if they’re stopped (ephemeral)
• Good for buffer / cache / scratch data / temporary content
• Risk of data loss if hardware fails
• Backups and Replication are your responsibility



EBS Multi-Attach – io1/io2 family:
Only io1/io2 family can have multi-attach.
Attach the same EBS volume to multiple EC2 instances in the same AZ.
Each instance has full read & write permissions to the volume.



#Amazon EFS – Elastic File System
-------------------------------------------------
AWS Elastic File System (EFS) is one of three main storage services offered by Amazon. 
It is a scalable, cloud-based file system for Linux-based applications and workloads that can be used in combination with 
AWS cloud services and on-premise resources. 
EFS offers a choice between two storage classes, Infrequent Access and Standard access.

EFS is a network file system (NFS) that allows you to mount the same file system on EC2 instances that are in different AZs.


• Managed NFS (network file system) that can be mounted on many EC2
• EFS works with EC2 instances in multi-AZ



EFS – Performance & Storage Classes:
EFS Scale
• 1000s of concurrent NFS clients, 10 GB+ /s throughput
• Grow to Petabyte-scale network file system, automatically

Performance mode (set at EFS creation time)
• General purpose (default): latency-sensitive use cases (web server, CMS, etc…)
• Max I/O – higher latency, throughput, highly parallel (big data, media processing)

Throughput mode
• Bursting (1 TB = 50MiB/s + burst of up to 100MiB/s)
• Provisioned: set your throughput regardless of storage size, ex: 1 GiB/s for 1 TB storage

Storage Tiers (lifecycle management feature – move file after N days) 
• Standard: for frequently accessed files 
• Infrequent access (EFS-IA): cost to retrieve files, lower price to store. 
Enable EFS -IA with a Lifecycle Policy.

Availability and durability 
• Standard: Multi-AZ, great for prod 
• One Zone: One AZ, great for dev, backup enabled by default, compatible with IA (EFS One Zone -IA)


EBS vs EFS:
-EBS have to pay for provisioning the Size and have to pay full capicity not the uses.
-EFS you do bill only what you used.


EFS multi AZ network file system for multiple EC2
EBS Single AZ single EC2
Instance Storage for single EC2 High permormance and Maximam I/O.


AWS S3:
Simple Storage Service (S3) provides object storage, without the use of EC2 instances, that is accessible directly through the internet.




=================================================
Section 8: High Availability and Scalability: ELB & ASG
=================================================

High Availability: 
Run instances for the same application across multi AZ
• Auto Scaling Group multi AZ
• Load Balancer multi AZ


The following cookie names are reserved by the ELB (AWSALB, AWSALBAPP, AWSALBTG).


Load Balances:
Load Balances are servers that forward traffic to multiple
servers (e.g., EC2 instances) downstream

It is integrated with many AWS offerings/services:
• EC2, EC2 Auto Scaling Groups, Amazon ECS
• AWS Certificate Manager (ACM), CloudWatch
• Route 53, AWS WAF, AWS Global Accelerator



AWS has 4 kinds of managed Load Balancers:

• Classic Load Balancer (v1 - old generation) – 2009 – CLB
HTTP, HTTPS, TCP, SSL (secure TCP)

• Application Load Balancer (v2 - new generation) – 2016 – ALB
HTTP, HTTPS, WebSocket

• Network Load Balancer (v2 - new generation) – 2017 – NLB
TCP, TLS (secure TCP), UDP

• Gateway Load Balancer – 2020 – GWLB
Operates at layer 3 (Network layer) – IP Protocol

Overall, it is recommended to use the newer generation load balancers as they
provide more features
Some load balancers can be setup as internal (private) or external (public) ELBs



Application Load Balancer (v2):
-------------------------------------------------

ALBs can route traffic to different Target Groups based on URL Path, Hostname, HTTP Headers, and Query Strings.
You can't attach an Elastic IP address to Application Load Balancers.

• Application load balancers is Layer 7 (HTTP)
• Load balancing to multiple HTTP applications across machines
(target groups)
• Load balancing to multiple applications on the same machine
(ex: containers)
• Support for HTTP/2 and WebSocket
• Support redirects (from HTTP to HTTPS for example)
 Routing tables to different target groups:
• Routing based on path in URL (example.com/users & example.com/posts)
• Routing based on hostname in URL (one.example.com & other.example.com)
• Routing based on Query String, Headers
(example.com/users?id=123&order=false)
• ALB are a great fit for micro services & container-based application
(example: Docker & Amazon ECS)
• Has a port mapping feature to redirect to a dynamic port in ECS
• In comparison, we’d need multiple Classic Load Balancer per application

• ALB can route to multiple target groups
• Health checks are at the target group level

Fixed hostname (XXX.region.elb.amazonaws.com)
The application servers don’t see the IP of the client directly
• The true IP of the client is inserted in the header X-Forwarded-For
• We can also get Port (X-Forwarded-Port) and proto (X-Forwarded-Proto)



#Network Load Balancer (v2):
-------------------------------------------------
Network Load Balancer has one static IP address per AZ and you can attach an Elastic IP address to it. 
Application Load Balancers and Classic Load Balancers have a static DNS name.
NLB has fixed IP, one fixep for per AZ.

• Network load balancers (Layer 4) allow to:
• Forward TCP & UDP traffic to your instances
• Handle millions of request per seconds
• Less latency ~100 ms (vs 400 ms for ALB)
• NLB has one static IP per AZ, and supports assigning Elastic IP
  (helpful for whitelisting specific IP)
• NLB are used for extreme performance, TCP or UDP traffic
• Not included in the AWS free tier



Network Load Balancer –Target Groups:
• EC2 instances
• IP Addresses – must be private IPs
• Application Load Balancer




#Gateway Load Balancer
-------------------------------------------------
• Deploy, scale, and manage a fleet of 3rd party network virtual appliances in AWS
• Example: Firewalls, Intrusion Detection and Prevention Systems, Deep Packet Inspection
Systems, payload manipulation


Operates at Layer 3 (Network Layer) – IPPackets, Combines the following functions:
• Transparent Network Gateway – single entry/exit for all traffic
• Load Balancer – distributes traffic to your virtual appliances
• Uses the GENEVE protocol on port 6081



Sticky Sessions (Session Affinity):
-------------------------------------------------
• It is possible to implement stickiness so that the same client is always redirected to the same instance behind a load balancer
• This works for Classic Load Balancers & Application Load Balancers
• The “cookie” used for stickiness has an expiration date you control
• Use case: make sure the user doesn’t lose his session data




Cross-Zone Load Balancing:
-------------------------------------------------
When Cross-Zone Load Balancing is enabled, ELB distributes traffic evenly across all registered EC2 instances in all AZs.

Application Load Balancer 
• Always on (can’t be disabled) 
• No charges for inter AZ data 

Network Load Balancer 
• Disabled by default 
• You pay charges ($) for inter AZ data if enabled 

Classic Load Balancer 
• Disabled by default 
• No charges for inter AZ data if enabled


Sticky Sessions – Cookie Names
• Application-based Cookies
• Custom cookie
• Generated by the target
• Can include any custom attributes required by the application
• Cookie name must be specified individually for each target group
• Don’t use AWSALB, AWSALBAPP, or AWSALBTG (reserved for use by the ELB)
• Application cookie
• Generated by the load balancer
• Cookie name is AWSALBAPP
• Duration-based Cookies
• Cookie generated by the load balancer
• Cookie name is AWSALB for ALB, AWSELB for CLB



Elastic Load Balancers – SSL Certificates:
-------------------------------------------------
Classic Load Balancer (v1)
• Support only one SSL certificate
• Must use multiple CLB for multiple hostname with multiple SSL certificates

Application Load Balancer (v2)
• Supports multiple listeners with multiple SSL certificates
• Uses Server Name Indication (SNI) to make it work

Network Load Balancer (v2)
• Supports multiple listeners with multiple SSL certificates
• Uses Server Name Indication (SNI) to make it work


Connection Draining
-------------------------------------------------
Feature naming
• Connection Draining – for CLB
• Deregistration Delay – for ALB & NLB

• Time to complete “in-flight requests” while the instance is de-registering or unhealthy
• Stops sending new requests to the EC2 instance which is de-registering
• Between 1 to 3600 seconds (default: 300 seconds)
• Can be disabled (set value to 0)
• Set to a low value if your requests are short


Auto Scaling Group Attributes
-------------------------------------------------

A Launch Template (older “Launch Configurations” are deprecated)
• AMI + Instance Type
• EC2 User Data
• EBS Volumes
• Security Groups
• SSH Key Pair
• IAM Roles for your EC2 Instances
• Network + Subnets Information
• Load Balancer Information
• Min Size / Max Size / Initial Capacity
• Scaling Policies

It is possible to scale an ASG based on CloudWatch alarms
An alarm monitors a metric (such as Average CPU, or a custom metric)



Auto Scaling Groups – Dynamic Scaling Policies
• Target Tracking Scaling
• Most simple and easy to set-up
• Example: I want the average ASG CPU to stay at around 40%
• Simple / Step Scaling
• When a CloudWatch alarm is triggered (example CPU > 70%), then add 2 units
• When a CloudWatch alarm is triggered (example CPU < 30%), then remove 1
• Scheduled Actions
• Anticipate a scaling based on known usage patterns
• Example: increase the min capacity to 10 at 5 pm on Fridays


VIP:
===
Only Network Load Balancer provides both static DNS name and static IP. 
While, Application Load Balancer provides a static DNS name but it does NOT provide a static IP. 
The reason being that AWS wants your Elastic Load Balancer to be accessible using a static endpoint, 
even if the underlying infrastructure that AWS manages changes.


Which feature in both Application Load Balancers and Network Load Balancers allows you to load multiple 
SSL certificates on one listener?
Server Name Indication (SNI)

Server Name Indication (SNI) allows you to expose multiple HTTPS applications each with its own 
SSL certificate on the same listener. 

Make sure you remember the Default Termination Policy for Auto Scaling Group. 
It tries to balance across AZs first, then terminates based on the age of the Launch Configuration.



=================================================
#Elastic Load Blancer |  | elb | ELB          
=================================================

Elastic Load Balancer are used to distribute traffic across EC2 instances in one or more AZs in a single region. 

The AWS ELB is an AWS service for automatic distribution of incoming application traffic across its components like Amazon EC2 instances, AWS Lambda, and containers..

A load balancer accepts incoming traffic from clients and routes requests to its registered targets (such as EC2) in one or more AZ. 
The load balancer also monitors the health of its registered targets and ensures that it routes traffic only to healthy targets.
When the load balancer detects an unhealthy target, it stops routing traffic to that target. 
It then resumes routing traffic to that target when it detects that the target is healthy again.

TypeOfLoadBalancer:
 - Application Load Balancers
 - Network Load Balancers
 - Gateway Load Balancers
 - Classic Load Balancers
 
Load balancers are a regional service. They do not balance load across different regions. 
You must provision a new ELB in each region that you operate out of.


With Application Load Balancers, Network Load Balancers, and Gateway Load Balancers, 
you register targets in target groups, and route traffic to the target groups. 
With Classic Load Balancers,you register instances with the load balancer.


Cross-zone load balancing is enable then every target equal load, if disable the  every AZ are equal. 
Application Load Balancers, cross-zone load balancing is always enabled.
With Network Load Balancers and Gateway Load Balancers, cross-zone load balancing is disabled by default. 


Elastic Load Balancing works with the following services:

 - Amazon EC2 — Virtual servers that run your applications in the cloud. 
 - Amazon EC2 Auto Scaling — Ensures that you are running your desired number of instances, even if an instance fails. 
 - AWS Certificate Manager — When you create an HTTPS listener, you can specify certificates provided by ACM. The load balancer uses certificates to terminate connections and decrypt requests from clients.
 - Amazon CloudWatch — Enables you to monitor your load balancer and to take action as needed. 
 - Amazon ECS — Enables you to run, stop, and manage Docker containers on a cluster of EC2 instances. 
 - AWS Global Accelerator — Improves the availability and performance of your application. Use an accelerator to distribute traffic across multiple load balancers in one or more AWS Regions.
 - Route 53 — Provides a reliable and cost-effective way to route visitors to websites by translating domain names into the numeric IP addresses that computers use to connect to each other.
 - AWS WAF — You can use AWS WAF (Web Application Firewall) with your Application Load Balancer to allow or block requests based on the rules in a web access control list (web ACL).

 
#AWS Classic Load Balancer:
------------------------------------------------- 
This simple load balancer operates both at the request level and the connection level and was originally used for classic EC2 instances. 
Classic Load balancer in AWS is used on EC2-classic instances. This is the previous generation’s load balancer and also it doesn’t allow host-based or path based routing.
Mostly it is used to route traffic to one single URL.


#NLB
------------------------------------------------- 
AWS recommends AWS Network Load Balancer (NLB) if the application needs to achieve static IP and extreme performance.
AWS network load balancers also avoid DNS caching problems and work with existing firewall security policies of users thanks to its static and resilient IP addresses. 
And AWS load balancer TLS termination is only possible with NLB.


#Create a ELB
------------------------------------------------- 
Select which type of AWS load balancer to use
Complete basic configuration
Configure a security group
Configure a target group
Register targets
Create a load balancer and test it
Get more details on how to configure AWS load balancers


#GateWay Load Balancer
-------------------------------------------------

Gateway Load Balancer helps you easily deploy, scale, and manage your third-party virtual appliances. 
It gives you one gateway for distributing traffic across multiple virtual appliances while scaling them up or down, 
based on demand. This decreases potential points of failure in your network and increases availability.

You can find, test, and buy virtual appliances from third-party vendors directly in AWS Marketplace.  

Use cases:
Centralize your third-party virtual appliances
Consolidating your third-party virtual appliances with Gateway Load Balancer can reduce operational overhead and costs.

=================================================
#ASG Auto Scaling | Auto Scaling groups        
================================================= 

An Auto Scaling group contains a collection of EC2 instances that are treated as a 
logical grouping for the purposes of automatic scaling and management. 
An Auto Scaling group also enables you to use Amazon EC2 Auto Scaling features such as health check replacements and scaling policies.

When you create an EC2 Auto Scaling group, you must specify a launch configuration. You can specify your launch configuration with multiple EC2 Auto Scaling groups. 

AutoScaling happend in a available zone.


Component of Authscaling
 - Launch configuation
 - AutoScaling Group
 - Scaling Policy
 
 
AutoScalingPolicies
 - Manual
 - Daynamic
 
 
You cannot modify a launch configuration after you've created it. If you want to change the launch 
configuration for an Auto Scaling group, you must create a new launch configuration and update your Auto Scaling group to 
inherit this new launch configuration.

The default termination policy for an Auto Scaling Group is to automatically terminate a stopped instance, 
so unless you've configured it to do otherwise, stopping an instance will result in termination regardless 
if you wanted that to happen or not. 
A new instance will be spun up in its place.
 
 


#Amazon EC2 Auto Scaling vs. AWS Auto Scaling           
--------------------------------------------------

AWS Auto Scaling:
You should use AWS Auto Scaling to manage scaling for multiple resources across multiple services. 
AWS Auto Scaling lets you define dynamic scaling policies for multiple EC2 Auto Scaling groups or other resources using predefined scaling strategies. Using AWS Auto Scaling to configure scaling policies for all of the scalable resources 
in your application is faster than managing scaling policies for each resource via its individual service console

Amazon EC2 Auto Scaling :
You should use EC2 Auto Scaling if you only need to scale Amazon EC2 Auto Scaling groups, or if you are only interested 
in maintaining the health of your EC2 fleet. You should also use EC2 Auto Scaling if you need 
to create or configure Amazon EC2 Auto Scaling groups, or if you need to set up scheduled or step 
scaling policies (as AWS Auto Scaling supports only target tracking scaling policies)



Predictive Scaling of AWS Auto Scaling pla:
Predictive Scaling Policy brings the similar prediction algorithm offered through AWS Auto Scaling 
plan as a native scaling policy in EC2 Auto Scaling. 
If you have predictable load changes, you can use Predictive Scaling policy to proactively increase capacity ahead of upcoming demand. 
Amazon EC2 Auto Scaling enables you to run your Amazon EC2 fleet at optimal utilization.



What is fleet management and how is it different from dynamic scaling:

Fleet management refers to the functionality that automatically replaces unhealthy instances and maintains your fleet at the 
desired capacity. Amazon EC2 Auto Scaling fleet management ensures that your application is able to receive traffic 
and that the instances themselves are working properly. 
When Auto Scaling detects a failed health check, it can replace the instance automatically.


The dynamic scaling capabilities of Amazon EC2 Auto Scaling refers to the functionality that automatically increases 
or decreases capacity based on load or other metrics. For example, if your CPU spikes above 80% (and you have an alarm setup)
 Amazon EC2 Auto Scaling can add a new instance dynamically.


================================================= 
##Section 9: AWS Fundamentals: RDS + Aurora + ElastiCache
================================================= 
RDS is a managed service that makes it easy to set up, operate, and scale a relational database in AWS. 
It provides cost-efficient and resizable capacity while automating or outsourcing time-consuming administration tasks such as 
hardware provisioning, database setup, patching and backups.

RDS comes in six different flavors:
SQL Server
Oracle
MySQL Server
PostgreSQL
MariaDB
Aurora


RDS has two key features when scaling out:
-Read replication for improved performance
-Multi-AZ for high availability


You cannot SSH into an RDS instance so therefore you cannot patch the OS. 
This means that AWS is responsible for the security and maintenance of RDS. 

You can provision an EC2 instance as a database if you need or want to manage the underlying server yourself, 
but not with an RDS engine.

Multi-AZ is supported for all DB flavors except aurora. This is because Aurora is completely fault-tolerant on its own.
Multi-AZ feature allows for high availability across availability zones and not regions.

During a failover, the recovered former primary becomes the new secondary and the promoted secondary becomes primary. 
Once the original DB is recovered, there will be a sync process kicked off where the two 
DBs mirror each other once to sync up on the new data that the failed former primary might have missed out on.



Read Replication is exclusively used for performance enhancement.

You can promote read replicas to be their very own production database if needed.
Read replicas are supported for all six flavors of DB on top of RDS.
Each Read Replica will have its own DNS endpoint.
Automated backups must be enabled in order to use read replicas.



RDS supports MySQL, PostgreSQL, MariaDB, Oracle, MS SQL Server, and Amazon Aurora.

IAM Auth is not supported for ElastiCache Redis. It works with both RDS MySQL and RDS PostgreSQL.
For your RDS database, you can have up to 5 Read Replicas.
You can not create encrypted Read Replicas from an unencrypted RDS DB instance.


RDS is a managed service:
• Automated provisioning, OS patching
• Continuous backups and restore to specific timestamp (Point in Time Restore)!
• Monitoring dashboards
• Read replicas for improved read performance
• Multi AZ setup for DR (Disaster Recovery)
• Maintenance windows for upgrades
• Scaling capability (vertical and horizontal)
• Storage backed by EBS (gp2 or io1)
• BUT you can’t SSH into your instances


Backups are automatically enabled in RDS,Automated backups:

• Daily full backup of the database (during the maintenance window)
• Transaction logs are backed-up by RDS every 5 minutes
• => ability to restore to any point in time (from oldest backup to 5 minutes ago)
• 7 days retention (can be increased to 35 days)



ReadReplica:
----------------------------------------------

Up to 5 Read Replicas
• Within AZ, Cross AZ or Cross Region
• Replication is ASYNC, so reads are eventually consistent (My Old data found some time)

Read replicas are used for SELECT (=read) only kind of statements(not INSERT, UPDATE, DELETE)

Multi-AZ keeps the same connection string regardless of which database is up.
Read Replicas add new endpoints with their own DNS name. We need to change our application to reference them individually 
to balance the read load.

Storing Session Data in ElastiCache is a common pattern to ensuring different EC2 instances can retrieve 
your user's state if needed.


Multi AZ:
SYNC replication and One DNS name – automatic app failover to standby
• Increase availability
• Failover in case of loss of AZ, loss of network, instance or storage failure
• No manual intervention in apps
• Not used for scaling
• Multi-AZ replication is free
• Note:The Read Replicas be setup as Multi AZ for Disaster Recovery (DR)
 

RDS – From Single-AZ to Multi-AZ:
• Zero downtime operation (no need to stop the DB)
• Just click on “modify” for the database
• The following happens internally:
• A snapshot is taken
• A new DB is restored from the snapshot in a new AZ
• Synchronization is established between the two databases



RDS Security – Network & IAM
--------------------------------------------
Network Security
• RDS databases are usually deployed within a private subnet, not in a public one
• RDS security works by leveraging security groups (the same concept as for EC2
  instances) – it controls which IP / security group can communicate with RDS
Access Management
• IAM policies help control who can manage AWS RDS (through the RDS API)
• Traditional Username and Password can be used to login into the database
• IAM-based authentication can be used to login into RDS MySQL & PostgreSQL-compatible


Aurora and RDS Security – Summary
--------------------------------------------
Encryption at rest:
• Is done only when you first create the DB instance
• or: unencrypted DB => snapshot => copy snapshot as encrypted => create DB from snapshot
• Your responsibility:
• Check the ports / IP / security group inbound rules in DB’s SG
• In-database user creation and permissions or manage through IAM
• Creating a database with or without public access
• Ensure parameter groups or DB is configured to only allow SSL connections

AWS responsibility:
• No SSH access
• No manual DB patching
• No manual OS patching
• No way to audit the underlying instance

ElastiCache – Redis vs Memcached
----------------------------------------
REDIS:
• Multi AZ with Auto-Failover
• Read Replicas to scale reads and have high availability
• Data Durability using AOF persistence
• Backup and restore features

MEMCACHED:
• Multi-node for partitioning of data (sharding)
• No high availability (replication)
• Non persistent
• No backup and restore
• Multi-threaded architecture


ElastiCache – Cache Security
----------------------------------------
All caches in ElastiCache:
• Do not support IAM authentication
• IAM policies on ElastiCache are only used for AWS API-level security

Redis AUTH
• You can set a “password/token” when you create a Redis cluster
• This is an extra level of security for your cach (on top of security groups)
• Support SSL in flight encryption

Memcached
• Supports SASL-based authentication (advanced)


RDS Backups:
When it comes to RDS, there are two kinds of backups:
-automated backups
-database snapshots

Automated backups allow you to recover your database to any point in time within a retention period (between one and 35 days). 
Automated backups will take a full daily snapshot and will also store transaction logs throughout the day. 
When you perform a DB recovery, RDS will first choose the most recent daily backup and apply the relevant transaction logs from that day. 


DB snapshots are done manually by the administrator. A key different from automated backups is that they are retained even 
after the original RDS instance is terminated. With automated backups, the backed up data in 
S3 is wiped clean along with the RDS engine. 


A relational database system does not scale well for the following reasons:
It normalizes data and stores it on multiple tables that require multiple queries to write to disk.
It generally incurs the performance costs of an ACID-compliant transaction system.
It uses expensive joins to reassemble required views of query results.


Scenario:
=========
-You want full control of OS or need elevated permissions	Consider going for a custom installation (EC2 + EBS)
-You want to migrate data from an on-premise database to cloud database of the same type	Consider using AWS Database Migration Service
-You want to migrate data from one database engine to another (Example : Microsoft SQL Server to Amazon Aurora)	Consider using AWS Schema Conversion Tool
-What are retained when you delete a RDS database instance?	All automatic backups are deleted
-All manual snapshots are retained (until explicit deletion)
(Optional) Take a final snapshot
-How do you reduce global latency and improve disaster recovery?	Use multi region read replicas
-How do you select the subnets a RDS instance is launched into?	Create DB Subnet groups
-How can you add encryption to an unencrypted database instance?	Create a DB snapshot
-Encrypt the database snapshot using keys from KMS
-Create a database from the encrypted snapshot
-Are you billed if you stop your DB instance?	You are billed for storage, IOPS, backups and snapshots. You are NOT billed for DB instance hours
-I will need RDS for at least one year. How can I reduce costs?	Use Amazon RDS reserved instances.
-Efficiently manage database connections	Use Amazon RDS Proxy
-Sits between client applications (including lambdas) and RDS




Let’s add a standby database in the second data center with replication.

Let’s consider some challenges:

Challenge 1 (SOLVED): Your database will go down if the data center crashes
You can switch to the standby database
Challenge 2 (SOLVED): You will lose data if the database crashes
Challenge 3 (SOLVED): Database will be slow when you take snapshots
Take snapshots from standby.
Applications connecting to master will get good performance always


=================================================
##Section 10: Route 53
=================================================


DNS Terminologies:
• Domain Registrar: Amazon Route 53, GoDaddy, …
• DNS Records: A, AAAA, CNAME, NS, …
• Zone File: contains DNS records
• Name Server: resolves DNS queries (Authoritative or Non-Authoritative)
• Top Level Domain (TLD): .com, .us, .in, .gov, .org, …
• Second Level Domain (SLD): amazon.com, google.com



Route 53 – Records:
• How you want to route traffic for a domain
Each record contains:
• Domain/subdomain Name – e.g., example.com
• Record Type – e.g., A or AAAA
• Value – e.g., 12.34.56.78
• Routing Policy – how Route 53 responds to queries
• TTL – amount of time the record cached at DNS Resolvers

Route 53 supports the following DNS record types:
• (must know) A / AAAA / CNAME / NS
• (advanced) CAA / DS / MX / NAPTR / PTR / SOA / TXT / SPF / SRV


Route 53 – Record Types:
• A – maps a hostname to IPv4
• AAAA – maps a hostname to IPv6

CNAME – maps a hostname to another hostname
• The target is a domain name which must have an A or AAAA record
• Can’t create a CNAME record for the top node of a DNS namespace (Zone Apex)
• Example: you can’t create for example.com, but you can create for www.example.com

NS – Name Servers for the Hosted Zone
• Control how traffic is routed for a domain





Route 53 – Alias Records Targets:
--------------------------------------------
• Elastic Load Balancers
• CloudFront Distributions
• API Gateway
• Elastic Beanstalk environments
• S3 Websites
• VPC Interface Endpoints
• Global Accelerator accelerator
• Route 53 record in the same hosted zone
• You cannot set an ALIAS record for an EC2 DNS name


CNAME vs Alias
--------------------------------------------
• AWS Resources (Load Balancer, CloudFront...) expose an AWS hostname:
• lb1-1234.us-east-2.elb.amazonaws.com and you want myapp.mydomain.com

CNAME:
• Points a hostname to any other hostname. (app.mydomain.com => blabla.anything.com)
• ONLY FOR NON ROOT DOMAIN (aka. something.mydomain.com)

Alias:
• Points a hostname to an AWS Resource (app.mydomain.com => blabla.amazonaws.com)
• Works for ROOT DOMAIN and NON ROOT DOMAIN (aka mydomain.com)
• Free of charge
• Native health check 



Route 53 Routing Policies:
--------------------------------------------
• Define how Route 53 responds to DNS queries 
• Don’t get confused by the word “Routing” 
• It’s not the same as Load balancer routing which routes the traffic 
• DNS does not route any traffic, it only responds to the DNS queries 

Route 53 Supports the following Routing Policies 
• Simple 
• Weighted 
• Failover 
• Latency based 
• Geolocation 
• Multi-Value Answer 
• Geoproximity (using Route 53 Traffic Flow feature)




Biull:
=======
1)Route53 Not free, 12dolar per year.
2)You pay $0.50 per month per hosted zone.



EXAMQ:
======
1)About Geoproximity routeing policy




VIP:
====
Weighted Routing Policy allows you to redirect part of the traffic based on weight (e.g., percentage). 
It's a common use case to send part of traffic to a new version of your application.

Each DNS record has a TTL (Time To Live) which orders clients for how long to cache these values and not overload the 
DNS Resolver with DNS requests. The TTL value should be set to strike a balance between how long the value should be cached vs. 
how many requests should go to the DNS Resolver.

Latency Routing Policy will evaluate the latency between your users and AWS Regions, and help them get a 
DNS response that will minimize their latency (e.g. response time)



You have a legal requirement that people in any country but France should NOT be able to access your website. 
Which Route 53 Routing Policy helps you in achieving this?
Geolocation.


You have purchased a domain on GoDaddy and would like to use Route 53 as the DNS Service Provider. 
What should you do to make this work?
Public Hosted Zones are meant to be used for people requesting your website through the Internet. Finally, NS records must be updated on the 3rd party Registrar.




=================================================
##Section 12: Amazon S3 Introduction
=================================================
Amazon S3 Overview – Objects (continued)
• Object values are the content of the body:
• Max Object Size is 5TB (5000GB)
• If uploading more than 5GB, must use “multi-part upload”
• Metadata (list of text key / value pairs – system or user metadata)
• Tags (Unicode key / value pair – up to 10) – useful for security / lifecycle
• Version ID (if versioning is enabled)

S3 Encryption for Objects, There are 4 methods of encrypting objects in S3:
• SSE-S3: encrypts S3 objects using keys handled & managed by AWS
• SSE-KMS: leverage AWS Key Management Service to manage encryption keys
• SSE-C: when you want to manage your own encryption keys
• Client Side Encryption 

Multi-Part Upload is recommended as soon as the file is over 100 MB.
5GB is Max file size to upload.

With SSE-C, the encryption happens in AWS and you have full control over the encryption keys.

With SSE-KMS, the encryption happens in AWS, and the encryption keys are managed by AWS but you 
have full control over the rotation policy of the encryption key. Encryption keys stored in AWS.

With Client-Side Encryption, you have to do the encryption yourself and you have full control over the encryption keys. 
You perform the encryption yourself and send the encrypted data to AWS. 
AWS does not know your encryption keys and cannot decrypt your data.

Explicit DENY in an IAM Policy will take precedence over an S3 bucket policy.


S3 Select & Glacier Select:
• Retrieve less data using SQL by performing server side filtering
• Can filter by rows & columns (simple SQL statements)
• Less network transfer, less CPU cost client-side


S3 Event Notifications with Amazon EventBridge:
• Advanced filtering options with JSON rules (metadata, object size, name...)
• Multiple Destinations – ex Step Functions, Kinesis Streams / Firehose…
• EventBridge Capabilities – Archive, Replay Events, Reliable delivery



S3 – Requester Pays
-------------------------------------------------
• In general, bucket owners pay for all
Amazon S3 storage and data transfer costs associated with their bucket

• With Requester Pays buckets, the
requester instead of the bucket owner pays the cost of the request and the data download from the bucket

• Helpful when you want to share large datasets with other accounts
• The requester must be authenticated in AWS (cannot be anonymous)




MFA Delete forces users to use MFA codes before deleting S3 objects. 
It's an extra level of security to prevent accidental deletions.

S3 Access Logs log all the requests made to S3 buckets and Amazon Athena can then be used to run 
cserverless analytics on top of the log files.

S3 Replication allows you to replicate data from an S3 bucket to another in the same/different AWS Region.

S3 Pre-Signed URLs are temporary URLs that you generate to grant time-limited access to some actions in your S3 bucket.


S3 MFA-Delete:
• MFA (multi factor authentication) forces user to generate a code on a device (usually a
mobile phone or hardware) before doing important operations on S3
• Only the bucket owner (root account) can enable/disable MFA-Delete
• MFA-Delete currently can only be enabled using the CLI


S3 Default Encryption vs Bucket Policies:
• One way to “force encryption” is to use a bucket policy and refuse any
API call to PUT an S3 object without encryption headers

Another way is to use the “default encryption” option in S3
• Note: Bucket Policies are evaluated before “default encryption”

S3 Access Logs: Warning
• Do not set your logging bucket to be the monitored bucket
• It will create a logging loop, and your bucket will grow in size exponentially


S3 Replication – Notes
• After activating, only new objects are replicated
• Optionally, you can replicate existing objects using S3 Batch Replication
• Replicates existing objects and objects that failed replication
• For DELETE operations:
• Can replicate delete markers from source to target (optional setting)
• Deletions with a version ID are not replicated (to avoid malicious deletes)
• There is no “chaining” of replication
• If bucket 1 has replication into bucket 2, which has replication into bucket 3
• Then objects created in bucket 1 are not replicated to bucket 3


S3 Pre-Signed URLs:
• Can generate pre-signed URLs using SDK or CLI
• For downloads (easy, can use the CLI)
• For uploads (harder, must use the SDK)
• Valid for a default of 3600 seconds, can change timeout with --expires-in [TIME_BY_SECONDS] argument
• Users given a pre-signed URL inherit the permissions of the person who generated the URL for GET / PUT
• Examples :
• Allow only logged-in users to download a premium video on your S3 bucket
• Allow an ever changing list of users to download files by generating URLs dynamically
• Allow temporarily a user to upload a file to a precise location in our bucket

=================================================
##Section 15: CloudFront & AWS Global Accelerator
=================================================
CloudFront is a global service.

CloudFront– Origins:
-------------------------------------------------
• S3 bucket 
• For distributing files and caching them at the edge 
• Enhanced security with CloudFront Origin Access Identity (OAI) 
• CloudFront can be used as an ingress (to upload files to S3) 
Custom Origin (HTTP) 
• Application Load Balancer 
• EC2 instance 
• S3 website (must first enable the bucket as a static S3 website) 
• Any HTTP backend you want


CloudFront Geo Restriction:
-------------------------------------------------
• You can restrict who can access your distribution
• Whitelist: Allow your users to access your content only if they're in one of the
countries on a list of approved countries.
• Blacklist: Prevent your users from accessing your content if they're in one of the

countries on a blacklist of banned countries.
• The “country” is determined using a 3rd party Geo-IP database
• Use case: Copyright Laws to control access to content



CloudFront vs S3 Cross Region Replication
-------------------------------------------------
CloudFront:
• Global Edge network
• Files are cached for a TTL (maybe a day)
• Great for static content that must be available everywhere

S3 Cross Region Replication:
• Must be setup for each region you want replication to happen
• Files are updated in near real-time
• Read only
• Great for dynamic content that needs to be available at low-latency in few regions



CloudFront Signed URL / Signed Cookies
-------------------------------------------------
• You want to distribute paid shared content to premium users over the world
• We can use CloudFront Signed URL / Cookie. We attach a policy with:
• Includes URL expiration
• Includes IP ranges to access the data from
• Trusted signers (which AWS accounts can create signed URLs)
• How long should the URL be valid for?
• Shared content (movie, music): make it short (a few minutes)
• Private content (private to the user): you can make it last for years
• Signed URL = access to individual files (one signed URL per file)
• Signed Cookies = access to multiple files (one signed cookie for many files)


CloudFront Signed URL vs S3 Pre-Signed URL
-------------------------------------------------
CloudFront Signed URL:
• Allow access to a path, no matter
the origin
• Account wide key-pair, only the root
can manage it
• Can filter by IP, path, date, expiration
• Can leverage caching features

S3 Pre-Signed URL:
• Issue a request as the person who
pre-signed the URL
• Uses the IAM key of the signing
IAM principal
• Limited lifetime

CloudFront – Origin Groups
• To increase high-availability and do failover
• Origin Group: one primary and one secondary origin
• If the primary origin fails, the second one is used


=================================================
#Subnet | cidr             
================================================= 
Subnet create on availablity zone not on Region. Same subnet can not take more then one.


Public Subnet
 - If a subnet traffic is routed to on Internet Gateway it is public with a public IP.
Private Subnet
 - If a Subnet dosnot have a route to the internet Gateway then it is private.
   When you create a VPC you must specify on IPv4 CIDR blica for the 
   , The allowed blick size is betwwn /16 to /28 netmask.
   The first four and last IP address of Subnet cannot be assigned.
    - Suppose a IP => 10.0.0.0/16
	- 10.0.0.0 Network address
	- 10.0.0.1 Reserved by AWS for VPC Route
	- 10.0.0.2 Reserved By AWS for DNS server
	- 10.0.0.3 Reserved By AWS for future use
	- 10.0.0.255 Brodycast Address
	AWS do not support brodcust in a VPC but reserve the address.
	
	


=================================================
#VPC | vpc | virtual private cloud                
================================================= 

Vpc is a Virtual Network or DataCenter inside AWS for one Client.
A VPC can span multiple availability zones in a region.
The CIDR block for the default VPC is always a 16 subnet mask; in this example, it's 172.31.0.0/16. It means this VPC can provide up to 65,536 IP addresses.

- It is logical Isolated from Other virtual Network in the AWS.
- Max 5 VPC can be created and 200 subnet in 1 VPC.
- We Can allocate Max 5 Elastic IP.
- Once a VPC created DHCP, NACL and Sucurity Group will be created automatically.
- A VPC is confied to on AWS Region and dos not extend between Region.

- Onece the VPC is created, Its CIDR block range cant change.
  (You can create another CIDR make it primary and older one will be Secondary then you can delete fitst one)
- If you need a diffenent CIDR size, create a new VPC.
- The different subnets wihtin a VPC cannot overlap.
- You can expend yor vpc CIDR by adding new/extra IP address.


VPC create on Region not available zone. all property of VPC are Regional becaus its exists on Region. 
One vpc cant extend more then one Region.


VPC Two Type

- Default Vpc
- Custom Vpc

Primary diffent of of both, default vpc has internet gatway custom vpc has not, you can added.

Default VPC has default CIDR, Security Group, NACL and Route table setting.
In Custom VPC has to be create considering its CIDR dos not have Internet Getwat be default.
 

VPC Peering: 
A peering connection can be made between your own VPCs or with a VPC in another AWS account, as long as it is in the same region.

VPCs, but transitive peering is not supported. In other words, VPC A can connect to B and C in the above diagram, 
but C cannot communicate with B unless directly paired.

We can have up to 200 Subnets per Amazon Virtual Private Cloud (VPC).

Monitor VPC by using:
CloudWatch and CloudWatch logs
VPC Flow Logs

#Component of VPC
-------------------------------------------------

 - CIDR and IP address subnet
 - Implied router and Routing Table
 - Internet Gatway
 - Security Group
 - Network ACL
 - Virtual Private gatway 
 - Peering connections
 - Elastic IP




-Each VPC has a main route table, by default
-Main route table has a default route enabling communication between resources in all subnets in a VPC
-Default route rule CANNOT be deleted/edited
-HOWEVER you can add/edit/delete other routing rules to the main route table


#Subnet 
-------------------------------------------------
Amazon defines a route table as a set of rules, called routes, which are used to determine where network traffic is directed.
Each subnet has to be linked to a route table, and a subnet can only be linked to one route table. On the other hand, one route table can have associations with multiple subnets. 


-Each subnet can have its own route table OR share its route table with the VPC
-If a subnet does not have a route table associated with it, it implicitly uses the route table of its VPC
-Multiple subnets can share a route table
-HOWEVER at any point in time, a subnet can be associated with one route table ONLY


Public Subnet:
-Communication is allowed from subnet to internet
-Communication is allowed from internet to subnet


Security products and features:
-Security groups - This acts as a firewall for the EC2 instances, controlling inbound and outbound traffic at the instance level.
-Network access control lists - It acts as a firewall for the subnets, controlling inbound and outbound traffic at the subnet level.
-Flow logs - These capture the inbound and outbound traffic from the network interfaces in your VPC.



=================================================
#VPN | VPNs | vpn             
================================================= 
VPCs can also serve as a bridge between your corporate data center and the AWS cloud. 
With a VPC Virtual Private Network (VPN), your VPC becomes an extension of your on-prem environment.



=================================================
#DirectConnect    | DX       | DC
================================================= 
AWS Direct Connect Private dedicated network connection from on-premises to AWS


Direct Connect is an AWS service that establishes a dedicated network connection between your premises and AWS. 
You can create this private connectivity to reduce network costs, increase bandwidth, and provide more consistent network experience 
compared to regular internet-based connections.

The use case for Direct Connect is high throughput workloads or if you need a stable or reliable connection


DirectConnect connects your on-prem with your VPC through a non-public tunnel.

=================================================
#NAT Gateway        
================================================= 
A Network Address Translation (NAT) device can be used to enable instances in a private subnet to connect to the internet 
or the AWS services, but this prevents the internet from initiating connections with the instances in a private subnet.

However, your private subnet database instance might still need internet access or the ability to connect to other AWS resources. 
You can use a NAT device to do so. 

The NAT device directs traffic from your private subnet to either the internet or other AWS services. 
It then sends the response back to your instances. When traffic is directed to the internet, the source 
IP address of your instance is replaced with the NAT device address, and when the internet traffic returns, 
the NAT device translates the address to your instance’s private IP address.


NAT device is added to the public subnet to get internet connectivity.



AWS provides two kinds of NAT devices:
NAT gateway 
NAT instance 


NAT Gateway
A NAT gateway must be launched in a public subnet because it needs internet connectivity. 
It also requires an elastic IP address, which you can select at the time of launch.

Once created, you need to update the route table associated with your private subnet to point internet-bound traffic to the NAT gateway. 
This way, the instances in your private subnet can communicate with the internet.

=================================================
#AWS PrivateLink
=================================================
In summary, Global Accelerator is a fast/reliable pipeline between user and application.

AWS PrivateLink simplifies the security of data shared with cloud-based applications by eliminating the exposure of 
data to the public Internet. AWS PrivateLink provides private connectivity between different VPCs, AWS services, 
and on-premises applications, securely on the Amazon network.

This is useful because different AWS services often talk to each other over the internet. 
If you do not want that behavior and instead want AWS services to only communicate within the AWS network, use AWS PrivateLink. 



Summary: AWS PrivateLink connects your AWS services with other AWS services through a non-public tunnel.



Network ACL:
-------------------------------------------------

Every subnet in your VPC must be associated with an ACL, failing which the subnet gets automatically associated with your default ACL.

One subnet can only be linked with one ACL. On the other hand, an ACL can be linked to multiple subnets.


Amazon VPC can contain anywhere from 16 to 65,536 IP addresses. You can select your CIDR block according to the number of instances needed.

VPC limits:

Five VPCs per region
200 subnets per VPC
200 route tables per VPC
500 security groups per VPC
50 inbound and outbound rules per VPC


Amazon VPC costs:
If you opt to create a hardware VPN connection associated with your VPC using Virtual Private Gateway, you will have to pay for each 
VPN connection hour that your VPN connection is provisioned and available. Each partial VPN connection hour consumed is 
billed as a full hour. You'll also incur standard AWS data transfer charges for all data transferred via the VPN connection. 

If you create a NAT gateway in your VPC, Charges are levied for each NAT gateway hour that your NAT gateway is 
provisioned and available for. Data processing charges apply for each gigabyte processed through a NAT gateway. 
Each partial NAT gateway hour consumed is billed as a full hour.



=================================================
#BGP |   Border Gateway Protocol          
================================================= 
BGP is a means by which all junction points on the internet (routers) communicate with each other to dynamically establish the 
correct (and correctly weighted) paths that network packets should follow to traverse the global networking

=================================================
#AWS Global Accelerator
=================================================

In summary, Global Accelerator is a fast/reliable pipeline between user and application.


AWS Global Accelerator accelerates connectivity to improve performance and availability for users. 
Global Accelerator sits on top of the AWS backbone and directs traffic to optimal endpoints worldwide. 
By default, Global Accelerator provides you two static IP addresses that you can make use of.

Global Accelerator also provides fast regional failover.




=================================================
#Internet Protocol (IP    
================================================= 
The Internet Protocol (IP) uses three types of addressing schemes: Unicast, Multicast, and Anycast.

Unicast:
A Unicast address is used to identify a single unique host. It is used to send data to a single destination. 
In computer networking, unicast communication is a one-to-one transmission from one point in the network to another. 


Multicast:
A Multicast address is used to deliver data to a group of destinations (a one-to-many transmission). 
IP multicast group addresses are represented by class-D IP addresses reserved specifically for multicast 
communications, ranging from 224.0.0.0 through 239.255.255.255. Any IP packet sent to a multicast address is 
delivered to only those hosts that have joined that particular IP Multicast group, resulting in less network traffic, 
thereby reducing bandwidth and network overhead. If the host hasn’t joined the group, the receiver ignores the packets 
at the hardware level, eliminating platform software resource consumption in that network element. 
IPv6 multicast replaces broadcast addresses that were supported in IPv4. 

Anycast:
Anycast, also known as IP Anycast or Anycast routing, is an IP network addressing scheme that allows multiple servers to share the same 
IP address, allowing for multiple physical destination servers to be logically identified by a single IP address. 
Based on the location of the user request, the anycast routers send it to the server in the network based on a least-cost analysis 
that includes assessing the number of hops, shortest distance, lowest transit cost, and minimum latency measurements to optimize the 
selection of a destination server.




=================================================
#Security Groups    
================================================= 

Security Groups are used to control access (SSH, HTTP, RDP, etc.) with EC2. 
They act as a virtual firewall for your instances to control inbound and outbound traffic. 
When you launch an instance in a VPC, you can assign up to five security groups to 
the instance and security groups act at the instance level, not the subnet level.

Security groups are specific to a single VPC, so you can't share a Security Group 
between multiple VPCs. However, you can copy a Security Group to create a new Security Group with the same 
rules in another VPC for the same AWS Account.


Security Groups are regional and can span AZs, but can't be cross-regional.

You can specify the source of your security group (basically who is allowed to bypass the virtual firewall) 
to be a single /32 IP address, an IP range, or even a separate security group.

You cannot block specific IP addresses with Security Groups (use NACLs instead)


Security Groups are stateful:
-If an outgoing request is allowed, the incoming response for it is automatically allowed.
-If an incoming request is allowed, an outgoing response for it is automatically allowed


=================================================
#WAF   | Web Application Firewall (WAF)
================================================= 
AWS WAF is a web application that lets you allow or block the HTTP(s) requests that are bound for CloudFront, 
API Gateway, Application Load Balancers, EC2, and other Layer 7 entry points into your AWS environment. 
AWS WAF gives you control over how traffic reaches your applications by enabling you to create security 
rules that block common attack patterns, such as SQL injection or cross-site scripting, 
and rules that filter out specific traffic patterns that you can define. 



=================================================
#Storage | DB | db | s3 | efs | ebs           
================================================= 
There are several categories of databases:
Relational (OLTP and OLAP), Document, Key Value, Graph, In Memory among others

Buckets:
-Buckets are a universal namespace, i.e., the bucket names must be unique.
-If uploading of an object to S3 bucket is successful, we receive a HTTP 200 code.
-S3, S3-IA, S3 Reduced Redundancy Storage are the storage classes.
-Encryption is of two types, i.e., Client Side Encryption and Server Side Encryption
-Access to the buckets can be controlled by using either ACL (Access Control List) or bucket policies.
-By default buckets are private and all the objects stored in a bucket are also private.


Amazon Simple Storage Service (Amazon S3) is an object storage service that offers 
industry-leading scalability, data availability, security, and performance.

Amazon S3 is an object storage service that stores data as objects within buckets. 
An object is a file and any metadata that describes the file. A bucket is a container for objects.
Each object has a key (or key name), which is the unique identifier for the object within the bucket.

Access control lists (ACLs)
You can use ACLs to grant read and write permissions to authorized users for individual buckets and objects. 
Each bucket and object has an ACL attached to it as a subresource. 
The ACL defines which AWS accounts or groups are granted access and the type of access.

Amazon S3 cloud storage is an object-based storage service. You cannot install an operating system when you use 
Amazon S3 storage because data cannot be accessed on the block level as it is required by an operating system.


AWS Storage:
  - Simple Storage service (S3)
  - Elastic file system (EFS)
  - Elastic Block Storage (EBS)
  - Glacier
  - Snowball
  
  
   
#Simple Storage Service (S3)
--------------------------------------------------
S3 Object base storage, its able to access via http/https.
Its a distrubute database and data keep in bucket.

TypeOfS3:
 - S3 Standard
 - Amazon Glacher
 - Glacher Deep archive
 - Standart infrequental acc
 - One zone IA
 - Intelactual
 
 
 
#Amazon S3 Replication
--------------------------------------------------

Replication is the automatic, asynchronous copying of objects across buckets in the same or different AWS Regions. 
Replication copies newly created objects and object updates from a source bucket to a destination bucket or buckets. 

When you configure replication, you add replication rules to the source bucket. 
Replication rules define which source bucket objects to replicate and the destination bucket or buckets where 
the replicated objects are stored. You can create a rule to replicate all the objects in a bucket or a subset of 
objects with a specific key name prefix, one or more object tags, or both. A destination bucket can be in the 
same AWS account as the source bucket, or it can be in a different account.

If you specify an object version ID to delete, Amazon S3 deletes that object version in the source bucket. 
But it doesn't replicate the deletion in the destination bucket. In other words, 
it doesn't delete the same object version from the destination bucket. This protects data from malicious deletions.

When you add a replication rule to a bucket, the rule is enabled by default, so it starts working as soon as you save it.

Amazon Simple Storage Service (S3) Replication is an elastic, fully managed, low cost feature that replicates objects 
between buckets. S3 Replication offers the most flexibility and functionality in cloud storage, giving you the controls you need to 
meet your data sovereignty and other business needs.

With Amazon S3 Replication, you can configure Amazon S3 to automatically replicate S3 objects across different 
AWS Regions by using S3 Cross-Region Replication (CRR) or between buckets in the same AWS Region by using S3 
Same-Region Replication (SRR). S3 Replication offers the flexibility of replicating to multiple destination buckets in the same, 
or different AWS Regions. S3 Replication supports two-way replication between two or more buckets in the same or different AWS Regions. 


 
#S3 security
-------------------------------------------------

Security responsibility includes the following areas:

-Managing your data, including object ownership and encryption.
-Classifying your assets.
-Managing access to your data using IAM roles and other service configurations to apply the appropriate permissions.
-Enabling detective controls such as AWS CloudTrail or Amazon GuardDuty for Amazon S3.


Server-Side Encryption:
Request Amazon S3 to encrypt your object before saving it on disks in its data centers and then decrypt it 
when you download the objects. Server-side encryption can help reduce risk to your data by encrypting 
the data with a key that is stored in a different mechanism than the mechanism that stores the data itself.

Amazon S3 provides these server-side encryption options:
-Server-side encryption with Amazon S3‐managed keys (SSE-S3).
-Server-side encryption with KMS key stored in AWS Key Management Service (SSE-KMS).
-Server-side encryption with customer-provided keys (SSE-C).


Client-Side Encryption:

Encrypt data client-side and upload the encrypted data to Amazon S3. In this case, you manage the encryption process, 
the encryption keys, and related tools. As with server-side encryption, client-side encryption can help 
reduce risk by encrypting the data with a key that is stored in a different mechanism than the mechanism that stores the data itself.




#EFS - Elastic File System
-------------------------------------------------
Simple, serverless, set-and-forget, elastic file system

Amazon Elastic File System (Amazon EFS) provides a simple, serverless, set-and-forget elastic file system for use with AWS Cloud services and on-premises resources. 

Amazon EFS provides a simple, serverless, set-and-forget elastic file system.
With Amazon EFS, you can create a file system, mount the file system on an Amazon EC2 instance,
and then read and write data to and from your file system. 
You can mount an Amazon EFS file system in your virtual private cloud (VPC), through the 
Network File System versions 4.0 and 4.1 (NFSv4) protocol.



How do I access a file system from an Amazon EC2 instance?
To access your file system, you mount the file system on an Amazon EC2 Linux-based instance using the standard Linux mount command 
and the file system’s DNS name. 
Once you’ve mounted, you can work with the files and directories in your file system just like you would with a local file system.



#Object Lifecycle Management
------------------------------------------------
A lifecycle configuration is a set of rules that define actions that AWS S3 applies to a group of objects. There are two types of actions:

Transaction Actions
This action defines objects’ transition from one storage class to another.

Expiration Actions
This action deletes objects in the Amazon S3 bucket.


#Amazon DynamoDB
------------------------------------------------
Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. 
It's a fully managed, multiregion, multimaster, durable non-SQL database. It comes with built-in security, backup and restore, 
and in-memory caching for internet-scale applications.


The main components of DynamoDB are:

-a collection which serves as the foundational table
-a document which is equivalent to a row in a SQL database
-key-value pairs which are the fields within the document or row

Amazon DynamoDB is a Serverless key-value and document database that delivers single-digit millisecond performance at any scale. 
It's a fully managed, multiregion, multimaster, durable database with built-in security, backup and restore, 
and in-memory caching for internet-scale applications. DynamoDB can handle more than 10 trillion requests 
per day and can support peaks of more than 20 million requests per second.

Amazon DynamoDB is a fully managed, serverless, key-value NoSQL database designed to run high-performance applications at any scale. 
DynamoDB offers built-in security, continuous backups, automated multi-Region replication, in-memory caching, and data export tools.


DynamoDB is accessible via an HTTP API and performs authentication & authorization via IAM roles, 
making it a perfect fit for building Serverless applications.


DAX:
===
DynamoDB Accelerator (DAX) delivers fast response times for accessing eventually consistent data. Certain usecases requires microseconds response times




#Amazon Aurora
------------------------------------------------
Amazon Aurora (Aurora) is a fully managed relational database engine that's compatible with MySQL and PostgreSQL.
The underlying storage grows automatically as needed. An Aurora cluster volume can grow to a maximum size of 128 tebibytes (TiB). 


Amazon Aurora is a relational database management system (RDBMS) built for the cloud with full MySQL and PostgreSQL compatibility. 
Aurora gives you the performance and availability of commercial-grade databases at one-tenth the cost.

Amazon Aurora provides built-in security, continuous backups, serverless compute, 
up to 15 read replicas, automated multi-Region replication, and integrations with other AWS services.

Amazon Aurora is a modern relational database service offering performance and high availability at scale, fully open source 
MySQL- and PostgreSQL-compatible editions, and a range of developer tools for 
building serverless and machine learning (ML)-driven applications.

Aurora features a distributed, fault-tolerant, and self-healing storage system that is 
decoupled from compute resources and auto-scales up to 128 TB per database instance. 
It delivers high performance and availability with up to 15 low-latency read replicas, 
point-in-time recovery, continuous backup to Amazon Simple Storage Service (Amazon S3), 
and replication across three Availability Zones (AZs).


#Data Warehouse
-------------------------------------------------
A data warehouse is a specialized type of relational database, which is optimized for
analysis and reporting of large amounts of data. It can be used to combine
transactional data from disparate sources (such as user behavior in a web application,
data from your finance and billing system, or customer relationship management or
CRM) to make them available for analysis and decision-making.

Traditionally, setting up, running, and scaling a data warehouse has been complicated
and expensive. On AWS, you can leverage Amazon Redshift, a managed data
warehouse service that is designed to operate at less than a tenth the cost of
traditional solutions.


#Redshift
-------------------------------------------------
Fastest, easiest, and most widely used cloud data warehouse

Amazon Redshift is a fully managed data warehouse service in the cloud. Its datasets range from 
100s of gigabytes to a petabyte. The initial process to create a data warehouse is to launch a 
set of compute resources called nodes, which are organized into groups called cluster. 
After that you can process your queries.

Amazon Redshift is a fully managed, scalable cloud data warehouse that accelerates your time to insights with fast, easy, 
and secure analytics at scale. Thousands of customers rely on 
Amazon Redshift to analyze data from terabytes to petabytes and run complex analytical queries.

Redshift is not multi-AZ, if you want multi-AZ you will need to spin up a separate cluster ingesting the same input. 
You can also manually restore snapshots to a new AZ in the event of an outage.


What are the differences between a database and a data warehouse? 
A database is any collection of data organized for storage, accessibility, and retrieval. 

A data warehouse is a type of database the integrates copies of transaction data 
from disparate source systems and provisions them for analytical use.


#Storage Gateway 
-------------------------------------------------
AWS Storage Gateway - Summary:
Key to look for : Hybrid storage (cloud + on premise)

File share (NFS or SMB) + Looking for S3 features and integrations => AWS Storage File Gateway

Tapes on cloud => AWS Storage Tape Gateway

Volumes on cloud (Block Storage) => AWS Storage Volume Gateway

High performance => Stored
Otherwise => Cached

Needs additional setup on-premises

VM image with AWS Storage Gateway software deployed on-premises or on EC2 instance





=================================================
# Edge locations             
================================================= 

Edge locations are AWS data centers designed to deliver services with the lowest latency possible.
Amazon has dozens of these data centers spread across the world. They’re closer to users than Regions or Availability Zones, 
often in major cities, so responses can be fast and snappy. 

UserCase:

CloudFront, 
which uses edge locations to cache copies of the content that it serves, so the content is closer to users and can be delivered to them faster.
Route 53, 
which serves DNS responses from edge locations, so that DNS queries that originate nearby can resolve faster (and, contrary to what you might think, is also Amazon’s premier database).
Web Application Firewall and AWS Shield, 
which filter traffic in edge locations to stop unwanted traffic as soon as possible.




=================================================
#ElastiCache  | Elasti Cache      
================================================= 

Amazon ElastiCache is a fully managed, in-memory caching service supporting flexible, real-time use cases. 
You can use ElastiCache for caching, which accelerates application and database performance, 
or as a primary data store for use cases that don't require durability like 
session stores, gaming leaderboards, streaming, and analytics. ElastiCache is compatible with Redis and Memcached. 


Use cases:
Accelerate application performance
Ease backend database load
Build low-latency data stores


Amazon ElastiCache is a web service that makes it easy to deploy and run Memcached or Redis 
protocol-compliant server nodes in the cloud. Amazon ElastiCache improves the performance of web applications by 
allowing you to retrieve information from a fast, managed, in-memory system, instead of relying entirely on slower disk-based databases. 

=================================================
#CloudFront  | cludFont      
================================================= 
Global Accelerator and CloudFront both use the AWS global network and its edge locations around the world.

CloudFront is Amazon’s content delivery network that is primarily used to speed up websites. 
It’s particularly useful for large, static assets—like images and videos. CloudFront sits in front of an “origin” server 
(which serves the original content), and caches it at the edge locations around the world.

When a user visits a site, they’re routed to the nearest edge location using DNS. 
CloudFront looks to see if the page they requested is cached. If it is, the page is served directly from the cache. 
If it isn’t, CloudFront fetches the page from the origin, stores it in the cache, and serves it to the user. 
The next user to hit the same edge location will get the page served from the cache.


The AWS CDN service is called CloudFront. It serves up cached content and assets for the increased global performance of your application. 
The main components of CloudFront are the edge locations (cache endpoints), the origin (original source of truth to be cached 
such as an EC2 instance, an S3 bucket, an Elastic Load Balancer or a Route 53 config), and the distribution (the arrangement 
of edge locations from the origin or basically the network itself).


Performance
CloudFront improves performance for both cacheable content (such as images and videos) and dynamic content (such as API acceleration and dynamic site delivery).
Global Accelerator improves performance for a wide range of applications over TCP or UDP by proxying packets at the edge to applications running in one or more AWS Regions.

Use Cases
CloudFront is a good fit for HTTP use cases
Global Accelerator is a good fit for non-HTTP use cases, such as gaming (UDP), IoT (MQTT), or VoIP, as well as for HTTP use cases that require static IP addresses or deterministic, fast regional failover.

Caching
CloudFront supports Edge caching
Global Accelerator does not support Edge Caching.

Geo-Targeting:
Geo-Targeting is a concept where businesses can show personalized content to their audience based on their geographic location 
without changing the URL. This helps you create customized content for the audience of a specific 
geographical area, keeping their needs in the forefront.



=================================================
#AWS Global Accelerator (AMS SSPS)          
================================================= 

Global Accelerator, your users' traffic is moved off the internet and onto Amazon’s private global network through 90+ global edge locations, then directed to your application origins. 

AWS Global Accelerator is a networking service that 
improves the performance of your users’ traffic by up to 60% using Amazon Web Services’ global network infrastructure.

Global Accelerator, you are provided two global static public IPs that act as a fixed entry point to your application, 
improving availability. On the back end, add or remove your AWS application endpoints, such as Application Load Balancers, 
Network Load Balancers, EC2 Instances, and Elastic IPs without making user-facing changes.

Global Accelerator automatically re-routes your traffic to your nearest healthy available endpoint to mitigate endpoint failure.


It provides static IP addresses that act as a fixed entry point to application endpoints in a single or multiple AWS Regions, such as Application Load Balancers, Network Load Balancers or EC2 instances.



=================================================
#Amazon FSx       
================================================= 

Amazon FSx for Windows File Server provides a fully managed native Microsoft File System.
You can use Microsoft Active Directory to authenticate into the file system.


Amazon FSx is a fully managed third-party file system solution. It uses SSD storage to provide fast performance with low latency.
There are four available FSx solutions available in AWS:

Amazon FSx makes it easy and cost effective to launch, run, and scale feature-rich, high-performance file systems in the cloud. 
It supports a wide range of workloads with its reliability, security, scalability, and broad set of capabilities. 

Four widely-used file systems: 
 - NetApp ONTAP, 
 - OpenZFS, 
 - Windows File Server 
 - Lustre.
 
 
NetApp ONTAP
In collaboration with NetApp, AWS has launched Amazon FSx for NetApp ONTAP, a new cloud-based managed shared file and block storage 
service that brings the best of both worlds to their customers.
FSx for ONTAP delivers NFS, SMB and iSCSI storage powered by NetApp’s advanced data management system
 
Lustre
Amazon FSx for Lustre offers fully-managed storage built especially to provide high-performance at scale for compute workloads. 
It is ideal for machine learning, video rendering, high performance computing and financial simulations.



Windows File Server:
FSx for Windows File Server provides fully managed Microsoft Windows file servers, that are backed by a fully native Windows file system. 


FSx has two key differentiators compared to other Amazon’s previous file service offerings such as Elastic File Service (EFS). 
It comes with a complete file server built in, and it offers superior performance for demanding use cases.

Amazon FSx offers file systems designed for a variety of workload types. 
You can use AWS FSx as storage for Windows applications, machine learning (ML) and high-performance computing (HPC). 
FSx can also help with electronic design automation.


You can deploy your Amazon FSx for Windows in a single AZ or in a Multi-AZ configuration.
By default, all data is encrypted at rest.


=================================================
#Amazon FSx for Lustre
================================================= 
Amazon FSx for Lustre makes it easy and cost effective to launch and run the open source Lustre file system for high-performance 
computing applications. With FSx for Lustre, you can launch and run a file system that can process massive data sets at up to 
hundreds of gigabytes per second of throughput, millions of IOPS, and sub-millisecond latencies.


FSx for Lustre is compatible with the most popular Linux-based AMIs, including Amazon Linux, 
Amazon Linux 2, Red Hat Enterprise Linux (RHEL), CentOS, SUSE Linux and Ubuntu.

Since the Lustre file system is designed for high-performance computing workloads that typically run on compute clusters, 
choose EFS for normal Linux file system if your requirements don't match this use case.

FSx Lustre has the ability to store and retrieve data directly on S3 on its own.



=================================================
#SQS, SNS, MQ and Amazon Kinesis        
================================================= 


Anytime multiple services need to receive the same event, you should consider SNS rather than SQS.

#AWS SQS
--------------------------------------------------
 The entire service is based on sending messages to the queue and allowing for applications (ex. ECS containers, Lambda functions)
 to poll for messages and process them. 
 The message stays in the queue until some application picks it up, processes it, and deletes the message when it’s done. 
 

#AWS SNS(Simple Notification Service)
--------------------------------------------------
It provides much more functionality than just the ability to send push notifications (emails, SMS, and mobile push). 
In fact, it’s a serverless publish-subscribe messaging system allowing to send events to multiple applications (subscribers) at the same time (fan-out), 
including SQS queues, Lambda functions, Kinesis Data Streams, and generic HTTP endpoints. 


In order to use the service, we only need to:
create a topic,
subscribe to a topic,
confirm the subscription,
start sending events to a topic to deliver them to all subscribers (potentially multiple applications and people).

#Amazon Kinesis 
--------------------------------------------------

Amazon Kinesis Data Streams is a serverless streaming data service that makes it easy to capture, process, 
and store data streams at any scale.
Amazon Kinesis makes it easy to collect, process, and analyze real-time, streaming data so you can get timely 
insights and react quickly to new information.

Amazon Kinesis is a managed, scalable, cloud-based service that allows real-time processing of streaming large amount of data per second. 
It is designed for real-time applications and allows developers to take in any amount of data from several sources, 
scaling up and down that can be run on EC2 instances.

It is used to capture, store, and process data from large, distributed streams such as event logs and social media feeds. 
After processing the data, Kinesis distributes it to multiple consumers simultaneously.

The producers continually push data to Kinesis Data Streams, and the consumers process the data in real time. 
Consumers (such as a custom application running on Amazon EC2 or an Amazon Kinesis Data Firehose delivery stream) 
can store their results using an AWS service such as Amazon DynamoDB, Amazon Redshift, or Amazon S3.

#MQ
--------------------------------------------------
Amazon MQ is a managed message broker service for Apache ActiveMQ and RabbitMQ that makes 
it easy to set up and operate message brokers in the cloud. 
You get direct access to the ActiveMQ and RabbitMQ consoles and industry standard APIs and protocols for messaging, 
including JMS, NMS, AMQP 1.0 and 0.9.1, STOMP, MQTT, and WebSocket. 
You can easily move from any message broker that uses these standards to Amazon MQ because you 
don’t have to rewrite any messaging code in your applications.



How do I migrate if I'm using a different message broker instead of ActiveMQ or RabbitMQ?
Amazon MQ provides compatibility with the most common messaging APIs, such as Java Message Service (JMS) and 
.NET Message Service (NMS), and protocols, including AMQP, STOMP, MQTT, and WebSocket. 
This makes it easy to switch from any standards-based message broker to Amazon MQ without rewriting the messaging code in your applications. 
In most cases, you can simply update the endpoints of your Amazon MQ broker to connect to your existing applications, 
and start sending messages.





=================================================
#CloudWatch  | cloudWatch      
================================================= 

Amazon CloudWatch is a monitoring and observability service built for DevOps engineers, developers, site reliability engineers (SREs), 
IT managers, and product owners. 

CloudWatch provides you with data and actionable insights to monitor your applications, respond to system-wide performance changes, 
and optimize resource utilization. 

CloudWatch collects monitoring and operational data in the form of logs, metrics, and events. 

CloudWatch collects monitoring and operational data in the form of logs, metrics, and events.
You can use CloudWatch to detect anomalous behavior in your environments, set alarms, visualize logs and metrics side by side, 
take automated actions, troubleshoot issues, and discover insights to keep your applications running smoothly.

You get a unified view of operational health and gain complete visibility of your AWS resources, applications, and services running on 
AWS and on-premises. You can use CloudWatch to detect anomalous behavior in your environments, set alarms, 
visualize logs and metrics side by side, take automated actions, troubleshoot issues, and discover insights to 
keep your applications running smoothly.
 
 - Use a single platform for observability
 - Collect metrics on AWS and on premises
 - Improve operational performance and resource optimization
 - Get operational visibility and insight
 - Derive actionable insights from logs
 
 
You can use metrics to calculate statistics and then present the data graphically in the CloudWatch console. 
You can configure alarm actions to stop, start, or terminate an Amazon EC2 instance when certain criteria are met.
 
Use cases:
-Monitor Amazon EC2
-Monitor Other Amazon Web Services Resources
-Monitor Custom Metrics
-Monitor and Store Logs
-Set Alarms
-Monitor and React to Resource Changes


CloudWatch is NOT CloudTrail so it is important to know that only CloudTrail can monitor AWS access for security and auditing reasons. 
CloudWatch is all about performance. CloudTrail is all about auditing.


=================================================
#AWS Event and EventBridge
================================================= 

#Event
--------------------------------------------------
An event indicates a change in an environment such as an AWS environment, a SaaS partner service or application, 
or one of your applications or services. The following are examples of events:

Amazon EC2 generates an event when the state of an instance changes from pending to running.

Amazon EC2 Auto Scaling generates events when it launches or terminates instances.

AWS CloudTrail publishes events when you make API calls.

You can also set up scheduled events that are generated on a periodic basis.

Events are represented as JSON objects and they all have a similar structure, and the same top-level fields.


#EventBridge
--------------------------------------------------
EventBridge is an event bus for messages that you want to propagate across your (micro)services. 
Those events can come from state changes of AWS services, other AWS accounts, or external 
applications like Auth0, Shopify, and others. You can, of course, also send your custom messages.

EventBridge delivers a stream of real-time data from event sources such as Zendesk or Shopify to targets like 
AWS Lambda and other SaaS applications. You can set up routing rules to determine 
where to send your data to build application architectures that react in real-time to your data sources 
with event publisher and consumer completely decoupled.

EventBridge sends metrics to Amazon CloudWatch every minute for everything from the number of 
matched events to the number of times a target is invoked by a rule.


In event-driven architecture, services interact with each other through events. 
An event is something that happened in your application (for example, an item was put into a cart, a new order was placed). 
Events are JSON objects that tell you information about something that happened in your application. 
In event-driven architecture, each component of the application raises an event whenever anything changes. 
Other components listen and decide what to do with it and how they would like to react.


#Glue
--------------------------------------------------

AWS Glue is a fully managed ETL (extract, transform, and load) service that makes it simple and cost-effective to categorize your data, 
clean it, enrich it, and move it reliably between various data stores and data streams. 


Athena uses the AWS Glue Data Catalog to store and retrieve table metadata for the Amazon S3 data in your Amazon Web Services account. 
The table metadata lets the Athena query engine know how to find, read, and process the data that you want to query.


Data integration is the process of preparing and combining data for analytics, machine learning, and application development. 
It involves multiple tasks, such as discovering and extracting data from various sources; enriching, cleaning, normalizing, 
and combining data; and loading and organizing data in databases, data warehouses, and data lakes. These tasks are 
often handled by different types of users that each use different products.

AWS Glue provides both visual and code-based interfaces to make data integration easier. 
Users can easily find and access data using the AWS Glue Data Catalog.


Glue uses ETL jobs to extract data from a combination of other Amazon Web Services and incorporates 
it into data lakes and data warehouses. It uses application programming interfaces (APIs) to transform the e
xtracted data set for integration, and to help users monitor jobs.

Users can put ETL jobs on a schedule or pick events that will trigger a job. 
Once triggered, Glue extracts the data, transforms it based on code that Glue generates automatically, 
and loads it into Amazon S3 or Amazon Redshift. Glue then writes metadata from the job into the AWS Glue Data Catalog.


AWS Glue DataBrew is a visual data preparation tool that enables users to clean and normalize data without writing any code. 

Simplify data preparation (capturing metadata) for analytics:
Connect AWS Glue to your data on AWS (Aurora, RDS, Redshift, S3 etc)
AWS Glue creates a AWS Glue Data Catalog with metadata abstracted from your data
Your data is ready for searching and querying






#CloudTrail 
--------------------------------------------------
CloudTrail logs actions inside your AWS environment.CloudTrail has a feature called CloudTrail Insights.
Insights let you detect unusual API activities on your account by automation.

AWS CloudTrail is an AWS service that helps you enable governance, compliance, and operational and risk auditing of your AWS account. 
Actions taken by a user, role, or an AWS service are recorded as events in CloudTrail. 
Events include actions taken in the AWS Management Console, AWS Command Line Interface, and AWS SDKs and APIs.

CloudTrail is enabled on your AWS account when you create it. 
When activity occurs in your AWS account, that activity is recorded in a CloudTrail event. 
You can easily view recent events in the CloudTrail console by going to Event history. 
For an ongoing record of activity and events in your AWS account, create a trail.



AWS CloudTrail is a service that automatically records events such as AWS API calls. 
You can create EventBridge rules that use the information from CloudTrail. 
For more information about CloudTrail, see What is AWS CloudTrail?.


There are two types of events that can be logged in CloudTrail: management events and data events.

Management events provide information about management operations that are performed on resources in your AWS account.
Think of Management events as things normally done by people when they are in AWS. 

Examples:
a user sign in
a policy changed
a newly created security configuration
a logging rule deletion


Data events provide information about the resource operations performed on or in a resource.
Think of Data events as things normally done by software when hitting various AWS endpoints. 

Examples:
S3 object-level API activity
Lambda function execution activity
By default, CloudTrail logs management events, but not data events.



Track events, API calls, changes made to your AWS resources:
-Who made the request?
-What action was performed?
-What are the parameters used?
-What was the end result?


#AWS Config | AWSConfig 
-------------------------------------------------
This helps you understand the configuration changes that happen in your environment. 
This service provides an AWS inventory that includes configuration history, configuration change notification, 
and relationships between AWS resources. It can also be configured to send information via AWS SNS when new logs are delivered.




#CloudFront
-------------------------------------------------

Amazon Cloudfront is a content delivery network (AWS CDN) that retrieves data stored in the 
Amazon S3 bucket and distributes it to numerous edge locations across the world. 
Edge locations are the network of data centers distributed worldwide through which content is delivered.

If the content is already cached in the edge location, CloudFront delivers it immediately with the lowest latency possible.
If the content is not present in the edge location, CloudFront retrieves it from the origin 
(like Amazon S3 bucket, a MediaPackage channel, or an HTTP server) that has been identified for your content.

Securely deliver content with low latency and high transfer speeds

Amazon CloudFront speeds up distribution of your static and dynamic web content, such as .html, .css, .php, image, and media files. 
When users request your content, CloudFront delivers it through a worldwide network of 
edge locations that provide low latency and high performance.

AWS CloudFront is a globally-distributed network offered by Amazon Web Services, 
which securely transfers content such as software, SDKs, videos, etc., to the clients, with high transfer speed.


Use cases:
-Deliver fast, secure websites
-Accelerate dynamic content delivery and APIs
-Stream live and on-demand video
-Distribute patches and updates



Scenario: Restrict content to users in certain countries
	Enable CloudFront Geo restriction
	Configure White list(countries to be allowed) and Blacklist(countries to be blocked)


=================================================
#Route 53 | Route53 
================================================= 
Amazon Route 53 is a highly available and scalable Domain Name System (DNS) service. 
You can use Route 53 to perform three main functions in any combination: domain registration, DNS routing, and health checking.


When you buy a domain name, every DNS address starts with an SOA (Start of Authority) record. 
The SOA record stores information about the name of the server that kicked off the transfer of ownership, 
the administrator who will now use the domain, the current metadata available, and the default number of seconds or TTL.

NS records, or Name Server records, are used by the Top Level Domain hosts (.org, .com, .uk, etc.) 
to direct traffic to the Content servers. The Content DNS servers contain the authoritative DNS records.

Browsers talk to the Top Level Domains whenever they are queried and encounter domain name that they do not recognize.
Browsers will ask for the authoritative DNS records associated with the domain.

Because the Top Level Domain contains NS records, the TLD can in turn queries the Name Servers for their own SOA.
Within the SOA, there will be the requested information.

Once this information is collected, it will then be returned all the way back to the original browser asking for it.



In summary: Browser -> TLD -> NS -> SOA -> DNS record. The pipeline reverses when the correct DNS record is found.


The routing policies available are:
-Simple Routing
-Weighted Routing
-Latency-based Routing
-Failover Routing
-Geolocation Routing
-Geo-proximity Routing
-Multivalue Answer Routing



================================================= 
# Encryption  | Security
================================================= 

#Symmetric Key 
-------------------------------------------------
Symmetric encryption algorithms use the same key for encryption and decryption
-Key Factor 1: Choose the right encryption algorithm
-Key Factor 2: How do we secure the encryption key?
-Key Factor 3: How do we share the encryption key?


#Asymmetric Key Encryption
-------------------------------------------------

Two Keys : Public Key and Private Key Also called Public Key Cyptography.

Encrypt data with Public Key and decrypt with Private Key
Share Public Key with everybody and keep the Private Key with you(YEAH, ITS PRIVATE!)

No crazy questions:
Will somebody not figure out private key using the public key?

How do you create Asymmetric Keys



#Amazon Cognito
-------------------------------------------------

Amazon Cognito provides authentication, authorization, and user management for your web and mobile apps. 
Your users can sign in directly with a user name and password, or through a third party such as Facebook, Amazon, Google or Apple.


Amazon Cognito lets you add user sign-up, sign-in, and access control to your web and mobile apps quickly and easily. 
Amazon Cognito scales to millions of users and supports sign-in with social identity providers, such as Apple, Facebook, Google, 
and Amazon, and enterprise identity providers via SAML 2.0 and OpenID Connect.  


Amazon Cognito is designed for developers who want to add user management and sync functionality to their mobile and web apps. 
Developers can use Cognito Identity to add sign-up and sign-in to their apps and to enable their 
users to securely access their app’s resources. 
Cognito also enables developers to sync data across devices, platforms, and applications.



#STS(Security Token Service)
-------------------------------------------------
WS STS is an AWS service that allows you to request temporary security credentials for your AWS resources, 
for IAM authenticated users and users that are authenticated in AWS such as federated users via OpenID or SAML2.0.

You use STS to provide trusted users with temporary access to resources via API calls, your AWS console or 
the AWS command line interface (CLI)


AWS provides AWS Security Token Service (AWS STS) as a web service that enables you to request temporary, 
limited-privilege credentials for AWS Identity and Access Management (IAM) users or for users you authenticate (federated users).

Recording API requests:
AWS STS supports AWS CloudTrail, a service that records AWS calls for your AWS account and delivers log files to an Amazon S3 bucket. 
By using information collected by CloudTrail, you can determine the requests successfully sent to AWS STS, 
as well as who sent the request, and when it was sent.

The STS token lifecycle is determined by you and can be anywhere from 15 minutes to 36 hours.

External web identities can be authenticated by a third party online identity manager like amazon, google, 
facebook or any other open-id connect compatible service. This web identity federation also removes the need to 
distribute long-term security credentials to 
facilitate access to your AWS resources.


Use-Case:
-Identity Federation Use-Case
-Cross-Account Access using AWS STS
-EC2 Instance STS Credentials


#KMS AWS Key Management Service 
-------------------------------------------------

Amazon S3 uses AWS KMS keys to encrypt your Amazon S3 objects. AWS KMS encrypts only the object data.
 
 
KMS is a managed service that makes it easy for you to create and control the encryption keys used to encrypt your data. 
AWS KMS uses Hardware Security Modules (HSMs) to protect the security of your keys. 
You can use AWS KMS to protect your data in AWS services and in your applications

AWS Key Management Service (AWS KMS) makes it easy for you to create and manage cryptographic keys and control their use across a 
wide range of AWS services and in your applications.

AWS KMS is integrated with AWS CloudTrail to provide you with logs of all key usage to help meet your regulatory and compliance needs.

AWS KMS Key Management Service is a useful and very beneficial service while dealing with sensitive data and it 
also makes it easy for you to create and manage cryptographic keys.




AWS KMS keys must be in the same Region as the bucket.




Features of AWS KMS:
----------------------------------------------------
It is an easy way to control and access your data using managed encryption.

With AWS Key Management Service, the process of key management is reduced to a few simple clicks.

It is also integrated with other AWS services including Amazon EBS, Amazon S3, and Amazon RedShift to 
simplify the encryption of your data within these services.

AWS KMS enables you to create, rotate, disable, enable, and define usage policies for master keys and audit their usage.

It is a centralized key management

It is secure and compliant.



CloudHSM
----------------------------------------------------
AWS CloudHSM is a cloud-based hardware security module (HSM) that enables you to easily 
generate and use your own encryption keys on the AWS Cloud. 

CloudHSM is standards-compliant and enables you to export all of your keys to most other
commercially-available HSMs, subject to your configurations. 


The AWS CloudHSM service helps you meet corporate, contractual, and regulatory compliance 
requirements for data security by using dedicated 
Hardware Security Module (HSM) instances within the AWS cloud. 

AWS CloudHSM provides hardware security modules in the AWS Cloud. A hardware security module (HSM) is a computing device that 
processes cryptographic operations and provides secure storage for cryptographic keys.



When you use an HSM from AWS CloudHSM, you can perform a variety of cryptographic tasks:

Generate, store, import, export, and manage cryptographic keys, including symmetric keys and asymmetric key pairs.

Use symmetric and asymmetric algorithms to encrypt and decrypt data.

Use cryptographic hash functions to compute message digests and hash-based message authentication codes (HMACs).

Cryptographically sign data (including code signing) and verify signatures.

Generate cryptographically secure random data.



Q: What is a Hardware Security Module (HSM)?

A Hardware Security Module (HSM) provides secure key storage and cryptographic operations within a 
tamper-resistant hardware device. HSMs are designed to securely store cryptographic 
key material and use the key material without exposing it outside the cryptographic boundary of the hardware.


Q: What can I do with CloudHSM?

You can use the CloudHSM service to support a variety of use cases and applications, 
such as database encryption, Digital Rights Management (DRM), Public Key Infrastructure (PKI), 
authentication and authorization, document signing, and transaction processing.

There are no upfront costs to use AWS CloudHSM. With CloudHSM, you pay an hourly fee for each HSM you launch until you terminate the HSM.


#Transit Gateway (TGW)
-------------------------------------------------
AWS Transit Gateway connects your Amazon Virtual Private Clouds (VPCs) and on-premises networks 
through a central hub. This simplifies your network and puts an end to complex peering relationships. 
It acts as a cloud router – each new connection is only made once.

Transit Gateway is a Regional resource and can connect thousands of VPCs within the same AWS Region. 
You can create multiple Transit Gateway instances per Region, and you can 
connect to a maximum of three Transit Gateway instances over a single Direct Connect connection for hybrid connectivity.



As your cloud infrastructure expands globally you need to find out a way to connect your resources which are in different VPCs. 
A Transit Gateway is a network hub that you can use to interconnect your virtual private clouds (VPCs) and on-premises networks. 

It is like a hub and spoke design or star topology design for connecting VPCs and on-premises networks. 
Transit Gateway allows customers to connect thousands of VPCs together. It is a regional service. 
It gives you simplified connectivity to the multiple VPC as compared to a complex VPC peering connection. 

Traffic between VPC and Transit Gateway remains on the AWS global private network and is not exposed to the public internet. 
Transit Gateways in different regions can peer with each other to enable VPC communications across regions. 
Transit Gateway inter-Region peering encrypts all traffic, with no single point of 
failure or bandwidth bottleneck which helps you to get improved security.


#ECMP
-------------------------------------------------
Equal-cost multi-path routing (ECMP) is a routing strategy where packet forwarding to a single destination can occur over multiple best paths with equal routing priority. 
Multi-path routing can be used in conjunction with most routing protocols because it is a per-hop local decision made independently at each router.


AWS Transit Gateway VPN supports ECMP protocol that can load balance traffic across multiple VPN tunnels. 
The question is, can Transit Gateway ECMP be used to deploy a transit DMZ as shown in the diagram below?


#Traffic Mirroring
-------------------------------------------------

Traffic Mirroring copies inbound and outbound traffic from the network interfaces that are attached to your instances. 
You can send the mirrored traffic to the network interface of another instance, a Network Load Balancer that has a UDP listener, 
or a Gateway Load Balancer that has a UDP listener. The traffic mirror source and the traffic mirror target (monitoring appliance) 
can be in the same VPC. Or they can be in a different VPCs that are connected 
through intra-Region VPC peering, a transit gateway, or by a Gateway Load Balancer endpoint to connect to a 
Gateway Load Balancer in a different VPC.


Traffic Mirroring is an Amazon VPC feature that you can use to copy network traffic from an elastic network interface of type interface. 
You can then send the traffic to out-of-band security and monitoring appliances for:

-Content inspection
-Threat monitoring
-Troubleshooting




#Directory Service
-------------------------------------------------
Microsoft AD is a Microsoft Active Directory hosted on the AWS Cloud. It integrates most Active Directory features with AWS applications.



AWS Directory Service lets you run Microsoft Active Directory (AD) as a managed service. 
AWS Directory Service for Microsoft Active Directory, also referred to as AWS Managed Microsoft AD, 
is powered by Windows Server 2012 R2. When you select and launch this directory type, 
it is created as a highly available pair of domain controllers connected to your virtual 
private cloud (VPC). The domain controllers run in different Availability Zones in a Region of your choice. 
Host monitoring and recovery, data replication, snapshots, and software updates are automatically configured and managed for you.

With AWS Managed Microsoft AD, you can run directory-aware workloads in the AWS Cloud, 
including Microsoft SharePoint and custom .NET and SQL Server-based applications. 
You can also configure a trust relationship between AWS Managed Microsoft AD in the AWS Cloud and your existing 
on-premises Microsoft Active Directory, providing users and groups with access to resources in either domain, 
using single sign-on (SSO).

AWS Directory Service makes it easy to set up and run directories in the AWS Cloud, or connect your 
AWS resources with an existing on-premises Microsoft Active Directory. Once your directory is created, 
you can use it for a variety of tasks:

-Manage users and groups
-Provide single sign-on to applications and services
-Create and apply group policy
-Simplify the deployment and management of cloud-based Linux and Microsoft Windows workloads
-You can use AWS Managed Microsoft AD to enable multi-factor authentication by integrating with your 
 existing RADIUS-based MFA infrastructure to provide an additional layer of security when users access AWS applications.
-Securely connect to Amazon EC2 Linux and Windows instances




AWS introduced AWS Directory Service for Microsoft Active Directory (Standard Edition), 
also known as AWS Microsoft AD (Standard Edition), which is managed Microsoft Active Directory (AD) 
that is performance optimized for small and midsize businesses. AWS Microsoft AD (Standard Edition) 
offers you a highly available and cost-effective primary directory in the 
AWS Cloud that you can use to manage users, groups, and computers.


AWS Managed Microsoft AD makes it easy to migrate AD-dependent applications and Windows workloads to AWS. 
With AWS Managed Microsoft AD, you can use Group Policies to manage EC2 instances and run AD-dependent applications in the 
AWS Cloud without the need to deploy your own AD infrastructure


#AWS Workspaces
-------------------------------------------------
-Desktop-as-a-Service (DaaS)
--Provision Windows or Linux desktops in minutes
-Eliminate traditional desktop management - Virtual Desktop Infrastructure (VDI)



#AWS Shield
-------------------------------------------------

Shields from Distributed Denial of Service (DDoS) attacks
Disrupt normal traffic of a server by overwhelming it with a flood of Internet traffic
Protect
-Amazon Route 53
-Amazon CloudFront
-AWS Global Accelerator
-Amazon Elastic Compute Cloud (EC2) instances
-Elastic Load Balancers (ELB)

=================================================
#HPC ENA, EFA      
=================================================

 
#Elastic Fabric Adapter (EFA)
-------------------------------------------------
An Elastic Fabric Adapter (EFA) is a network device that you can attach to your Amazon EC2 instance to accelerate 
High Performance Computing (HPC) and machine learning applications. EFA enables you to achieve the application performance of 
an on-premises HPC cluster, with the scalability, flexibility, and elasticity provided by the AWS Cloud.

Elastic Fabric Adapter (EFA) is a network interface for Amazon EC2 instances that enables customers to run applications 
requiring high levels of inter-node communications at scale on AWS. 
Its custom-built operating system (OS) bypass hardware interface enhances the performance of inter-instance communications, 
which is critical to scaling these applications. 


You can create, use, and manage an EFA much like any other elastic network interface in Amazon EC2. 
However, unlike elastic network interfaces, EFAs cannot be attached to or detached from an instance in a running state.

Elastic Fabric Adapter (EFA) is a network interface for Amazon EC2 instances that enables customers to run HPC applications 
requiring high levels of inter-instance communications, like computational fluid dynamics, weather modeling, 
and reservoir simulation, at scale on AWS. It uses a custom-built operating system bypass technique to enhance the 
performance of inter-instance communications, which is critical to scaling HPC applications. With EFA, 
HPC applications using popular HPC technologies like Message Passing Interface (MPI) can scale to thousands of CPU cores. 
EFA supports industry-standard libfabric APIs, so applications that use a supported MPI library can be 
migrated to AWS with little or no modification.

 
#ENA (Elastic Network Adapter)
-------------------------------------------------
The Elastic Network Adapter (ENA) is designed to improve operating system health and reduce 
the chances of long-term disruption because of unexpected hardware behavior and or failures. 
The ENA architecture keeps device or driver failures as transparent to the system as possible. 
This topic provides troubleshooting information for ENA.

#With EC2
The Elastic Network Adapter (ENA) driver publishes network performance metrics from the instances where they are enabled. 
You can use these metrics to troubleshoot instance performance issues, choose the right instance size for a workload, 
plan scaling activities proactively, and benchmark applications to determine whether they maximize the
performance available on an instance.

#HPC (High Performance Computing)
-------------------------------------------------
Run your large, complex simulations and deep learning workloads in the cloud with a complete suite of 
high performance computing (HPC) products and services on AWS. Gain insights faster, and quickly
move from idea to market with virtually unlimited compute capacity, a high-performance file system,
and high-throughput networking.


#ParallelCluster 
-------------------------------------------------
AWS ParallelCluster is an open source cluster management tool that makes it easy for you to deploy and manage High Performance 
Computing (HPC) clusters on AWS. ParallelCluster uses a simple text file to model and provision all the 
resources needed for your HPC applications in an automated and secure manner. It also supports multiple 
instance types and job submission queues, and job schedulers like AWS Batch and Slurm.

AWS ParallelCluster is built on the popular open source CfnCluster project and is released via the Python Package Index (PyPI). 
ParallelCluster's source code is hosted on the Amazon Web Services repository on GitHub. 
AWS ParallelCluster is available at no additional charge, and you pay only for the AWS resources needed to run your applications.




=================================================
#CloudFormation         
=================================================

AWS CloudFormation is an AWS service that uses template files to automate the setup of AWS resources.

An AWS CloudFormation template is a formatted text file in JSON or YAML language that describes your AWS infrastructure. 
To create, view and modify templates, you can use AWS CloudFormation Designer or any text editor tool. 
An AWS CloudFormation template consists of nine main objects:

Format version: Format version defines the capability of a template.
Description: Any comments about your template can be specified in the description.
Metadata: Metadata can be used in the template to provide further information using JSON or YAML objects. 

CloudFormation is an infrastructure automation platform for AWS that deploys AWS resources in a repeatable, testable and auditable manner.
AWS CloudFormation provides users with a simple way to create and manage a collection of Amazon Web Services (AWS)
resources by provisioning and updating them in a predictable way. AWS CloudFormation enables you to manage your 
complete infrastructure or AWS resources in a text file.



You can use CloudFormation to automate the configuration of workloads that run on the most popular AWS services, 
like the EC2 compute service, the S3 storage service, and the IAM service for configuring access control.

You can also apply CloudFormation templates to AWS services that cater to niche use cases, like Ground Station, 
the AWS satellite management solution.

In general, if a service runs on AWS, it is a safe bet that you can use CloudFormation to automate its configuration and deployment.

It is worth noting that CloudFormation is not the only way to configure and deploy services on AWS. 
You can handle these processes manually using the AWS command-line interface, API, or Web console. 

AWS CloudFormation enables you to manage your complete infrastructure or AWS resources in a text file, or template. 
A collection of AWS resources is called a stack. AWS resources can be created or updated by using a stack.


HowToWork:
-Create or use an existing CloudFormation template using JSON or YAML format.
-Save the code in an S3 bucket, which serves as a repository for the code.
-Use AWS CloudFormation to call the bucket and create a stack on your template. 
-CloudFormation reads the file and understands the services that are called, their order, 
the relationship between the services, and provisions the services one after the other.



#StepFunction
-------------------------------------------------

AWS Step Functions is a serverless orchestration service that lets developers create and manage multi-step 
application workflows in the cloud. By using the service’s drag-and-drop visual editor, 
teams can easily assemble individual microservices into unified workflows. At each step of a given workflow, 
Step Functions manages input, output, error handling, and retries, so that developers can focus on 
higher-value business logic for their applications.




#EMR
-------------------------------------------------

With it, organizations can process and analyze massive amounts of data.

Amazon Elastic MapReduce (Amazon EMR) is a web service that makes it easy to quickly and cost-effectively process vast amounts of data.
Amazon EMR is the industry-leading cloud big data platform for processing vast amounts of data using 
open source tools such as Apache Spark, Apache Hive, Apache HBase, Apache Flink, Apache Hudi, and Presto. 
Amazon EMR makes it easy to set up, operate, and scale your big data environments 
by automating time-consuming tasks like provisioning capacity and tuning clusters and uses 
Hadoop, an open source framework, to distribute your data and processing across a resizable cluster of Amazon EC2 instances. 




3
#GraphQL:
------------------------------------------------
GraphQL is a query language for APIs and a runtime for fulfilling those queries with your existing data. 
GraphQL provides a complete and understandable description of the data in your API, gives clients the 
power to ask for exactly what they need and nothing more, makes it easier to evolve APIs over time, 
and enables powerful developer tools.

Send a GraphQL query to your API and get exactly what you need, nothing more and nothing less. 
GraphQL queries always return predictable results. Apps using GraphQL are fast and stable because they 
control the data they get, not the server.


#AppSync
-------------------------------------------------


The fundamental idea of a GraphQL API is that all API functionality is available via a unified query language 
(the Graph Query Language) under a single endpoint. Rather than making requests to various endpoints to get 
different parts of the data needed to build a webpage, developers can issue a single request to a 
GraphQL API and immediately get back all the data they need. This model reduces the complexity of 
web applications and improves the experience for website visitors with faster load times.

AWS AppSync is a fully managed GraphQL API layer developed by Amazon Web Services. 
AppSync allows developers to build GraphQL APIs without much of the usual work; it handles the parsing and resolution of 
requests as well as connecting to other AWS services like AWS Lambda, NoSQL and SQL data stores, and HTTP APIs to gather 
backend data for the API.

AWS AppSync is a fully managed AWS serverless service for real-time data queries, synchronization, and communications. 
In AppSync, AWS has a GraphQL-as-a-Service offering that makes it easy to build scalable and resilient GraphQL APIs in the cloud.

With AppSync, you can build scalable applications, including those requiring real-time updates, on a range of data sources 
such as NoSQL data stores, relational databases, HTTP APIs, and your custom data sources with Amazon Lambda. 
For mobile and web apps, AppSync additionally provides local data access when devices go offline, 
and data synchronization with customizable conflict resolution, when they are back online.

AWS AppSync is a serverless GraphQL and Pub/Sub API service that simplifies building modern web and mobile applications.

AWS AppSync GraphQL APIs simplify application development by providing a single endpoint to securely query or update data from multiple databases, microservices, and APIs.

AWS AppSync Pub/Sub APIs make it easy to create engaging real-time experiences by automatically publishing data updates to subscribed API clients via serverless WebSockets connections. 



=================================================
#Elastic Beanstalk        
================================================= 

AWS Elastic Beanstalk allows you to quickly deploy applications and services without having to worry about 
configuring underlying resources, services, operating systems or web servers.

Elastic Beanstalk takes care of the hosting infrastructure, coding language interpreter, operating system, 
security, https service and application layer. All you need to worry about is writing your code.

You can develop code in a number of languages which is then zipped up and the zip file is used 
when instantiating a new elastic beanstalk instance.

Elastic Beanstalk is a complete application management solution, and manages all infrastructure and platform tasks on your behalf.

When using Elastic Beanstalk as your deployment solution, simply upload your source code and 
Elastic Beanstalk will provision and operate all necessary infrastructure, including servers, databases, 
load balancers, networks, and auto scaling groups. Although these resources are created on your behalf, 
you retain full control of these resources, allowing developers to customize as needed.

AWS Elastic Beanstalk, you can quickly deploy and manage applications in the AWS Cloud without worrying about the 
infrastructure that runs those applications. AWS Elastic Beanstalk reduces management complexity 
without restricting choice or control. You simply upload your application, and AWS Elastic Beanstalk 
automatically handles the details of capacity provisioning, load balancing, scaling, and application health monitoring.


Elastic Beanstalk is the fastest and simplest way to deploy your application on AWS. You simply use the AWS Management Console, 
a Git repository, or an integrated development environment (IDE) such as Eclipse or Visual Studio to upload your application, 
and Elastic Beanstalk automatically handles the deployment details of capacity 
provisioning, load balancing, auto-scaling, and application health monitoring. 
Within minutes, your application will be ready to use without any infrastructure or resource configuration work on your part.



AWS Elastic Beanstalk provides an environment that makes it easy to deploy and run applications in the cloud.
AWS Elastic Beanstalk is combined with the developer tools to help you manage the lifecycle of your applications.


=================================================
#Global Accelerator      
================================================= 

Global Accelerator is a network layer service in which you create accelerators to improve availability and performance for internet applications used by a global audience. 


With Global Accelerator, you are provided two global static public IPs that act as a fixed entry point to your application, 
improving availability. On the back end, add or remove your AWS application endpoints, such as Application Load Balancers, 
Network Load Balancers, EC2 Instances, and Elastic IPs without making user-facing changes. 
Global Accelerator automatically re-routes your traffic to your nearest healthy available endpoint to mitigate endpoint failure.


It provides static IP addresses that act as a fixed entry point to application endpoints in a 
single or multiple AWS Regions, such as Application Load Balancers, Network Load Balancers or EC2 instances.



Uses the AWS global network to optimize the path from users to applications, improving the performance of TCP and UDP traffic.

AWS Global Accelerator continually monitors the health of application endpoints and will detect an unhealthy endpoint and redirect 
traffic to healthy endpoints in less than 1 minute.


#RTO and RPO
-------------------------------------------------
Among the components of a DR plan are two key parameters that define how long your business can afford to be offline and how
much data loss it can tolerate. These are the Recovery Time Objective (RTO) and Recovery Point Objective (RPO).


RTO:
RTO is the goal your organization sets for the maximum length of time it should take to restore normal 
operations following an outage or data loss.

RTO refers to how much time an application can be down without causing significant damage to the business. 
Some applications can be down for days without significant consequences. 

RTO is not simply the duration of time between loss and recovery. The objective also accounts for the steps 
IT must take to restore the application and its data.



RPO:
RPO is your goal for the maximum amount of data the organization can tolerate losing. 
This parameter is measured in time: from the moment a failure occurs to your last valid data backup. 
For example, if you experience a failure now and your last full data backup was 24 hours ago, the RPO is 24 hours. 


Recovery point objectives refer to your company’s loss tolerance: the amount of data that can be lost before significant 
harm to the business occurs. 

If you back up all or most of your data in regularly scheduled 24-hour increments, then in the worst-case scenario you 
will lose 24 hours’ worth of data.

For example, if you have a 4-hour RPO for an application then you will have a maximum 4-hour gap between backup and data loss.
Having a 4-hour RPO does not necessarily mean you will lose 4 hours’ worth of data.


=================================================
#AWS Organizations  
================================================= 
Organizations typically have multiple AWS accounts
Different business units
Different environments

How do you centralize your management (billing, access control, compliance and security) across multiple AWS accounts?
Welcome AWS Organizations!
Organize accounts into Organizational Units (OU)
Provides API to automate creation of new accounts


Features:
-One consolidated bill for all AWS accounts
-Centralized compliance management for AWS Config Rules
-Send AWS CloudTrail data to one S3 bucket (across accounts)
-AWS Firewall Manager to manage firewall rules (across accounts)
-AWS WAF, AWS Shield Advanced protections and Security Groups
-Use Service control policies (SCPs) to define restrictions for actions (across accounts):
-Prevent users from disabling AWS Config or changing its rules
-Require Amazon EC2 instances to use a specific type
-Require MFA to stop an Amazon EC2 instance
-Require a tag upon resource creation



=================================================
#AWS Trusted Advisor
================================================= 
Recommendations for cost optimization, performance, security and fault tolerance
-Red - Action recommended Yellow - investigate and Green - Good to go
-All AWS customers get 4 checks for free:
-Service limits (usage > 80%)
-Security groups having unrestricted access (0.0.0.0/0)
-Proper use of IAM
-MFA on Root Account
-Business or Enterprise AWS support plan provides over 50 checks
-Disable those you are not interested in
-How much will you save by using Reserved Instances?
-How does your resource utilization look like? Are you right sized?


AWS Directory Service
=================================================
#AWS Well-Architected  | Architected Framework      
================================================= 

The AWS Well-Architected Framework is based on five pillars:
-operational excellence, 
-security, 
-reliability, 
-performance efficiency
-cost optimization


DevOps 



=================================================
#DevOps | CI  |CD | cicd | DevOps - CI, CD Tools       
================================================= 

Tools:
-CodeCommit - Private source control (Git)
-CodePipeline - Orchestrate CI/CD pipelines
-CodeBuild - Build and Test Code (application packages and containers)
-CodeDeploy - Automate Deployment (EC2, ECS, Elastic Beanstalk, EKS, Lambda etc)






=================================================
#Managed Services8 | IAAS| PAAS       
================================================= 

AWS Managed Service Offerings:

-Elastic Load Balancing - Distribute incoming traffic across multiple targets
-AWS Elastic Beanstalk - Run and Manage Web Apps
-Amazon Elastic Container Service (ECS) - Containers orchestration on AWS
-AWS Fargate - Serverless compute for containers
-Amazon Elastic Kubernetes Service (EKS) - Run Kubernetes on AWS
-Amazon RDS - Relational Databases - MySQL, Oracle, SQL Server etc



IAAS:

IAAS (Infrastructure as a Service):
IAAS (Infrastructure as a Service) is all about using only infrastructure from cloud provider. It is also called “Lift and Shift”. Example: Using EC2 to deploy your applications or databases

With IAAS, you are responsible for:

Application Code and Runtime
Configuring load balancing
Auto scaling
OS upgrades and patches
Availability






PAAS:

PAAS (Platform as a Service)
PAAS (Platform as a Service) is all about using a platform provided by cloud

Cloud provider is responsible for:

OS (incl. upgrades and patches)
Application Runtime
Auto scaling, Availability & Load balancing etc..
You are responsible for:

Application code
Configuration

Examples of PAAS
CAAS (Container as a Service): Containers instead of Applications
FAAS (Function as a Service) or Serverless: Functions instead of Applications



=================================================
#Shared Responsibility Model         
================================================= 
Security & Compliance is shared responsibility between AWS and customer



Amazon EC2 instances is Infrastructure as a Service (IaaS).
You are responsible for:
Guest OS (incl. security patches)
Application software installed
Configuring Security Groups (or firewalls)


AWS is responsible for infrastructure layer only.


Amazon S3 & DynamoDB are managed services.

AWS manages infrastructure layer, OS, and platform.

You are responsible for
-Managing your data
-Managing security of data at rest(encryption)
-Managing security of data in transit
-Mandating SSL/HTTPS
-Using the right network - AWS global network or dedicated private network when possible
-Managing access to the service
-Configure right permissions (IAM users/roles/user policies/resource policies)
(FOR AWS RDS) Managing in database users
-Configuring the right security groups (control inbound and outbound traffic)
-Disabling external access (public vs private)


=================================================
# API Gateway       
================================================= 
How about a fully managed service with auto scaling that can act as a “front door” to your APIs? Welcome “Amazon API Gateway”

Amazon API Gateway helps you to “publish, maintain, monitor, and secure APIs at any scale”

You can authorize users by integrating with:

AWS IAM (for AWS users using signature version 4)
Amazon Cognito
Lambda authorizer (custom authorization with JWT tokens or SAML)

Features:
-Integrates with AWS Lambda, Amazon EC2, Amazon ECS or any web application
-Supports HTTP(S) and WebSockets (two way communication - chat apps and streaming dashboards)
-Serverless. Pay for use (API calls and connection duration)
-Provides API Lifecycle Management for RESTful APIs and WebSocket APIs
-You can Run multiple versions of the same API
-Supports Rate Limits(request quota limits), throttling and fine-grained access permissions using API Keys for Third-Party Developers
-Lifecycle management for REST APIs
-Versioning and multiple environments
-API keys - Generate API keys to monitor usage
-Implement plans and quota limits for external applications (or developer)
-WARNING - Do NOT use API keys for Authorization
-Enable caching for API calls with TTL
-Protect backends by throttling requests
-Integrates with
-Amazon CloudWatch - Performance metrics, API calls, latency data and error rates
-Amazon CloudWatch Logs - Debug logging
-WS CloudTrail - Complete history of changes to your REST API




=================================================
#Amazon Elastic Container Service (ECS)      
================================================= 

Amazon Elastic Container Service (Amazon ECS) is a fully managed service for container orchestration.


Amazon ECR is a Fully-managed Docker container registry provided by AWS. Its an alternative to Docker Hub.



AWS Fargate is a serverless option.

-Service: Allows you to run and maintain a specified number (the “desired count”) of tasks
-ECS cluster: Grouping of one or more container instances (EC2 instances) where you run your tasks
-Container Instance - EC2 instance in the cluster running a container agent (helps it communicate with the cluster)
-AWS provides ECS ready AMIs with container agents pre-installed.


Remember:
-AWS Fargate does NOT give you visibility into the EC2 instances in the cluster.
-You can use On-Demand instances or Spot instances to create your cluster.
-You can load balance using Application Load Balancers
-Two features of ALB are important for ECS:
-Dynamic host port mapping: Multiple tasks from the same service are allowed per EC2 (container) instance
-Path-based routing: Multiple services can use the same listener port on same ALB and be routed based on path (www.app.com/microservice-a and www.app.com/microservice-b)


Elastic Beanstalk:
Single container or multiple containers in same EC2 instance
Recommended for simple web applications

Amazon ECS:
AWS specific solution for container orchestration
Ideal for microservices


Amazon Fargate:
Serverless version of Amazon ECS
You want to run microservices and you don’t want to manage the cluster


Amazon EKS:
AWS managed service for Kubernetes
Recommended if you are already using Kubernetes and would want to move the workload to AWS



=================================================
#ML | machine-learning
================================================= 


#SageMaker
-------------------------------------------------
Amazon SageMaker is a cloud-based machine-learning platform that helps users create, design, train, tune, and deploy machine-learning models in a production-ready hosted environment. The AWS SageMaker comes with a pool of advantages (know all about it in the next section)




#FAQ
================================================= 
Amazon EC2 Auto Scaling FAQs:
================================================= 


21) How is AWS CloudFormation different from AWS Elastic Beanstalk?
Here are some differences between AWS CloudFormation and AWS Elastic Beanstalk:
AWS CloudFormation helps you provision and describe all of the infrastructure resources that are present in your cloud environment. 
On the other hand, AWS Elastic Beanstalk provides an environment that makes it easy to deploy and run applications in the cloud.

AWS CloudFormation supports the infrastructure needs of various types of applications, like legacy applications and 
existing enterprise applications. On the other hand, AWS Elastic Beanstalk is combined with the developer tools to
help you manage the lifecycle of your applications.



22) What are the elements of an AWS CloudFormation template?
AWS CloudFormation templates are YAML or JSON formatted text files that are comprised of five essential elements, they are:
-Template parameters
-Output values
-Data tables
-Resources
-File format versi


23) What happens when one of the resources in a stack cannot be created successfully?
If the resource in the stack cannot be created, then the CloudFormation automatically rolls back and terminates all the 
resources that were created in the CloudFormation template. This is a handy feature 
when you accidentally exceed your limit of Elastic IP addresses or don’t have access to an EC2 AMI.



23) What Is Identity and Access Management (IAM) and How Is It Used?
Identity and Access Management (IAM) is a web service for securely controlling access to AWS services. 
IAM lets you manage users, security credentials such as access keys, and permissions that control which 
AWS resources users and applications can access.




24) What are the policies that you can set for your users’ passwords?
Here are some of the policies that you can set:
You can set a minimum length of the password, or you can ask the users to add at least one number or special characters in it.
You can assign requirements of particular character types, including uppercase letters, 
lowercase letters, numbers, and non-alphanumeric characters.
You can enforce automatic password expiration, prevent reuse of old passwords, and request for a password 
reset upon their next AWS sign in.
You can have the AWS users contact an account administrator when the user has allowed the password to expire. 



25) Can AWS Config aggregate data across different AWS accounts?
Yes, you can set up AWS Config to deliver configuration updates from different accounts to one S3 bucket, 
once the appropriate IAM policies are applied to the S3 bucket.



26) What happens to my Amazon EC2 instances if I delete my ASG?
If you have an EC2 Auto Scaling group (ASG) with running instances and you choose to delete the ASG, the instances will be 
terminated and the ASG will be deleted.



27) How do I know when EC2 Auto Scaling is launching or terminating the EC2 instances in an EC2 Auto Scaling group?
When you use Amazon EC2 Auto Scaling to scale your applications automatically, 
it is useful to know when EC2 Auto Scaling is launching or terminating the EC2 instances in your EC2 Auto Scaling group. 
Amazon SNS coordinates and manages the delivery or sending of notifications to subscribing clients or endpoints. 



28) What is a launch configuration?
A launch configuration is a template that an EC2 Auto Scaling group uses to launch EC2 instances.
You can specify your launch configuration with multiple EC2 Auto Scaling groups. However, you can only specify one launch configuration 
for an EC2 Auto Scaling group at a time, and you can't modify a launch configuration after you've created it. 

Therefore, if you want to change the launch configuration for your EC2 Auto Scaling group, 
you must create a launch configuration and then update your EC2 Auto Scaling group with the new launch configuration. 

When you change the launch configuration for your EC2 Auto Scaling group, any new instances are launched using the new 
configuration parameters, but existing instances are not affected. You can see the launch configurations section of the 
EC2 Auto Scaling User Guide for more details.



29) What happens if a scaling activity causes me to reach my Amazon EC2 limit of instances?
Amazon EC2 Auto Scaling cannot scale past the Amazon EC2 limit of instances that you can run. 
If you need more Amazon EC2 instances, complete the Amazon EC2 instance request form.



30) Can EC2 Auto Scaling groups span multiple AWS regions?
EC2 Auto Scaling groups are regional constructs. They can span Availability Zones, but not AWS regions.



31) How can I implement changes across multiple instances in an EC2 Auto Scaling group?
You can use AWS CodeDeploy or CloudFormation to orchestrate code changes to multiple instances in your EC2 Auto Scaling group.



32) If I have data installed in an EC2 Auto Scaling group, and a new instance is dynamically created later, is the data copied over to the new instances?
Data is not automatically copied from existing instances to new instances. You can use lifecycle hooks to copy the data, or an Amazon RDS database including replicas.



33) When I create an EC2 Auto Scaling group from an existing instance, does it create a new AMI (Amazon Machine Image)?
When you create an Auto Scaling group from an existing instance, it does not create a new AMI. 



34) What are lifecycle hooks?
Lifecycle hooks let you take action before an instance goes into service or before it gets terminated. 
This can be especially useful if you are not baking your software environment into an Amazon Machine Image (AMI). 



35) Which health check type should I select?
If you are using Elastic Load Balancing (ELB) with your group, you should select an ELB health check. 
If you’re not using ELB with your group, you should select the EC2 health check.



36) How do I control which instances Amazon EC2 Auto Scaling terminates when scaling in, and how do I protect data on an instance?
You can configure this through the use of a termination policy. You can also use instance protection to prevent 
Amazon EC2 Auto Scaling from selecting specific instances for termination when scaling in. 

If you have data on an instance, 
and you need that data to be persistent even if your instance is scaled in, then you can use a service like S3, RDS, or DynamoDB, 
to make sure that it is stored off the instance.



37) How long is the turn-around time for Amazon EC2 Auto Scaling to spin up a new instance at inService state after 
detecting an unhealthy server?
The turnaround time is within minutes. The majority of replacements happen within less than 5 minutes, and on average 
it is significantly less than 5 minutes. It depends on a variety of factors, including how long it takes to boot up the AMI 
of your instance.



38) If Elastic Load Balancing (ELB) determines that an instance is unhealthy, and moved offline, 
will the previous requests sent to the failed instance be queued and rerouted to other instances within the group?

When ELB notices that the instance is unhealthy, it will stop routing requests to it. 
However, prior to discovering that the instance is unhealthy, some requests to that instance will fail.


39) If you don’t use Elastic Load Balancing (ELB) how would users be directed to the other servers in a group if there was a failure?
You can integrate with Route53 (which Amazon EC2 Auto Scaling does not currently support out of the box, but many customers use).
You can also use your own reverse proxy, or for internal microservices, can use service discovery solutions.


================================================= 
#Security:
================================================= 

40) Are CloudWatch agents automatically installed on EC2 instances when you create an Amazon EC2 Auto Scaling group?
If your AMI contains a CloudWatch agent, it’s automatically installed on EC2 instances when you create an EC2 Auto Scaling group. With the stock Amazon Linux AMI, you need to install it (recommended, via yum).


Cost Optimization:
=================

41) Can I create a single ASG to scale instances across different purchase options?
Yes. You can provision and automatically scale EC2 capacity across different EC2 instance types, Availability Zones, and On-Demand, RIs and Spot purchase options in a single Auto Scaling Group. 


42)Can I use ASGs to launch and manage just Spot Instances or just On-Demand instances and RIs?
Yes. You can configure your ASG specifying all capacity to be only Spot instances or all capacity to be only On-Demand instances and RIs.


43) Can I specify instances of different sizes (CPU cores, memory) in my Auto Scaling group?
Yes. You can specify any instance type available in a region. Additionally, you can specify an optional weight for each instance type, which defines the capacity units that each instance would contribute to your application’s performance.


44) What are the costs for using Amazon EC2 Auto Scaling?
Amazon EC2 Auto Scaling fleet managment for EC2 instances carries no additional fees. The dynamic scaling capabilities of 
Amazon EC2 Auto Scaling are enabled by Amazon CloudWatch and also carry no additional fees. 
Amazon EC2 and Amazon CloudWatch service fees apply and are billed separately.




================================================= 
Amazon Elastic Container Registry FAQs:
================================================= 

1) What is Amazon Elastic Container Registry (Amazon ECR)?
Amazon ECR is a fully managed container registry that makes it easy for developers to share and deploy container images and artifacts. Amazon ECR is integrated with Amazon Elastic Container Service (Amazon ECS),  Amazon Elastic Kubernetes Service (Amazon EKS), and AWS Lambda, simplifying your development to production workflow.


2) Is Amazon ECR a global service?
Amazon ECR is a Regional service and is designed to give you flexibility in how images are deployed. You have the ability to push/pull images to the same AWS Region where your Docker cluster runs for the best performance. 


3) What is the difference between Amazon ECR public and private repositories?
A private repository does not offer content search capabilities and requires Amazon IAM-based authentication using 
AWS account credentials before allowing images to be pulled. A public repository has descriptive content and allows 
anyone anywhere to pull images without needing an AWS account or using IAM credentials. Public repository 
images are also available in the Amazon ECR public gallery.


4) What compliance capabilities can I enable on Amazon ECR?
You can use AWS CloudTrail on Amazon ECR to provide a history of all API actions such as who pulled an image and 
when tags were moved between images. Administrators can also find which EC2 instances pulled which images.


5) Can I access Amazon ECR inside a VPC?
Yes. You can set up AWS PrivateLink endpoints to allow your instances to pull images from your private repositories without traversing through the public internet.


6)Does Amazon ECR work with AWS Elastic Beanstalk?
Yes. AWS Elastic Beanstalk supports Amazon ECR for both single and multi-container Docker environments, allowing you to easily 
deploy container images stored in Amazon ECR with AWS Elastic Beanstalk. All you need to do is specify the 
Amazon ECR repository in your Dockerrun.aws.json configuration and attach the 
AmazonEC2ContainerRegistryReadOnly policy to your container instance role.


7) Does Amazon ECR scan container images for vulnerabilities?
You can enable Amazon ECR to automatically scan your container images for a broad range of operating system vulnerabilities. 
You can also scan images using an API command, and Amazon ECR will notify you over API and in the console when a scan completes. 
For enhanced image scanning, you can turn on Amazon Inspector.




================================================= 
Amazon EC2 Container Service FAQ:
================================================= 

1) What is Amazon Elastic Container Service?
Amazon Elastic Container Service (ECS) is a highly scalable, high performance container management service that supports 
Docker containers and allows you to easily run applications on a managed cluster of Amazon Elastic Compute Cloud (Amazon EC2) instances.

Amazon ECS makes it easy to use containers as a building block for your applications by eliminating the need for you to install, 
operate, and scale your own cluster management infrastructure. Amazon ECS lets you schedule long-running 
applications, services, and batch processes using Docker containers.



2) What is the pricing for Amazon ECS?
There is no additional charge for Amazon ECS. You pay for AWS resources (e.g. Amazon EC2 instances or EBS volumes) 
you create to store and run your application. You only pay for what you use, as you use it; there are no 
minimum fees and no upfront commitments.


3) How is Amazon ECS different from AWS Elastic Beanstalk?
AWS Elastic Beanstalk is an application management platform that helps customers easily deploy and scale web applications and services. 
It keeps the building block provisioning (e.g., EC2, Amazon RDS, Elastic Load Balancing, AWS Auto Scaling, and Amazon CloudWatch), 
application deployment, and health monitoring abstracted from the user so they can focus on writing code. You simply specify which 
container images to  deploy, the CPU and memory requirements, the port mappings, and the container links.

Elastic Beanstalk will automatically handle all the details such as provisioning an Amazon ECS cluster, balancing load, auto-scaling, 
monitoring, and container placement across your cluster. Elastic Beanstalk is ideal if you want to leverage the benefits of containers 
withthe simplicity of deploying applications from development to production by uploading a container image. You can work with 
Amazon ECS directly if you want more fine-grained control for custom application architectures.


4) Does Amazon ECS support any other container types?
No. Docker is the only container platform supported by Amazon ECS at this time.



================================================= 
Amazon EC2 Windows FAQ:
================================================= 

AWS is a member of the Microsoft Partner Network, licensed to sell Microsoft software under the Service 
Provider License Agreement (SPLA), and a Microsoft Gold Certified Hosting Partner. 
AWS is an authorized Microsoft License Mobility Partner and has an active Premier Support agreement with Microsoft.


1) Are there regional restrictions on accessing the benefit of the expanded Support agreement with Microsoft?
No, there are no regional restrictions to using this benefit.


2) What types of Microsoft software can I run on AWS?
You can run many types of Microsoft software on AWS, including but not limited to: Microsoft Office, Windows Server, SQL Server, 
Exchange, SharePoint, Skype for Business, Microsoft Dynamics products, System Center, BizTalk, and Remote Desktop Services. 
You can use license included instances that include the license for Windows Server and SQL Server on Amazon EC2 or Amazon RDS. 
AWS customers have the flexibility of bringing on-premises Microsoft volume licenses and deploying them on 
Amazon EC2 instances subject to Microsoft license terms.



3) What’s the difference between Dedicated Hosts and Dedicated Instances?
Both offerings provide instances that are dedicated to your use. However, Dedicated Hosts provide additional control 
over your instances and visibility into Host level resources and tooling that allows you to manage software that consumes 
licenses on a per-core or per-socket basis, such as Windows Server and SQL Server. In addition, AWS Config will keep a record 
of how your instances use these Dedicated Host resources which will allow you to create your own license usage reports.


4)Can I buy MSDN from AWS?
No, AWS does not sell MSDN licenses.


5)What’s new in Windows Server 2022?
The latest server OS released by Microsoft, Windows Server 2022, offers a variety of features and improvements in performance, 
connectivity and security. AWS customers can make the best out of running Windows Server 2022 on EC2 by leveraging the 
elasticity and breadth of resources offered on AWS. Customers can start using various features of Windows Server 2022 
readily by accessing the Windows AMIs offered by AWS.




================================================= 
Amazon EC2 FAQ:
================================================= 

1)What operating system environments are supported?
Amazon EC2 currently supports a variety of operating systems including: Amazon Linux, Ubuntu, Windows Server, 
Red Hat Enterprise Linux, SUSE Linux Enterprise Server, openSUSE Leap, Fedora, Fedora CoreOS, Debian, CentOS, Gentoo Linux, 
Oracle Linux, and FreeBSD. We are looking for ways to expand it to other platforms.



2)Are these On-Demand Instance vCPU-based limits regional?
Yes, the On-Demand Instance limits for an AWS account are set on a per-region basis.



3)Will these limits change over time?
Yes, limits can change over time. Amazon EC2 is constantly monitoring your usage within each region and your limits are raised 
automatically based on your use of EC2.


4)How can I view my current On-Demand Instance limits?
You can find your current On-Demand Instance limits on the EC2 Service Limits page in the Amazon EC2 console, 
or from the Service Quotas console and APIs.




================================================= 
Amazon Elastic Kubernetes Service (EKS) FAQ:
================================================= 

Kubernetes is an open-source container orchestration system allowing you to deploy and manage containerized applications at scale. 
Kubernetes arranges containers into logical groupings for management and discoverability, 
then launches them onto clusters of Amazon Elastic Compute Cloud (Amazon EC2) instances.

1) What is Amazon Elastic Kubernetes Service (Amazon EKS)?
Amazon EKS is a managed service that makes it easy for you to run Kubernetes on AWS without installing and operating 
your own Kubernetes control plane or worker nodes.



2) Which operating systems does Amazon EKS support?
Amazon EKS supports Kubernetes-compatible Linux x86, ARM, and Windows Server operating system distributions. 
Amazon EKS provides optimized AMIs for Amazon Linux 2 and Windows Server 2019. EKS- optimized AMIs for other Linux distributions, 
such as Ubuntu, are available from their respective vendors.


3) Does Amazon EKS work with AWS Fargate?
Yes. You can run Kubernetes applications as serverless containers using AWS Fargate and Amazon EKS.


4)How much does Amazon EKS cost?

You pay $0.10 per hour for each Amazon EKS cluster you create and for the AWS resources you create to run your Kubernetes worker nodes.
You only pay for what you use, as you use it; there are no minimum fees and no upfront commitments. 


================================================= 
Amazon Lightsail FAQ:
================================================= 

1) What is a Virtual Private Server?
A virtual private server, also known as an "instance", allows users to run websites and web
applications in a highly secure and available environment, while being cost effective.


2) What is Amazon Lightsail?
Amazon Lightsail is a virtual private server (VPS) provider and is the easiest way to get started with AWS for developers, 
small businesses, students, and other users who need a solution to build and host their applications on cloud. 
Lightsail provides developers compute, storage, and networking capacity and capabilities to deploy and manage 
websites and web applications in the cloud. Lightsail includes everything you need to launch your 
project quickly – virtual machines, containers, databases, CDN, load balancers, DNS management etc. 
– for a low, predictable monthly price.


3)What is a Lightsail instance?
A Lightsail instance is a virtual private server (VPS) that lives in the AWS Cloud. Use your Lightsail instances to store your data,
run your code, and build web-based applications or websites. Your instances can connect to each other and to other 
AWS resources through both public (Internet) and private (VPC) networking. You can create, manage, and 
connect easily to instances right from the Lightsail console.


4)Does Lightsail offer an API?
Yes. Everything you do in the Lightsail console is backed by a publicly available API. 




================================================= 
AWS Batch FAQ:
================================================= 

1)What is AWS Batch?
AWS Batch is a set of batch management capabilities that enables developers, scientists, and engineers to easily and efficiently 
run hundreds of thousands of batch computing jobs on AWS. AWS Batch dynamically provisions the optimal quantity and type 
of compute resources (e.g., CPU or memory optimized compute resources) based on the volume and specific resource 
requirements of the batch jobs submitted.


2)What is Batch Computing?
Batch computing is the execution of a series of programs ("jobs") on one or more computers without manual intervention. 


3)What types of batch jobs does AWS Batch support?
AWS Batch supports any job that can executed as a Docker container. Jobs specify their memory requirements and number of vCPUs.  


4)What is a Compute Resource?
An AWS Batch Compute Resource is an EC2 instance or AWS Fargate compute resource.



5)What is the pricing for AWS Batch?
There is no additional charge for AWS Batch. You only pay for the AWS Resources (e.g. EC2 instances or AWS Fargate) 
you create to store and run your batch jobs.



================================================= 
AWS Elastic Beanstalk FAQ:
================================================= 

1) What is AWS Elastic Beanstalk?
AWS Elastic Beanstalk makes it even easier for developers to quickly deploy and manage applications in the AWS Cloud. 
Developers simply upload their application, and Elastic Beanstalk automatically handles the deployment details of capacity provisioning, 
load balancing, auto-scaling, and application health monitoring.

AWS Elastic Beanstalk stores your application files and, optionally, server log files in Amazon S3.


2)Who should use AWS Elastic Beanstalk?
Those who want to deploy and manage their applications within minutes in the AWS Cloud. 
You don’t need experience with cloud computing to get started. AWS Elastic Beanstalk 
supports Java, .NET, PHP, Node.js, Python, Ruby, Go, and Docker web applications.

development stacks Also:
Apache Tomcat for Java applications
Apache HTTP Server for PHP applications
Apache HTTP Server for Python applications
Nginx or Apache HTTP Server for Node.js applications
Passenger or Puma for Ruby applications
Microsoft IIS 7.5, 8.0, and 8.5 for .NET applications



3)Why should I use IAM with AWS Elastic Beanstalk?
IAM allows you to manage users and groups in a centralized manner. 
You can control which IAM users have access to AWS Elastic Beanstalk, and limit permissions to read-only access to Elastic 
Beanstalk for operators who should not be able to perform actions against Elastic Beanstalk resources. 
All user activity within your account will be aggregated under a single AWS bill.


4)How much does AWS Elastic Beanstalk cost?
There is no additional charge for AWS Elastic Beanstalk–you pay only for the AWS resources actually used to store and run your application.



================================================= 
AWS Fargate FAQ:
================================================= 

1)What is AWS Fargate?
AWS Fargate is a serverless compute engine for containers that works with both Amazon Elastic Container Service (ECS) and 
Amazon Elastic Kubernetes Service (EKS). AWS Fargate makes it easy to focus on building your applications. 
Fargate eliminates the need to provision and manage servers.



2)What is the pricing of AWS Fargate?
With AWS Fargate, you pay only for the amount of vCPU, memory, and storage resources consumed by your containerized applications.
vCPU and memory resources are calculated from the time your container images are pulled until the 
Amazon ECS task or EKS pod terminates, rounded up to the nearest second.



3)How should I choose when to use AWS Fargate?  
Choose AWS Fargate for its isolation model and security. You should also select Fargate if you want to launch containers 
without having to provision or manage EC2 instances. If you require greater control of your 
EC2 instances or broader customization options, then use ECS or EKS without Fargate. 
Use EC2 for GPU workloads, which are not supported on Fargate today.



================================================= 
AWS Lambda FAQ:
================================================= 

1) What is serverless computing?
Serverless computing allows you to build and run applications and services without thinking about servers. 
With serverless computing, your application still runs on servers, but all the server management is done by AWS. 

At the core of serverless computing is AWS Lambda, which lets you run your code without provisioning or managing servers.


2)Can I access the infrastructure that AWS Lambda runs on?
No. AWS Lambda operates the compute infrastructure on your behalf, allowing it to perform health checks, 
apply security patches, and do other routine maintenance.



3)How does AWS Lambda secure my code?
AWS Lambda stores code in Amazon S3 and encrypts it at rest. 
AWS Lambda performs additional integrity checks while your code is in use.



4)What is an AWS Lambda function?
The code you run on AWS Lambda is uploaded as a “Lambda function”. Each function has associated configuration information, such as 
its name, description, entry point, and resource requirements. The code must be written in a “stateless” style i.e. it should assume 
there is no affinity to the underlying compute infrastructure. Local file system access, child processes, and similar artifacts 
may not extend beyond the lifetime of the request, and any persistent state should be stored in Amazon S3, Amazon DynamoDB, 
Amazon EFS, or another Internet-available storage service. Lambda functions can include libraries, even native ones.



5)Does AWS Lambda support environment variables?
Yes. You can easily create and modify environment variables from the AWS Lambda Console, CLI, or SDKs. 
To learn more about environment variables, see the documentation.



6)How do I use an AWS Lambda function to respond to Amazon CloudWatch alarms?
First, configure the alarm to send Amazon SNS notifications. Then from the AWS Lambda console, 
select a Lambda function and associate it with that Amazon SNS topic. 


7)How do I set up Amazon EFS for Lambda?
Developers can easily connect an existing EFS file system to a Lambda function via an EFS Access Point by using the console, CLI, or SDK. 
When the function is first invoked, the file system is automatically mounted and made available to function code.


6)What is AWS Lambda Extensions?
AWS Lambda Extensions lets you integrate Lambda with your favorite tools for monitoring, observability, security, and governance. 
Extensions enable you and your preferred tooling vendors to plug into Lambda’s lifecycle and integrate 
more deeply into the Lambda execution environment.




================================================= 
AWS Outposts FAQ:
================================================= 

Why would I use AWS Outposts rack instead of operating in an AWS Region?

You can use Outposts rack to support your applications that have low latency or local data processing requirements. 
These applications may need to generate near real-time responses to end user applications or need to communicate with other 
on-premises systems or control on-site equipment. These can include workloads running on factory floors for automated operations in 
manufacturing, real-time patient diagnosis or medical imaging, and content and media streaming. You can use Outposts rack to securely 
store and process customer data that needs to remain on premises or in countries where there is no AWS Region. You can run data intensive 
workloads on Outposts rack and process data locally when transmitting data to AWS Regions is expensive and 
wasteful and for better control on data analysis, backup and restore.




================================================= 
Storage FAQ:
================================================= 


Amazon EFS vs. Amazon EBS vs. Amazon S3?

AWS offers cloud storage services to support a wide range of storage workloads.

-Amazon EFS is a file storage service for use with Amazon compute (EC2, containers, serverless) and on-premises servers. 
EFS provides a file system interface, file system access semantics (such as strong consistency and file locking), 
and concurrently accessible storage for up to thousands of EC2 instances.

-Amazon Elastic Block Store (EBS) is a block-level storage service for use with EC2. Amazon EBS can deliver performance 
for workloads that require the lowest-latency access to data from a single EC2 instance.

-Amazon Simple Storage Service (S3) is an object storage service. Amazon S3 makes data available through an internet 
API that can be accessed anywhere.


1)What is AWS Storage Gateway?
The AWS Storage Gateway sits between your applications and Amazon storage services. 
AWS Storage Gateway is a hybrid cloud storage service that gives you on-premises access to virtually unlimited cloud storage. 
Storage Gateway provides a standard set of storage protocols such as iSCSI, SMB, and NFS, 
which allow you to use AWS storage without rewriting your existing applications.


2)Where can I deploy a Storage Gateway appliance?
On-premises, you can deploy a virtual machine containing the Storage Gateway software on VMware ESXi, 
Microsoft Hyper-V, or Linux KVM, or you can deploy Storage Gateway as a hardware appliance. 
You can also deploy the Storage Gateway VM in VMware Cloud on AWS, or as an AMI in Amazon EC2.



3)How does AWS Storage Gateway provide on-premises applications access to cloud storage?
Depending on your use case, Storage Gateway provides three types of storage interfaces for your on-premises 
applications: file, volume, and tape.

The Amazon S3 File Gateway enables you to store and retrieve objects in Amazon Simple Storage Service (S3) using 
file protocols such as Network File System (NFS) and Server Message Block (SMB). Objects written through S3 
File Gateway can be directly accessed in S3.

The Amazon FSx File Gateway enables you to store and retrieve files in Amazon FSx for Windows File Server using the SMB protocol. 
Files written through Amazon FSx File Gateway are directly accessible in Amazon FSx for Windows File Server.

The Volume Gateway provides block storage to your on-premises applications using iSCSI connectivity. 
Data on the volumes is stored in Amazon S3 and you can take point-in-time copies of volumes that are stored in AWS as 
Amazon EBS snapshots. You can also take copies of volumes and manage their retention using AWS Backup. 
You can restore EBS snapshots to a Volume Gateway volume or an EBS volume.

The Tape Gateway provides your backup application with an iSCSI virtual tape library (VTL) interface, consisting of a virtual media 
changer, virtual tape drives, and virtual tapes. 
Virtual tapes are stored in Amazon S3 and can be archived to Amazon S3 Glacier or Amazon S3 Glacier Deep Archive.


4)Can I use AWS Storage Gateway with AWS Direct Connect?
Yes, you can use AWS Direct Connect to increase throughput and reduce your network costs by establishing a dedicated network 
connection between your on-premises gateway and AWS. Note that AWS Storage Gateway efficiently uses your 
internet bandwidth to help speed up the upload of your on-premises application data to AWS.


5)How will I be billed for my use of AWS Storage Gateway?
There are 3 elements to how you will be billed for AWS Storage Gateway: Storage, requests, and data transfer. 



================================================= 
Amazon EBS FAQs:
================================================= 

1)Amazon EBS volumes Type?
Amazon EBS provides seven volume types: Provisioned IOPS SSD (io2 Block Express, io2, and io1), 
General Purpose SSD (gp3 and gp2), Throughput Optimized HDD (st1) and Cold HDD (sc1). 


2)Does EBS encryption support boot volumes?
Yes.


================================================= 
Amazon EFS FAQs:
================================================= 

Amazon EFS is compatible with all Linux-based AMIs for Amazon EC2.


1)What is Amazon Elastic File System?
Amazon Elastic File System (Amazon EFS) is a simple, serverless, set-and-forget elastic file system that makes it easy to set up, 
scale, and cost-optimize file storage in AWS. With a few clicks in the AWS Management Console, 
you can create file systems that are accessible to Amazon Elastic Compute Cloud (EC2) instances, 
Amazon container services (Amazon Elastic Container Service [ECS], Amazon Elastic Kubernetes Service [EKS], and AWS Fargate), and 
AWS Lambda functions through a file system interface (using standard operating system file I/O APIs). 
They also support full file system access semantics, such as strong consistency and file locking.



2)How do I access a file system from an Amazon EC2 instance?
To access your file system, mount the file system on an Amazon EC2 Linux-based instance using the standard Linux mount command and the 
 system’s DNS name. To simplify accessing your Amazon EFS file systems, we recommend using the Amazon EFS mount helper utility. 
 Once mounted, you can work with the files and directories in your file system just like you would with a local file system.
EFS uses the Network File System version 4 (NFS v4) protocol. For a step-by-step example of how to access a file system from an 
EC2 instance, see the guide here.



3)What is an Amazon EFS Access Point?
Amazon EFS Access Points simplify providing applications with access to shared datasets in an Amazon EFS file system. 
Amazon EFS Access Points work together with AWS IAM and enforce an operating system user and group, and a directory for every file 
system request made through the access point. You can create multiple access points per file system and use them to provide access 
to specific applications.



4)How do I access an Amazon EFS file system from servers in my on-premises datacenter?
To access Amazon EFS file systems from on premises, you must have an AWS Direct Connect or 
AWS VPN connection between your on-premises datacenter and your Amazon VPC.
You mount an Amazon EFS file system on your on-premises Linux server using the standard Linux mount command for mounting a file system 
using the NFS v4.1 protocol.

You can access your Amazon EFS file system concurrently from servers in your on-premises datacenter as well as Amazon EC2 instances in your Amazon VPC.


5)How much does Amazon EFS cost?

With Amazon EFS, you pay only for what you use per month.
When using the Provisioned Throughput mode, you pay for the throughput you provision per month. 
There is no minimum fee and no setup charges.



================================================= 
Amazon FSx for Lustre FAQs:
================================================= 

1)What is Amazon FSx for Lustre?
Amazon FSx for Lustre makes it easy and cost effective to launch, run, and scale the world’s most popular high-performance file system.
The open source Lustre file system is designed for applications that require fast storage – where you want your storage to keep up 
with your compute. Lustre was built to solve the problem of quickly and cheaply processing the world’s ever-growing data sets, 
and it’s the most widely used file system for the 500 fastest computers in the world.


2)What use cases does Amazon FSx for Lustre support?
Use Amazon FSx for Lustre for workloads where speed matters, such as machine learning, 
high performance computing (HPC), video processing, financial modeling, genome sequencing, and electronic design automation (EDA).


3)What is the difference between scratch and persistent deployment options?
Amazon FSx for Lustre provides two deployment options: scratch and persistent.
Scratch file systems are designed for temporary storage and shorter-term processing of data. Data is not replicated and does not persist if a file server fails.
Persistent file systems are designed for longer-term storage and workloads. The file servers are highly available, 
and data is automatically replicated within the AWS Availability Zone (AZ) that is associated with the file system. 
The data volumes attached to the file servers are replicated independently from the file servers to which they are attached.


4)What instance types and AMIs work with Amazon FSx for Lustre?
FSx for Lustre is compatible with the most popular Linux-based AMIs, including Amazon Linux, Amazon Linux 2, 
Red Hat Enterprise Linux (RHEL), CentOS, SUSE Linux and Ubuntu.


5)How do I monitor my file system’s activity?
Amazon FSx for Lustre provides native CloudWatch integration, allowing you to monitor file system health and performance 
metrics in real time.


6) How many instances can connect to a file system?
An FSx for Lustre file system can be concurrently accessed by thousands of compute instances.


7)How will I be charged and billed for my use of Amazon FSx for Lustre?
You pay only for the resources you use.
Storage capacity scaling requests are processed by adding new storage capacity to your file system. 
You will be billed for new storage capacity 
once the new file servers have been added to your file system, and the file system status changes from UPDATING to AVAILABLE.



================================================= 
Amazon FSx for Windows File Server FAQs:
================================================= 

1) What is Amazon FSx for Windows File Server?
Amazon FSx for Windows File Server provides fully managed, highly reliable, and scalable file storage that is 
accessible over the industry-standard Service Message Block (SMB) protocol. It is built on Windows Server, 
delivering a wide range of administrative features such as user quotas, end-user file restore, and 
Microsoft Active Directory (AD) integration.



2)How will I be charged and billed for my use of Amazon FSx for Windows File Server?
You pay only for the resources you use. You are billed hourly for your file systems, based on their deployment type 
(Single-AZ or Multi-AZ), storage type (SSD or HDD), storage capacity (priced per GB-month), and throughput capacity 
(priced per MBps-month). You are billed hourly for your backup storage (priced per GB-month). 
For pricing information, please visit the Amazon FSx pricing page.



================================================= 
Amazon S3 FAQ:
================================================= 

1) Does Amazon store its own data in Amazon S3?
Yes. Developers within Amazon use Amazon S3 for a wide variety of projects. Many of these projects use 
Amazon S3 as their authoritative data store and rely on it for business-critical operations.


2)Where is my data stored?
You specify an AWS Region when you create your Amazon S3 bucket. For S3 Standard, S3 Standard-IA, S3 Intelligent-Tiering, S3 Glacier 
Instant Retrieval, S3 Glacier Flexible Retrieval, and S3 Glacier Deep Archive storage classes, your objects are automatically stored 
across multiple devices spanning a minimum of three Availability Zones, each separated by miles across an AWS Region. Objects stored 
in the S3 One Zone-IA storage class are stored redundantly within a single Availability Zone in the AWS Region you select. 
For S3 on Outposts, your data is stored in your Outpost on-premises environment, unless you manually choose to transfer it to an 
AWS Region. Please refer to Regional Products and Services for details of Amazon S3 service availability by AWS Region.


3)How much does Amazon S3 cost?
With Amazon S3, you pay only for what you use. There is no minimum charge. 
You can estimate your monthly bill using the AWS Pricing Calculator.


4)What are Amazon S3 Event Notifications?
You can enable Amazon S3 Event Notifications and receive them in response to specific events in your S3 bucket, 
such as PUT, POST, COPY, and DELETE events. You can publish notifications to Amazon EventBridge, 
Amazon SNS, Amazon SQS, or directly to AWS Lambda.


Amazon S3 Event Notifications let you to run workflows, send alerts, or perform other actions in response to 
changes in your objects stored in S3. 


5) What is S3 Transfer Acceleration?
Amazon S3 Transfer Acceleration creates fast, easy, and secure transfers of files over long distances between your client and your 
Amazon S3 bucket. S3 Transfer Acceleration leverages Amazon CloudFront’s globally distributed AWS Edge Locations. 
As data arrives at an AWS Edge Location, data is routed to your Amazon S3 bucket over an optimized network path.


6)How secure is my data in Amazon S3?     
Amazon S3 is secure by default. Upon creation, only you have access to Amazon S3 buckets that you create, 
and you have complete control over who has access to your data. Amazon S3 supports user authentication to control access to data. 
You can use access control mechanisms such as bucket policies to selectively grant permissions to users and groups of users. 
The Amazon S3 console highlights your publicly accessible buckets, indicates the source of public accessibility, and also warns 
you if changes to your bucket policies or bucket ACLs would make your bucket publicly accessible. You should enable Block Public 
Access for all accounts and buckets that you do not want publicly accessible. 

You can securely upload/download your data to Amazon S3 via SSL endpoints using the HTTPS protocol. 
If you need extra security you can use the Server-Side Encryption (SSE) option to encrypt data stored at rest. 
You can configure your Amazon S3 buckets to automatically encrypt objects before storing them if the incoming storage 
requests do not have any encryption information. Alternatively, you can use your own encryption libraries to encrypt data before 
storing it in Amazon S3.



7)What is S3 Select?
S3 Select is an Amazon S3 feature that makes it easy to retrieve specific data from the contents of an object using simple 
SQL expressions without having to retrieve the entire object. S3 Select simplifies and improves the performance of scanning and 
filtering the contents of objects into a smaller, targeted dataset by up to 400%. With S3 Select, you can also perform operational 
investigations on log files in Amazon S3 without the need to operate or manage a compute cluster. 




================================================= 
AWS Backup FAQs:
================================================= 


1)What is AWS Backup?
AWS Backup is a fully managed service that enables you to centralize and automate data protection across on-premises and AWS services. 
Together with AWS Organizations, AWS Backup allows you to centrally deploy data protection (backup) policies to configure, manage, 
and govern your backup activity across your organization’s AWS accounts and resources. AWS Backup also enables you to audit and 
report on the compliance of your data protection policies with AWS Backup Audit Manager.


2)What can I back up using AWS Backup?
You can use AWS Backup to create and manage the backups of the following AWS services:
Amazon Elastic Block Store (Amazon EBS) volumes
Amazon Elastic Compute Cloud (Amazon EC2) instances (including Windows applications)
Windows Volume Shadow Copy Service (VSS) supported applications (including Windows Server, Microsoft SQL Server, 
and Microsoft Exchange Server) on Amazon EC2.
Amazon Relational Database Service (Amazon RDS) databases (including Amazon Aurora clusters)
Amazon DynamoDB tables, Amazon Elastic File System (Amazon EFS) file systems
Amazon FSx for NetApp ONTAP file systems
Amazon FSx for OpenZFS file systems
Amazon FSx for Windows File Server file systems
Amazon FSx for Lustre file systems
Amazon Neptune databases
Amazon DocumentDB (with MongoDB compatibility) databases
AWS Storage Gateway volumes
Amazon Simple Storage Service (Amazon S3).
You can also use AWS Backup to create and manage backups of Amazon Outposts, 
VMware CloudTM on AWS, and on-premises VMware virtual machines.



3)Can I use AWS Backup to back up on-premises data?
Yes, you can use AWS Backup to back up your on-premises Storage Gateway volumes and VMware virtual machines, 
providing a common way to manage the backups of your application data both on premises and on AWS.



4)What is a backup plan?
A backup plan is a policy expression that defines when and how you want to back up your AWS resources, such as DynamoDB tables or 
EFS file systems. You assign resources to backup plans and AWS Backup will then automatically make and retain backups for those 
resources according to the backup plan. Backup plans are composed of one or more backup rules. Each backup rule is composed of 
1) a backup schedule, which includes the backup frequency (Recovery Point Objective - RPO) and backup window, 
2) a lifecycle rule that specifies when to transition a backup from one storage tier to another and when to expire the recovery point, 
3) the backup vault in which to place the created recovery points, and 
4) the tags to be added to backups upon creation. For example, a backup plan might have a “daily backup rule” and a “monthly backup rule.”
 The daily rule backs up resources every day at midnight and retains the backups for one month. 
 The monthly rule takes a backup once a month on the beginning of every month and retains the backups for one year.
 
 
5)What is AWS Backup Audit Manager?
AWS Backup Audit Manager allows you to audit and report on the compliance of your data protection policies to 
help you meet your business and regulatory needs. AWS Backup enables you to centralize and automate data protection 
policies across AWS services based on organizational best practices and regulatory standards, and AWS Backup Audit 
Manager helps you maintain and demonstrate compliance to those policies.


6)What is AWS Backup Audit Manager?
AWS Backup Audit Manager allows you to audit and report on the compliance of your data protection policies to help you meet your 
business and regulatory needs. AWS Backup enables you to centralize and automate data protection policies across AWS services based 
on organizational best practices and regulatory standards, and AWS Backup Audit Manager helps you maintain and demonstrate compliance 
to those policies.


7)What is AWS Backup Vault Lock?
AWS Backup Vault Lock is a feature that enables you to prevent changes to backup lifecycle as well as prevent manual deletion of backups,
 helping you meet your compliance requirements. AWS Backup Vault Lock implements safeguards that ensure you are 
 storing your backups using a Write-Once-Read-Many (WORM) model.



================================================= 
AWS DataSync FAQs:
================================================= 

1)  What is AWS DataSync?
AWS DataSync is an online data transfer service that simplifies, automates, and accelerates copying large amounts of data between 
on-premises storage systems and AWS Storage services, as well as between AWS Storage services.

AWS DataSync reduces the complexity and cost of online data transfer, making it simple to transfer datasets 
between on-premises, edge, or other cloud storage and AWS Storage services, as well as between AWS Storage services.

2)Where can I move data to and from?
DataSync supports the following storage location types: Network File System (NFS) shares, Server Message Block (SMB) shares, 
Hadoop Distributed File Systems (HDFS), self-managed object storage, Google Cloud Storage, Azure Files, AWS Snowcone, 
Amazon Simple Storage Service (Amazon S3), Amazon Elastic File System (Amazon EFS) file systems, Amazon FSx for 
Windows File Server file systems, Amazon FSx for Lustre file systems, Amazon FSx for OpenZFS file systems, and 
Amazon FSx for NetApp ONTAP file systems.


3)How do I deploy an AWS DataSync agent?
You deploy an AWS DataSync agent to your on-premises hypervisor, in your public cloud environment, or in Amazon EC2. 
To copy data to or from an on-premises file server, you download the agent virtual machine image from the AWS Console and 
deploy to your on-premises VMware ESXi, Linux Kernel-based Virtual Machine (KVM), or Microsoft Hyper-V hypervisor. 
When a DataSync agent is used, the agent must be deployed so that it can access your file server using the NFS, SMB protocol.



4)How does AWS DataSync access my Amazon EFS file system?
AWS DataSync accesses your Amazon EFS file system using the NFS protocol. The DataSync service mounts your file system from within 
your VPC from Elastic Network Interfaces (ENIs) managed by the DataSync service. DataSync fully manages the creation, use, 
and deletion of these ENIs on your behalf. You can choose to mount your EFS file system using a mount target or an EFS Access Point.




================================================= 
AWS Snow FAQs:
================================================= 

1)What is AWS Snowball?
AWS Snowball is a service that provides secure, rugged devices, so you can bring AWS computing and storage 
capabilities to your edge environments, and transfer data into and out of AWS. 


2)Who should use Snowball Edge?
Consider Snowball Edge if you need to run computing in rugged, austere, mobile, or disconnected (or intermittently connected) environments. 
Also consider it for large-scale data transfers and migrations when bandwidth is not available for use of a 
high-speed online transfer service, such as AWS DataSync.


3)What is AWS OpsHub for Snow Family?
AWS OpsHub is an application that you can download from the Snowball resources page. 
It offers a graphical user interface for managing the AWS Snow Family devices. AWS OpsHub makes it easy to setup and manage 
AWS Snowball devices enabling you to rapidly deploy edge computing workloads and simplify data migration to the cloud. 


4) How does AWS OpsHub for Snow Family work?
AWS OpsHub is an application that you can download and install on any Windows or Mac client machine, such as a laptop. 
Once you have installed AWS OpsHub and have your AWS Snow Family device on site, open AWS OpsHub and unlock the device. 
You will then be presented with a dashboard showing your device and its system metrics. 
You can then begin deploying your edge applications or migrating your data to the device with just a few clicks.



5) What is AWS Snowmobile?
AWS Snowmobile is the first exabyte-scale data migration service that allows you to move very large datasets from on-premises to AWS. 
Each Snowmobile is a secured data truck with up to 100PB storage capacity that can be dispatched to your site and connected directly 
to your network backbone to perform high-speed data migration. You can quickly migrate an exabyte of 
data with ten Snowmobiles in parallel from a single location or multiple data centers. 
Snowmobile is offered by AWS as a managed service.



================================================= 
AWS Transfer for SFTP FAQ:
================================================= 

1) Why should I use the AWS Transfer Family?
AWS Transfer Family supports multiple protocols for business-to-business (B2B) file transfers so data can easily and securely be 
exchanged across stakeholders, third-party vendors, business partners, or customers. Without using Transfer Family, 
you have to host and manage your own file transfer service which requires you to invest in operating and managing infrastructure, 
patching servers, monitoring for uptime and availability, and building one-off mechanisms to provision users and audit their activity. 


2) How do I get started with AWS Transfer for SFTP, FTPS, and FTP?
In 3 simple steps, you get an always-on server endpoint enabled for SFTP, FTPS, and/or FTP. First, you select the protocol(s) 
you want to enable your end users to connect to your endpoint. Next, you configure user access using AWS Transfer 
Family built-in authentication manager (service managed), Microsoft Active Directory (AD), or by integrating your own or a 
third party identity provider such as Okta or Microsoft AzureAD (“BYO” authentication). Finally, select the server to access 
S3 buckets or EFS file systems. Once the protocol(s), identity provider, and the access to file systems are enabled, 
your users can continue to use their existing SFTP, FTPS, or FTP clients and configurations, 
while the data accessed is stored in the chosen file systems.


3)Which protocols should I use for securing data while in-transit over a public network?
Either SFTP or FTPS should be used for secure transfers over public networks. 
Due to the underlying security of the protocols based on SSH and TLS cryptographic algorithms, 
data and commands are transferred through a secure, encrypted channel.


4) How am I billed for use of the service?
You are billed on an hourly basis for each of the protocols enabled, from the time you create and configure your server endpoint, 
until the time you delete it. You are also billed based on the amount of data uploaded and downloaded over 
SFTP, FTPS, or FTP and number of messages exchanged over AS2.



================================================= 
Amazon Aurora FAQs:
================================================= 

1) What is Amazon Aurora?
Amazon Aurora is a modern relational database service offering performance and high availability at scale, fully open source 
MySQL- and PostgreSQL-compatible editions, and a range of developer tools for building serverless and machine 
learning (ML)-driven applications.

Aurora features a distributed, fault-tolerant, and self-healing storage system that is decoupled from compute 
resources and auto-scales up to 128 TB per database instance. It delivers high performance and availability with up to 15 low-latency 
read replicas, point-in-time recovery, continuous backup to Amazon Simple Storage Service (Amazon S3), 
and replication across three Availability Zones (AZs).


2)What does "MySQL compatible" mean?
Amazon Aurora is drop-in compatible with existing MySQL open-source databases and adds support for new releases regularly. 
This means you can easily migrate MySQL databases to and from Aurora using standard import/export tools or snapshots. 
It also means that most of the code, applications, drivers, and tools you already use with MySQL databases 
today can be used with Aurora with little or no change. When considering Aurora vs. 
MySQL, it is important to understand that the Amazon Aurora database engine is designed to be 
wire-compatible with MySQL 5.6 and 5.7 using the InnoDB storage engine. 
This makes it easy to move applications between the two engines. 
Certain MySQL features, such as the MyISAM storage engine, can’t be used with persistent tables.


3)What does “PostgreSQL compatible” mean?
This means you can easily migrate PostgreSQL databases to and from Aurora using standard import/export tools or snapshots.
It also means that most of the code, applications, drivers, and tools you already use with 
PostgreSQL databases today can be used with Aurora with little or no change. 


4)What are the minimum and maximum storage limits of an Amazon Aurora database?
The minimum storage is 10 GB. Based on your database usage, your Amazon Aurora storage will 
automatically grow, up to 128 TB, in 10 GB increments with no impact to database performance. 
There is no need to provision storage in advance.



5)How do I enable backups for my DB Instance?
Automated backups are always enabled on Amazon Aurora DB Instances. Backups do not impact database performance.


6)Can I take DB Snapshots and keep them around as long as I want?
Yes, and there is no performance impact when taking snapshots. 
Note that restoring data from DB Snapshots requires creating a new DB Instance.


7)If my database fails, what is my recovery path?
Amazon Aurora automatically maintains six copies of your data across three Availability Zones (AZs) and will 
automatically attempt to recover your database in a healthy AZ with no data loss. 
In the unlikely event your data is unavailable within Amazon Aurora storage, you can restore from a DB 
Snapshot or perform a point-in-time restore operation to a new instance. 
Note that the latest restorable time for a point-in-time restore operation can be up to five minutes in the past.


8)How do I access my Amazon Aurora database?
Amazon Aurora databases must be accessed through the database port entered on database creation. 
This provides an additional layer of security for your data. 


9)What is Amazon Aurora Serverless?
Aurora Serverless is an on-demand, auto-scaling configuration for Amazon Aurora. It enables you to run your database in the 
cloud without managing database capacity. Manually managing database capacity can take up valuable time and can lead to inefficient 
use of database resources. With Aurora Serverless, you simply create a database, specify the desired database capacity range, and 
connect your application. Aurora automatically adjusts the capacity within the range your specified based on your application’s needs. 
You pay on a per-second basis for the database capacity you use when the database is active.



10)What all Aurora features does Aurora Serverless v2 support?
Aurora Serverless v2 supports all features of provisioned Aurora, including read replica, 
multi-AZ configuration, Global Database, RDS proxy, and Performance Insights.


11)What is Amazon DevOps Guru for RDS?
Amazon DevOps Guru for RDS is a new ML-powered capability for Amazon RDS that is designed to automatically detect and 
diagnose database performance and operational issues, enabling you to resolve issues in minutes rather than days.



================================================= 
Amazon RDS FAQs:
================================================= 

1)Which relational database engines does Amazon RDS support?
Amazon RDS supports Amazon Aurora, MySQL, MariaDB, Oracle, SQL Server, and PostgreSQL database engines.


2)What is a database instance (DB instance)?
You can think of a DB instance as a database environment in the cloud with the compute and storage resources you specify. 
You can create and delete DB instances; define/refine infrastructure attributes of your DB instance(s); and control access and 
security via the AWS Management Console, Amazon RDS APIs, and AWS Command Line Interface. You can run one or more 
DB instances and each DB instance can support one or more databases or database schemas, depending on engine type.



3)How do I import data into an Amazon RDS DB instance?
There are a number of simple ways to import data into Amazon RDS, such as with the mysqldump or mysqlimport utilities for MySQL; 
Data Pump, import/export, or SQL Loader for Oracle; Import/Export wizard, full backup files (.bak files), or 
Bulk Copy Program (BCP) for SQL Server; or pg_dump for PostgreSQL


4)Can I test my DB instance with a new version before upgrading?
Yes. You can do so by creating a DB snapshot of your existing DB instance, restoring from the DB snapshot to create a new DB instance, 
and then initiating a version upgrade for the new DB instance.




================================================= 
Amazon ElastiCache FAQ:
================================================= 

1) What is Amazon ElastiCache?
Amazon ElastiCache is a web service that makes it easy to deploy and run Memcached or Redis protocol-compliant server nodes in the cloud. 
Amazon ElastiCache improves the performance of web applications by allowing you to retrieve information from a fast, managed, 
in-memory system, instead of relying entirely on slower disk-based databases. 


2)Which engines does Amazon ElastiCache support?
Amazon ElastiCache offers fully managed Redis, voted the most loved database by developers in the Stack Overflow Developer Survey for 5 years in a row, and 
Memcached for your most demanding applications that require sub-millisecond response times.



================================================= 
Amazon Redshift FAQs:
================================================= 

What is a Data Warehouse?
A data warehouse is a system that pulls together data from many different sources within an organization for reporting and analysis. 
The reports created from complex queries within a data warehouse are used to make business decisions.
A data warehouse stores historical data about your business so that you can analyze and extract insights from it. 
It does not store current information, nor is it updated in real-time.



1)What is Amazon Redshift?
Amazon Redshift is a fully managed, scalable cloud data warehouse that accelerates your time to 
insights with fast, easy, and secure analytics at scale. 
Thousands of customers rely on Amazon Redshift to analyze data from terabytes to petabytes and run complex analytical queries.



2)What is Amazon Redshift Serverless (preview)?
Amazon Redshift Serverless (preview) is a serverless option of Amazon Redshift that makes it easy to run and scale analytics in 
seconds without the need to set up and manage data warehouse infrastructure. With Redshift Serverless, any user—including data analysts, 
developers, business professionals, and data scientists—can get insights from data by simply 
loading and querying data in the data warehouse.



3)How does Amazon Redshift keep my data secure?
Amazon Redshift supports industry-leading security with built-in AWS IAM integration, identity federation for single-sign on (SSO), 
multi-factor authentication, column-level access control, row-level security, Amazon Virtual Private Cloud (Amazon VPC), and provides 
built-in AWS KMS integration to protect your data in transit and at rest. Amazon Redshift encrypts and keeps your data secure 
in transit and at rest using industry-standard encryption techniques. To keep data secure in transit, Amazon Redshift supports 
SSL-enabled connections between your client application and your Redshift data warehouse cluster. To keep your data secure at rest, 
Amazon Redshift encrypts each block using hardware-accelerated AES-256 as it is written to disk.





================================================= 
Amazon Neptune FAQs:
================================================= 
1) What is Amazon Neptune?
Amazon Neptune is a fast, reliable, fully-managed graph database service that makes it easy to build and run 
applications that work with highly connected datasets. 


2)What popular graph query languages does Amazon Neptune support?
Amazon Neptune supports both the open source Apache TinkerPop Gremlin graph traversal language and the 
W3C standard Resource Description Framework’s (RDF) SPARQL query language.



3)How do I access my Amazon Neptune database?
Access to Amazon Neptune databases must be done through the HTTP port entered on database creation within your VPC. 



================================================= 
AWS Database Migration Service FAQs:
================================================= 


1)What sources and targets does AWS Database Migration Service support?
AWS Database Migration Service (DMS) supports a range of homogeneous and heterogeneous data replications.
Either the source or the target database (or both) need to reside in RDS or on EC2. 
Replication between on-premises to on-premises databases is not supported.


=====================================================
Amazon DocumentDB (with MongoDB compatibility) FAQs:
=====================================================

1)What is Amazon DocumentDB (with MongoDB compatibility)?
Amazon DocumentDB (with MongoDB compatibility) is a fast, scalable, highly available, and fully managed document database service 
that supports MongoDB workloads. As a document database, Amazon DocumentDB makes it easy to store, query, and index JSON data. 
Developers can use the same MongoDB application code, drivers, and tools as they do 
today to run, manage, and scale workloads on Amazon DocumentDB.



2)What does "MongoDB-compatible" mean?
MongoDB compatible” means that Amazon DocumentDB interacts with the Apache 2.0 open source 
MongoDB 3.6 and 4.0 APIs. As a result, you can use the 
same MongoDB drivers, applications, and tools with Amazon DocumentDB with little or no changes. 




================================================= 
Amazon VPC FAQs:
================================================= 

1)Why should I use Amazon VPC?
Amazon VPC enables you to build a virtual network in the AWS cloud - no VPNs, hardware, or physical datacenters required. 
You can define your own network space, and control how your network and the Amazon EC2 resources inside 
your network are exposed to the Internet. You can also leverage the enhanced security options in 
Amazon VPC to provide more granular access to and from the Amazon EC2 instances in your virtual network.



2)What are the components of Amazon VPC?
-A Virtual Private Cloud: A logically isolated virtual network in the AWS cloud. 
You define a VPC’s IP address space from ranges you select.

-Subnet: A segment of a VPC’s IP address range where you can place groups of isolated resources.

-Internet Gateway: The Amazon VPC side of a connection to the public Internet.

-NAT Gateway: A highly available, managed Network Address Translation (NAT) service for your 
resources in a private subnet to access the Internet.

-Virtual private gateway: The Amazon VPC side of a VPN connection.

-Peering Connection: A peering connection enables you to route traffic via private IP addresses between two peered VPCs.

-VPC Endpoints: Enables private connectivity to services hosted in AWS, from within your 
VPC without using an Internet Gateway, VPN, Network Address Translation (NAT) devices, or firewall proxies.

-Egress-only Internet Gateway: A stateful gateway to provide egress only access for IPv6 traffic from the VPC to the Internet.


3) How will I be charged and billed for my use of Amazon VPC?
There are no additional charges for creating and using the VPC itself. 
If you connect your VPC to your corporate datacenter using the optional hardware VPN connection, pricing is per VPN connection-hour 
(the amount of time you have a VPN connection in the "available" state.) Partial hours are billed as full hours. 
Data transferred over VPN connections will be charged at standard AWS Data Transfer rates.


4)What are the connectivity options for my Amazon VPC?
-The internet (via an internet gateway)
-Your corporate data center using an AWS Site-to-Site VPN connection (via the virtual private gateway)
-Both the internet and your corporate data center (utilizing both an internet gateway and a virtual private gateway)
-Other AWS services (via internet gateway, NAT, virtual private gateway, or VPC endpoints)
-Other Amazon VPCs (via VPC peering connections)


5) When is an IP address considered a Public IP address?
Any IP address that is assigned to an instance or a service hosted in a VPC that can be accessed over the internet is considered a 
public IP address. Only public IPv4 addresses, including Elastic IP addresses (EIPs) and IPv6 GUA can be routable on the internet. 
To do so, you would need to first connect the VPC to the internet and then update the route table to 
make them reachable to/from the internet.


6)How do instances without public IP addresses access the Internet?
Instances without public IP addresses can access the Internet in one of two ways:
Instances without public IP addresses can route their traffic through a NAT gateway or a NAT instance to access the Internet. 
These instances use the public IP address of the NAT gateway or NAT instance to traverse the Internet. 
The NAT gateway or NAT instance allows outbound communication but doesn’t allow machines on the Internet to initiate a 
connection to the privately addressed instances.

For VPCs with a hardware VPN connection or Direct Connect connection, instances can route their Internet traffic down the 
virtual private gateway to your existing datacenter. From there, it can access the Internet via your existing egress points 
and network security/monitoring devices.


7)Can I connect to my VPC using a software VPN?
Yes. You may use a third-party software VPN to create a site to site or remote access 
VPN connection with your VPC via the Internet gateway.



8)Does traffic go over the internet when two instances communicate using public IP addresses, or when instances 
communicate with a public AWS service endpoint?
No. When using public IP addresses, all communication between instances and services hosted in AWS use AWS's private network. 
Packets that originate from the AWS network with a destination on the AWS network stay on the AWS global network, except 
traffic to or from AWS China Regions.
 
In addition, all data flowing across the AWS global network that interconnects our data centers and Regions is 
automatically encrypted at the physical layer before it leaves our secured facilities. 
Additional encryption layers exist as well; for example, all VPC cross-region peering traffic, 
and customer or service-to-service Transport Layer Security (TLS) connections. 


9)What IP address ranges are assigned to a default Amazon VPC?
Default VPCs are assigned a CIDR range of 172.31.0.0/16. Default subnets within a default VPC are assigned /20 
netblocks within the VPC CIDR range. 



10)How large of a VPC can I create?
Currently, Amazon VPC supports five (5) IP address ranges, one (1) primary and four (4) secondary for IPv4. 
Each of these ranges can be between /28 (in CIDR notation) and /16 in size. 
The IP address ranges of your VPC should not overlap with the IP address ranges of your existing network.
For IPv6, the VPC is a fixed size of /56 (in CIDR notation). A VPC can have both IPv4 and IPv6 CIDR blocks associated to it.



11)Is there a limit on how large or small a subnet can be?
The minimum size of a subnet is a /28 (or 14 IP addresses.) for IPv4. Subnets cannot be larger than the VPC in which they are created.
For IPv6, the subnet size is fixed to be a /64. Only one IPv6 CIDR block can be allocated to a subnet.


12)Can I assign any IP address to an instance?
You can assign any IP address to your instance as long as it is:
Part of the associated subnet's IP address range
Not reserved by Amazon for IP networking purposes
Not currently assigned to another interfac
Not currently assigned to another interfac




13)Can I assign multiple IP addresses to an instance?
Yes. You can assign one or more secondary private IP addresses to an Elastic Network Interface or an EC2 instance in Amazon VPC. 
The number of secondary private IP addresses you can assign depends on the instance type. 


14) Can I monitor the network traffic in my VPC?
Yes. You can use Amazon VPC traffic mirroring and Amazon VPC flow logs features to monitor the network traffic in your Amazon VPC.

VPC flow logs is a feature that enables you to capture information about the IP traffic going to and from network interfaces in your VPC. 
Flow logs data can be published to either Amazon CloudWatch Logs or Amazon S3.


15)What is Amazon VPC traffic mirroring?
Amazon VPC traffic mirroring makes it easy for customers to replicate network traffic to and from an 
Amazon EC2 instance and forward it to out-of-band security and monitoring appliances for use-cases such as content inspection, 
threat monitoring, and troubleshooting. 



16)Can a VPC span multiple Availability Zones?
Yes. 


17) Can a subnet span Availability Zones?
No. A subnet must reside within a single Availability Zone.


18)Am I charged for network bandwidth between instances in different subnets?
If the instances reside in subnets in different Availability Zones, you will be charged $0.01 per GB for data transfer.



19)What are instance hostnames?
When you launch an instance, it is assigned a hostname. There are two options available, an IP based name or a Resource based name, 
and this parameter is configurable at instance launch. 
The IP based name uses a form of the Private IPv4 address while the Resource based name uses a form of the instance-id.

 you can change the hostname of an instance form IP based to 
 Resource based or vice versa by stopping the instance and then changing the resource based naming options.
 the instance hostname can be used as DNS hostnames.



20)What is AWS PrivateLink?
AWS PrivateLink enables customers to access services hosted on AWS in a highly available and scalable manner, 
while keeping all the network traffic within the AWS network.



21)How many VPCs, subnets, Elastic IP addresses, and internet gateways can I create?

Five Amazon VPCs per AWS account per region
Two hundred subnets per Amazon VPC
Five Amazon VPC Elastic IP addresses per AWS account per region
One internet gateway per Amazon VPC




================================================= 
Amazon CloudFront FAQs:
================================================= 

With CloudFront, your files are delivered to end-users using a global network of edge locations.
Amazon CloudFront is a good choice for distribution of frequently accessed static content that benefits from edge 
delivery—like popular website images, videos, media files or software downloads.

1)How does Amazon CloudFront speed up my entire website?
Amazon CloudFront uses standard cache control headers you set on your files to identify static and dynamic content. 
Delivering all your content using a single Amazon CloudFront distribution helps you make sure that performance 
optimizations are applied to your entire website or web application.


2)What types of content does Amazon CloudFront support?
Amazon CloudFront supports content that can be sent using the HTTP or WebSocket protocols. 
This includes dynamic web pages and applications, such as HTML or PHP pages or WebSocket-based applications, 
and any popular static files that are a part of your web application, such as website images, audio, video, 
media files or software downloads. Amazon CloudFront also supports delivery of live or on-demand media streaming over HTTP.


3)What types of HTTP requests are supported by Amazon CloudFront?
Amazon CloudFront currently supports GET, HEAD, POST, PUT, PATCH, DELETE and OPTIONS reques

4)What are WebSockets?
WebSocket is a real-time communication protocol that provides bidirectional communication between a client and a server 
over a long-held TCP connection.



4) What is streaming? Why would I want to stream?
Generally, streaming refers to delivering audio and video to end users over the Internet without having to download the 
media file prior to playback. The protocols used for streaming include those that use HTTP for delivery such as Apple’s 
HTTP Live Streaming (HLS), MPEG Dynamic Adaptive Streaming over HTTP (MPEG-DASH), 
Adobe’s HTTP Dynamic Streaming (HDS) and Microsoft’s Smooth Streaming.



5)What is Lambda@Edge?
Lambda@Edge  is an extension of AWS Lambda allowing you to run code at global edge locations without provisioning or managing servers. 
Lambda@Edge offers powerful and flexible serverless computing for complex functions and full application logic closer to your viewers. 




================================================= 
Amazon Route 53 FAQs:
================================================= 

1) What is Private DNS?
Private DNS is a Route 53 feature that lets you have authoritative DNS within your VPCs without exposing your 
DNS records (including the name of the resource and its IP address(es) to the Internet.




================================================= 
Amazon API Gateway FAQs:
================================================= 


1)What is Amazon API Gateway?
Amazon API Gateway is a fully managed service that makes it easy for developers to publish, maintain, monitor, 
and secure APIs at any scale.


2)How am I charged for using Amazon API Gateway?
Amazon API Gateway bills per million API calls, plus the cost of data transfer out, in gigabytes. 
If you choose to provision a cache for your API, hourly rates apply. For WebSocket APIs, API Gateway 
bills based on messages sent and received and the number of minutes a client is connected to the API.



================================================= 
AWS App Mesh FAQs:
================================================= 

1)What is AWS App Mesh?

AWS App Mesh makes it easy to monitor, control, and debug the communications between services.

App Mesh makes it easier to get visibility, security, and control over the communications between your services 
without writing new code or running additional AWS infrastructure. Using App Mesh, you can standardize 
how services communicate, implement rules for communications between services, and capture metrics, logs, 
and traces directly into AWS services and third-party tools of your choice.



================================================= 
AWS Direct Connect FAQs:
================================================= 

What is AWS Direct Connect?
AWS Direct Connect is a networking service that provides an alternative to using the internet to connect to AWS. 
Using AWS Direct Connect, data that would have previously been transported over the internet is delivered through a 
private network connection between your facilities and AWS. In many circumstances, private network connections can reduce costs, 
increase bandwidth, and provide a more consistent network experience than internet-based connections. All AWS services, including 
Amazon Elastic Compute Cloud (EC2), Amazon Virtual Private Cloud (VPC), Amazon Simple Storage Service (S3), and 
Amazon DynamoDB can be used with AWS Direct Connect.



2)What is an AWS Direct Connect gateway?
An AWS Direct Connect gateway is a grouping of virtual private gateways (VGWs) and private virtual interfaces (VIFs). 
An AWS Direct Connect gateway is a globally available resource. You can create the AWS Direct Connect gateway in any 
Region and access it from all other Regions. 


3)What is a virtual interface (VIF)?

A virtual interface (VIF) is necessary to access AWS services, and is either public or private. 
A public virtual interface enables access to public services, such as Amazon S3. A private virtual interface enables access to your VPC. 


4) What is a virtual private gateway (VGW)?
A virtual private gateway (VGW) is part of a VPC that provides edge routing for AWS managed VPN connections and AWS 
Direct Connect connections. You associate an AWS Direct Connect gateway with the virtual private gateway for the VPC. 


5)What is AWS Direct Connect SiteLink?
When the AWS Direct Connect SiteLink feature is enabled at two or more AWS Direct Connect locations, 
you can send data between those locations, bypassing AWS Regions. 
AWS Direct Connect SiteLink works with both hosted and dedicated connections.


6)How will I be charged and billed for my use of AWS Direct Connect?
AWS Direct Connect has two separate charges: port hours and data transfer. Pricing is per port-hour consumed for each port type. 
Partial port hours consumed are billed as full hours. The account that owns the port will be charged the port hour charges.
Data transfer through AWS Direct Connect will be billed in the same month in which the usage occurred.



7)How does AWS Direct Connect differ from an IPsec VPN Connection?
VPN connections use IPsec to establish encrypted network connectivity between your intranet and an Amazon VPC over the public internet. 
VPN Connections can be configured in minutes and are a good solution if you have an immediate need, have low to modest 
bandwidth requirements, and can tolerate the inherent variability of internet-based connectivity. AWS Direct Connect bypasses 
the internet; instead, it uses dedicated, private network connections between your network and AWS.



8)Does MACsec replace other encryption technologies I currently use in my network?
MACsec is not intended as a replacement for any specific encryption technology. For simplicity, and for defense in depth,
you should continue to use any encryption technologies that you already use. We offer MACsec as an encryption option you can 
integrate into your network in addition to other encryption technologies you currently use.



================================================= 
Elastic Load Balancing FAQs:
================================================= 

1)How do I decide which load balancer to select for my application?
Elastic Load Balancing (ELB) supports four types of load balancers. 
You can select the appropriate load balancer based on your application needs. 
If you need to load balance HTTP requests, we recommend you use the Application Load Balancer (ALB). 
For network/transport protocols (layer4 – TCP, UDP) load balancing, and for extreme performance/low latency 
applications we recommend using Network Load Balancer. If your application is built within the Amazon Elastic Compute Cloud (Amazon EC2) 
Classic network, you should use Classic Load Balancer. If you need to deploy and run 
third-party virtual appliances, you can use Gateway Load Balancer.


2)Can Network Load Balancer process both TCP and UDP protocol traffic on the same port?
Yes. To achieve this, you can use a TCP+UDP listener. For example, for a DNS service using both TCP and UDP, 
you can create a TCP+UDP listener on port 53, and the load balancer will process traffic for both UDP and TCP requests on that port. 
You must associate a TCP+UDP listener with a TCP+UDP target group.



3) How does Gateway Load Balancer pricing work?
You are charged for each hour or partial hour that a Gateway Load Balancer is running and the number of Load Balancer 
Capacity Units (LCU) used by Gateway Load Balancer per hour.



================================================= 
Amazon Athena FAQs:
================================================= 

1)What is Amazon Athena?
Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. 
Athena is serverless, so there is no infrastructure to setup or manage, and you can start analyzing data immediately. 
You don’t even need to load your data into Athena, it works directly with data stored in S3. To get started, 
just log into the Athena Management Console, define your schema, and start querying. 

Amazon Athena uses Presto with full standard SQL support and works with a variety of standard data formats, 
including CSV, JSON, ORC, Avro, and Parquet. Athena can handle complex analysis, including large joins, window functions, and arrays. 



2)How do you access Amazon Athena?
Amazon Athena can be accessed via the AWS Management Console, an API, or an ODBC or JDBC driver. 
You can programmatically run queries, add tables or partitions using the ODBC or JDBC driver.



3) What is federated query?
If you have data in sources other than Amazon S3, you can use Athena to query the data in place or build pipelines that extract 
data from multiple data sources and store them in Amazon S3. With Athena Federated Query, you can run SQL queries across data 
stored in relational, non-relational, object, and custom data sources.


4)How is Amazon Athena priced?
Amazon Athena is priced per query and charges based on the amount of data scanned by the query. 
You can store data in a variety of formats on Amazon S3. If you compress your data, partition, 
or convert it to columnar storage formats, you pay less because you scan less data. 
Converting data to the columnar format allows Athena to read only the columns it needs to process the query.

No, you are not charged for failed queries.
Amazon Athena queries data directly from Amazon S3, so your source data is billed at S3 rates. When Amazon Athena runs a query, 
it stores the results in an S3 bucket of your choice and you are billed at standard S3 rates for these result sets. 




================================================= 
Amazon EMR FAQs:
================================================= 


1)What is Amazon EMR?
Amazon EMR is the industry-leading cloud big data platform for data processing, interactive analysis, and machine learning using 
open source frameworks such as Apache Spark, Apache Hive, and Presto. With EMR you can run petabyte-scale analysis at less than 
half of the cost of traditional on-premises solutions and over 1.7x faster than standard Apache Spark.


2)What is EMR Studio?
EMR Studio is an integrated development environment (IDE) that makes it easy for data scientists and data engineers to develop, 
visualize, and debug data engineering and data science applications written in R, Python, Scala, and PySpark.


3)How is EMR Studio different from SageMaker Studio?
You can use both EMR Studio and SageMaker Studio with Amazon EMR. EMR Studio provides an integrated development environment (IDE) 
that makes it easy for you to develop, visualize, and debug data engineering and data science applications written in R, Python, 
Scala, and PySpark. Amazon SageMaker Studio provides a single, web-based visual interface where you can perform all 
machine learning development steps. 


4)How much does Amazon EMR cost?
Amazon EMR pricing is simple and predictable: you pay a per-second rate for every second you use, with a one-minute minimum. 
You can estimate your bill using the AWS Pricing Calculator. Usage for other Amazon Web Services including 
Amazon EC2 is billed separately from Amazon EMR.



5)What is Amazon EMR Serverless?

Amazon EMR Serverless is a new deployment option in Amazon EMR that allows you to run big data frameworks such as 
Apache Spark and Apache Hive without configuring, managing, and scaling clusters.



================================================= 
Amazon CloudSearch FAQs:
================================================= 

1)What is Amazon CloudSearch?
Amazon CloudSearch is a fully-managed service in the AWS Cloud that makes it easy to set up, manage, and scale a 
search solution for your website or application.


2)What benefits does Amazon CloudSearch offer?
Amazon CloudSearch is a fully managed search service that automatically scales with the volume of data and complexity of search 
requests to deliver fast and accurate results. Amazon CloudSearch lets customers add search capability without needing to 
manage hosts, traffic and data scaling, redundancy, or software packages. Users pay low hourly rates only for the resources consumed. 
Amazon CloudSearch can offer significantly lower total cost of ownership compared to operating and managing your own search environment.



================================================= 
Amazon Kinesis Data Streams FAQs:
================================================= 

1)What is Amazon Kinesis Data Streams?
With Amazon Kinesis Data Streams, you can build custom applications that process or analyze streaming data for specialized needs. 
You can add various types of data such as clickstreams, application logs, and social media to a Kinesis data 
stream from hundreds of thousands of sources. 
Within seconds, the data will be available for your applications to read and process from the stream.


2)When I use Kinesis Data Streams, how secure is my data?

Amazon Kinesis is secure by default. Only the account and data stream owners have access to the Kinesis resources they create. 
Kinesis supports user authentication to control access to data. You can use AWS IAM policies to selectively grant 
permissions to users and groups of users. 
You can securely put and get your data from Kinesis through SSL endpoints using the HTTPS protocol.



3)How does Amazon Kinesis Data Streams pricing work?

Kinesis Data Streams uses simple pay-as-you-go pricing. There are no upfront costs or minimum fees, and you pay only for the 
resources you use. Kinesis Data Streams has two capacity modes—on-demand and provisioned—and both come with specific billing options.




=================================================
#Security, Identity, & Compliance
=================================================



AWS Identity and Access Management FAQ:
======================================

1) What is AWS Identity and Access Management (IAM)?
IAM provides fine-grained access control across all of AWS. With IAM, you can control access to services and resources under specific 
conditions. 
IAM provides authentication and authorization for AWS services. A service evaluates if an AWS request is allowed or denied. 
Access is denied by default and is allowed only when a policy explicitly grants access. You can attach policies to roles and 
resources to control access across AWS.


2)What are IAM roles and how do they work?
AWS Identity and Access Management (IAM) roles provide a way to access AWS by relying on temporary security credentials. 
Each role has a set of permissions for making AWS service requests, and a role is not associated with a specific user or group. 
Instead, trusted entities such as identity providers or AWS services assume roles.



3)What are IAM policies?
IAM policies define permissions for the entities you attach them to. For example, to grant access to an IAM role, 
attach a policy to the role. The permissions defined in the policy determine whether requests are allowed or denied. 
You also can attach policies to some resources, such as Amazon S3 buckets, to grant direct, cross-account access.



4)What are customer managed policies and when should I use them?
To grant only the permissions required to perform tasks, you can create customer managed policies that are specific to your use 
cases and resources. Use customer managed policies to continue refining permissions for your specific requirements.


5)What are inline policies and when should I use them?
Inline policies are embedded in and inherent to specific IAM roles. Use inline policies if you want to maintain a strict 
one-to-one relationship between a policy and the identity to which it is applied. 
For example, you can grant administrative permissions to ensure they are not attached to other roles. 


6)What are resource-based policies and when should I use them?
Resource-based policies are permissions policies that are attached to resources. For example, you can attach resource-based policies 
to Amazon S3 buckets, Amazon SQS queues, VPC endpoints, and AWS Key Management Service encryption keys. 
For a list of services that support resource-based policies, see AWS services that work with IAM. 
Use resource-based policies to grant direct, cross-account access. 
With resource-based policies, you can define who has access to a resource and which actions they can perform with it.



AWS IAM Identity Center (Successor to AWS Single Sign-On) FAQs:
==============================================================

1) What is AWS IAM Identity Center (successor to AWS Single Sign-On)?
IAM Identity Center is built on top of AWS Identity and Access Management (IAM) to simplify access management to multiple AWS accounts, 
AWS applications, and other SAML-enabled cloud applications. In IAM Identity Center, you create, or connect, 
your workforce users for use across AWS. You can choose to manage access just to your AWS accounts, just to your cloud applications, 
or to both. You can create users directly in IAM Identity Center, or you can bring them from your existing workforce directory. 
With IAM Identity Center, you get a unified administration experience to define, customize, and assign fine-grained access. 
Your workforce users get a user portal to access their assigned AWS accounts or cloud applications.



================================================= 
Amazon Cognito FAQs:
================================================= 

1)What is Amazon Cognito?
Amazon Cognito lets you easily add user sign-up and authentication to your mobile and web apps. 
Amazon Cognito also enables you to authenticate users through an external identity provider and provides 
temporary security credentials to access your app’s backend resources in AWS or any service behind Amazon API Gateway. 
Amazon Cognito works with external identity providers that support SAML or OpenID Connect, social 
identity providers (such as Facebook, Twitter, Amazon) and you can also integrate your own identity provider.



2)What is a User Pool?
A User Pool is your user directory that you can configure for your web and mobile apps. 
A User Pool securely stores your users’ profile attributes. You can create and manage a 
User Pool using the AWS console, AWS CLI, or AWS SDK.


3)What are unauthenticated users?
Unauthenticated users are users who do not authenticate with any identity provider, but instead access your app as a guest. 
You can define a separate IAM role for these users to provide limited permissions to access your backend resources.


4)How much does Cognito Identity cost?
With Amazon Cognito, you pay only for what you use. There are no minimum fees and no upfront commitments.
If you are using the Cognito Identity to create a User Pool, you pay based on your monthly active users (MAUs) only. 
A user is counted as a MAU if, within a calendar month, there is an identity operation related to that user, such as sign-up, 
sign-in, token refresh, password change, or a user account attribute is updated. You are not charged for subsequent sessions or 
for inactive users with in that calendar month. Separate charges apply for optional use of SMS messaging as described below.



================================================= 
AWS Directory Service FAQ:
================================================= 

1)What can I do with AWS Directory Service?
AWS Directory Service makes it easy for you to setup and run directories in the AWS cloud, or connect your AWS resources 
with an existing on-premises Microsoft Active Directory. Once your directory is created, you can use it to manage users and groups, 
provide single sign-on to applications and services, create and apply group policy, join Amazon EC2 instances to a domain, 
as well as simplify the deployment and management of cloud-based Linux and Microsoft Windows workloads. 
AWS Directory Service enables your end users to use their existing corporate credentials when accessing AWS applications, 
such as Amazon WorkSpaces, Amazon WorkDocs and Amazon WorkMail, as well as directory-aware Microsoft workloads, 
including custom .NET and SQL Server-based applications. Finally, you can use your existing corporate credentials to 
administer AWS resources via AWS Identity and Access Management (IAM) role-based access to the AWS Management Console, 
so you do not need to build out more identity federation infrastructure.


2)How do I create an AWS Managed Microsoft AD directory?
You can launch the AWS Directory Service console from the AWS Management Console to create an AWS Managed Microsoft AD directory. 
Alternatively, you can use the AWS SDK or AWS CLI.


3)Which applications are compatible with AWS Managed Microsoft AD?
Amazon Chime
Amazon Connect
Amazon EC2 Instances
Amazon FSx for Windows File Server
Amazon QuickSight
Amazon RDS for MySQL
Amazon RDS for Oracle
Amazon RDS for PostgreSQL
Amazon RDS for SQL Server
Amazon Single Sign On
Amazon WorkDocs
Amazon WorkMail
Amazon WorkSpaces
AWS Client VPN
AWS Management Console
Note that not all configurations of these applications may be supported.


================================================= 
AWS Key Management Service FAQs:
================================================= 

1) What is AWS Key Management Service (KMS)?
AWS KMS is a managed service that enables you to easily create and control the keys used for cryptographic operations. 
The service provides a highly available key generation, storage, management, and auditing solution for you to encrypt or 
digitally sign data within your own applications or control the encryption of data across AWS services.


2) How will I be charged and billed for my use of AWS KMS?
With AWS KMS, you pay only for what you use, there is no minimum fee. There are no set-up fees or commitments to begin using the service. 
At the end of the month, your credit card will automatically be charged for that month’s usage.



3)Who can use and manage my keys in AWS KMS?
AWS KMS enforces usage and management policies that you define. You choose to allow AWS Identity and 
Access Management (IAM) users and roles from your account or other accounts to use and manage your keys.



================================================= 
AWS Organizations FAQs:
================================================= 


1)What is AWS Organizations?
An organization is a collection of AWS accounts that you can organize into a hierarchy and manage centrally.

AWS Organizations helps you centrally govern your environment as you scale your workloads on AWS. 
Whether you are a growing startup or a large enterprise, Organizations helps you to programmatically create new accounts and 
allocate resources, simplify billing by setting up a single payment method for all of your accounts, create groups of accounts 
to organize your workflows, and apply policies to these groups for governance. 
In addition, AWS Organizations is integrated with other AWS services so you can define central 
configurations, security mechanisms, and resource sharing across accounts in your organization.



2)Which central governance and management capabilities does AWS Organizations enable?

Automate AWS account creation and management, and provision resources with AWS CloudFormation Stacksets
Maintain a secure environment with policies and management of AWS security services
Govern access to AWS services, resources, and regions
Centrally manage policies across multiple AWS accounts
Audit your environment for compliance 
View and manage costs with consolidated billing 
Configure AWS services across multiple accounts



3)What is AWS Control Tower?
AWS Control Tower, built on AWS services such as AWS Organizations, offers the easiest way to set up and govern a new, secure, 
multi-account AWS environment. It establishes a landing zone, which is a well-architected, multi-account environment based on 
best-practice blueprints, and enables governance using guardrails you can choose.



4)What does AWS Organizations cost?
AWS Organizations is offered at no additional charge.



================================================= 
Amazon MQ FAQs
================================================= 

1) What is Amazon MQ?
Amazon MQ is a managed message broker service for Apache ActiveMQ and RabbitMQ that makes it easy to set up and operate message 
brokers in the cloud. You get direct access to the ActiveMQ and RabbitMQ consoles and industry standard APIs and protocols for 
messaging, including JMS, NMS, AMQP 1.0 and 0.9.1, STOMP, MQTT, and WebSocket. You can easily move from any message 
broker that uses these standards to Amazon MQ because you don’t have to rewrite any messaging code in your applications.



2)Who should use Amazon MQ?
Amazon MQ is suitable for enterprise IT pros, developers, and architects who are managing a message broker themselves–whether 
on-premises or in the cloud–and want to move to a fully managed cloud service without rewriting the messaging code in their applications.


3)Who should use Amazon MQ?
Amazon MQ is suitable for enterprise IT pros, developers, and architects who are managing a message broker themselves–whether 
on-premises or in the cloud–and want to move to a fully managed cloud service without rewriting the messaging code in their applications.



4)How does Amazon MQ work with other AWS services?

Any application that runs on an AWS compute service, such as Amazon EC2, Amazon ECS, or AWS Lambda, can use Amazon MQ. 
Amazon MQ is also integrated with the following AWS services:

Amazon CloudWatch - monitor metrics and generate alarms
Amazon CloudWatch Logs - publish logs from your Amazon MQ brokers to Amazon CloudWatch Logs
AWS CloudTrail - log, continously monitor, and retain Amazon MQ API calls
AWS CloudFormation - automate the process of creating, updating, and deleting message brokers
AWS Identity and Access Management (IAM) - authentication and authorization of the service API
AWS Key Management Service (KMS) - create and control the keys used to encrypt your data




================================================= 
Amazon SQS FAQs:
================================================= 

1) How is Amazon SQS different from Amazon Simple Notification Service (SNS)?
Amazon SNS allows applications to send time-critical messages to multiple subscribers through a “push” mechanism, 
eliminating the need to periodically check or “poll” for updates. Amazon SQS is a message queue service used by distributed 
applications to exchange messages through a polling model, and can be used to decouple sending and receiving components. 



2)How is Amazon SQS different from Amazon MQ?
If you're using messaging with existing applications, and want to move your messaging to the cloud quickly and easily, 
we recommend you consider Amazon MQ. 
It supports industry-standard APIs and protocols so you can switch from any standards-based message broker to 
Amazon MQ without rewriting the messaging code in your applications. If you are building brand new applications in the cloud, 
we recommend you consider Amazon SQS and Amazon SNS. Amazon SQS and SNS are lightweight, fully managed message queue and topic 
services that scale almost infinitely and provide simple, easy-to-use APIs.



3)How is Amazon SQS different from Amazon Kinesis Streams?
Amazon SQS offers a reliable, highly-scalable hosted queue for storing messages as they travel between applications or microservices. 
It moves data between distributed application components and helps you decouple these components. 
Amazon SQS provides common middleware constructs such as dead-letter queues and poison-pill management. 
It also provides a generic web services API and can be accessed by any programming language that the AWS SDK supports. 
Amazon SQS supports both standard and FIFO queues.

Amazon Kinesis Streams allows real-time processing of streaming big data and the ability to read and replay records to multiple 
Amazon Kinesis Applications. The Amazon Kinesis Client Library (KCL) delivers all records for a given partition key to the same 
record processor, making it easier to build multiple applications that read from the same 
Amazon Kinesis stream (for example, to perform counting, aggregation, and filtering).


4)How much does Amazon SQS cost?

You pay only for what you use, and there is no minimum fee.
The cost of Amazon SQS is calculated per request, plus data transfer charges for data transferred out of 
Amazon SQS (unless data is transferred to Amazon Elastic Compute Cloud (EC2) 
instances or to AWS Lambda functions within the same region). 



5)Can I use Amazon SQS with other AWS services?
Yes. You can make your applications more flexible and scalable by using Amazon SQS with compute services such as 
Amazon EC2, Amazon Elastic Container Service (ECS), and AWS Lambda, as well as with storage and database services such as 
Amazon Simple Storage Service (Amazon S3) and Amazon DynamoDB.


6) What is Amazon SQS long polling?
Amazon SQS long polling is a way to retrieve messages from your Amazon SQS queues. 
While the regular short polling returns immediately, even if the message queue being polled is empty, 
long polling doesn’t return a response until a message arrives in the message queue, 
or the long poll times out.

Long polling makes it inexpensive to retrieve messages from your Amazon SQS queue as soon as the messages are available. 
Using long polling might reduce the cost of using SQS, because you can reduce the number of empty receives



7) How does Amazon SQS handle messages that can't be processed?
In Amazon SQS, you can use the API or the console to configure dead letter queues, which receive messages from other source queues. 
When configuring a dead letter queue, you are required to set appropriate permissions for the dead letter queue 
redrive using RedriveAllowPolicy.
RedriveAllowPolicy includes the parameters for the dead-letter queue redrive permission. 
It defines which source queues can specify dead-letter queues as a JSON object.
Once you make a dead letter queue, it receives messages after a maximum number of processing attempts cannot be completed. You can use dead letter queues to isolate messages that can't be processed for later analysis.



8) What is a visibility timeout?
The visibility timeout is a period of time during which Amazon SQS prevents other consuming components 
from receiving and processing a message.



9)What are message groups?
Messages are grouped into distinct, ordered "bundles" within a FIFO queue. For each message group ID, all messages are sent and 
received in strict order. However, messages with different message group ID values might be sent and received out of order. 
You must associate a message group ID with a message. If you don't provide a message group ID, the action fails.

If multiple hosts (or different threads on the same host) send messages with the same message group ID are sent to a FIFO queue, 
Amazon SQS delivers the messages in the order in which they arrive for processing. 
To ensure that Amazon SQS preserves the order in which messages are sent and received, ensure that multiple senders send each 
message with a unique message group ID.



================================================= 
Amazon SNS FAQs:
================================================= 

1)What is Amazon Simple Notification Service (Amazon SNS)?
Amazon Simple Notification Service (Amazon SNS) is a web service that makes it easy to set up, operate, and send notifications from 
the cloud. It provides developers with a highly scalable, flexible, and cost-effective capability to publish messages 
from an application and immediately deliver them to subscribers or other applications. 

It is designed to make web-scale computing easier for developers. Amazon SNS follows the “publish-subscribe” (pub-sub) messaging paradigm,
 with notifications being delivered to clients using a “push” mechanism that eliminates the need to 
 periodically check or “poll” for new information and updates. 
 
With simple APIs requiring minimal up-front development effort, 
 no maintenance or management overhead and pay-as-you-go pricing, Amazon SNS gives developers an easy mechanism to 
 incorporate a powerful notification system with their applications.


2)What are the benefits of using Amazon SNS?
Instantaneous, push-based delivery (no polling)
Simple APIs and easy integration with applications
Flexible message delivery over multiple transport protocols
Inexpensive, pay-as-you-go model with no up-front costs
Web-based AWS Management Console offers the simplicity of a point-and-click interface



3)What are some example uses for Amazon SNS notifications?
The Amazon SNS service can support a wide variety of needs including event notification, monitoring applications, 
workflow systems, time-sensitive information updates, mobile applications, and any other application that generates or 
consumes notifications. 

For example, Amazon SNS can be used in workflow systems to relay events among distributed computer 
applications, move data between data stores or update records in business systems. Event updates and notifications 
concerning validation, approval, inventory changes and shipment status are immediately delivered to relevant system components as 
well as end-users. A common pattern is to use SNS to publish messages to Amazon SQS message queues to reliably send messages to one or 
many system components asynchronously. 

Another example use for Amazon SNS is to relay time-critical events to mobile applications and 
devices. Since Amazon SNS is both highly reliable and scalable, it provides significant advantages to developers who build 
applications that rely on real-time events.


4)How much does Amazon SNS cost?
With Amazon SNS, there is no minimum fee and you pay only for what you use. Users pay $0.50 per 1 million 
Amazon SNS Requests, $0.06 per 100,000 notification deliveries over HTTP, and $2.00 per 100,000 notification deliveries over email. 
For SMS messaging, charges vary by destination country.


5) Are there quotas for the number of topics or number of subscribers per topic?
By default, SNS offers 10 million subscriptions per topic, and 100,000 topics per account.



================================================= 
Amazon SWF FAQs:
================================================= 

1)What is Amazon SWF?
Amazon Simple Workflow Service (SWF) is a web service that makes it easy to coordinate work across distributed application components. 
Amazon SWF enables applications for a range of use cases, including media processing, web application back-ends, 
business process workflows, and analytics pipelines, to be designed as a coordination of tasks.


2)What can I do with Amazon SWF?
Amazon SWF can be used to address many challenges that arise while building applications with distributed components. 
For example, you can use Amazon SWF and the accompanying AWS Flow Framework for:

-Writing your applications as asynchronous programs using simple programming constructs that abstract details such as initiating tasks to run remotely and tracking the program’s runtime state.

-Maintaining your application’s execution state (e.g. which steps have completed, which ones are running, etc.). 
You do not have to use databases, custom systems, or ad hoc solutions to keep execution state.
Communicating and managing the flow of work between your application components. 

With Amazon SWF, you do not need to design a messaging protocol or worry about lost and duplicated tasks.

Centralizing the coordination of steps in your application. Your coordination logic does not have to be scattered across 
different components, but can be encapsulated in a single program.

Integrating a range of programs and components, including legacy systems and 3rd party cloud services, into your applications. 

By allowing your application flexibility in where and in what combination the application components are deployed, 

Amazon SWF helps you gradually migrate application components from private data centers to public cloud infrastructure 
without disrupting the application availability or performance.

Automating workflows that include long-running human tasks (e.g. approvals, reviews, investigations, etc.) 
Amazon SWF reliably tracks the status of processing steps that run up to several days or months.

Building an application layer on top of Amazon SWF to support domain specific languages for your end users. 

Since Amazon SWF gives you full flexibility in choosing your programming language, you can conveniently build interpreters for 
specialized languages (e.g. XPDL) and customized user-interfaces including modeling tools.

Getting detailed audit trails and visibility into all running instances of your applications. 

You can also incorporate visibility capabilities provided by Amazon SWF into your own user interfaces using the 
APIs provided by Amazon SWF.

ustomers have used Amazon SWF to build applications for video encoding, social commerce, infrastructure provisioning, 
MapReduce pipelines, business process management, and several other use cases. For more details on use cases, please see What are some use cases that can be solved with SWF?. To see how customers are using Amazon SWF today, please read our case studies.




================================================= 
Amazon AppFlow FAQs:
================================================= 

1)What is Amazon AppFlow?
Amazon AppFlow is a fully managed integration service that enables you to securely transfer data between Software-as-a-Service 
(SaaS) applications like Salesforce, Marketo, Slack, and ServiceNow, and AWS services like Amazon S3 and Amazon Redshift, 
in just a few clicks. With AppFlow, you can run data flows at nearly any scale at the frequency you choose - on a schedule, 
in response to a business event, or on demand. You can configure powerful data transformation capabilities like filtering and 
validation to generate rich, ready-to-use data as part of the flow itself, without additional steps. AppFlow automatically encrypts 
data in motion, and allows users to restrict data from flowing over the public Internet for SaaS applications that are 
integrated with AWS PrivateLink, reducing exposure to security threats.




================================================= 
AWS Step Functions FAQs:
================================================= 

1)What is AWS Step Functions?
AWS Step Functions is a fully managed service that makes it easy to coordinate the components of distributed applications and 
microservices using visual workflows. 

Building applications from individual components that each perform a discrete 
function lets you scale easily and change applications quickly. 

Step Functions is a reliable way to coordinate 
components and step through the functions of your application. Step Functions provides a graphical console 
to arrange and visualize the components of your application as a series of steps. 

This makes it simple to build and run multi-step applications. Step Functions automatically triggers and tracks each step, and 
retries when there are errors, so your application executes in order and as expected. 

Step Functions logs the state of each step, so when things do go wrong, you can diagnose and debug problems quickly. 
You can change and add steps without even writing code, so you can easily evolve your application and innovate faster.



2)What are some common AWS Step Functions use cases?
AWS Step Functions helps with any computational problem or business process that can be subdivided into a series of steps. 
It’s also useful for creating end-to-end workflows to manage jobs with interdependencies. 
Common use cases include:

Data processing: consolidate data from multiple databases into unified reports, refine and reduce large data sets into useful formats, 
or coordinate multi-step analytics and machine learning workflows

DevOps and IT automation: build tools for continuous integration and continuous deployment, or create event-driven applications 
that automatically respond to changes in infrastructure

E-commerce: automate mission-critical business processes, such as order fulfillment and inventory tracking

Web applications: implement robust user registration processes and sign-on authentication



================================================= 
Amazon CloudWatch FAQs:
================================================= 


1)What is Amazon CloudWatch?
Amazon CloudWatch is a monitoring service for AWS cloud resources and the applications you run on AWS. 
You can use Amazon CloudWatch to collect and track metrics, collect and monitor log files, and set alarms. 
Amazon CloudWatch can monitor AWS resources such as Amazon EC2 instances, Amazon DynamoDB tables, and Amazon RDS DB instances, 
as well as custom metrics generated by your applications and services, and any log files your applications generate. 
You can use Amazon CloudWatch to gain system-wide visibility into resource utilization, application performance, and 
operational health. You can use these insights to react and keep your application running smoothly.



2)What can I use to access CloudWatch?
Amazon CloudWatch can be accessed via API, command-line interface, AWS SDKs, and the AWS Management Console.


3)How do I retrieve my log data?
You can retrieve any of your log data using the CloudWatch Logs console or through the CloudWatch Logs CLI. 
Log events are retrieved based on the Log Group, Log Stream and time with which they are associated. 
The CloudWatch Logs API for retrieving log events is GetLogEvents.

You can use the CLI to retrieve your log events and search through them using command line grep or similar search functions.


4)What is CloudWatch Dashboards?
Amazon CloudWatch Dashboards allow you to create, customize, interact with, and save graphs of AWS resources and custom metrics.


5)What is CloudWatch Events?
Amazon CloudWatch Events (CWE) is a stream of system events describing changes in your AWS resources. 
The events stream augments the existing CloudWatch Metrics and Logs streams to provide a more complete picture of the 
health and state of your applications. You write declarative rules to associate events of interest with automated actions to be taken.


6) What services emit CloudWatch Events?
Currently, Amazon EC2, Auto Scaling, and AWS CloudTrail are supported. 
Via AWS CloudTrail, mutating API calls (i.e., all calls except Describe*, List*, and Get*) across all services are 
visible in CloudWatch Events.


================================================= 
AWS Auto Scaling FAQs:
================================================= 
1)What is AWS Auto Scaling? 
AWS Auto Scaling helps you configure consistent and congruent scaling policies across the full infrastructure stack 
backing your application. AWS Auto Scaling will automatically scale resources as needed to align to your selected scaling strategy, 
so you maintain performance and pay only for the resources you actually need.


2)When should I use AWS Auto Scaling?
You should use AWS Auto Scaling if you have an application that uses one or more scalable resources and experiences variable load. 
A good example would be an e-commerce web application that receives variable traffic through the day. 
It follows a standard three tier architecture with Elastic Load Balancing for distributing incoming traffic, 
Amazon EC2 for the compute layer, and DynamoDB for the data layer. 
In this case, AWS Auto Scaling will scale one or more EC2 Auto Scaling groups and DynamoDB tables that are powering the application 
in response to the demand curve.



3) How much does AWS Auto Scaling cost?
Similar to Auto Scaling on individual AWS resources, AWS Auto Scaling is free to use. 
AWS Auto Scaling is enabled by Amazon CloudWatch, so service fees apply for CloudWatch and your application resources 
(such as Amazon EC2 instances, Elastic Load Balancing load balancers, etc.).




================================================= 
AWS CloudFormation FAQs:
================================================= 

1)What is AWS CloudFormation?
CloudFormation introduces four concepts: A template is a JSON or YAML declarative code file that describes the intended 
state of all the resources you need to deploy your application


AWS CloudFormation is a service that gives developers and businesses an easy way to create a collection of related AWS and 
third-party resources, and provision and manage them in an orderly and predictable fashion.

Developers can deploy and update compute, database, and many other resources in a simple, declarative style that 
abstracts away the complexity of specific resource APIs. 


2)How is CloudFormation different from AWS Elastic Beanstalk?
These services are designed to complement each other. AWS Elastic Beanstalk provides an environment where you can easily 
deploy and run applications in the cloud. It is integrated with developer tools and provides a one-stop experience for 
managing application lifecycle. If your application workloads can be managed as Elastic Beanstalk workloads, 
you can enjoy a more turn-key experience in creating and updating applications. Behind the scenes, 
Elastic Beanstalk uses CloudFormation to create and maintain resources. If your application requirements dictate more custom control, 
the additional functionality of CloudFormation gives you more options to control your workloads.

AWS CloudFormation is a convenient provisioning mechanism for a broad range of AWS and third-party resources. 
It supports the infrastructure needs of many different types of applications such as existing enterprise applications, 
legacy applications, applications built using a variety of AWS resources, and container-based solutions 
(including those built using AWS Elastic Beanstalk).



3)How much does AWS CloudFormation cost?
There is no additional charge for using AWS CloudFormation with resource providers in the following 
namespaces: AWS::*, Alexa::*, and Custom::*. In this case, you pay for AWS resources (such as Amazon EC2 instances, 
Elastic Load Balancing load balancers, etc.) created using AWS CloudFormation just as if you had created them manually. 
You only pay for what you use, as you use it; there are no minimum fees and no required upfront commitments.



================================================= 
AWS CloudTrail FAQs:
================================================= 

1)What is AWS CloudTrail?
AWS CloudTrail enables auditing, security monitoring, and operational troubleshooting by tracking user activity and API usage. 
CloudTrail logs, continuously monitors, and retains account activity related to actions across your AWS infrastructure, 
giving you control over storage, analysis, and remediation actions.


2)Who should use CloudTrail?
You should use CloudTrail if you need to audit activity, monitor security, or troubleshoot operational issues.


3)How can I secure my CloudTrail log files?
By default, CloudTrail log files are encrypted using Amazon S3 Server Side Encryption (SSE) and placed into your S3 bucket. 
You can control access to log files by applying IAM or S3 bucket policies. You can add an additional layer of security by 
enabling S3 Multi Factor Authentication (MFA) Delete on your S3 bucket. 


4)Why should I use AWS CloudTrail Lake?
CloudTrail Lake allows you to examine incidents by querying all actions logged by CloudTrail. 
It simplifies incident logging by helping to remove operational dependencies and provides tools that can help reduce 
your reliance on complex data process pipelines that span across teams. 


5)How do I get charged for AWS CloudTrail?
AWS CloudTrail allows you to view, search, and download the last 90 days of your account’s management events for free. 
You can deliver one copy of your ongoing management events to Amazon S3 for free by creating a trail. 
Once a CloudTrail trail is set up, Amazon S3 charges apply based on your usage.



================================================= 
AWS Config FAQs:
================================================= 

1)What is AWS Config?
AWS Config is a fully managed service that provides you with an AWS resource inventory, configuration history, 
and configuration change notifications to enable security and governance. With AWS Config you can discover existing AWS resources, 
export a complete inventory of your AWS resources with all configuration details, and determine how a resource was 
configured at any point in time. These capabilities enable compliance auditing, security analysis, 
resource change tracking, and troubleshooting.


2)How will I be charged for AWS Config?
With AWS Config, you are charged based on the number of configuration items recorded, the number of active AWS Config rule 
evaluations and the number of conformance pack evaluations in your account. A configuration item is a record of the 
configuration state of a resource in your AWS account. An AWS Config rule evaluation is a compliance state evaluation of a 
resource by a AWS Config rule in your AWS account, and a conformance pack evaluation is the evaluation of a resource by a 
AWS Config rule within the conformance pack. 





=================================================
#Exam content         
================================================= 



#Exam Content Outline
-------------------------------------------------

The exam contains five main sections, which are:

-Design Resilient Architectures (34%)
-Define Performant Architecture (24%)
-Specify Secure Applications and Architectures (24%)
-Design Cost-Optimized Architectures (10%)
-Define Operationally Excellent Architectures (6%)Now


Design Resilient Architecture:
-How to choose reliable and resilient storage using services like AWS S3, AWS Glacier, and AWS EBS
-How to design decoupling mechanisms using AWS services like AWS SNS
-How to create a multi-tier architecture
-How to architect for high availability and fault-tolerance


Define Performant Architectures:
How to choose performant storage and databases using AWS RDS, AWS Redshift, and AWS DynamoDB
How to improve performance using AWS Elasticache
How to design elastic and scalable solutions through AWS Lambda, AWS CloudWatch, and AWS Data Pipeline



Specify Secure Applications and Architectures:
How to secure applications using AWS Inspector, AWS CloudTrail, and AWS IAM
How to secure data using AWS CloudHSM and AWS Macie
How to define the network infrastructure with AWS CloudFront, AWS VPC, and Elastic Load Balancer


Design Cost-Optimized Architectures:
How to design cost-optimized compute solutions using AWS EC2, AWS Elastic Beanstalk, AWS Lambda, and Aws Lightsail
Design cost-effective storage solutions using AWS S3, AWS Glacier, AWS EBS, and AWS Elastic File System


Define Operationally Excellent Architectures:
Perform operations as code
Annotate documentation
Make frequent, small, and reversible changes
Anticipate and tackle failures



================================================= 
#Exam | Question | Mock
================================================= 

1) How do you choose aws region ?

2) How do you upgrade or downgrade a system with near-zero downtime?
You can upgrade or downgrade a system with near-zero downtime using the following steps of migration:
-Open EC2 console
-Choose Operating System AMI
-Launch an instance with the new instance type
-Install all the updates
-Install applications
-Test the instance to see if it’s working
-If working, deploy the new instance and replace the older instance
-Once it’s deployed, you can upgrade or downgrade the system with near-zero downtime.


3) Is there any other alternative tool to log into the cloud environment other than console?
The that can help you log into the AWS resources are:
Putty
AWS CLI for Linux
AWS CLI for Windows
AWS CLI for Windows CMD
AWS SDK
Eclipse


4) What services can be used to create a centralized logging solution?
The essential services that you can use are Amazon CloudWatch Logs, store them in Amazon S3, and then use 
Amazon Elastic Search to visualize them. You can use Amazon Kinesis Firehose to move the data from Amazon S3 to Amazon ElasticSearch.




5) Name some of the AWS services that are not region-specific ?
AWS services that are not region-specific are:
IAM
Route 53
Web Application Firewall 
CloudFront



6) What is CloudWatch?
The Amazon CloudWatch has the following features:
Depending on multiple metrics, it participates in triggering alarms.
Helps in monitoring the AWS environments like CPU utilization, EC2, Amazon RDS instances, Amazon SQS, S3, Load Balancer, SNS, etc.




7) What is an Elastic Transcoder?
To support multiple devices with various resolutions like laptops, tablets, and smartphones, we need to change the resolution and 
format of the video. This can be done easily by an AWS Service tool called the Elastic Transcoder, 
which is a media transcoding in the cloud that exactly lets us do the needful. It is easy to use, cost-effective, 
and highly scalable for businesses and developers.


8) How do you set up SSH agent forwarding so that you do not have to copy the key every time you log in?
Here’s how you accomplish this:
Go to your PuTTY Configuration
Go to the category SSH -> Auth
Enable SSH agent forwarding to your instance



9) How do you configure CloudWatch to recover an EC2 instance?
Here’s how you can configure them:
Create an Alarm using Amazon CloudWatch
In the Alarm, go to Define Alarm -> Actions tab
Choose Recover this instance option



10) What are the common types of AMI designs?
There are many types of AMIs, but some of the common AMIs are:
Fully Baked AMI
Just Enough Baked AMI (JeOS AMI)
Hybrid AMI




11) What are Key-Pairs in AWS?
The Key-Pairs are password-protected login credentials for the Virtual Machines that are used to prove our 
identity while connecting the Amazon EC2 instances. The Key-Pairs are made up of a Private Key and a 
Public Key which lets us connect to the instances.



12) How can you recover/login to an EC2 instance for which you have lost the key?
Follow the steps provided below to recover an EC2 instance if you have lost the key:

Verify that the EC2Config service is running
Detach the root volume for the instance
Attach the volume to a temporary instance
Modify the configuration file
Restart the original instance



13) How do you monitor Amazon VPC?
You can monitor VPC by using:
-CloudWatch and CloudWatch logs
-VPC Flow Logs



14) When Would You Prefer Provisioned IOPS over Standard Rds Storage?
You would use Provisioned IOPS when you have batch-oriented workloads. 
]Provisioned IOPS delivers high IO rates, but it is also expensive. 
However, batch processing workloads do not require manual intervention. 



15) How Do Amazon Rds, Dynamodb, and Redshift Differ from Each Other?
Amazon RDS is a database management service for relational databases. It manages patching, upgrading, and data backups automatically. 
It’s a database management service for structured data only. On the other hand, DynamoDB is a NoSQL 
database service for dealing with unstructured data. Redshift is a data warehouse product used in data analysis.



16) What are the factors to consider while migrating to Amazon Web Services?
Here are the factors to consider during AWS migration:
Operational Costs - These include the cost of infrastructure, ability to match demand and supply, transparency, and others.
Workforce Productivity 
Cost avoidance
Operational resilience
Business agility




17) What is RTO and RPO in AWS?
RTO or Recovery Time Objective is the maximum time your business or organization is willing to wait for a recovery to 
complete in the wake of an outage. On the other hand, RPO or Recovery Point Objective is the 
maximum amount of data loss your company is willing to accept as measured in time.





18) What are the advantages of AWS IAM?
AWS IAM allows an administrator to provide multiple users and groups with granular access. Various user groups and users may require 
varying levels of access to the various resources that have been developed. 
We may assign roles to users and create roles with defined access levels using IAM.

It further gives us Federated Access, which allows us to grant applications and users access to 
resources without having to create IAM Roles.


19) Explain Connection Draining ?
Connection Draining is an AWS service that allows us to serve current requests on the servers that are either being 
decommissioned or updated.

By enabling this Connection Draining, we let the Load Balancer make an outgoing instance finish its 
existing requests for a set length of time before sending it any new requests. A departing instance will 
immediately go off if Connection Draining is not enabled, and all pending requests will fail.


20) What is Power User Access in AWS?
The AWS Resources owner is identical to an Administrator User. ,
The Administrator User can build, change, delete, and inspect resources, as well as grant permissions to other AWS users.

Administrator Access without the ability to control users and permissions is provided to a Power User. 
A Power User Access user cannot provide permissions to other users but has the ability to modify, remove, view, and create resources.


================================================= 
#ExamAdv | advice
================================================= 

#Section 8: High Availability and Scalability: ELB & ASG
-------------------------------------------------

-Gateway Load balancer
1)If you see GENEvE protocol on port 608
=> Think it is Gateway Load balancer


2)Any Time you see multiple SSL certificates 
=> Think this is ALB or NLB, Using  SNI (Server Name Indication) make it work.

-ASG solution Architeure
3)Lifecycle Hooks of ASG are doing something before instance gos Termination
Like backup loggs or DB dump



#Section 15: CloudFront & AWS Global Accelerator
-------------------------------------------------
1)Global Acceleration come to in exam



#On Section 23: Identity and Access Management (IAM) - Advanced
-------------------------------------------------

-Identity Federation
1)If taking about AusumeRole or Cross-Account 
 => Its about STS (AWS Security Token Service)


-Directory Services 
2)May exam ask about MS-AD from of High level of 3type AWS AD 
 => AWS AD Manager | AD Connector  with proxy | Simple AD



-Resource Access Manager (RAM)
3)Subnet or Other resource sharing with other account (Generally exam asking about Subnet)
 => Resource Access Manager (RAM)



#Section 24: AWS Security & Encryption: KMS, SSM Parameter Store, CloudHSM, Shield, WAF
-------------------------------------------------

-KMS Overview
4)Exam generally asking about Symmetric (AES-256 Keys) Encryption.


-AWS Secrets Manager - Overview
5)Exam ask you about Store thing about its AWS Secrets Manager

-WAF
6)Need to remember that WAF (Web Application Firewall) Deploy onlyh three thing 
  Application Load Balancer | API Gateway | CloudFont

-GuardDuty 
7)GuardDuty protect you against CryptoCurrency Attacks


-Shared Responsibility Model
8)Exam ask you about your responsibility and AwS Responsibility
Need to undestanding clearly about Shared Responsibility



#Section 26: Disaster Recovery & Migrations
-------------------------------------------------


1)Exam ask you base on scenario base question, what should is choose the solution for.






























#Domain Of Exam
-------------------------------------------------
Domain 1: Design Resilient Architectures 30%
Domain 2: Design High-Performing Architectures 28%
Domain 3: Design Secure Applications and Architectures 24%
Domain 4: Design Cost-Optimized Architectures 18%
TOTAL 100%


Domain 1: Design Resilient Architectures

	1.1 Design a multi-tier architecture solution
		 Determine a solution design based on access patterns.
		 Determine a scaling strategy for components used in a design.
		 Select an appropriate database based on requirements.
		 Select an appropriate compute and storage service based on requirements.
	1.2 Design highly available and/or fault-tolerant architectures
		 Determine the amount of resources needed to provide a fault-tolerant architecture across
		Availability Zones.
		 Select a highly available configuration to mitigate single points of failure.
		 Apply AWS services to improve the reliability of legacy applications when application changes
		are not possible.
		 Select an appropriate disaster recovery strategy to meet business requirements.
		 Identify key performance indicators to ensure the high availability of the solution.
	1.3 Design decoupling mechanisms using AWS services
		 Determine which AWS services can be leveraged to achieve loose coupling of components.
		 Determine when to leverage serverless technologies to enable decoupling.
	1.4 Choose appropriate resilient storage
		 Define a strategy to ensure the durability of data.
		 Identify how data service consistency will affect the operation of the application.
		 Select data services that will meet the access requirements of the application.
		 Identify storage services that can be used with hybrid or non-cloud-native applications.


Domain 2: Design High-Performing Architectures

	2.1 Identify elastic and scalable compute solutions for a workload
		 Select the appropriate instance(s) based on compute, storage, and networking requirements.
		 Choose the appropriate architecture and services that scale to meet performance
		requirements.
		 Identify metrics to monitor the performance of the solution. 
	2.2 Select high-performing and scalable storage solutions for a workload
		 Select a storage service and configuration that meets performance demands.
		 Determine storage services that can scale to accommodate future needs.
	2.3 Select high-performing networking solutions for a workload
		 Select appropriate AWS connectivity options to meet performance demands.
		 Select appropriate features to optimize connectivity to AWS public services.
		 Determine an edge caching strategy to provide performance benefits.
		 Select appropriate data transfer service for migration and/or ingestion.
	2.4 Choose high-performing database solutions for a workload
		 Select an appropriate database scaling strategy.
		 Determine when database caching is required for performance improvement.
		 Choose a suitable database service to meet performance needs.
		
		
Domain 3: Design Secure Applications and Architectures

	3.1 Design secure access to AWS resources
		 Determine when to choose between users, groups, and roles.
		 Interpret the net effect of a given access policy.
		 Select appropriate techniques to secure a root account.
		 Determine ways to secure credentials using features of AWS IAM.
		 Determine the secure method for an application to access AWS APIs.
		 Select appropriate services to create traceability for access to AWS resources.
	3.2 Design secure application tiers
		 Given traffic control requirements, determine when and how to use security groups and
		network ACLs.
		 Determine a network segmentation strategy using public and private subnets.
		 Select the appropriate routing mechanism to securely access AWS service endpoints or
		internet-based resources from Amazon VPC.
		 Select appropriate AWS services to protect applications from external threats.
	3.3 Select appropriate data security options
		 Determine the policies that need to be applied to objects based on access patterns.
		 Select appropriate encryption options for data at rest and in transit for AWS services.
		 Select appropriate key management options based on requirements.
		
		
Domain 4: Design Cost-Optimized Architectures

	4.1 Identify cost-effective storage solutions
		 Determine the most cost-effective data storage options based on requirements.
		 Apply automated processes to ensure that data over time is stored on storage tiers that
		minimize costs.
	4.2 Identify cost-effective compute and database services
		 Determine the most cost-effective Amazon EC2 billing options for each aspect of the
		workload.
		 Determine the most cost-effective database options based on requirements.
		 Select appropriate scaling strategies from a cost perspective.
		 Select and size compute resources that are optimally suited for the workload.
		 Determine options to minimize total cost of ownership (TCO) through managed services and
		serverless architectures.
	4.3 Design cost-optimized network architectures
		 Identify when content delivery can be used to reduce costs.
		 Determine strategies to reduce data transfer costs within AWS.
		 Determine the most cost-effective connectivity options between AWS and on-premises
		environments.
		
	
	
#Which key tools, technologies, and concepts might be covered on the exam?
-------------------------------------------------
	 Compute
	 Cost management
	 Database
	 Disaster recovery
	 High availability
	 Management and governance
	 Microservices and component decoupling
	 Migration and data transfer
	 Networking, connectivity, and content delivery
	 Security
	 Serverless design principles
	 Storage


#AWS services and features
-------------------------------------------------
Analytics:
	 Amazon Athena
	 Amazon Elasticsearch Service (Amazon ES)
	 Amazon EMR
	 AWS Glue
	 Amazon Kinesis
	 Amazon QuickSight


AWS Billing and Cost Management:
	 AWS Budgets
	 Cost Explorer
	
Application Integration:
	 Amazon Simple Notification Service (Amazon SNS)
	 Amazon Simple Queue Service (Amazon SQS)
	
Compute:
	 Amazon EC2
	 AWS Elastic Beanstalk
	 Amazon Elastic Container Service (Amazon ECS)
	 Amazon Elastic Kubernetes Service (Amazon EKS)
	 Elastic Load Balancing
	 AWS Fargate
	 AWS Lambda
	
Database:
	 Amazon Aurora
	 Amazon DynamoDB
	 Amazon ElastiCache
	 Amazon RDS
	 Amazon Redshift
	
Management and Governance:
	 AWS Auto Scaling
	 AWS Backup
	 AWS CloudFormation
	 AWS CloudTrail
	 Amazon CloudWatch
	 AWS Config
	 Amazon EventBridge (Amazon CloudWatch Events)
	 AWS Organizations
	 AWS Resource Access Manager
	 AWS Systems Manager
	 AWS Trusted Advisor
	
Migration and Transfer:
	 AWS Database Migration Service (AWS DMS)
	 AWS DataSync
	 AWS Migration Hub
	 AWS Server Migration Service (AWS SMS)
	 AWS Snowball
	 AWS Transfer Family
	
Networking and Content Delivery:
	 Amazon API Gateway
	 Amazon CloudFront
	 AWS Direct Connect
	 AWS Global Accelerator
	 Amazon Route 53
	 AWS Transit Gateway
	 Amazon VPC (and associated features)
	
Security, Identity, and Compliance:
	 AWS Certificate Manager (ACM)
	 AWS Directory Service
	 Amazon GuardDuty
	 AWS Identity and Access Management (IAM)
	 Amazon Inspector
	 AWS Key Management Service (AWS KMS)
	 Amazon Macie
	 AWS Secrets Manager
	 AWS Shield
	 AWS Single Sign-On
	 AWS WAF
	
Storage:
	 Amazon Elastic Block Store (Amazon EBS)
	 Amazon Elastic File System (Amazon EFS)
	 Amazon FSx
	 Amazon S3
	 Amazon S3 Glacier
	 AWS Storage Gateway
	 
 

 

 
 





  